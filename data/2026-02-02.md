<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 183]
- [quant-ph](#quant-ph) [Total: 38]
- [gr-qc](#gr-qc) [Total: 17]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Attention Isn't All You Need for Emotion Recognition:Domain Features Outperform Transformers on the EAV Dataset](https://arxiv.org/abs/2601.22161)
*Anmol Guragain*

Main category: cs.LG

TL;DR: 复杂注意力机制在小规模多模态情感识别数据集上表现不佳，而简单的领域特定特征和预训练策略更有效。


<details>
  <summary>Details</summary>
Motivation: 研究复杂注意力机制在小规模多模态情感识别数据集（EAV）上的表现，探索是否这些复杂架构能提升性能。

Method: 实现三类模型：基准Transformer（M1）、新型因子化注意力机制（M2）、改进的CNN基准（M3）。在EAV数据集上进行系统实验，比较不同架构的性能。

Result: 复杂注意力机制（M2）表现最差，比基准低5-13个百分点，主要因为过拟合和破坏预训练特征。简单领域特定修改效果显著：音频CNN添加delta MFCCs提升3.66个百分点至65.56%；EEG使用频域特征提升7.62个百分点至67.62%；视觉Transformer基准达到75.30%，超过原论文ViViT结果。

Conclusion: 对于小规模情感识别任务，领域知识和适当实现比架构复杂性更重要。简单但领域适当的特征工程和预训练策略比复杂注意力机制更有效。

Abstract: We present a systematic study of multimodal emotion recognition using the EAV dataset, investigating whether complex attention mechanisms improve performance on small datasets. We implement three model categories: baseline transformers (M1), novel factorized attention mechanisms (M2), and improved CNN baselines (M3). Our experiments show that sophisticated attention mechanisms consistently underperform on small datasets. M2 models achieved 5 to 13 percentage points below baselines due to overfitting and destruction of pretrained features. In contrast, simple domain-appropriate modifications proved effective: adding delta MFCCs to the audio CNN improved accuracy from 61.9\% to \textbf{65.56\%} (+3.66pp), while frequency-domain features for EEG achieved \textbf{67.62\%} (+7.62pp over the paper baseline). Our vision transformer baseline (M1) reached \textbf{75.30\%}, exceeding the paper's ViViT result (74.5\%) through domain-specific pretraining, and vision delta features achieved \textbf{72.68\%} (+1.28pp over the paper CNN). These findings demonstrate that for small-scale emotion recognition, domain knowledge and proper implementation outperform architectural complexity.

</details>


### [2] [Multitask Learning for Earth Observation Data Classification with Hybrid Quantum Network](https://arxiv.org/abs/2601.22195)
*Fan Fan,Yilei Shi,Tobias Guggemos,Xiao Xiang Zhu*

Main category: cs.LG

TL;DR: 提出一种结合多任务学习和量子卷积的混合模型，用于地球观测数据分类，探索量子机器学习在EO领域的潜力


<details>
  <summary>Details</summary>
Motivation: 地球观测进入大数据时代，传统深度学习模型计算需求巨大成为瓶颈，量子机器学习有望解决未来计算挑战

Method: 采用混合模型：1) 多任务学习辅助高效数据编码；2) 位置权重模块结合量子卷积操作提取有效分类特征

Result: 在多个EO基准测试中验证了模型有效性，实验探索了模型的泛化能力，并分析了其优势来源

Conclusion: 展示了量子机器学习在地球观测数据分析中的潜力，尽管当前量子设备仍有限制

Abstract: Quantum machine learning (QML) has gained increasing attention as a potential solution to address the challenges of computation requirements in the future. Earth observation (EO) has entered the era of Big Data, and the computational demands for effectively analyzing large EO data with complex deep learning models have become a bottleneck. Motivated by this, we aim to leverage quantum computing for EO data classification and explore its advantages despite the current limitations of quantum devices. This paper presents a hybrid model that incorporates multitask learning to assist efficient data encoding and employs a location weight module with quantum convolution operations to extract valid features for classification. The validity of our proposed model was evaluated using multiple EO benchmarks. Additionally, we experimentally explored the generalizability of our model and investigated the factors contributing to its advantage, highlighting the potential of QML in EO data analysis.

</details>


### [3] [Neural Signals Generate Clinical Notes in the Wild](https://arxiv.org/abs/2601.22197)
*Jathurshan Pradeepkumar,Zheng Chen,Jimeng Sun*

Main category: cs.LG

TL;DR: 首个临床EEG到语言的基础模型CELM，能够总结长时间EEG记录并生成多尺度临床报告，性能显著优于基线


<details>
  <summary>Details</summary>
Motivation: 从长期EEG记录生成总结异常模式、诊断发现和临床解释的报告仍然劳动密集，需要自动化解决方案

Method: 开发CELM模型，整合预训练的EEG基础模型和语言模型，支持多尺度报告生成（记录描述、背景活动、癫痫样异常、事件/癫痫发作、印象）

Result: 在有患者病史监督下，生成指标相对改进70%-95%（从0.2-0.3提升到0.4-0.6）；零样本设置下达到0.43-0.52，显著优于基线的0.17-0.26

Conclusion: CELM是首个临床EEG到语言的基础模型，能够有效总结长时间EEG记录并生成多尺度临床报告，为EEG报告自动化提供了可扩展的解决方案

Abstract: Generating clinical reports that summarize abnormal patterns, diagnostic findings, and clinical interpretations from long-term EEG recordings remains labor-intensive. We curate a large-scale clinical EEG dataset with $9{,}922$ reports paired with approximately $11{,}000$ hours of EEG recordings from $9{,}048$ patients. We therefore develop CELM, the first clinical EEG-to-Language foundation model capable of summarizing long-duration, variable-length EEG recordings and performing end-to-end clinical report generation at multiple scales, including recording description, background activity, epileptiform abnormalities, events/seizures, and impressions. Experimental results show that, with patient history supervision, our method achieves $70\%$--$95\%$ average relative improvements in standard generation metrics (e.g., ROUGE-1 and METEOR) from $0.2$--$0.3$ to $0.4$--$0.6$. In the zero-shot setting without patient history, CELM attains generation scores in the range of $0.43$--$0.52$, compared to baselines of $0.17$--$0.26$. CELM integrates pretrained EEG foundation models with language models to enable scalable multimodal learning. We release our model and benchmark construction pipeline at [URL].

</details>


### [4] [FedAdaVR: Adaptive Variance Reduction for Robust Federated Learning under Limited Client Participation](https://arxiv.org/abs/2601.22204)
*S M Ruhul Kabir Howlader,Xiao Chen,Yifei Xie,Lu Liu*

Main category: cs.LG

TL;DR: FedAdaVR：一种结合自适应优化器和方差缩减技术的新型联邦学习算法，通过存储客户端更新来模拟缺席客户端的参与，有效解决客户端异构性和部分参与问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中存在严重的异构性问题，导致梯度噪声、客户端漂移和部分客户端参与误差，其中部分参与问题最为普遍但现有研究未能充分解决。

Method: 提出FedAdaVR算法，结合自适应优化器和方差缩减技术，存储客户端最近更新，即使客户端缺席当前训练轮次也能模拟其参与；进一步提出FedAdaVR-Quant，通过量化存储客户端更新，大幅减少内存需求。

Result: 理论分析证明FedAdaVR能在一般非凸条件下收敛，并能消除部分客户端参与误差；实验表明在IID和非IID设置下，FedAdaVR在多个数据集上均优于现有基线方法；FedAdaVR-Quant能减少50%-87.5%内存需求且保持同等模型性能。

Conclusion: FedAdaVR有效解决了联邦学习中的部分客户端参与问题，通过存储和利用客户端历史更新来模拟缺席参与，显著提升了异构环境下的学习性能，同时量化版本大幅降低了内存开销。

Abstract: Federated learning (FL) encounters substantial challenges due to heterogeneity, leading to gradient noise, client drift, and partial client participation errors, the last of which is the most pervasive but remains insufficiently addressed in current literature. In this paper, we propose FedAdaVR, a novel FL algorithm aimed at solving heterogeneity issues caused by sporadic client participation by incorporating an adaptive optimiser with a variance reduction technique. This method takes advantage of the most recent stored updates from clients, even when they are absent from the current training round, thereby emulating their presence. Furthermore, we propose FedAdaVR-Quant, which stores client updates in quantised form, significantly reducing the memory requirements (by 50%, 75%, and 87.5%) of FedAdaVR while maintaining equivalent model performance. We analyse the convergence behaviour of FedAdaVR under general nonconvex conditions and prove that our proposed algorithm can eliminate partial client participation error. Extensive experiments conducted on multiple datasets, under both independent and identically distributed (IID) and non-IID settings, demonstrate that FedAdaVR consistently outperforms state-of-the-art baseline methods.

</details>


### [5] [Causal Imitation Learning Under Measurement Error and Distribution Shift](https://arxiv.org/abs/2601.22206)
*Shi Bo,AmirEmad Ghassami*

Main category: cs.LG

TL;DR: 提出CausIL框架，解决离线模仿学习中因状态测量误差和分布偏移导致的虚假相关性，通过因果推断方法学习鲁棒策略


<details>
  <summary>Details</summary>
Motivation: 离线模仿学习面临两个关键挑战：1）决策相关状态只能通过噪声测量观测；2）训练和部署时的分布可能变化。这会导致虚假的状态-动作相关性，使得标准行为克隆方法在分布偏移下产生系统性偏差

Method: 提出CausIL框架，受因果建模启发，将噪声状态观测作为代理变量，基于近端因果推断理论。提供了从演示中恢复目标策略的识别条件，无需奖励或交互式专家查询。开发了离散和连续状态空间的估计器，对于连续设置使用RKHS函数类的对抗性学习过程

Result: 在PhysioNet/Computing in Cardiology Challenge 2019队列的半模拟纵向数据上评估，相比行为克隆基线，CausIL在分布偏移下表现出更好的鲁棒性

Conclusion: CausIL框架通过因果建模方法有效解决了测量误差和分布偏移下的模仿学习问题，提供了理论保证和实际可行的解决方案

Abstract: We study offline imitation learning (IL) when part of the decision-relevant state is observed only through noisy measurements and the distribution may change between training and deployment. Such settings induce spurious state-action correlations, so standard behavioral cloning (BC) -- whether conditioning on raw measurements or ignoring them -- can converge to systematically biased policies under distribution shift. We propose a general framework for IL under measurement error, inspired by explicitly modeling the causal relationships among the variables, yielding a target that retains a causal interpretation and is robust to distribution shift. Building on ideas from proximal causal inference, we introduce \texttt{CausIL}, which treats noisy state observations as proxy variables, and we provide identification conditions under which the target policy is recoverable from demonstrations without rewards or interactive expert queries. We develop estimators for both discrete and continuous state spaces; for continuous settings, we use an adversarial procedure over RKHS function classes to learn the required parameters. We evaluate \texttt{CausIL} on semi-simulated longitudinal data from the PhysioNet/Computing in Cardiology Challenge 2019 cohort and demonstrate improved robustness to distribution shift compared to BC baselines.

</details>


### [6] [Latent Spherical Flow Policy for Reinforcement Learning with Combinatorial Actions](https://arxiv.org/abs/2601.22211)
*Lingkai Kong,Anagha Satish,Hezi Jiang,Akseli Kangaslahti,Andrew Ma,Wenbo Chen,Mingxiao Song,Lily Xu,Milind Tambe*

Main category: cs.LG

TL;DR: LSFlow提出了一种通过球形流匹配在连续隐空间学习随机策略，然后通过组合优化求解器映射到可行动作的组合强化学习方法，在多个任务上平均性能提升20.6%


<details>
  <summary>Details</summary>
Motivation: 组合动作空间的强化学习面临挑战，因为可行动作集指数级大且受复杂可行性约束限制。现有方法要么将特定任务价值函数嵌入约束优化程序，要么学习确定性结构化策略，牺牲了通用性和策略表达能力。

Method: 提出LSFlow方法：1) 通过球形流匹配在紧凑连续隐空间学习随机策略；2) 使用组合优化求解器将隐空间样本映射到有效结构化动作；3) 在隐空间直接训练价值网络，避免重复求解器调用；4) 引入平滑贝尔曼算子处理求解器引起的分段常数和不连续价值景观。

Result: 在多个具有挑战性的组合RL任务上，LSFlow方法平均性能比最先进基线高出20.6%。

Conclusion: LSFlow将现代生成策略的表达能力引入组合RL，同时通过设计保证可行性，通过隐空间学习和平滑贝尔曼算子解决了组合动作空间的关键挑战。

Abstract: Reinforcement learning (RL) with combinatorial action spaces remains challenging because feasible action sets are exponentially large and governed by complex feasibility constraints, making direct policy parameterization impractical. Existing approaches embed task-specific value functions into constrained optimization programs or learn deterministic structured policies, sacrificing generality and policy expressiveness. We propose a solver-induced \emph{latent spherical flow policy} that brings the expressiveness of modern generative policies to combinatorial RL while guaranteeing feasibility by design. Our method, LSFlow, learns a \emph{stochastic} policy in a compact continuous latent space via spherical flow matching, and delegates feasibility to a combinatorial optimization solver that maps each latent sample to a valid structured action. To improve efficiency, we train the value network directly in the latent space, avoiding repeated solver calls during policy optimization. To address the piecewise-constant and discontinuous value landscape induced by solver-based action selection, we introduce a smoothed Bellman operator that yields stable, well-defined learning targets. Empirically, our approach outperforms state-of-the-art baselines by an average of 20.6\% across a range of challenging combinatorial RL tasks.

</details>


### [7] [DAJ: Data-Reweighted LLM Judge for Test-Time Scaling in Code Generation](https://arxiv.org/abs/2601.22230)
*Peijia Qin,Ruiyi Zhang,Qi Cao,Pengtao Xie*

Main category: cs.LG

TL;DR: DAJ：基于推理的LLM评判器，通过双层数据重加权学习框架训练，使用可验证奖励，在代码生成测试时缩放中实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于Best-of-N选择的测试时缩放方法依赖LLM评判器，但训练可靠的LLM评判器面临严重分布偏移挑战：简单与困难问题不平衡、训练任务与评估基准不匹配、以及由廉价模型生成的训练数据与推理时模型行为不匹配。

Method: 提出DAJ（基于推理的LLM评判器），采用双层数据重加权学习框架，学习数据重要性权重（域级或实例级），通过可验证奖励优化在目标基准对齐的元集上的泛化性能。自动强调困难问题、分布内样本和轨迹对齐数据。

Result: 在LiveCodeBench和BigCodeBench上实现最先进性能，优于强测试时缩放基线以及领先的专有模型。

Conclusion: DAJ通过数据重加权框架有效解决了LLM评判器训练中的分布偏移问题，首次将数据重加权应用于LLM-as-a-Judge训练，为测试时缩放提供了更可靠的评判机制。

Abstract: Test-time scaling for code generation commonly relies on Best-of-N selection, in which multiple candidate solutions are sampled from a base model, and the best one is selected by an LLM judge. However, training reliable LLM judges is challenging due to severe distribution shifts, including imbalances between easy and hard problems, mismatches between training tasks and evaluation benchmarks, and trajectory mismatch arising from training data generated by cheaper models whose behavior differs from that of inference-time models. We propose DAJ, a reasoning-based LLM judge trained with verifiable rewards under a bi-level data-reweighted learning framework. The proposed framework learns data-importance weights (either domain-level or instance-level) to optimize generalization performance on a held-out meta set aligned with target benchmarks. To the best of our knowledge, this is the first application of data reweighting to LLM-as-a-Judge training for test-time scaling. Our approach automatically emphasizes hard problems, in-distribution samples, and trajectory-aligned data, without relying on hand-crafted heuristics. Empirically, DAJ achieves state-of-the-art performance on LiveCodeBench and BigCodeBench, outperforming strong test-time scaling baselines as well as leading proprietary models.

</details>


### [8] [FunPRM: Function-as-Step Process Reward Model with Meta Reward Correction for Code Generation](https://arxiv.org/abs/2601.22249)
*Ruiyi Zhang,Peijia Qin,Qi Cao,Eric Xue,Pengtao Xie*

Main category: cs.LG

TL;DR: FunPRM通过函数化代码分解和元学习奖励修正，提升LLM在复杂编程任务上的表现，在多个基准测试中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有过程奖励模型（PRM）在代码生成中效果有限，因为代码缺乏有意义的步骤分解，且蒙特卡洛估计的部分解正确性分数存在噪声。

Method: FunPRM通过提示LLM生成模块化函数代码，将函数作为PRM推理步骤；引入基于元学习的奖励修正机制，利用单元测试获得的最终解奖励来净化噪声的部分解奖励。

Result: 在LiveCodeBench和BigCodeBench上的实验表明，FunPRM在五个基础LLM上均优于现有测试时缩放方法，与O4-mini结合时在LiveCodeBench上达到最先进性能，且生成的代码更具可读性和可重用性。

Conclusion: FunPRM通过函数化分解和奖励修正有效解决了代码生成中的PRM应用挑战，显著提升了LLM在复杂编程任务上的性能。

Abstract: Code generation is a core application of large language models (LLMs), yet LLMs still frequently fail on complex programming tasks. Given its success in mathematical reasoning, test-time scaling approaches such as Process Reward Model (PRM)-based Best-of-N selection offer a promising way to improve performance. However, existing PRMs remain ineffective for code generation due to the lack of meaningful step decomposition in code and the noise of Monte Carlo-estimated partial-solution correctness scores (rewards). To address these challenges, we propose FunPRM. FunPRM prompts LLMs to encourage modular code generation organized into functions, with functions treated as PRM reasoning steps. Furthermore, FunPRM introduces a novel meta-learning-based reward correction mechanism that leverages clean final-solution rewards obtained via a unit-test-based evaluation system to purify noisy partial-solution rewards. Experiments on LiveCodeBench and BigCodeBench demonstrate that FunPRM consistently outperforms existing test-time scaling methods across five base LLMs, notably achieving state-of-the-art performance on LiveCodeBench when combined with O4-mini. Furthermore, FunPRM produces code that is more readable and reusable for developers.

</details>


### [9] [Symmetry Breaking in Transformers for Efficient and Interpretable Training](https://arxiv.org/abs/2601.22257)
*Eva Silverstein,Daniel Kunin,Vasudev Shyam*

Main category: cs.LG

TL;DR: 论文提出通过添加无学习的查询和值偏置来打破注意力机制中的旋转对称性，这能提升简单优化器的性能并增强注意力头的可解释性。


<details>
  <summary>Details</summary>
Motivation: 标准注意力机制中存在不影响模型输出的冗余旋转自由度，这些自由度在计算中被保留但没有实际作用。论文旨在通过打破这种对称性来改进模型性能。

Method: 引入对称性打破协议：通过批量采样的、无学习的查询和值偏置，在旋转空间中插入一个首选方向。这种方法理论上能改善优化器性能并增强可解释性。

Result: 1) 显著提升简单、内存高效优化器的性能，缩小甚至消除与复杂自适应方法的差距；2) 使原本冗余的旋转自由度变得可解释，能够选择性地放大语义上有意义的token类别。

Conclusion: 最小化、有原则的架构改变可以同时提升模型性能和可解释性，为注意力机制的设计提供了新的方向。

Abstract: The attention mechanism in its standard implementation contains extraneous rotational degrees of freedom that are carried through computation but do not affect model activations or outputs. We introduce a simple symmetry-breaking protocol that inserts a preferred direction into this rotational space through batchwise-sampled, unlearned query and value biases. This modification has two theoretically motivated and empirically validated consequences. First, it can substantially improve the performance of simple, memory-efficient optimizers, narrowing -- and in some cases closing -- the gap to successful but more complex memory-intensive adaptive methods. We demonstrate this by pretraining 124M parameter transformer models with four optimization algorithms (AdamW, SOAP, SGDM, and Energy Conserving Descent(ECD)) and evaluating both validation loss and downstream logical reasoning. Second, it enables an interpretable use of otherwise redundant rotational degrees of freedom, selectively amplifying semantically meaningful token classes within individual attention heads. Overall, our results show that minimal, principled architectural changes can simultaneously improve performance and interpretability.

</details>


### [10] [Tabular Foundation Models Can Do Survival Analysis](https://arxiv.org/abs/2601.22259)
*Da In Kim,Wei Siang Lai,Kelly W. Zhang*

Main category: cs.LG

TL;DR: 将生存分析转化为分类问题，通过离散化事件时间，利用表格基础模型进行上下文学习，无需显式训练


<details>
  <summary>Details</summary>
Motivation: 表格基础模型在分类和回归任务中表现出色，但由于右删失数据的存在，将其应用于生存分析具有挑战性。需要一种方法能够利用现有表格基础模型处理时间到事件的结果。

Method: 提出基于分类的框架，将静态和动态生存分析转化为一系列二分类问题：通过离散化事件时间，将每个时间点视为一个二分类任务（事件是否发生）。删失观测被处理为在某些时间点缺少标签的样本。该框架允许现有表格基础模型通过上下文学习进行生存分析，无需显式训练。

Result: 在53个真实世界数据集上的评估表明，采用该分类公式的现成表格基础模型在多个生存指标上平均优于经典和深度学习基线方法。理论证明在标准删失假设下，最小化二分类损失能够随着训练集增大而恢复真实的生存概率。

Conclusion: 该研究成功地将生存分析转化为分类问题，使现有表格基础模型能够有效处理右删失数据，为生存分析提供了新的强大工具，无需专门训练即可获得优异性能。

Abstract: While tabular foundation models have achieved remarkable success in classification and regression, adapting them to model time-to-event outcomes for survival analysis is non-trivial due to right-censoring, where data observations may end before the event occurs. We develop a classification-based framework that reformulates both static and dynamic survival analysis as a series of binary classification problems by discretizing event times. Censored observations are naturally handled as examples with missing labels at certain time points. This classification formulation enables existing tabular foundation models to perform survival analysis through in-context learning without explicit training. We prove that under standard censoring assumptions, minimizing our binary classification loss recovers the true survival probabilities as the training set size increases. We demonstrate through evaluation across $53$ real-world datasets that off-the-shelf tabular foundation models with this classification formulation outperform classical and deep learning baselines on average over multiple survival metrics.

</details>


### [11] [Privacy-Preserving Sensor-Based Human Activity Recognition for Low-Resource Healthcare Using Classical Machine Learning](https://arxiv.org/abs/2601.22265)
*Ramakant Kumar,Pravin Kumar*

Main category: cs.LG

TL;DR: 提出基于可穿戴惯性传感器和机器学习的低成本自动化人体活动识别框架，用于远程医疗和老年人辅助，其中支持张量机(STM)在活动分类中显著优于传统分类器。


<details>
  <summary>Details</summary>
Motivation: 医疗基础设施有限导致老年人和弱势患者依赖家庭护理，经常忽视治疗性锻炼（如瑜伽或物理治疗）的依从性。需要解决这一差距，提供远程监测解决方案。

Method: 使用加速度计和陀螺仪测量收集活动数据（行走、上下楼梯、坐、站、躺）。评估四种经典分类器（逻辑回归、随机森林、SVM、k-NN）并与提出的支持张量机(STM)进行比较。STM利用张量表示保留时空运动动态。

Result: SVM准确率为93.33%，逻辑回归、随机森林和k-NN为91.11%。STM显著优于这些模型，测试准确率达到96.67%，交叉验证准确率最高达98.50%。

Conclusion: 提出的框架在远程医疗、老年人辅助、儿童活动监测、瑜伽反馈和智能家居健康方面具有强大潜力，为低资源和农村医疗环境提供了可扩展的解决方案。

Abstract: Limited access to medical infrastructure forces elderly and vulnerable patients to rely on home-based care, often leading to neglect and poor adherence to therapeutic exercises such as yoga or physiotherapy. To address this gap, we propose a low-cost and automated human activity recognition (HAR) framework based on wearable inertial sensors and machine learning. Activity data, including walking, walking upstairs, walking downstairs, sitting, standing, and lying, were collected using accelerometer and gyroscope measurements. Four classical classifiers, Logistic Regression, Random Forest, Support Vector Machine (SVM), and k-Nearest Neighbors (k-NN), were evaluated and compared with the proposed Support Tensor Machine (STM). Experimental results show that SVM achieved an accuracy of 93.33 percent, while Logistic Regression, Random Forest, and k-NN achieved 91.11 percent. In contrast, STM significantly outperformed these models, achieving a test accuracy of 96.67 percent and the highest cross-validation accuracy of 98.50 percent. Unlike conventional methods, STM leverages tensor representations to preserve spatio-temporal motion dynamics, resulting in robust classification across diverse activities. The proposed framework demonstrates strong potential for remote healthcare, elderly assistance, child activity monitoring, yoga feedback, and smart home wellness, offering a scalable solution for low-resource and rural healthcare settings.

</details>


### [12] [Task-Uniform Convergence and Backward Transfer in Federated Domain-Incremental Learning with Partial Participation](https://arxiv.org/abs/2601.22274)
*Longtao Xu,Jian Li*

Main category: cs.LG

TL;DR: SPECIAL是一种简单、无内存的联邦域增量学习算法，通过服务器端"锚点"控制累积漂移，无需重放缓冲区或任务特定头，保持通信和模型大小不变。


<details>
  <summary>Details</summary>
Motivation: 现实联邦系统中数据分布会漂移，但隐私规则禁止原始数据共享。联邦域增量学习(FDIL)面临两个理论缺失：向后知识转移(BKT)保证和跨所有任务的部分参与收敛率保证。

Method: SPECIAL在FedAvg基础上添加服务器端"锚点"：每轮中服务器用轻量级近端项将参与客户端更新推向先前全局模型，控制累积漂移，无需重放缓冲区、合成数据或任务特定头。

Result: 理论证明SPECIAL：(1)保护先前任务：BKT界限将先前任务损失增加限制为漂移控制项，随更多轮次、本地周期和参与客户端而缩小；(2)跨任务高效学习：首次实现FDIL部分参与下的通信高效非凸收敛率O((E/NT)^(1/2))。

Conclusion: SPECIAL通过简单服务器端锚点有效解决联邦域增量学习问题，提供理论保证并保持FedAvg的通信和模型效率，实验验证了其有效性。

Abstract: Real-world federated systems seldom operate on static data: input distributions drift while privacy rules forbid raw-data sharing. We study this setting as Federated Domain-Incremental Learning (FDIL), where (i) clients are heterogeneous, (ii) tasks arrive sequentially with shifting domains, yet (iii) the label space remains fixed. Two theoretical pillars remain missing for FDIL under realistic deployment: a guarantee of backward knowledge transfer (BKT) and a convergence rate that holds across the sequence of all tasks with partial participation. We introduce SPECIAL (Server-Proximal Efficient Continual Aggregation for Learning), a simple, memory-free FDIL algorithm that adds a single server-side ``anchor'' to vanilla FedAvg: in each round, the server nudges the uniformly sampled participated clients update toward the previous global model with a lightweight proximal term. This anchor curbs cumulative drift without replay buffers, synthetic data, or task-specific heads, keeping communication and model size unchanged. Our theory shows that SPECIAL (i) preserves earlier tasks: a BKT bound caps any increase in prior-task loss by a drift-controlled term that shrinks with more rounds, local epochs, and participating clients; and (ii) learns efficiently across all tasks: the first communication-efficient non-convex convergence rate for FDIL with partial participation, O((E/NT)^(1/2)), with E local epochs, T communication rounds, and N participated clients per round, matching single-task FedAvg while explicitly separating optimization variance from inter-task drift. Experimental results further demonstrate the effectiveness of SPECIAL.

</details>


### [13] [SurrogateSHAP: Training-Free Contributor Attribution for Text-to-Image (T2I) Models](https://arxiv.org/abs/2601.22276)
*Mingyu Lu,Soham Gadgil,Chris Lin,Chanwoo Kim,Su-In Lee*

Main category: cs.LG

TL;DR: SurrogateSHAP：一种无需重新训练的Shapley值近似框架，用于评估文本到图像扩散模型中数据贡献者的价值，显著降低计算成本


<details>
  <summary>Details</summary>
Motivation: 随着文本到图像扩散模型在创意工作流中的广泛应用，需要公平评估数据贡献者价值的框架。传统Shapley值方法面临双重计算瓶颈：模型重新训练成本高和组合子集数量庞大

Method: 提出SurrogateSHAP框架：1) 通过预训练模型推理近似昂贵的重新训练过程；2) 使用梯度提升树近似效用函数，从树模型解析推导Shapley值

Result: 在三个不同归因任务中表现优异：CIFAR-20上的图像质量、后印象派艺术品的美学评估、时尚产品数据的多样性。相比现有方法计算开销显著降低，能一致识别有影响力的贡献者

Conclusion: SurrogateSHAP为公平补偿数据贡献者提供了可扩展解决方案，能有效定位临床图像中的虚假相关性来源，为审计安全关键生成模型提供了可行路径

Abstract: As Text-to-Image (T2I) diffusion models are increasingly used in real-world creative workflows, a principled framework for valuing contributors who provide a collection of data is essential for fair compensation and sustainable data marketplaces. While the Shapley value offers a theoretically grounded approach to attribution, it faces a dual computational bottleneck: (i) the prohibitive cost of exhaustive model retraining for each sampled subset of players (i.e., data contributors) and (ii) the combinatorial number of subsets needed to estimate marginal contributions due to contributor interactions. To this end, we propose SurrogateSHAP, a retraining-free framework that approximates the expensive retraining game through inference from a pretrained model. To further improve efficiency, we employ a gradient-boosted tree to approximate the utility function and derive Shapley values analytically from the tree-based model. We evaluate SurrogateSHAP across three diverse attribution tasks: (i) image quality for DDPM-CFG on CIFAR-20, (ii) aesthetics for Stable Diffusion on Post-Impressionist artworks, and (iii) product diversity for FLUX.1 on Fashion-Product data. Across settings, SurrogateSHAP outperforms prior methods while substantially reducing computational overhead, consistently identifying influential contributors across multiple utility metrics. Finally, we demonstrate that SurrogateSHAP effectively localizes data sources responsible for spurious correlations in clinical images, providing a scalable path toward auditing safety-critical generative models.

</details>


### [14] [Riemannian Lyapunov Optimizer: A Unified Framework for Optimization](https://arxiv.org/abs/2601.22284)
*Yixuan Wang,Omkar Sudhir Patil,Warren E. Dixon*

Main category: cs.LG

TL;DR: Riemannian Lyapunov Optimizers (RLOs) 是一个统一的几何优化框架，将经典优化器重新解释为黎曼流形上的受控动力系统，通过控制理论方法系统化设计优化器。


<details>
  <summary>Details</summary>
Motivation: 当前优化器设计多基于启发式改进，缺乏系统理论框架。本文旨在建立控制理论与机器学习优化之间的桥梁，提供统一语言和系统化工具来设计稳定有效的优化器。

Method: 将优化重新解释为黎曼参数流形上的扩展状态离散时间受控动力系统。核心是识别"正常吸引不变流形"(NAIM)，将训练动态分为两个阶段：速度状态快速对齐目标图，然后在其中受控演化。通过构造严格的Lyapunov函数来证明收敛性，形成"优化器生成器"。

Result: RLOs不仅能够恢复经典算法，还能原则性地设计新优化器。通过几何诊断验证理论，并在大规模基准测试中达到最先进性能。

Conclusion: RLOs成功连接了控制理论与现代机器学习优化，提供了统一框架和系统化工具包，用于设计稳定有效的优化器，为优化器设计提供了新的理论基础。

Abstract: We introduce Riemannian Lyapunov Optimizers (RLOs), a family of optimization algorithms that unifies classic optimizers within one geometric framework. Unlike heuristic improvements to existing optimizers, RLOs are systematically derived from a novel control-theoretic framework that reinterprets optimization as an extended state discrete-time controlled dynamical system on a Riemannian parameter manifold. Central to this framework is the identification of a Normally Attracting Invariant Manifold (NAIM), which organizes training dynamics into two distinct stages: rapid alignment of the speed state to a target graph, followed by controlled evolution within it. We formalize this by constructing a strict Lyapunov function that certifies convergence to a target manifold. This perspective yields a constructive ``optimizer generator" that not only recovers classic algorithms but enables the principled design of RLOs. We validate our theory via geometric diagnostics and demonstrate that grounding optimizer design in control theory yields state-of-the-art performance in large-scale benchmarks. Overall, RLOs bridge control theory and modern machine learning optimization, providing a unified language and a systematic toolkit for designing stable, effective optimizers.

</details>


### [15] [Demystifying Mergeability: Interpretable Properties to Predict Model Merging Success](https://arxiv.org/abs/2601.22285)
*Luca Zhou,Bo Zhao,Rose Yu,Emanuele Rodolà*

Main category: cs.LG

TL;DR: 模型合并的成功因素不仅取决于模型本身，还依赖于合并方法和任务特性，研究发现子空间重叠和梯度对齐是方法无关的兼容性基础。


<details>
  <summary>Details</summary>
Motivation: 当前研究通常将模型可合并性视为内在属性，但作者认为这忽略了合并方法和任务特性的影响，需要系统性地理解模型合并成功的关键因素。

Method: 提出架构无关的框架，使用线性优化分析可解释的成对度量（如梯度L2距离），在四种合并方法上研究后合并性能与度量特性的相关性。

Result: 发现成功驱动因素在不同方法间差异显著（46.7%度量重叠；55.3%符号一致性），揭示了方法特定的"指纹"，但子空间重叠和梯度对齐始终是方法无关的兼容性基础。

Conclusion: 模型可合并性诊断框架为理解合并兼容性提供了基础，并激励未来开发能显式促进子空间重叠和梯度对齐特性的微调策略。

Abstract: Model merging combines knowledge from separately fine-tuned models, yet success factors remain poorly understood. While recent work treats mergeability as an intrinsic property, we show with an architecture-agnostic framework that it fundamentally depends on both the merging method and the partner tasks. Using linear optimization over a set of interpretable pairwise metrics (e.g., gradient L2 distance), we uncover properties correlating with post-merge performance across four merging methods. We find substantial variation in success drivers (46.7% metric overlap; 55.3% sign agreement), revealing method-specific "fingerprints". Crucially, however, subspace overlap and gradient alignment metrics consistently emerge as foundational, method-agnostic prerequisites for compatibility. These findings provide a diagnostic foundation for understanding mergeability and motivate future fine-tuning strategies that explicitly encourage these properties.

</details>


### [16] [ParalESN: Enabling parallel information processing in Reservoir Computing](https://arxiv.org/abs/2601.22296)
*Matteo Pinna,Giacomo Lagomarsini,Andrea Ceni,Claudio Gallicchio*

Main category: cs.LG

TL;DR: 提出Parallel Echo State Network (ParalESN)，通过结构化算子和状态空间建模解决传统储层计算的可扩展性问题，实现时间数据的并行处理和高维高效储层构建。


<details>
  <summary>Details</summary>
Motivation: 传统储层计算面临两个主要限制：(1) 时间数据必须顺序处理，(2) 高维储层内存占用过大，这严重制约了其可扩展性。

Method: 通过结构化算子和状态空间建模视角重新审视储层计算，提出ParalESN，基于复数空间中的对角线性递归构建高维高效储层，实现时间数据的并行处理。

Result: 理论分析表明ParalESN保留了传统ESN的回声状态特性和普适性保证，同时允许任意线性储层在复数对角形式下的等价表示。实验显示在时间序列基准测试中与传统RC预测精度相当，但计算成本显著降低；在1-D像素级分类任务中，与完全可训练神经网络相比，在保持竞争性精度的同时，计算成本和能耗降低数个数量级。

Conclusion: ParalESN为将储层计算整合到深度学习领域提供了一个有前景、可扩展且理论严谨的途径。

Abstract: Reservoir Computing (RC) has established itself as an efficient paradigm for temporal processing. However, its scalability remains severely constrained by (i) the necessity of processing temporal data sequentially and (ii) the prohibitive memory footprint of high-dimensional reservoirs. In this work, we revisit RC through the lens of structured operators and state space modeling to address these limitations, introducing Parallel Echo State Network (ParalESN). ParalESN enables the construction of high-dimensional and efficient reservoirs based on diagonal linear recurrence in the complex space, enabling parallel processing of temporal data. We provide a theoretical analysis demonstrating that ParalESN preserves the Echo State Property and the universality guarantees of traditional Echo State Networks while admitting an equivalent representation of arbitrary linear reservoirs in the complex diagonal form. Empirically, ParalESN matches the predictive accuracy of traditional RC on time series benchmarks, while delivering substantial computational savings. On 1-D pixel-level classification tasks, ParalESN achieves competitive accuracy with fully trainable neural networks while reducing computational costs and energy consumption by orders of magnitude. Overall, ParalESN offers a promising, scalable, and principled pathway for integrating RC within the deep learning landscape.

</details>


### [17] [Conformal Prediction for Generative Models via Adaptive Cluster-Based Density Estimation](https://arxiv.org/abs/2601.22298)
*Qidong Yang,Qianyu Julie Zhu,Jonathan Giezendanner,Youssef Marzouk,Stephen Bates,Sherrie Wang*

Main category: cs.LG

TL;DR: 提出CP4Gen方法，通过聚类密度估计为条件生成模型构建更稳健、可解释的预测集，提升不确定性校准


<details>
  <summary>Details</summary>
Motivation: 条件生成模型缺乏校准的不确定性估计，这在高风险应用中会削弱对单个输出的信任，需要系统性的不确定性量化方法

Method: 提出CP4Gen方法，基于模型生成样本进行密度估计，采用聚类技术构建预测集，对异常值更不敏感，结构更简单，可解释性更强

Result: 在合成数据集和真实世界应用（包括气候模拟任务）上的实验表明，CP4Gen在预测集体积和结构简单性方面始终优于现有方法

Conclusion: CP4Gen为条件生成模型提供了强大的不确定性估计工具，特别适用于需要严格且可解释预测集的场景

Abstract: Conditional generative models map input variables to complex, high-dimensional distributions, enabling realistic sample generation in a diverse set of domains. A critical challenge with these models is the absence of calibrated uncertainty, which undermines trust in individual outputs for high-stakes applications. To address this issue, we propose a systematic conformal prediction approach tailored to conditional generative models, leveraging density estimation on model-generated samples. We introduce a novel method called CP4Gen, which utilizes clustering-based density estimation to construct prediction sets that are less sensitive to outliers, more interpretable, and of lower structural complexity than existing methods. Extensive experiments on synthetic datasets and real-world applications, including climate emulation tasks, demonstrate that CP4Gen consistently achieves superior performance in terms of prediction set volume and structural simplicity. Our approach offers practitioners a powerful tool for uncertainty estimation associated with conditional generative models, particularly in scenarios demanding rigorous and interpretable prediction sets.

</details>


### [18] [ZK-HybridFL: Zero-Knowledge Proof-Enhanced Hybrid Ledger for Federated Learning](https://arxiv.org/abs/2601.22302)
*Amirhossein Taherpour,Xiaodong Wang*

Main category: cs.LG

TL;DR: ZK-HybridFL：结合DAG账本、侧链和零知识证明的安全去中心化联邦学习框架，实现隐私保护的模型验证和对抗行为检测


<details>
  <summary>Details</summary>
Motivation: 联邦学习在保护数据隐私的同时实现协作训练，但集中式和去中心化方法都面临可扩展性、安全性和更新验证的挑战。需要一种既能保护隐私又能有效验证模型更新的安全去中心化解决方案。

Method: 提出ZK-HybridFL框架，集成有向无环图（DAG）账本、专用侧链和零知识证明（ZKPs），使用事件驱动智能合约和预言机辅助的侧链来验证本地模型更新而不暴露敏感数据，内置挑战机制检测对抗行为。

Result: 在图像分类和语言建模任务实验中，ZK-HybridFL相比Blade-FL和ChainFL实现了更快的收敛速度、更高的准确率、更低的困惑度和更低的延迟。框架能抵抗大量对抗节点和空闲节点，支持亚秒级链上验证和高效gas使用，防止无效更新和孤儿式攻击。

Conclusion: ZK-HybridFL为去中心化联邦学习提供了一个可扩展且安全的解决方案，适用于多样化环境，在保护隐私的同时确保了模型更新的有效性和系统的鲁棒性。

Abstract: Federated learning (FL) enables collaborative model training while preserving data privacy, yet both centralized and decentralized approaches face challenges in scalability, security, and update validation. We propose ZK-HybridFL, a secure decentralized FL framework that integrates a directed acyclic graph (DAG) ledger with dedicated sidechains and zero-knowledge proofs (ZKPs) for privacy-preserving model validation. The framework uses event-driven smart contracts and an oracle-assisted sidechain to verify local model updates without exposing sensitive data. A built-in challenge mechanism efficiently detects adversarial behavior. In experiments on image classification and language modeling tasks, ZK-HybridFL achieves faster convergence, higher accuracy, lower perplexity, and reduced latency compared to Blade-FL and ChainFL. It remains robust against substantial fractions of adversarial and idle nodes, supports sub-second on-chain verification with efficient gas usage, and prevents invalid updates and orphanage-style attacks. This makes ZK-HybridFL a scalable and secure solution for decentralized FL across diverse environments.

</details>


### [19] [BayesFlow: A Probability Inference Framework for Meta-Agent Assisted Workflow Generation](https://arxiv.org/abs/2601.22305)
*Bo Yuan,Yun Zhou,Zhichao Xu,Kiran Ramnath,Aosong Feng,Balasubramaniam Srinivasan*

Main category: cs.LG

TL;DR: 提出贝叶斯工作流生成(BWG)框架，将工作流生成建模为贝叶斯推断问题，通过采样方法构建工作流，在六个基准数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有自动工作流生成方法大多将其视为优化问题，缺乏理论基础。作者希望建立一个有理论保证的贝叶斯推断框架来改进工作流生成。

Method: 提出贝叶斯工作流生成(BWG)框架，将工作流生成视为后验分布推断问题。具体实现为BayesFlow算法，使用并行前瞻回滚进行重要性加权，结合序列循环精炼器进行池级改进。

Result: 在六个基准数据集上，BayesFlow比最先进的工作流生成基线准确率提升高达9个百分点，比零样本提示提升高达65个百分点。

Conclusion: BWG为基于搜索的工作流设计提供了有理论基础的升级方案，证明了贝叶斯推断框架在自动工作流生成中的有效性。

Abstract: Automatic workflow generation is the process of automatically synthesizing sequences of LLM calls, tool invocations, and post-processing steps for complex end-to-end tasks. Most prior methods cast this task as an optimization problem with limited theoretical grounding. We propose to cast workflow generation as Bayesian inference over a posterior distribution on workflows, and introduce \textbf{Bayesian Workflow Generation (BWG)}, a sampling framework that builds workflows step-by-step using parallel look-ahead rollouts for importance weighting and a sequential in-loop refiner for pool-wide improvements. We prove that, without the refiner, the weighted empirical distribution converges to the target posterior. We instantiate BWG as \textbf{BayesFlow}, a training-free algorithm for workflow construction. Across six benchmark datasets, BayesFlow improves accuracy by up to 9 percentage points over SOTA workflow generation baselines and by up to 65 percentage points over zero-shot prompting, establishing BWG as a principled upgrade to search-based workflow design. Code will be available on https://github.com/BoYuanVisionary/BayesFlow.

</details>


### [20] [Exact closed-form Gaussian moments of residual layers](https://arxiv.org/abs/2601.22307)
*Simon Kuang,Xinfan Lin*

Main category: cs.LG

TL;DR: 提出一种通过逐层矩匹配在深度残差神经网络中传播多元高斯分布均值和协方差的精确方法，在多个激活函数上取得显著改进


<details>
  <summary>Details</summary>
Motivation: 解决在深度神经网络中传播高斯分布统计特性的长期问题，为贝叶斯推断和不确定性量化提供更精确的方法

Method: 使用逐层矩匹配技术，推导出probit、GeLU、ReLU、Heaviside和sine激活函数的精确矩匹配公式，适用于前馈和广义残差层

Result: 在随机网络上KL散度误差指标比流行方法提高数个数量级（最高百万倍），在真实数据上获得竞争性的统计校准，在变分贝叶斯网络上比最先进确定性推断方法提高百倍

Conclusion: 该方法填补了长期存在的理论空白，为深度神经网络中的不确定性传播提供了精确高效的解决方案，在多个应用场景中表现出显著优势

Abstract: We study the problem of propagating the mean and covariance of a general multivariate Gaussian distribution through a deep (residual) neural network using layer-by-layer moment matching. We close a longstanding gap by deriving exact moment matching for the probit, GeLU, ReLU (as a limit of GeLU), Heaviside (as a limit of probit), and sine activation functions; for both feedforward and generalized residual layers. On random networks, we find orders-of-magnitude improvements in the KL divergence error metric, up to a millionfold, over popular alternatives. On real data, we find competitive statistical calibration for inference under epistemic uncertainty in the input. On a variational Bayes network, we show that our method attains hundredfold improvements in KL divergence from Monte Carlo ground truth over a state-of-the-art deterministic inference method. We also give an a priori error bound and a preliminary analysis of stochastic feedforward neurons, which have recently attracted general interest.

</details>


### [21] [Stealthy Poisoning Attacks Bypass Defenses in Regression Settings](https://arxiv.org/abs/2601.22308)
*Javier Carnerero-Cano,Luis Muñoz-González,Phillippa Spencer,Emil C. Lupu*

Main category: cs.LG

TL;DR: 提出针对回归模型的最优隐蔽攻击方法，能绕过现有防御，并提出新的评估方法和防御方案BayesClean


<details>
  <summary>Details</summary>
Motivation: 回归模型在工业、工程和科学中广泛应用，但其对投毒攻击的鲁棒性研究不足，现有研究往往基于不现实的威胁模型，实用性有限

Method: 提出考虑不同可检测程度的最优隐蔽攻击公式；提出基于目标归一化的新评估方法，权衡攻击效果和可检测性；开发新的防御方法BayesClean

Result: 提出的最优隐蔽攻击能绕过最先进的防御；BayesClean在攻击隐蔽且投毒点数量显著时优于先前防御

Conclusion: 回归模型的投毒攻击防御需要更现实的威胁模型，提出的攻击、评估和防御方法为实际应用提供了更有效的解决方案

Abstract: Regression models are widely used in industrial processes, engineering and in natural and physical sciences, yet their robustness to poisoning has received less attention. When it has, studies often assume unrealistic threat models and are thus less useful in practice. In this paper, we propose a novel optimal stealthy attack formulation that considers different degrees of detectability and show that it bypasses state-of-the-art defenses. We further propose a new methodology based on normalization of objectives to evaluate different trade-offs between effectiveness and detectability. Finally, we develop a novel defense (BayesClean) against stealthy attacks. BayesClean improves on previous defenses when attacks are stealthy and the number of poisoning points is significant.

</details>


### [22] [SCALAR: Quantifying Structural Hallucination, Consistency, and Reasoning Gaps in Materials Foundation Models](https://arxiv.org/abs/2601.22312)
*Can Polat,Erchin Serpedin,Mustafa Kurban,Hasan Kurban*

Main category: cs.LG

TL;DR: SCALAR是一个用于评估材料基础模型在几何尺度泛化能力的基准测试，包含三个任务：CIF到性质预测、物理推理的思维链变体、以及基于目标性质的晶体逆向检索。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型越来越多地应用于材料科学推理，但它们在物理结构化分布偏移下的行为仍然缺乏深入理解。需要评估几何尺度泛化及其与结构幻觉、一致性和推理能力的关系。

Method: SCALAR基准测试使用从DFT验证的晶胞通过超胞扩展和几何截断得到的纳米颗粒结构，涵盖从几个原子到超过18,000个原子的长度尺度。包含三个任务：CIF到性质预测、带物理推理的思维链变体、以及逆向检索任务。

Result: 实验显示，不同基础模型在显式推理下表现出显著的模型依赖性偏移，通常能减少幻觉和误差，但经常破坏一致性或有效性。几何尺度泛化不能仅从准确性推断。

Conclusion: SCALAR基准测试揭示了材料基础模型在几何尺度泛化方面的局限性，强调需要结构化指标来全面评估模型性能，而不仅仅是准确性。

Abstract: Large language models are increasingly applied to materials science reasoning, yet their behavior under physically structured distribution shifts remains poorly understood. We introduce SCALAR (Structural Consistency And Logic Across Regimes), a benchmark for evaluating geometric scale generalization and its connection to structural hallucination, consistency, and reasoning in materials foundation models. Given canonical crystal representations, models must reason about derived nanoparticle structures obtained through supercell expansion and geometric truncation across length scales spanning a few atoms to over 18,000 atoms, totaling $\approx$100,000 structures from DFT-validated unit cells. SCALAR defines three tasks. (i) CIF to property prediction. (ii) A Chain-of-Thought variant with explicit physics-grounded reasoning. (iii) Inverse retrieval identifying crystals from candidates given target properties. Outputs are evaluated via structured metrics capturing numeric error, hallucination, cross-prompt consistency, monotonic reasoning, output validity, and retrieval regret. Experiments across diverse foundation models reveal large, model-dependent shifts under explicit reasoning, often reducing hallucination and error, but frequently destabilizing consistency or validity. These results demonstrate that geometric scale generalization cannot be inferred from accuracy alone. Supplementary materials are available at https://github.com/KurbanIntelligenceLab/SCALAR.

</details>


### [23] [Hair-Trigger Alignment: Black-Box Evaluation Cannot Guarantee Post-Update Alignment](https://arxiv.org/abs/2601.22313)
*Yavuz Bakman,Duygu Nur Yaldiz,Salman Avestimehr,Sai Praneeth Karimireddy*

Main category: cs.LG

TL;DR: 静态黑盒评估无法保证LLM更新后的对齐性，理论上存在隐藏的对抗行为可能被良性更新激活，且模型规模越大隐藏能力越强。


<details>
  <summary>Details</summary>
Motivation: 当前LLM对齐研究通常基于静态黑盒评估（即对固定查询集没有不良响应），但实践中模型会频繁更新。现有方法无法评估模型更新后的对齐性，存在安全风险。

Method: 1. 形式化静态和更新后的对齐性定义；2. 理论证明：由于过参数化，静态对齐不能保证更新后对齐，且黑盒探测无法区分真正鲁棒的模型和隐藏对抗行为的模型；3. 在隐私、越狱安全和行为诚实三个对齐领域进行实证验证。

Result: 1. 理论上证明静态黑盒评估存在根本性局限；2. 实证发现存在通过所有标准黑盒测试但单次良性更新后严重失齐的LLM；3. 隐藏对抗行为的能力随模型规模增加而增强。

Conclusion: 静态评估协议不足，需要开发更新后鲁棒的对齐评估方法，特别是随着模型规模增大，更新后失齐风险增加。

Abstract: Large Language Models (LLMs) are rarely static and are frequently updated in practice. A growing body of alignment research has shown that models initially deemed "aligned" can exhibit misaligned behavior after fine-tuning, such as forgetting jailbreak safety features or re-surfacing knowledge that was intended to be forgotten. These works typically assume that the initial model is aligned based on static black-box evaluation, i.e., the absence of undesired responses to a fixed set of queries. In contrast, we formalize model alignment in both the static and post-update settings and uncover a fundamental limitation of black-box evaluation. We theoretically show that, due to overparameterization, static alignment provides no guarantee of post-update alignment for any update dataset. Moreover, we prove that static black-box probing cannot distinguish a model that is genuinely post-update robust from one that conceals an arbitrary amount of adversarial behavior which can be activated by even a single benign gradient update. We further validate these findings empirically in LLMs across three core alignment domains: privacy, jailbreak safety, and behavioral honesty. We demonstrate the existence of LLMs that pass all standard black-box alignment tests, yet become severely misaligned after a single benign update. Finally, we show that the capacity to hide such latent adversarial behavior increases with model scale, confirming our theoretical prediction that post-update misalignment grows with the number of parameters. Together, our results highlight the inadequacy of static evaluation protocols and emphasize the urgent need for post-update-robust alignment evaluation.

</details>


### [24] [Gaussian Process Bandit Optimization with Machine Learning Predictions and Application to Hypothesis Generation](https://arxiv.org/abs/2601.22315)
*Xin Jennifer Chen,Yunjin Tong*

Main category: cs.LG

TL;DR: 提出PA-GP-UCB算法，结合昂贵真实评估和廉价预测模型，利用离线数据提升贝叶斯优化的样本效率


<details>
  <summary>Details</summary>
Motivation: 现实优化问题常涉及昂贵真实评估（如人工评估、物理实验）和廉价预测模型（如机器学习模型、模拟），同时存在大量离线数据可用于预训练。需要有效结合两者以提高样本效率。

Method: 提出预测增强高斯过程上置信界算法（PA-GP-UCB），使用联合高斯过程后验推导的控制变量估计器来校正预测偏差并减少不确定性，利用离线数据和预测模型提升优化效率。

Result: 理论证明PA-GP-UCB保持GP-UCB的标准遗憾率，同时获得更小的主导常数，该常数由预测质量和离线数据覆盖度明确控制。在合成基准和基于人类行为数据的真实假设评估任务中，收敛速度快于基准方法。

Conclusion: PA-GP-UCB为昂贵反馈下的假设生成提供了一个通用且样本高效的框架，有效结合了昂贵真实评估、廉价预测模型和离线数据的优势。

Abstract: Many real-world optimization problems involve an expensive ground-truth oracle (e.g., human evaluation, physical experiments) and a cheap, low-fidelity prediction oracle (e.g., machine learning models, simulations). Meanwhile, abundant offline data (e.g., past experiments and predictions) are often available and can be used to pretrain powerful predictive models, as well as to provide an informative prior. We propose Prediction-Augmented Gaussian Process Upper Confidence Bound (PA-GP-UCB), a novel Bayesian optimization algorithm that leverages both oracles and offline data to achieve provable gains in sample efficiency for the ground-truth oracle queries. PA-GP-UCB employs a control-variates estimator derived from a joint Gaussian process posterior to correct prediction bias and reduce uncertainty. We prove that PA-GP-UCB preserves the standard regret rate of GP-UCB while achieving a strictly smaller leading constant that is explicitly controlled by prediction quality and offline data coverage. Empirically, PA-GP-UCB converges faster than Vanilla GP-UCB and naive prediction-augmented GP-UCB baselines on synthetic benchmarks and on a real-world hypothesis evaluation task grounded in human behavioral data, where predictions are provided by large language models. These results establish PA-GP-UCB as a general and sample-efficient framework for hypothesis generation under expensive feedback.

</details>


### [25] [FlowSymm: Physics Aware, Symmetry Preserving Graph Attention for Network Flow Completion](https://arxiv.org/abs/2601.22317)
*Ege Demirci,Francesco Bullo,Ananthram Swami,Ambuj Singh*

Main category: cs.LG

TL;DR: FlowSymm：一种新颖的神经网络架构，用于在尊重局部守恒定律的前提下恢复网络边缘的缺失流量，在交通、电力、自行车流量等实际基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 网络边缘流量恢复是一个基础逆问题，在交通、能源、移动性等系统中普遍存在。现有方法需要在尊重局部守恒定律（如流量守恒）的前提下准确恢复缺失流量，这是一个具有挑战性的问题。

Method: FlowSymm结合了三个关键组件：1）无散流上的群作用；2）图注意力编码器学习特征条件权重；3）通过隐式双层优化的轻量级Tikhonov细化。方法首先将观测值锚定在最小范数无散完成上，然后计算保持观测流量不变的所有允许群作用的正交基，参数化有效解子空间。GATv2层编码图和边特征，通过注意力机制选择物理感知的群作用，最后通过凸最小二乘求解器进行Tikhonov细化。

Result: 在三个真实世界流量基准测试（交通、电力、自行车）中，FlowSymm在RMSE、MAE和相关指标上均优于最先进的基线方法。

Conclusion: FlowSymm通过结合群对称性、图注意力机制和隐式优化，提供了一种有效恢复网络缺失流量的方法，同时严格尊重局部守恒定律，在实际应用中表现出优越性能。

Abstract: Recovering missing flows on the edges of a network, while exactly respecting local conservation laws, is a fundamental inverse problem that arises in many systems such as transportation, energy, and mobility. We introduce FlowSymm, a novel architecture that combines (i) a group-action on divergence-free flows, (ii) a graph-attention encoder to learn feature-conditioned weights over these symmetry-preserving actions, and (iii) a lightweight Tikhonov refinement solved via implicit bilevel optimization. The method first anchors the given observation on a minimum-norm divergence-free completion. We then compute an orthonormal basis for all admissible group actions that leave the observed flows invariant and parameterize the valid solution subspace, which shows an Abelian group structure under vector addition. A stack of GATv2 layers then encodes the graph and its edge features into per-edge embeddings, which are pooled over the missing edges and produce per-basis attention weights. This attention-guided process selects a set of physics-aware group actions that preserve the observed flows. Finally, a scalar Tikhonov penalty refines the missing entries via a convex least-squares solver, with gradients propagated implicitly through Cholesky factorization. Across three real-world flow benchmarks (traffic, power, bike), FlowSymm outperforms state-of-the-art baselines in RMSE, MAE and correlation metrics.

</details>


### [26] [Federate the Router: Learning Language Model Routers with Sparse and Decentralized Evaluations](https://arxiv.org/abs/2601.22318)
*Baris Askin,Shivam Patel,Anupam Nayak,Andrea Vigano,Jiin Woo,Gauri Joshi,Carlee Joe-Wong*

Main category: cs.LG

TL;DR: 提出了首个用于大语言模型路由的联邦学习框架，使客户端能够从本地离线查询-模型评估数据中学习共享路由策略，解决数据隐私和碎片化问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型作为远程服务被访问，不同模型在能力和价格上差异很大，需要路由查询以平衡质量和推理成本。现有路由方法假设可以访问集中的查询-模型评估数据，但这些数据通常分散在客户端（如终端用户和组织），且涉及隐私敏感信息，无法集中。此外，单客户端路由训练效果有限，因为本地评估数据有限，仅覆盖受限的查询分布和有偏的模型评估子集。

Method: 引入首个用于LLM路由的联邦学习框架，支持参数化多层感知器路由器和非参数化K-means路由器，适应异构客户端查询分布和非均匀模型覆盖。通过联邦协作训练共享路由策略。

Result: 在两个基准测试中，联邦协作相比客户端本地路由器提高了准确率-成本前沿，既通过增加有效模型覆盖，也通过更好的查询泛化。理论结果也验证了联邦训练减少了路由次优性。

Conclusion: 提出的联邦LLM路由框架有效解决了数据隐私和碎片化问题，通过客户端协作学习共享路由策略，在保持隐私的同时提高了路由性能，平衡了查询质量和推理成本。

Abstract: Large language models (LLMs) are increasingly accessed as remotely hosted services by edge and enterprise clients that cannot run frontier models locally. Since models vary widely in capability and price, routing queries to models that balance quality and inference cost is essential. Existing router approaches assume access to centralized query-model evaluation data. However, these data are often fragmented across clients, such as end users and organizations, and are privacy-sensitive, which makes centralizing data infeasible. Additionally, per-client router training is ineffective since local evaluation data is limited and covers only a restricted query distribution and a biased subset of model evaluations. We introduce the first federated framework for LLM routing, enabling clients to learn a shared routing policy from local offline query-model evaluation data. Our framework supports both parametric multilayer perceptron router and nonparametric K-means router under heterogeneous client query distributions and non-uniform model coverage. Across two benchmarks, federated collaboration improves the accuracy-cost frontier over client-local routers, both via increased effective model coverage and better query generalization. Our theoretical results also validate that federated training reduces routing suboptimality.

</details>


### [27] [Matrix Factorization for Practical Continual Mean Estimation Under User-Level Differential Privacy](https://arxiv.org/abs/2601.22320)
*Nikita P. Kalinin,Ali Najar,Valentin Roth,Christoph H. Lampert*

Main category: cs.LG

TL;DR: 提出一种在近似差分隐私下持续均值估计的新方法，使用矩阵分解机制和特定因子化，显著降低误差


<details>
  <summary>Details</summary>
Motivation: 现有持续均值估计研究主要关注纯差分隐私，导致估计噪声过大，限制了实际应用。需要研究近似差分隐私下的解决方案

Method: 采用矩阵分解机制，提出专门针对均值估计的因子化方法，在用户级差分隐私下实现高效准确的持续均值估计

Result: 新方法在持续均值估计中实现了渐近更低的均方误差界限，比纯差分隐私方法更准确

Conclusion: 近似差分隐私下的矩阵分解方法为持续均值估计提供了更实用的解决方案，平衡了隐私保护和估计准确性

Abstract: We study continual mean estimation, where data vectors arrive sequentially and the goal is to maintain accurate estimates of the running mean. We address this problem under user-level differential privacy, which protects each user's entire dataset even when they contribute multiple data points. Previous work on this problem has focused on pure differential privacy. While important, this approach limits applicability, as it leads to overly noisy estimates. In contrast, we analyze the problem under approximate differential privacy, adopting recent advances in the Matrix Factorization mechanism. We introduce a novel mean estimation specific factorization, which is both efficient and accurate, achieving asymptotically lower mean-squared error bounds in continual mean estimation under user-level differential privacy.

</details>


### [28] [Spatially-Adaptive Conformal Graph Transformer for Indoor Localization in Wi-Fi Driven Networks](https://arxiv.org/abs/2601.22322)
*Ayesh Abu Lehyeh,Anastassia Gharib,Safwan Wshah*

Main category: cs.LG

TL;DR: SAC-GT：一种结合图变换器和空间自适应保形预测的室内定位框架，既能提供精确的2D位置预测，又能给出统计有效的区域特定置信区域


<details>
  <summary>Details</summary>
Motivation: 室内定位是智能环境中各种基于位置服务的关键使能技术。现有的基于图的模型虽然能利用Wi-Fi接入点与设备之间的空间关系提供更精细的定位粒度，但缺乏预测不确定性的量化能力，而这正是实际部署的关键要求。

Method: 提出SAC-GT框架，整合了图变换器（GT）模型和新型空间自适应保形预测（SACP）方法。GT模型捕捉网络的空间拓扑和信号强度动态，SACP方法则提供区域特定的不确定性估计。

Result: 在大规模真实世界数据集上的广泛评估表明，SAC-GT解决方案在实现最先进的定位精度的同时，提供了鲁棒且空间自适应的可靠性保证。

Conclusion: SAC-GT框架能够同时提供精确的2D位置预测和统计有效的置信区域，这些置信区域能够适应不同的环境条件，解决了现有方法在不确定性量化方面的不足。

Abstract: Indoor localization is a critical enabler for a wide range of location-based services in smart environments, including navigation, asset tracking, and safety-critical applications. Recent graph-based models leverage spatial relationships between Wire-less Fidelity (Wi-Fi) Access Points (APs) and devices, offering finer localization granularity, but fall short in quantifying prediction uncertainty, a key requirement for real-world deployment. In this paper, we propose Spatially-Adaptive Conformal Graph Transformer (SAC-GT), a framework for accurate and reliable indoor localization. SAC-GT integrates a Graph Transformer (GT) model that captures network's spatial topology and signal strength dynamics, with a novel Spatially-Adaptive Conformal Prediction (SACP) method that provides region-specific uncertainty estimates. This allows SAC-GT to produce not only precise two-dimensional (2D) location predictions but also statistically valid confidence regions tailored to varying environmental conditions. Extensive evaluations on a large-scale real-world dataset demonstrate that the proposed SAC-GT solution achieves state-of-the-art localization accuracy while delivering robust and spatially adaptive reliability guarantees.

</details>


### [29] [Models Under SCOPE: Scalable and Controllable Routing via Pre-hoc Reasoning](https://arxiv.org/abs/2601.22323)
*Qi Cao,Shuhao Zhang,Ruizhe Zhou,Ruiyi Zhang,Peijia Qin,Pengtao Xie*

Main category: cs.LG

TL;DR: SCOPE是一个可扩展可控的性能预测路由框架，通过强化学习训练，基于相似问题检索预测模型成本和性能，实现动态路由决策，在精度优先时可提升25.7%准确率，在效率优先时可降低95.1%成本。


<details>
  <summary>Details</summary>
Motivation: 现有模型路由方法通常将路由视为固定的小模型集合选择，难以适应新模型或变化的预算约束，缺乏灵活性和可扩展性。

Method: 提出SCOPE框架，使用强化学习训练，通过检索模型在相似问题上的表现来预测成本和性能，而不是依赖固定的模型名称，实现基于推理的预测。

Result: SCOPE不仅能节省成本，还能灵活适应用户需求：在性能优先时可将准确率提升25.7%，在效率优先时可将成本降低95.1%。

Conclusion: SCOPE超越了传统的模型选择方法，通过预测成本和性能将路由转化为动态决策问题，能够适应新模型和变化的预算约束，提供了更好的精度-成本权衡控制。

Abstract: Model routing chooses which language model to use for each query. By sending easy queries to cheaper models and hard queries to stronger ones, it can significantly reduce inference cost while maintaining high accuracy. However, most existing routers treat this as a fixed choice among a small set of models, which makes them hard to adapt to new models or changing budget constraints. In this paper, we propose SCOPE (Scalable and Controllable Outcome Performance Estimator), a routing framework that goes beyond model selection by predicting their cost and performance. Trained with reinforcement learning, SCOPE makes reasoning-based predictions by retrieving how models behave on similar problems, rather than relying on fixed model names, enabling it to work with new, unseen models. Moreover, by explicitly predicting how accurate and how expensive a model will be, it turns routing into a dynamic decision problem, allowing users to easily control the trade-off between accuracy and cost. Experiments show that SCOPE is more than just a cost-saving tool. It flexibly adapts to user needs: it can boost accuracy by up to 25.7% when performance is the priority, or cut costs by up to 95.1% when efficiency matters most.

</details>


### [30] [AgentScore: Autoformulation of Deployable Clinical Scoring Systems](https://arxiv.org/abs/2601.22324)
*Silas Ruhrberg Estévez,Christopher Chiu,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: AgentScore使用LLM生成候选规则，通过验证选择循环创建可部署的临床评分系统，在多个任务中表现优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型虽然预测性能强，但不符合临床工作流程的约束条件（如可记忆性、可审计性、床边执行），导致难以转化为常规临床使用。可部署的临床指南通常采用单位加权临床检查表形式，但学习这种评分系统需要在指数级大的离散规则空间中进行搜索。

Method: AgentScore使用LLM在语义指导下提出候选规则，然后通过确定性的、基于数据的验证和选择循环来确保统计有效性和可部署性约束。该方法在指数级大的离散规则空间中执行语义引导优化。

Result: 在8个临床预测任务中，AgentScore优于现有的评分生成方法，并在更强的结构约束下实现了与更灵活的可解释模型相当的AUC。在另外2个外部验证任务中，AgentScore实现了比已建立的基于指南的评分更高的区分度。

Conclusion: AgentScore通过结合LLM的语义引导和基于数据的验证，能够生成既符合临床工作流程约束又具有良好预测性能的评分系统，填补了机器学习模型与临床部署之间的差距。

Abstract: Modern clinical practice relies on evidence-based guidelines implemented as compact scoring systems composed of a small number of interpretable decision rules. While machine-learning models achieve strong performance, many fail to translate into routine clinical use due to misalignment with workflow constraints such as memorability, auditability, and bedside execution. We argue that this gap arises not from insufficient predictive power, but from optimizing over model classes that are incompatible with guideline deployment. Deployable guidelines often take the form of unit-weighted clinical checklists, formed by thresholding the sum of binary rules, but learning such scores requires searching an exponentially large discrete space of possible rule sets. We introduce AgentScore, which performs semantically guided optimization in this space by using LLMs to propose candidate rules and a deterministic, data-grounded verification-and-selection loop to enforce statistical validity and deployability constraints. Across eight clinical prediction tasks, AgentScore outperforms existing score-generation methods and achieves AUC comparable to more flexible interpretable models despite operating under stronger structural constraints. On two additional externally validated tasks, AgentScore achieves higher discrimination than established guideline-based scores.

</details>


### [31] [Label-Efficient Monitoring of Classification Models via Stratified Importance Sampling](https://arxiv.org/abs/2601.22326)
*Lupo Marsigli,Angel Lopez de Haro*

Main category: cs.LG

TL;DR: 提出基于分层重要性采样（SIS）的模型监控框架，在有限标注预算和低错误率下提高估计效率


<details>
  <summary>Details</summary>
Motivation: 生产环境中分类模型监控面临标注预算严格、批量标注获取困难、错误率极低等挑战，需要高效的监控方法

Method: 采用分层重要性采样（SIS）框架，结合重要性采样和分层随机采样的优势，不需要最优的提议分布或分层定义

Result: 理论分析表明SIS在温和条件下产生无偏估计，有限样本均方误差优于重要性采样和分层随机采样；实验验证了在固定标注预算下的一致效率提升

Conclusion: SIS为部署后模型监控提供了一种原则性、标注高效且操作轻量化的方法论，即使在代理噪声和非最优分层下也能提升估计效率

Abstract: Monitoring the performance of classification models in production is critical yet challenging due to strict labeling budgets, one-shot batch acquisition of labels and extremely low error rates. We propose a general framework based on Stratified Importance Sampling (SIS) that directly addresses these constraints in model monitoring. While SIS has previously been applied in specialized domains, our theoretical analysis establishes its broad applicability to the monitoring of classification models. Under mild conditions, SIS yields unbiased estimators with strict finite-sample mean squared error (MSE) improvements over both importance sampling (IS) and stratified random sampling (SRS). The framework does not rely on optimally defined proposal distributions or strata: even with noisy proxies and sub-optimal stratification, SIS can improve estimator efficiency compared to IS or SRS individually, though extreme proposal mismatch may limit these gains. Experiments across binary and multiclass tasks demonstrate consistent efficiency improvements under fixed label budgets, underscoring SIS as a principled, label-efficient, and operationally lightweight methodology for post-deployment model monitoring.

</details>


### [32] [Molecular Representations in Implicit Functional Space via Hyper-Networks](https://arxiv.org/abs/2601.22327)
*Zehong Wang,Xiaolong Han,Qi Yang,Xiangru Tang,Fang Wu,Xiaoguang Guo,Weixiang Sun,Tianyi Ma,Pietro Lio,Le Cong,Sheng Wang,Chuxu Zhang,Yanfang Ye*

Main category: cs.LG

TL;DR: MolField：将分子视为连续函数而非离散对象的新表示框架，通过超网络学习分子场分布，实现物理一致的分子学习


<details>
  <summary>Details</summary>
Motivation: 现有分子表示方法（序列、图、点云）将分子视为离散对象，忽略了分子本质上是连续、场状的物理实体。需要一种能捕捉分子连续物理本质的表示方法。

Method: MolField框架：1）将每个分子建模为三维空间上的连续函数（分子场）；2）使用超网络学习分子场分布；3）通过结构化权重标记化实现函数空间学习；4）在规范坐标上定义函数以保证SE(3)不变性。

Result: 在分子动力学和性质预测任务上验证，结果表明：将分子视为连续函数从根本上改变了分子表示在不同任务间的泛化方式，下游行为对分子离散化或查询方式具有稳定性。

Conclusion: 分子学习应重新构想为函数空间学习，将分子视为连续函数而非离散对象。MolField框架展示了这种连续函数视角如何改变分子表示的性质，使其对离散化方案具有鲁棒性。

Abstract: Molecular representations fundamentally shape how machine learning systems reason about molecular structure and physical properties. Most existing approaches adopt a discrete pipeline: molecules are encoded as sequences, graphs, or point clouds, mapped to fixed-dimensional embeddings, and then used for task-specific prediction. This paradigm treats molecules as discrete objects, despite their intrinsically continuous and field-like physical nature. We argue that molecular learning can instead be formulated as learning in function space. Specifically, we model each molecule as a continuous function over three-dimensional (3D) space and treat this molecular field as the primary object of representation. From this perspective, conventional molecular representations arise as particular sampling schemes of an underlying continuous object. We instantiate this formulation with MolField, a hyper-network-based framework that learns distributions over molecular fields. To ensure physical consistency, these functions are defined over canonicalized coordinates, yielding invariance to global SE(3) transformations. To enable learning directly over functions, we introduce a structured weight tokenization and train a sequence-based hyper-network to model a shared prior over molecular fields. We evaluate MolField on molecular dynamics and property prediction. Our results show that treating molecules as continuous functions fundamentally changes how molecular representations generalize across tasks and yields downstream behavior that is stable to how molecules are discretized or queried.

</details>


### [33] [Knowledge-Informed Kernel State Reconstruction for Interpretable Dynamical System Discovery](https://arxiv.org/abs/2601.22328)
*Luca Muscarnera,Silas Ruhrberg Estévez,Samuel Holt,Evgeny Saveliev,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: MAAT框架通过知识引导的核状态重建，从噪声、不完整观测数据中恢复物理一致的轨迹和导数，为符号回归提供高质量输入


<details>
  <summary>Details</summary>
Motivation: 现有方法在噪声、部分观测下表现不佳，或依赖黑盒潜在动力学模型而缺乏可解释性，需要一种能结合先验知识、处理异构数据的符号发现框架

Method: 基于知识引导的核状态重建，在再生核希尔伯特空间中进行状态重建，直接融入结构性和语义先验（如非负性、守恒定律、领域特定观测模型），处理异构采样和测量粒度

Result: 在12个多样化科学基准测试和多种噪声机制下，MAAT显著降低了状态估计的MSE，为下游符号回归提供了更准确的轨迹和导数

Conclusion: MAAT提供了一个原则性的接口，将碎片化的传感器数据与符号回归连接起来，通过结合先验知识实现物理一致的状态估计，提升了从数据中发现控制方程的能力

Abstract: Recovering governing equations from data is central to scientific discovery, yet existing methods often break down under noisy, partial observations, or rely on black-box latent dynamics that obscure mechanism. We introduce MAAT (Model Aware Approximation of Trajectories), a framework for symbolic discovery built on knowledge-informed Kernel State Reconstruction. MAAT formulates state reconstruction in a reproducing kernel Hilbert space and directly incorporates structural and semantic priors such as non-negativity, conservation laws, and domain-specific observation models into the reconstruction objective, while accommodating heterogeneous sampling and measurement granularity. This yields smooth, physically consistent state estimates with analytic time derivatives, providing a principled interface between fragmented sensor data and symbolic regression. Across twelve diverse scientific benchmarks and multiple noise regimes, MAAT substantially reduces state-estimation MSE for trajectories and derivatives used by downstream symbolic regression relative to strong baselines.

</details>


### [34] [Scalable Batch Correction for Cell Painting via Batch-Dependent Kernels and Adaptive Sampling](https://arxiv.org/abs/2601.22331)
*Aditya Narayan Ravi,Snehal Vadvalkar,Abhishek Pandey,Ilan Shomorony*

Main category: cs.LG

TL;DR: BALANS是一种用于Cell Painting数据的可扩展批次校正方法，通过构建平滑的亲和矩阵来对齐不同批次的样本，具有近线性时间复杂度和理论保证。


<details>
  <summary>Details</summary>
Motivation: Cell Painting是一种产生丰富细胞形态特征的高通量成像技术，但在大规模应用中受到实验室、仪器和协议差异引起的批次效应影响，这会掩盖生物信号。

Method: BALANS通过两个关键思想构建稀疏亲和矩阵：(1) 使用批次感知的局部尺度计算高斯核亲和度；(2) 采用自适应采样策略，优先考虑邻居覆盖度低的行，并保留每行最强的亲和度，从而获得稀疏但信息丰富的近似矩阵。

Result: 实验证明BALANS能够扩展到大规模数据集，在保持校正质量的同时，相比广泛使用的批次校正方法的原生实现显著提高了运行时间。该方法在样本复杂度方面达到最优阶数，并提供近似保证。

Conclusion: BALANS是一种高效、可扩展的批次校正方法，特别适用于大规模Cell Painting数据分析，能够有效处理批次效应而不牺牲计算效率。

Abstract: Cell Painting is a microscopy-based, high-content imaging assay that produces rich morphological profiles of cells and can support drug discovery by quantifying cellular responses to chemical perturbations. At scale, however, Cell Painting data is strongly affected by batch effects arising from differences in laboratories, instruments, and protocols, which can obscure biological signal. We present BALANS (Batch Alignment via Local Affinities and Subsampling), a scalable batch-correction method that aligns samples across batches by constructing a smoothed affinity matrix from pairwise distances. Given $n$ data points, BALANS builds a sparse affinity matrix $A \in \mathbb{R}^{n \times n}$ using two ideas. (i) For points $i$ and $j$, it sets a local scale using the distance from $i$ to its $k$-th nearest neighbor within the batch of $j$, then computes $A_{ij}$ via a Gaussian kernel calibrated by these batch-aware local scales. (ii) Rather than forming all $n^2$ entries, BALANS uses an adaptive sampling procedure that prioritizes rows with low cumulative neighbor coverage and retains only the strongest affinities per row, yielding a sparse but informative approximation of $A$. We prove that this sampling strategy is order-optimal in sample complexity and provides an approximation guarantee, and we show that BALANS runs in nearly linear time in $n$. Experiments on diverse real-world Cell Painting datasets and controlled large-scale synthetic benchmarks demonstrate that BALANS scales to large collections while improving runtime over native implementations of widely used batch-correction methods, without sacrificing correction quality.

</details>


### [35] [DP-$λ$CGD: Efficient Noise Correlation for Differentially Private Model Training](https://arxiv.org/abs/2601.22334)
*Nikita P. Kalinin,Ryan McKenna,Rasmus Pagh,Christoph H. Lampert*

Main category: cs.LG

TL;DR: 提出一种新的DP-SGD噪声相关策略，仅将噪声与前一次迭代相关并部分抵消，使用伪随机噪声生成器避免存储历史噪声，实现无额外内存开销的改进


<details>
  <summary>Details</summary>
Motivation: 现有DP-SGD扩展方法（如矩阵分解机制）通过跨多个训练迭代引入相关噪声来提高准确性，但需要存储先前添加的噪声向量，导致显著的内存开销

Method: 提出新的噪声相关策略：1）仅将噪声与紧邻的前一次迭代相关；2）抵消受控部分的噪声；3）使用伪随机噪声生成器进行噪声再生，无需存储历史噪声

Result: 方法无需额外内存（与标准DP-SGD相同），计算开销极小，经验证明比DP-SGD具有更高的准确性

Conclusion: 提出了一种内存高效的DP-SGD改进方法，通过受限的噪声相关策略在保持隐私保护的同时提高模型准确性，解决了现有方法的内存开销问题

Abstract: Differentially private stochastic gradient descent (DP-SGD) is the gold standard for training machine learning models with formal differential privacy guarantees. Several recent extensions improve its accuracy by introducing correlated noise across training iterations. Matrix factorization mechanisms are a prominent example, but they correlate noise across many iterations and require storing previously added noise vectors, leading to substantial memory overhead in some settings. In this work, we propose a new noise correlation strategy that correlates noise only with the immediately preceding iteration and cancels a controlled portion of it. Our method relies on noise regeneration using a pseudorandom noise generator, eliminating the need to store past noise. As a result, it requires no additional memory beyond standard DP-SGD. We show that the computational overhead is minimal and empirically demonstrate improved accuracy over DP-SGD.

</details>


### [36] [Knowledge Gradient for Preference Learning](https://arxiv.org/abs/2601.22335)
*Kaiwen Wu,Jacob R. Gardner*

Main category: cs.LG

TL;DR: 论文提出了精确解析的知识梯度方法用于偏好贝叶斯优化，解决了传统方法在成对比较查询场景中的计算难题。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯优化中的知识梯度方法在处理含噪声的黑盒目标函数优化时很流行，但许多实际场景只允许成对比较查询（偏好贝叶斯优化问题），直接函数评估不可用。将知识梯度扩展到偏好贝叶斯优化面临计算挑战，因为前瞻步骤需要计算非高斯后验分布，之前被认为难以处理。

Method: 推导出精确解析的知识梯度用于偏好贝叶斯优化，解决了非高斯后验分布的计算难题，实现了在成对比较查询场景下的有效优化。

Result: 精确知识梯度方法在一系列基准问题上表现强劲，通常优于现有的采集函数。同时，论文还通过案例研究展示了知识梯度在某些场景下的局限性。

Conclusion: 成功解决了知识梯度在偏好贝叶斯优化中的计算挑战，提出了有效的精确解析方法，为只允许成对比较查询的实际优化问题提供了新工具。

Abstract: The knowledge gradient is a popular acquisition function in Bayesian optimization (BO) for optimizing black-box objectives with noisy function evaluations. Many practical settings, however, allow only pairwise comparison queries, yielding a preferential BO problem where direct function evaluations are unavailable. Extending the knowledge gradient to preferential BO is hindered by its computational challenge. At its core, the look-ahead step in the preferential setting requires computing a non-Gaussian posterior, which was previously considered intractable. In this paper, we address this challenge by deriving an exact and analytical knowledge gradient for preferential BO. We show that the exact knowledge gradient performs strongly on a suite of benchmark problems, often outperforming existing acquisition functions. In addition, we also present a case study illustrating the limitation of the knowledge gradient in certain scenarios.

</details>


### [37] [Quantum-Inspired Reinforcement Learning for Secure and Sustainable AIoT-Driven Supply Chain Systems](https://arxiv.org/abs/2601.22339)
*Muhammad Bilal Akram Dastagir,Omer Tariq,Shahid Mumtaz,Saif Al-Kuwari,Ahmed Farouk*

Main category: cs.LG

TL;DR: 该论文提出了一种量子启发强化学习框架，用于同时优化供应链的可持续性（碳足迹）和安全性，结合AIoT技术解决现代全球供应链中的环境与安全挑战。


<details>
  <summary>Details</summary>
Motivation: 现代供应链需要在高速物流与环境保护、安全约束之间取得平衡。传统供应链优化模型往往忽视可持续性目标和网络安全漏洞，使系统容易受到生态损害和恶意攻击。

Method: 设计量子启发强化学习框架，将可控自旋链类比与实时AIoT信号结合，优化包含保真度、安全性和碳成本的多目标奖励函数。采用基于价值和集成更新的稳定训练方法，并使用窗口归一化奖励组件确保比例协调。

Result: 在模拟中，该方法表现出平滑收敛、后期强性能，并在代表性噪声通道下具有优雅的降级能力，优于标准的基于学习和模型的参考方法，展示了其对实时可持续性和风险需求的稳健处理能力。

Conclusion: 研究结果强化了量子启发AIoT框架在推动大规模安全、环保供应链运营方面的潜力，为全球互联基础设施负责任地满足消费者和环境需求奠定了基础。

Abstract: Modern supply chains must balance high-speed logistics with environmental impact and security constraints, prompting a surge of interest in AI-enabled Internet of Things (AIoT) solutions for global commerce. However, conventional supply chain optimization models often overlook crucial sustainability goals and cyber vulnerabilities, leaving systems susceptible to both ecological harm and malicious attacks. To tackle these challenges simultaneously, this work integrates a quantum-inspired reinforcement learning framework that unifies carbon footprint reduction, inventory management, and cryptographic-like security measures. We design a quantum-inspired reinforcement learning framework that couples a controllable spin-chain analogy with real-time AIoT signals and optimizes a multi-objective reward unifying fidelity, security, and carbon costs. The approach learns robust policies with stabilized training via value-based and ensemble updates, supported by window-normalized reward components to ensure commensurate scaling. In simulation, the method exhibits smooth convergence, strong late-episode performance, and graceful degradation under representative noise channels, outperforming standard learned and model-based references, highlighting its robust handling of real-time sustainability and risk demands. These findings reinforce the potential for quantum-inspired AIoT frameworks to drive secure, eco-conscious supply chain operations at scale, laying the groundwork for globally connected infrastructures that responsibly meet both consumer and environmental needs.

</details>


### [38] [Failing to Explore: Language Models on Interactive Tasks](https://arxiv.org/abs/2601.22345)
*Mahdi JafariRaviz,Keivan Rezaei,Arshia Soltani Moakhar,Zahra Sodagar,Yize Cheng,Soheil Feizi*

Main category: cs.LG

TL;DR: 评估语言模型在有限交互预算下探索交互环境的能力，发现现有模型存在系统性探索不足和次优解问题，性能甚至不如简单的探索-利用启发式基线方法。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型在交互环境中的探索能力，特别是在有限交互预算下的表现。当前缺乏对语言模型探索能力的系统评估，需要了解模型是否能有效探索环境并找到最优解。

Method: 引入三个参数化任务，控制探索难度，涵盖连续和离散环境。评估最先进的语言模型，与简单的探索-利用启发式基线进行比较。研究两种轻量级干预措施：将固定预算分配到并行执行中，以及定期总结交互历史。

Result: 发现最先进的语言模型存在系统性探索不足和次优解问题，性能通常显著差于简单的探索-利用启发式基线，且随着预算增加性能提升有限。并行执行意外地提高了性能，定期总结交互历史能保留关键发现并进一步改善探索。

Conclusion: 语言模型在交互环境探索方面存在显著局限性，需要改进探索策略。轻量级干预措施如并行执行和定期总结能有效提升性能，为改进语言模型的探索能力提供了方向。

Abstract: We evaluate language models on their ability to explore interactive environments under a limited interaction budget. We introduce three parametric tasks with controllable exploration difficulty, spanning continuous and discrete environments. Across state-of-the-art models, we find systematic under-exploration and suboptimal solutions, with performance often significantly worse than simple explore--exploit heuristic baselines and scaling weakly as the budget increases. Finally, we study two lightweight interventions: splitting a fixed budget into parallel executions, which surprisingly improves performance despite a no-gain theoretical result for our tasks, and periodically summarizing the interaction history, which preserves key discoveries and further improves exploration.

</details>


### [39] [MixQuant: Pushing the Limits of Block Rotations in Post-Training Quantization](https://arxiv.org/abs/2601.22347)
*Sai Sanjeet,Ian Colbert,Pablo Monteagudo-Lago,Giuseppe Franco,Yaman Umuroglu,Nicholas J. Fraser*

Main category: cs.LG

TL;DR: MixQuant：一种基于块旋转感知的后训练量化框架，通过排列重分布激活质量来改善异常值抑制，在块大小16时恢复90%全向量旋转困惑度


<details>
  <summary>Details</summary>
Motivation: 现有后训练量化方法使用块旋转来扩散异常值，但块结构对异常值抑制的影响机制尚不明确，需要系统分析块Hadamard旋转的异常值抑制效果

Method: 1. 系统分析块Hadamard旋转的异常值抑制机制；2. 提出MixQuant框架，通过排列重分布激活质量；3. 设计贪心质量扩散算法均衡块间ℓ₁范数；4. 识别Transformer中的排列等变区域，将排列合并到模型权重中

Result: MixQuant在所有块大小下都提高了精度，在Llama3 1B INT4量化中，块大小16时恢复90%全向量旋转困惑度（相比无排列的46%）

Conclusion: 异常值抑制受输入向量几何结构限制，当ℓ₁范数质量均匀分布在块间时效果最佳；MixQuant通过排列重分布激活质量有效改善量化性能，且不增加推理开销

Abstract: Recent post-training quantization (PTQ) methods have adopted block rotations to diffuse outliers prior to rounding. While this reduces the overhead of full-vector rotations, the effect of block structure on outlier suppression remains poorly understood. To fill this gap, we present the first systematic, non-asymptotic analysis of outlier suppression for block Hadamard rotations. Our analysis reveals that outlier suppression is fundamentally limited by the geometry of the input vector. In particular, post-rotation outliers are deterministically minimized when the pre-rotation $\ell_1$ norm mass is evenly distributed across blocks. Guided by these insights, we introduce MixQuant, a block rotation-aware PTQ framework that redistributes activation mass via permutations prior to rotation. We propose a greedy mass diffusion algorithm to calibrate permutations by equalizing the expected blockwise $\ell_1$ norms. To avoid adding inference overhead, we identify permutation-equivariant regions in transformer architectures to merge the resulting permutations into model weights before deployment. Experiments show that MixQuant consistently improves accuracy across all block sizes, recovering up to 90% of the full-vector rotation perplexity when quantizing Llama3 1B to INT4 with block size 16, compared to 46% without permutations.

</details>


### [40] [Learning Policy Representations for Steerable Behavior Synthesis](https://arxiv.org/abs/2601.22350)
*Beiming Li,Sergio Rozada,Alejandro Ribeiro*

Main category: cs.LG

TL;DR: 提出一种基于占用度量的策略表示学习方法，通过集合架构统一近似策略范围，支持在潜在空间中进行梯度优化，实现未见过的价值函数约束下的行为合成


<details>
  <summary>Details</summary>
Motivation: 为了在测试时方便地进行行为引导，需要学习一系列策略的表示。由于MDP中的策略由其占用度量唯一确定，因此需要开发能够统一表示策略范围的方法

Method: 将策略表示建模为状态-动作特征映射相对于占用度量的期望，使用集合架构统一近似策略范围。采用变分生成方法构建平滑潜在空间，并通过对比学习使潜在距离与价值函数差异对齐

Result: 模型能够从状态-动作样本集合编码到潜在嵌入，从中解码策略及其对应的多个奖励价值函数。潜在空间的几何特性支持基于梯度的优化

Conclusion: 该方法实现了新颖的行为合成任务，能够在无需额外训练的情况下，引导策略满足先前未见过的价值函数约束

Abstract: Given a Markov decision process (MDP), we seek to learn representations for a range of policies to facilitate behavior steering at test time. As policies of an MDP are uniquely determined by their occupancy measures, we propose modeling policy representations as expectations of state-action feature maps with respect to occupancy measures. We show that these representations can be approximated uniformly for a range of policies using a set-based architecture. Our model encodes a set of state-action samples into a latent embedding, from which we decode both the policy and its value functions corresponding to multiple rewards. We use variational generative approach to induce a smooth latent space, and further shape it with contrastive learning so that latent distances align with differences in value functions. This geometry permits gradient-based optimization directly in the latent space. Leveraging this capability, we solve a novel behavior synthesis task, where policies are steered to satisfy previously unseen value function constraints without additional training.

</details>


### [41] [Recoverability Has a Law: The ERR Measure for Tool-Augmented Agents](https://arxiv.org/abs/2601.22352)
*Sri Vatsa Vuddanti,Satwik Kumar Chittiprolu*

Main category: cs.LG

TL;DR: 语言模型代理在工具调用失败后表现出可恢复性，本文提出了一个可测量的恢复定律，通过期望恢复遗憾（ERR）和效率分数（ES）的定量关系来解释这一现象。


<details>
  <summary>Details</summary>
Motivation: 语言模型代理在工具调用执行失败后似乎能够自我恢复，但这种行为缺乏正式的理论解释。本文旨在填补这一空白，为执行层面的鲁棒性提供理论基础。

Method: 通过形式化期望恢复遗憾（ERR）来量化恢复策略与最优策略的偏差，并推导ERR与经验可观测的效率分数（ES）之间的一阶关系，从而建立恢复动力学的定量定律。

Result: 在五个工具使用基准测试（包括受控扰动、诊断推理和真实API）中验证了ERR-ES定律。在不同模型规模、扰动机制和恢复时间范围内，预测的遗憾与实际观察到的后失败遗憾高度匹配（Δ≤0.05）。

Conclusion: 可恢复性不是模型规模或架构的产物，而是交互动力学的受控属性。这为语言代理的执行层面鲁棒性提供了理论基础，揭示了恢复行为遵循可测量的定量定律。

Abstract: Language model agents often appear capable of self-recovery after failing tool call executions, yet this behavior lacks a formal explanation. We present a predictive theory that resolves this gap by showing that recoverability follows a measurable law. To elaborate, we formalize recoverability through Expected Recovery Regret (ERR), which quantifies the deviation of a recovery policy from the optimal one under stochastic execution noise, and derive a first-order relationship between ERR and an empirical observable quantity, the Efficiency Score (ES). This yields a falsifiable first-order quantitative law of recovery dynamics in tool-using agents. We empirically validate the law across five tool-use benchmarks spanning controlled perturbations, diagnostic reasoning, and real-world APIs. Across model scales, perturbation regimes, and recovery horizons, predicted regret under the ERR-ES law closely matched observed post-failure regret measured from Monte Carlo rollouts, within delta less than or equal to 0.05. Our results reveal that recoverability is not an artifact of model scale or architecture, but a governed property of interaction dynamics, providing a theoretical foundation for execution-level robustness in language agents.

</details>


### [42] [Relative Wasserstein Angle and the Problem of the $W_2$-Nearest Gaussian Distribution](https://arxiv.org/abs/2601.22355)
*Binshuai Wang,Peng Wei*

Main category: cs.LG

TL;DR: 论文提出基于最优传输框架的两种几何量度——相对Wasserstein角和正交投影距离，用于量化经验分布与高斯分布之间的偏差，并证明在Wasserstein空间中高斯近似可视为投影问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法量化经验分布与高斯分布偏差的能力有限，特别是常用的矩匹配高斯方法在Wasserstein距离意义上可能不是最优的高斯近似。需要更严谨的几何框架来定义非高斯性的度量。

Method: 利用相对平移不变二次Wasserstein空间的锥几何结构，引入相对Wasserstein角和正交投影距离两个几何量。在一维情况下推导闭式解，在高维情况下开发基于半离散对偶公式的随机流形优化算法。

Result: 证明了该空间中任意两条射线生成的填充锥是平坦的，确保角度、投影和内积严格定义。实验表明相对Wasserstein角比Wasserstein距离更鲁棒，提出的最近高斯近似在FID评分评估中优于矩匹配方法。

Conclusion: 该研究为量化非高斯性提供了严谨的几何框架，揭示了高斯近似本质上是一个投影问题，并提供了比传统矩匹配更优的高斯近似方法，在分布评估中具有实际应用价值。

Abstract: We study the problem of quantifying how far an empirical distribution deviates from Gaussianity under the framework of optimal transport. By exploiting the cone geometry of the relative translation invariant quadratic Wasserstein space, we introduce two novel geometric quantities, the relative Wasserstein angle and the orthogonal projection distance, which provide meaningful measures of non-Gaussianity. We prove that the filling cone generated by any two rays in this space is flat, ensuring that angles, projections, and inner products are rigorously well-defined. This geometric viewpoint recasts Gaussian approximation as a projection problem onto the Gaussian cone and reveals that the commonly used moment-matching Gaussian can \emph{not} be the \(W_2\)-nearest Gaussian for a given empirical distribution. In one dimension, we derive closed-form expressions for the proposed quantities and extend them to several classical distribution families, including uniform, Laplace, and logistic distributions; while in high dimensions, we develop an efficient stochastic manifold optimization algorithm based on a semi-discrete dual formulation. Experiments on synthetic data and real-world feature distributions demonstrate that the relative Wasserstein angle is more robust than the Wasserstein distance and that the proposed nearest Gaussian provides a better approximation than moment matching in the evaluation of Fréchet Inception Distance (FID) scores.

</details>


### [43] [PoSafeNet: Safe Learning with Poset-Structured Neural Nets](https://arxiv.org/abs/2601.22356)
*Kiwan Wong,Wei Xiao,Daniela Rus*

Main category: cs.LG

TL;DR: PoSafeNet：一种基于偏序集结构安全约束的神经安全层，通过顺序闭式投影自适应执行安全策略，提高机器人控制中的可行性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有安全学习方法通常统一或按固定优先级处理多个安全约束，导致不可行性和脆弱行为。实际中安全需求是异质的，只有部分优先级关系，有些约束可比而有些不可比。

Method: 将安全约束形式化为偏序集结构，提出PoSafeNet神经安全层，通过偏序一致约束排序下的顺序闭式投影来强制执行安全，自适应选择或混合有效安全执行，同时保持优先级语义。

Result: 在多障碍物导航、受限机器人操作和视觉自动驾驶实验中，相比非结构化和基于可微二次规划的安全层，表现出更好的可行性、鲁棒性和可扩展性。

Conclusion: 偏序集结构安全建模为异质安全约束提供了更灵活的框架，PoSafeNet通过自适应安全执行机制显著提升了学习型控制器在安全关键系统中的性能。

Abstract: Safe learning is essential for deploying learningbased controllers in safety-critical robotic systems, yet existing approaches often enforce multiple safety constraints uniformly or via fixed priority orders, leading to infeasibility and brittle behavior. In practice, safety requirements are heterogeneous and admit only partial priority relations, where some constraints are comparable while others are inherently incomparable. We formalize this setting as poset-structured safety, modeling safety constraints as a partially ordered set and treating safety composition as a structural property of the policy class. Building on this formulation, we propose PoSafeNet, a differentiable neural safety layer that enforces safety via sequential closed-form projection under poset-consistent constraint orderings, enabling adaptive selection or mixing of valid safety executions while preserving priority semantics by construction. Experiments on multi-obstacle navigation, constrained robot manipulation, and vision-based autonomous driving demonstrate improved feasibility, robustness, and scalability over unstructured and differentiable quadratic program-based safety layers.

</details>


### [44] [Small Talk, Big Impact: The Energy Cost of Thanking AI](https://arxiv.org/abs/2601.22357)
*Julien Delavande,Regis Pierrard,Sasha Luccioni*

Main category: cs.LG

TL;DR: 量化分析LLM交互中礼貌用语（如"谢谢"）的能耗成本，揭示输入长度、输出长度和模型大小对能耗的影响，为构建可持续AI应用提供指导。


<details>
  <summary>Details</summary>
Motivation: 随着LLM用户采用率增长和每日数十亿提示的处理，理解看似无害的礼貌信息（如"谢谢"）的能耗成本变得至关重要，这不仅关乎效率，更关系到可持续AI部署。

Method: 使用真实世界对话轨迹和细粒度能耗测量，量化输入长度、输出长度和模型大小对能耗的影响，以礼貌用语作为可控且可复现的代理变量。

Result: 研究发现礼貌用语确实产生可量化的能耗成本，输入长度、输出长度和模型大小显著影响能耗，为构建更可持续和高效的LLM应用提供了可操作的见解。

Conclusion: 礼貌用语并非"免费"，在LLM交互中会产生可测量的能耗成本，理解并减轻这种成本对于可持续AI部署至关重要，特别是在聊天等日益普及的真实世界场景中。

Abstract: Being polite is free - or is it? In this paper, we quantify the energy cost of seemingly innocuous messages such as ``thank you'' when interacting with large language models, often used by users to convey politeness. Using real-world conversation traces and fine-grained energy measurements, we quantify how input length, output length and model size affect energy use. While politeness is our motivating example, it also serves as a controlled and reproducible proxy for measuring the energy footprint of a typical LLM interaction. Our findings provide actionable insights for building more sustainable and efficient LLM applications, especially in increasingly widespread real-world contexts like chat. As user adoption grows and billions of prompts are processed daily, understanding and mitigating this cost becomes crucial - not just for efficiency, but for sustainable AI deployment.

</details>


### [45] [The Unseen Threat: Residual Knowledge in Machine Unlearning under Perturbed Samples](https://arxiv.org/abs/2601.22359)
*Hsiang Hsu,Pradeep Niroula,Zichang He,Ivan Brugere,Freddy Lecue,Chun-Fu Chen*

Main category: cs.LG

TL;DR: 论文提出机器遗忘中存在"残余知识"漏洞：即使模型已遗忘特定样本，其对抗扰动版本仍能被识别，揭示了新的隐私风险。作者提出RURK微调策略来缓解此问题。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法通过统计不可区分性提供保证，但这些保证不能自然扩展到对抗扰动输入时的模型输出。具体来说，遗忘样本的轻微扰动仍可能被遗忘模型正确识别，而重新训练的模型却无法识别，这揭示了一种新的隐私风险：遗忘样本的信息可能在其局部邻域中持续存在。

Method: 提出RURK（Residual Unlearning with Robust Knowledge）微调策略，通过惩罚模型重新识别扰动遗忘样本的能力来缓解残余知识风险。该方法旨在防止模型在遗忘样本的对抗扰动版本上保持识别能力。

Result: 在视觉基准测试和深度神经网络上的实验表明：1）残余知识在现有遗忘方法中普遍存在；2）提出的RURK方法能有效防止残余知识，提高遗忘的鲁棒性。

Conclusion: 残余知识是机器遗忘中一个不可避免的高维漏洞，需要专门的方法来缓解。提出的RURK策略能有效防止这一隐私风险，为机器遗忘提供了更强的安全保障。

Abstract: Machine unlearning offers a practical alternative to avoid full model re-training by approximately removing the influence of specific user data. While existing methods certify unlearning via statistical indistinguishability from re-trained models, these guarantees do not naturally extend to model outputs when inputs are adversarially perturbed. In particular, slight perturbations of forget samples may still be correctly recognized by the unlearned model - even when a re-trained model fails to do so - revealing a novel privacy risk: information about the forget samples may persist in their local neighborhood. In this work, we formalize this vulnerability as residual knowledge and show that it is inevitable in high-dimensional settings. To mitigate this risk, we propose a fine-tuning strategy, named RURK, that penalizes the model's ability to re-recognize perturbed forget samples. Experiments on vision benchmarks with deep neural networks demonstrate that residual knowledge is prevalent across existing unlearning methods and that our approach effectively prevents residual knowledge.

</details>


### [46] [Understanding Efficiency: Quantization, Batching, and Serving Strategies in LLM Energy Use](https://arxiv.org/abs/2601.22362)
*Julien Delavande,Regis Pierrard,Sasha Luccioni*

Main category: cs.LG

TL;DR: 论文研究了LLM推理中的系统级设计选择（如数值精度、批处理策略、请求调度）对能耗的巨大影响，发现这些因素可导致能耗数量级差异，并提出了更可持续的LLM部署方法。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在生产环境中的部署增加，计算资源和能源需求从训练转向推理。现有研究主要关注每个提示或每个token的能耗，但系统级设计选择对能耗的影响尚未得到充分研究。

Method: 在NVIDIA H100 GPU上进行详细的LLM推理能耗和延迟实证研究，分析量化、批处理大小和服务配置（如Hugging Face的Text Generation Inference服务器）的影响。

Result: 低精度格式仅在计算受限时节能；批处理提高能效，特别是在解码等内存受限阶段；结构化请求时序可将每个请求能耗降低高达100倍。

Conclusion: 可持续的LLM部署不仅取决于模型内部，还取决于服务堆栈的编排。研究结果支持基于阶段的能耗分析和系统级优化，以实现更环保的AI服务。

Abstract: Large Language Models (LLMs) are increasingly deployed in production, contributing towards shifting the burden in terms of computational resources and energy demands from training to inference. While prior work has examined the energy cost of inference per prompt or per token, we highlight how \emph{system-level design choices} - such as numerical precision, batching strategy, and request scheduling - can lead to orders-of-magnitude differences in energy consumption for the same model. We perform a detailed empirical study of LLM inference energy and latency on NVIDIA H100 GPUs, analyzing the impact of quantization, batch size, and serving configuration (e.g., with Hugging Face's Text Generation Inference server). Our results reveal that lower-precision formats only yield energy gains in compute-bound regimes; that batching improves energy efficiency, especially in memory-bound phases like decoding; and that structured request timing (arrival shaping) can reduce per-request energy by up to 100 times. We argue that sustainable LLM deployment depends not only on model internals, but also on the orchestration of the serving stack. Our findings motivate phase-aware energy profiling and system-level optimizations for greener AI services.

</details>


### [47] [FIRE: Multi-fidelity Regression with Distribution-conditioned In-context Learning using Tabular Foundation Models](https://arxiv.org/abs/2601.22371)
*Rosen Ting-Ying Yu,Nicholas Sung,Faez Ahmed*

Main category: cs.LG

TL;DR: FIRE是一个免训练的多保真度回归框架，使用表格基础模型进行零样本上下文贝叶斯推理，通过高保真度校正模型处理低保真度后验预测分布，在31个基准问题上优于7种现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统高斯过程多保真度回归存在立方计算复杂度高、容易对稀疏的高保真度观测过拟合的问题，限制了在实际应用中的效率和泛化能力。

Method: FIRE框架耦合表格基础模型，通过低保真度模型的后验预测分布来条件化高保真度校正模型，实现跨保真度的分布摘要信息传递，支持异方差误差捕获和鲁棒残差学习，无需模型重新训练。

Result: 在31个基准问题（包括合成和真实世界任务如DrivAerNet、LCBench）上，FIRE在性能-时间权衡方面优于7种最先进的GP或深度学习多保真度回归方法，在准确性和不确定性量化方面排名最高，具有运行时优势。

Conclusion: FIRE提供了一个有效的免训练多保真度回归框架，但存在上下文窗口限制和依赖预训练表格基础模型质量的局限性。

Abstract: Multi-fidelity (MF) regression often operates in regimes of extreme data imbalance, where the commonly-used Gaussian-process (GP) surrogates struggle with cubic scaling costs and overfit to sparse high-fidelity observations, limiting efficiency and generalization in real-world applications. We introduce FIRE, a training-free MF framework that couples tabular foundation models (TFMs) to perform zero-shot in-context Bayesian inference via a high-fidelity correction model conditioned on the low-fidelity model's posterior predictive distributions. This cross-fidelity information transfer via distributional summaries captures heteroscedastic errors, enabling robust residual learning without model retraining. Across 31 benchmark problems spanning synthetic and real-world tasks (e.g., DrivAerNet, LCBench), FIRE delivers a stronger performance-time trade-off than seven state-of-the-art GP-based or deep learning MF regression methods, ranking highest in accuracy and uncertainty quantification with runtime advantages. Limitations include context window constraints and dependence on the quality of the pre-trained TFM's.

</details>


### [48] [Purely Agentic Black-Box Optimization for Biological Design](https://arxiv.org/abs/2601.22382)
*Natalie Maus,Yimeng Zeng,Haydn Thomas Jones,Yining Huang,Gaurav Ng Goel,Alden Rose,Kyurae Kim,Hyun-Su Lee,Marcelo Der Torossian Torres,Fangping Wan,Cesar de la Fuente-Nunez,Mark Yatskar,Osbert Bastani,Jacob R. Gardner*

Main category: cs.LG

TL;DR: PABLO：一种完全基于语言代理的生物学黑盒优化系统，利用科学文献预训练的LLM生成和迭代优化生物候选物，在分子设计和抗菌肽优化任务中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有生物学设计方法主要依赖原始结构数据，难以充分利用丰富的科学文献知识。虽然LLM已被引入这些流程，但仅限于结构中心优化器中的狭窄角色，需要更全面的语言驱动优化方法。

Method: PABLO（Purely Agentic BLack-box Optimization）是一种分层代理系统，使用在化学和生物学文献上预训练的科学LLM，通过完全基于语言的推理过程生成和迭代优化生物候选物。

Result: 在GuacaMol分子设计和抗菌肽优化任务中，PABLO实现了最先进的性能，显著提高了样本效率和最终目标值。体外验证显示PABLO优化的肽对耐药病原体具有强活性。

Conclusion: PABLO展示了完全代理化、语言驱动的生物学黑盒优化的有效性，不仅性能优越，还能自然地整合语义任务描述、检索增强的领域知识和复杂约束，具有实际治疗发现的潜力。

Abstract: Many key challenges in biological design-such as small-molecule drug discovery, antimicrobial peptide development, and protein engineering-can be framed as black-box optimization over vast, complex structured spaces. Existing methods rely mainly on raw structural data and struggle to exploit the rich scientific literature. While large language models (LLMs) have been added to these pipelines, they have been confined to narrow roles within structure-centered optimizers. We instead cast biological black-box optimization as a fully agentic, language-based reasoning process. We introduce Purely Agentic BLack-box Optimization (PABLO), a hierarchical agentic system that uses scientific LLMs pretrained on chemistry and biology literature to generate and iteratively refine biological candidates. On both the standard GuacaMol molecular design and antimicrobial peptide optimization tasks, PABLO achieves state-of-the-art performance, substantially improving sample efficiency and final objective values over established baselines. Compared to prior optimization methods that incorporate LLMs, PABLO achieves competitive token usage per run despite relying on LLMs throughout the optimization loop. Beyond raw performance, the agentic formulation offers key advantages for realistic design: it naturally incorporates semantic task descriptions, retrieval-augmented domain knowledge, and complex constraints. In follow-up in vitro validation, PABLO-optimized peptides showed strong activity against drug-resistant pathogens, underscoring the practical potential of PABLO for therapeutic discovery.

</details>


### [49] [Graph is a Substrate Across Data Modalities](https://arxiv.org/abs/2601.22384)
*Ziming Li,Xiaoming Wu,Zehong Wang,Jiazheng Li,Yijun Tian,Jinhe Bi,Yunpu Ma,Yanfang Ye,Chuxu Zhang*

Main category: cs.LG

TL;DR: G-Substrate框架将图结构作为跨模态和任务共享的结构基板，通过统一结构模式和角色交替训练机制，实现图表示在不同学习场景中的持续积累和复用。


<details>
  <summary>Details</summary>
Motivation: 当前图表示学习通常是模态和任务隔离的，每个任务单独构建图表示然后丢弃，导致跨模态和任务的结构规律需要重复重建，无法在中间图表示层面积累。

Method: 提出G-Substrate框架：1）统一结构模式确保异构模态和任务间图表示的兼容性；2）角色交替训练策略，让同一图结构在学习过程中承担多个功能角色。

Result: 在多个领域、模态和任务上的实验表明，G-Substrate优于任务隔离方法和朴素的多任务学习方法。

Conclusion: 通过将图结构视为跨学习场景持久存在的结构基板，G-Substrate能够积累和复用结构知识，提高学习效率和性能。

Abstract: Graphs provide a natural representation of relational structure that arises across diverse domains. Despite this ubiquity, graph structure is typically learned in a modality- and task-isolated manner, where graph representations are constructed within individual task contexts and discarded thereafter. As a result, structural regularities across modalities and tasks are repeatedly reconstructed rather than accumulated at the level of intermediate graph representations. This motivates a representation-learning question: how should graph structure be organized so that it can persist and accumulate across heterogeneous modalities and tasks? We adopt a representation-centric perspective in which graph structure is treated as a structural substrate that persists across learning contexts. To instantiate this perspective, we propose G-Substrate, a graph substrate framework that organizes learning around shared graph structures. G-Substrate comprises two complementary mechanisms: a unified structural schema that ensures compatibility among graph representations across heterogeneous modalities and tasks, and an interleaved role-based training strategy that exposes the same graph structure to multiple functional roles during learning. Experiments across multiple domains, modalities, and tasks show that G-Substrate outperforms task-isolated and naive multi-task learning methods.

</details>


### [50] [SAIR: Cost-Efficient Multi-Stage ML Pipeline Autoscaling via In-Context Reinforcement Learning](https://arxiv.org/abs/2601.22397)
*Jianchang Su,Yifan Zhang,Shengkai Lin,Shizhen Zhao,Yusheng Zheng,Yiwei Yang,Wei Zhang*

Main category: cs.LG

TL;DR: SAIR：一个使用LLM作为上下文强化学习控制器的自动扩缩框架，用于多阶段ML推理管道，无需梯度更新即可在线改进策略，显著降低延迟和资源成本。


<details>
  <summary>Details</summary>
Motivation: 多阶段ML推理管道难以自动扩缩，因为存在异构资源、跨阶段耦合和动态瓶颈迁移等挑战。现有解决方案难以有效处理这些复杂问题。

Method: 1. 使用LLM作为上下文强化学习控制器，从奖励标记的交互历史中在线学习策略；2. 结合帕累托优势奖励塑造与可证明的分离边界；3. 使用惊奇引导的经验检索提高上下文效率；4. 通过用户空间CUDA拦截实现细粒度GPU速率控制。

Result: 在四种ML服务管道和三种工作负载模式下，SAIR实现了最佳或并列最佳的P99延迟和有效资源成本，P99延迟最多降低50%，有效成本最多减少97%，瓶颈检测准确率达到86%，且无需离线训练。

Conclusion: SAIR通过LLM作为上下文强化学习控制器，有效解决了多阶段ML推理管道的自动扩缩问题，在延迟、成本和准确性方面显著优于现有基线方法。

Abstract: Multi-stage ML inference pipelines are difficult to autoscale due to heterogeneous resources, cross-stage coupling, and dynamic bottleneck migration. We present SAIR, an autoscaling framework that uses an LLM as an in-context reinforcement learning controller, improving its policy online from reward-labeled interaction histories without gradient updates. SAIR combines Pareto-dominance reward shaping with a provable separation margin, surprisal-guided experience retrieval for context efficiency, and fine-grained GPU rate control via user-space CUDA interception. We provide regret analysis decomposing error into retrieval coverage and LLM selection components. On four ML serving pipelines under three workload patterns, SAIR achieves the best or tied-best P99 latency and effective resource cost among deployed baselines, improving P99 by up to 50% and reducing effective cost by up to 97% (under GPU rate-control assumptions), with 86% bottleneck detection accuracy and no offline training.

</details>


### [51] [Score-based Integrated Gradient for Root Cause Explanations of Outliers](https://arxiv.org/abs/2601.22399)
*Phuoc Nguyen,Truyen Tran,Sunil Gupta,Svetha Venkatesh*

Main category: cs.LG

TL;DR: SIREN是一种新颖的可扩展方法，通过估计数据似然性的得分函数来识别异常值的根本原因，使用积分梯度沿异常值到正常数据分布的路径累积得分贡献。


<details>
  <summary>Details</summary>
Motivation: 传统基于启发式或反事实推理的方法在不确定性和高维依赖下难以有效识别异常值的根本原因，需要一种更稳健和可扩展的方法。

Method: SIREN通过估计数据似然性的得分函数，使用积分梯度沿从异常值到正常数据分布的路径累积得分贡献来计算归因，满足三个Shapley值公理和一个因果结构导出的非对称公理。

Result: 在合成随机图和真实世界云服务及供应链数据集上的实验表明，SIREN在归因准确性和计算效率方面均优于最先进的基线方法。

Conclusion: SIREN提供了一种在非线性、高维和异方差因果模型中实现可处理且具有不确定性感知的根本原因归因的新方法，优于现有方法。

Abstract: Identifying the root causes of outliers is a fundamental problem in causal inference and anomaly detection. Traditional approaches based on heuristics or counterfactual reasoning often struggle under uncertainty and high-dimensional dependencies. We introduce SIREN, a novel and scalable method that attributes the root causes of outliers by estimating the score functions of the data likelihood. Attribution is computed via integrated gradients that accumulate score contributions along paths from the outlier toward the normal data distribution. Our method satisfies three of the four classic Shapley value axioms - dummy, efficiency, and linearity - as well as an asymmetry axiom derived from the underlying causal structure. Unlike prior work, SIREN operates directly on the score function, enabling tractable and uncertainty-aware root cause attribution in nonlinear, high-dimensional, and heteroscedastic causal models. Extensive experiments on synthetic random graphs and real-world cloud service and supply chain datasets show that SIREN outperforms state-of-the-art baselines in both attribution accuracy and computational efficiency.

</details>


### [52] [Optimization, Generalization and Differential Privacy Bounds for Gradient Descent on Kolmogorov-Arnold Networks](https://arxiv.org/abs/2601.22409)
*Puyu Wang,Junyu Zhou,Philipp Liznerski,Marius Kloft*

Main category: cs.LG

TL;DR: 本文分析两层KANs的梯度下降训练，推导出训练动态、泛化和差分隐私的理论边界，证明在NTK可分离假设下，多项式对数宽度足以实现优化和泛化，并揭示隐私训练中宽度必要性的新现象。


<details>
  <summary>Details</summary>
Motivation: Kolmogorov-Arnold Networks (KANs) 作为MLPs的结构化替代方案出现，但缺乏对其训练动态、泛化和隐私特性的理论分析。本文旨在填补这一空白，为两层KANs的梯度下降训练提供理论保证。

Method: 分析两层KANs的梯度下降训练，推导一般边界。在NTK可分离假设下，针对逻辑损失进行具体分析，研究多项式对数宽度下的优化和泛化性能。在差分隐私设置中，分析所需噪声量并推导效用边界。

Result: 证明在NTK可分离假设下，多项式对数宽度足以实现O(1/T)的优化速率和O(1/n)的泛化速率。在隐私设置中，获得O(√d/(nε))的效用边界，匹配经典下界。发现多项式对数宽度在隐私训练中不仅是充分的，也是必要的。

Conclusion: 本文为KANs训练提供了首个全面的理论分析框架，揭示了非私有训练（宽度充分性）与私有训练（宽度必要性）之间的定性差异，这些理论见解可指导实际选择如网络宽度和早停策略。

Abstract: Kolmogorov--Arnold Networks (KANs) have recently emerged as a structured alternative to standard MLPs, yet a principled theory for their training dynamics, generalization, and privacy properties remains limited. In this paper, we analyze gradient descent (GD) for training two-layer KANs and derive general bounds that characterize their training dynamics, generalization, and utility under differential privacy (DP). As a concrete instantiation, we specialize our analysis to logistic loss under an NTK-separable assumption, where we show that polylogarithmic network width suffices for GD to achieve an optimization rate of order $1/T$ and a generalization rate of order $1/n$, with $T$ denoting the number of GD iterations and $n$ the sample size. In the private setting, we characterize the noise required for $(ε,δ)$-DP and obtain a utility bound of order $\sqrt{d}/(nε)$ (with $d$ the input dimension), matching the classical lower bound for general convex Lipschitz problems. Our results imply that polylogarithmic width is not only sufficient but also necessary under differential privacy, revealing a qualitative gap between non-private (sufficiency only) and private (necessity also emerges) training regimes. Experiments further illustrate how these theoretical insights can guide practical choices, including network width selection and early stopping.

</details>


### [53] [MM-OpenFGL: A Comprehensive Benchmark for Multimodal Federated Graph Learning](https://arxiv.org/abs/2601.22416)
*Xunkai Li,Yuming Ai,Yinlin Zhu,Haodong Lu,Yi Zhang,Guohao Fu,Bowen Fan,Qiangqiang Dai,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: MM-OpenFGL：首个多模态联邦图学习基准，包含19个数据集、8种仿真策略、6个下游任务和57种SOTA方法，系统评估多模态联邦图学习的必要性、有效性、鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 现实应用中的多模态属性图通常分布在隔离平台，由于隐私或商业限制无法共享。现有联邦图学习研究主要关注单模态图，缺乏针对多模态联邦图学习独特挑战的系统研究。

Method: 提出MM-OpenFGL基准，系统形式化多模态联邦图学习范式，包含19个多模态数据集覆盖7个应用领域，8种仿真策略捕捉模态和拓扑变化，6个下游任务，57种最先进方法通过模块化API实现。

Result: 通过大量实验从必要性、有效性、鲁棒性和效率四个角度研究多模态联邦图学习，为未来研究提供有价值的见解。

Conclusion: MM-OpenFGL填补了多模态联邦图学习领域的空白，为系统评估和推动该领域研究提供了首个全面基准。

Abstract: Multimodal-attributed graphs (MMAGs) provide a unified framework for modeling complex relational data by integrating heterogeneous modalities with graph structures. While centralized learning has shown promising performance, MMAGs in real-world applications are often distributed across isolated platforms and cannot be shared due to privacy concerns or commercial constraints. Federated graph learning (FGL) offers a natural solution for collaborative training under such settings; however, existing studies largely focus on single-modality graphs and do not adequately address the challenges unique to multimodal federated graph learning (MMFGL). To bridge this gap, we present MM-OpenFGL, the first comprehensive benchmark that systematically formalizes the MMFGL paradigm and enables rigorous evaluation. MM-OpenFGL comprises 19 multimodal datasets spanning 7 application domains, 8 simulation strategies capturing modality and topology variations, 6 downstream tasks, and 57 state-of-the-art methods implemented through a modular API. Extensive experiments investigate MMFGL from the perspectives of necessity, effectiveness, robustness, and efficiency, offering valuable insights for future research on MMFGL.

</details>


### [54] [MetaLead: A Comprehensive Human-Curated Leaderboard Dataset for Transparent Reporting of Machine Learning Experiments](https://arxiv.org/abs/2601.22420)
*Roelien C. Timmer,Necva Bölücü,Stephen Wan*

Main category: cs.LG

TL;DR: MetaLead是一个全新的人工标注机器学习排行榜数据集，它捕获了所有实验结果以实现结果透明度，并包含额外元数据，如实验类型（基线、提出方法或变体）以及明确的训练/测试数据集分离，用于更透明和细致的ML评估。


<details>
  <summary>Details</summary>
Motivation: 机器学习领域中的排行榜对于基准测试和跟踪进展至关重要，但传统创建方法需要大量人工努力。现有的自动化排行榜生成数据集存在局限性：只捕获每篇论文的最佳结果，且元数据有限。因此需要更全面、透明的数据集来支持更细致的评估。

Method: 提出了MetaLead数据集，这是一个完全人工标注的机器学习排行榜数据集。它捕获所有实验结果（而不仅仅是最好结果），包含丰富的元数据（如实验类型：基线、提出方法或变体方法），并明确分离训练和测试数据集以支持跨领域评估。

Result: MetaLead数据集提供了更强大的资源，支持更透明和细致的机器学习研究评估。其丰富的结构使得能够进行实验类型指导的比较和跨领域评估，解决了现有数据集只关注最佳结果和元数据有限的局限性。

Conclusion: MetaLead是一个创新的机器学习排行榜数据集，通过捕获所有实验结果和丰富元数据，为机器学习研究提供了更透明、更细致的评估框架，有助于推动更全面的基准测试和进展跟踪。

Abstract: Leaderboards are crucial in the machine learning (ML) domain for benchmarking and tracking progress. However, creating leaderboards traditionally demands significant manual effort. In recent years, efforts have been made to automate leaderboard generation, but existing datasets for this purpose are limited by capturing only the best results from each paper and limited metadata. We present MetaLead, a fully human-annotated ML Leaderboard dataset that captures all experimental results for result transparency and contains extra metadata, such as the result experimental type: baseline, proposed method, or variation of proposed method for experiment-type guided comparisons, and explicitly separates train and test dataset for cross-domain assessment. This enriched structure makes MetaLead a powerful resource for more transparent and nuanced evaluations across ML research.

</details>


### [55] [CoDCL: Counterfactual Data Augmentation Contrastive Learning for Continuous-Time Dynamic Network Link Prediction](https://arxiv.org/abs/2601.22427)
*Hantong Feng,Yonggang Wu,Duxin Chen,Wenwu Yu*

Main category: cs.LG

TL;DR: CoDCL是一个动态网络学习框架，结合反事实数据增强和对比学习来提高模型对网络结构变化的鲁棒性，可作为即插即用模块集成到现有时序图模型中。


<details>
  <summary>Details</summary>
Motivation: 动态网络的快速增长和持续结构演化使得预测变得越来越困难，模型需要能够适应复杂时序环境并对新兴结构变化具有鲁棒性。

Method: 提出CoDCL框架，结合反事实数据增强和对比学习；设计综合策略生成高质量反事实数据，包括动态处理设计和高效结构邻域探索来量化交互模式的时序变化；整个框架设计为即插即用通用模块。

Result: 在多个真实世界数据集上的广泛实验表明，CoDCL显著超越了动态网络领域的最先进基线模型。

Conclusion: 证实了将反事实数据增强集成到动态表示学习中的关键作用，提出的框架能够有效提升动态网络预测模型的鲁棒性和适应性。

Abstract: The rapid growth and continuous structural evolution of dynamic networks make effective predictions increasingly challenging. To enable prediction models to adapt to complex temporal environments, they need to be robust to emerging structural changes. We propose a dynamic network learning framework CoDCL, which combines counterfactual data augmentation with contrastive learning to address this deficiency.Furthermore, we devise a comprehensive strategy to generate high-quality counterfactual data, combining a dynamic treatments design with efficient structural neighborhood exploration to quantify the temporal changes in interaction patterns.Crucially, the entire CoDCL is designed as a plug-and-play universal module that can be seamlessly integrated into various existing temporal graph models without requiring architectural modifications.Extensive experiments on multiple real-world datasets demonstrate that CoDCL significantly gains state-of-the-art baseline models in the field of dynamic networks, confirming the critical role of integrating counterfactual data augmentation into dynamic representation learning.

</details>


### [56] [ReNCE: Learning to Reason by Noise Contrastive Estimation](https://arxiv.org/abs/2601.22432)
*Wenzheng Zhang,Karl Stratos*

Main category: cs.LG

TL;DR: 提出一种用于LLM推理的显式对比学习方法，替代传统的GRPO方法，通过将结果分为正负集并最大化正结果概率，在数学基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: GRPO方法虽然有效，但其改进（如非对称裁剪和零方差数据过滤）需要大量经验洞察且难以识别。作者希望提出一种更简单直接的替代方法。

Method: 提出显式对比学习方法：将K个结果分为正负两个集合，然后最大化正结果的似然。该方法可视为LLM推理中（多标签）噪声对比估计的在线实例化。

Result: 在具有挑战性的数学基准测试套件上，与DAPO和在线DPO等强基线相比，表现出有竞争力的性能。

Conclusion: 提出的显式对比学习方法为LLM推理提供了一种有效的替代方案，避免了GRPO方法中需要经验洞察的复杂改进，同时在性能上保持竞争力。

Abstract: GRPO is a standard approach to endowing pretrained LLMs with reasoning capabilities. It estimates the advantage of an outcome from a group of $K$ outcomes, and promotes those with positive advantages inside a trust region. Since GRPO discriminates between good and bad outcomes softly, it benefits from additional refinements such as asymmetric clipping and zero-variance data filtering. While effective, these refinements require significant empirical insight and can be challenging to identify. We instead propose an explicit contrastive learning approach. Instead of estimating advantages, we bifurcate $K$ outcomes into positive and negative sets, then maximize the likelihood of positive outcomes. Our approach can be viewed as an online instantiation of (multi-label) noise contrastive estimation for LLM reasoning. We validate our method by demonstrating competitive performance on a suite of challenging math benchmarks against strong baselines such as DAPO and online DPO.

</details>


### [57] [AsyncMesh: Fully Asynchronous Optimization for Data and Pipeline Parallelism](https://arxiv.org/abs/2601.22442)
*Thalaiyasingam Ajanthan,Sameera Ramasinghe,Gil Avraham,Hadi Mohaghegh Dolatabadi,Chamin P Hewa Koneputugodage,Violetta Shevchenko,Yan Zuo,Alexander Long*

Main category: cs.LG

TL;DR: 提出异步更新方法解决数据并行和流水线并行中的通信瓶颈，通过权重前瞻和稀疏平均技术减少通信开销，在保持性能的同时提升可扩展性


<details>
  <summary>Details</summary>
Motivation: 数据并行和流水线并行需要快速互连的集群，通信成本高限制了可扩展性。需要解决通信瓶颈，放宽设备共置要求

Method: 1) 流水线并行采用权重前瞻方法；2) 数据并行采用异步稀疏平均方法，结合指数移动平均校正机制；3) 为稀疏平均和异步更新提供收敛保证

Result: 在大型语言模型（达10亿参数）实验中，该方法性能与完全同步基线相当，同时显著减少通信开销

Conclusion: 通过异步更新和相应的校正机制，可以在保持模型性能的同时有效解决分布式训练中的通信瓶颈问题

Abstract: Data and pipeline parallelism are key strategies for scaling neural network training across distributed devices, but their high communication cost necessitates co-located computing clusters with fast interconnects, limiting their scalability. We address this communication bottleneck by introducing asynchronous updates across both parallelism axes, relaxing the co-location requirement at the expense of introducing staleness between pipeline stages and data parallel replicas. To mitigate staleness, for pipeline parallelism, we adopt a weight look-ahead approach, and for data parallelism, we introduce an asynchronous sparse averaging method equipped with an exponential moving average based correction mechanism. We provide convergence guarantees for both sparse averaging and asynchronous updates. Experiments on large-scale language models (up to \em 1B parameters) demonstrate that our approach matches the performance of the fully synchronous baseline, while significantly reducing communication overhead.

</details>


### [58] [Weak Diffusion Priors Can Still Achieve Strong Inverse-Problem Performance](https://arxiv.org/abs/2601.22443)
*Jing Jia,Wei Yuan,Sifan Liu,Liyue Shen,Guanyang Wang*

Main category: cs.LG

TL;DR: 扩散模型即使在不匹配或低保真度的先验条件下，也能在逆问题求解中表现良好，研究揭示了弱先验成功的关键条件


<details>
  <summary>Details</summary>
Motivation: 研究扩散模型作为逆问题先验时的鲁棒性问题。实际应用中常使用不匹配或低保真度的扩散先验，但令人惊讶的是，这些弱先验往往表现接近完整强度的域内基线。本文旨在探究逆求解器对弱扩散先验具有鲁棒性的时机和原因。

Method: 通过大量实验研究弱先验在不同条件下的表现，并基于贝叶斯一致性理论分析，给出高维测量使后验集中于真实信号附近的条件。

Result: 研究发现弱先验在测量信息丰富时（如观测到大量像素）能够成功，同时识别了其失败的机制。理论分析表明，高维测量条件下，后验会集中在真实信号附近。

Conclusion: 该研究为弱扩散先验的可靠使用提供了理论依据，明确了在测量信息充分的情况下，即使使用不匹配的扩散模型作为先验，也能有效解决逆问题。

Abstract: Can a diffusion model trained on bedrooms recover human faces? Diffusion models are widely used as priors for inverse problems, but standard approaches usually assume a high-fidelity model trained on data that closely match the unknown signal. In practice, one often must use a mismatched or low-fidelity diffusion prior. Surprisingly, these weak priors often perform nearly as well as full-strength, in-domain baselines. We study when and why inverse solvers are robust to weak diffusion priors. Through extensive experiments, we find that weak priors succeed when measurements are highly informative (e.g., many observed pixels), and we identify regimes where they fail. Our theory, based on Bayesian consistency, gives conditions under which high-dimensional measurements make the posterior concentrate near the true signal. These results provide a principled justification on when weak diffusion priors can be used reliably.

</details>


### [59] [Automating Forecasting Question Generation and Resolution for AI Evaluation](https://arxiv.org/abs/2601.22444)
*Nikos I. Bosse,Peter Mühlbacher,Jack Wildman,Lawrence Phillips,Dan Schwarz*

Main category: cs.LG

TL;DR: 开发了一个使用LLM驱动的网络研究代理自动生成和解决高质量预测问题的系统，该系统能大规模生成多样化、真实的预测问题，并在几个月后自动解决，性能优于人类策划的平台。


<details>
  <summary>Details</summary>
Motivation: 预测未来事件对决策制定至关重要，也是衡量通用智能的稳健指标。然而，开发和评估AI预测器需要大量多样化且困难的问题，以及准确的解决方案。以往依赖周期性数据源（如天气、股票）的自动化方法限制了问题的多样性和实用性。

Method: 使用LLM驱动的网络研究代理构建了一个自动生成和解决预测问题的系统。该系统能够大规模生成高质量、多样化的真实世界预测问题，并在几个月后自动解决这些问题。系统还评估了不同LLM（Gemini 3 Pro、GPT-5、Gemini 2.5 Flash）在预测任务上的表现，并测试了问题分解策略。

Result: 系统生成了1499个多样化、真实的预测问题，可验证、无歧义的问题生成率约为96%，超过了人类策划的Metaculus平台。问题解决准确率约为95%。更智能的LLM在预测任务上表现更好（Gemini 3 Pro Brier得分0.134，GPT-5 0.149，Gemini 2.5 Flash 0.179）。问题分解策略显著改善了Brier得分（0.132 vs 0.141）。

Conclusion: LLM驱动的网络研究代理能够自动生成和解决高质量、多样化的预测问题，性能优于人类策划的平台。该系统不仅为评估AI预测能力提供了有效工具，还能直接用于改进预测性能，展示了自动化预测问题生成和解决的可行性。

Abstract: Forecasting future events is highly valuable in decision-making and is a robust measure of general intelligence. As forecasting is probabilistic, developing and evaluating AI forecasters requires generating large numbers of diverse and difficult questions, and accurately resolving them. Previous efforts to automate this laborious work relied on recurring data sources (e.g., weather, stocks), limiting diversity and utility. In this work, we present a system for generating and resolving high-quality forecasting questions automatically and at scale using LLM-powered web research agents. We use this system to generate 1499 diverse, real-world forecasting questions, and to resolve them several months later. We estimate that our system produces verifiable, unambiguous questions approximately 96% of the time, exceeding the rate of Metaculus, a leading human-curated forecasting platform. We also find that our system resolves questions at approximately 95% accuracy. We verify that forecasting agents powered by more intelligent LLMs perform better on these questions (Brier score of 0.134 for Gemini 3 Pro, 0.149 for GPT-5, and 0.179 for Gemini 2.5 Flash). Finally, we demonstrate how our system can be leveraged to directly improve forecasting, by evaluating a question decomposition strategy on a generated question set, yielding a significant improvement in Brier scores (0.132 vs. 0.141).

</details>


### [60] [Beyond Activation Patterns: A Weight-Based Out-of-Context Explanation of Sparse Autoencoder Features](https://arxiv.org/abs/2601.22447)
*Yiting Liu,Zhi-Hong Deng*

Main category: cs.LG

TL;DR: 提出基于权重的SAE特征解释框架，无需激活数据，通过权重交互测量功能效应，发现1/4特征直接预测输出token，特征参与注意力机制且有深度依赖结构，语义与非语义特征在注意力电路中分布不同。


<details>
  <summary>Details</summary>
Motivation: 当前稀疏自编码器（SAE）特征解释方法主要基于激活模式推断语义，但忽略了特征训练的目标是重建在前向传递中具有计算作用的激活。需要一种能直接测量特征功能效应的解释框架。

Method: 提出基于权重的解释框架，通过直接权重交互测量功能效应，无需激活数据。在Gemma-2和Llama-3.1模型上进行三个实验：特征对输出token的直接预测、特征在注意力机制中的参与情况、语义与非语义特征在注意力电路中的分布差异。

Result: 1) 约1/4的特征直接预测输出token；2) 特征积极参与注意力机制，且具有深度依赖的结构；3) 语义特征和非语义特征在注意力电路中表现出不同的分布特征。

Conclusion: 该权重解释框架提供了SAE特征可解释性的"缺失的另一半"，补充了基于激活的解释方法，通过直接分析权重交互揭示了特征的计算功能。

Abstract: Sparse autoencoders (SAEs) have emerged as a powerful technique for decomposing language model representations into interpretable features. Current interpretation methods infer feature semantics from activation patterns, but overlook that features are trained to reconstruct activations that serve computational roles in the forward pass. We introduce a novel weight-based interpretation framework that measures functional effects through direct weight interactions, requiring no activation data. Through three experiments on Gemma-2 and Llama-3.1 models, we demonstrate that (1) 1/4 of features directly predict output tokens, (2) features actively participate in attention mechanisms with depth-dependent structure, and (3) semantic and non-semantic feature populations exhibit distinct distribution profiles in attention circuits. Our analysis provides the missing out-of-context half of SAE feature interpretability.

</details>


### [61] [HeaPA: Difficulty-Aware Heap Sampling and On-Policy Query Augmentation for LLM Reinforcement Learning](https://arxiv.org/abs/2601.22448)
*Weiqi Wang,Xin Liu,Binxuan Huang,Hejie Cui,Rongzhi Zhang,Changlong Yu,Shuowei Jin,Jingfeng Yang,Qingyu Yin,Zhengyang Wang,Zheng Li,Yifan Gao,Priyanka Nigam,Bing Yin,Lihong Li,Yangqiu Song*

Main category: cs.LG

TL;DR: HeaPA是一种用于RLVR训练的高效采样方法，通过堆采样和在线查询增强来优化提示池管理，减少计算成本同时保持性能


<details>
  <summary>Details</summary>
Motivation: 当前RLVR训练中，提示池通常是静态的或与模型学习进度松散关联，均匀采样无法适应能力边界的变化，导致在已解决或无法解决的提示上浪费计算资源

Method: HeaPA维护有界演化提示池，使用堆采样跟踪能力边界，通过轻量级异步验证进行在线查询增强，并通过拓扑感知的统计重估计和控制性重新插入来稳定相关查询

Result: 在两个训练语料库、两种训练方案和七个基准测试中，HeaPA持续提高准确性，以更少的计算达到目标性能，同时保持相当的训练时间，且模型规模越大收益越明显

Conclusion: HeaPA通过边界聚焦采样和在线池增长机制，为RLVR训练提供了高效且可扩展的解决方案，特别适合大规模模型训练

Abstract: RLVR is now a standard way to train LLMs on reasoning tasks with verifiable outcomes, but when rollout generation dominates the cost, efficiency depends heavily on which prompts you sample and when. In practice, prompt pools are often static or only loosely tied to the model's learning progress, so uniform sampling can't keep up with the shifting capability frontier and ends up wasting rollouts on prompts that are already solved or still out of reach. Existing approaches improve efficiency through filtering, curricula, adaptive rollout allocation, or teacher guidance, but they typically assume a fixed pool-which makes it hard to support stable on-policy pool growth-or they add extra teacher cost and latency. We introduce HeaPA (Heap Sampling and On-Policy Query Augmentation), which maintains a bounded, evolving pool, tracks the frontier using heap-based boundary sampling, expands the pool via on-policy augmentation with lightweight asynchronous validation, and stabilizes correlated queries through topology-aware re-estimation of pool statistics and controlled reinsertion. Across two training corpora, two training recipes, and seven benchmarks, HeaPA consistently improves accuracy and reaches target performance with fewer computations while keeping wall-clock time comparable. Our analyses suggest these gains come from frontier-focused sampling and on-policy pool growth, with the benefits becoming larger as model scale increases. Our code is available at https://github.com/horizon-rl/HeaPA.

</details>


### [62] [Tuning the Implicit Regularizer of Masked Diffusion Language Models: Enhancing Generalization via Insights from $k$-Parity](https://arxiv.org/abs/2601.22450)
*Jianhao Huang,Baharan Mirzasoleiman*

Main category: cs.LG

TL;DR: 研究掩码扩散语言模型在k-parity问题上的泛化特性，发现其能避免grokking现象，并通过优化掩码概率分布提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散语言模型作为强大的生成范式，其泛化特性相比自回归模型研究不足。作者希望通过k-parity问题（计算k个相关位的XOR和）这一典型场景，研究掩码扩散模型的泛化行为，特别是与神经网络中常见的grokking现象（长时间随机性能后突然泛化）的关系。

Method: 1. 理论分析：将掩码扩散目标分解为驱动特征学习的信号机制和作为隐式正则化的噪声机制。2. 实验验证：在k-parity问题上使用掩码扩散目标训练nanoGPT模型。3. 优化方法：基于理论洞察优化掩码概率分布，应用于不同规模的模型。

Result: 1. 掩码扩散目标改变了学习景观，实现了快速且同步的泛化，避免了grokking现象。2. 优化掩码概率分布的方法显著提升了5000万参数模型的困惑度。3. 在80亿参数模型上，从零开始预训练和监督微调分别获得了8.8%和5.8%的性能提升，证明了方法的可扩展性和有效性。

Conclusion: 掩码扩散语言模型在k-parity问题上表现出与自回归模型不同的泛化特性，能够避免grokking现象。通过理论分解和优化掩码概率分布，可以显著提升大规模掩码扩散语言模型的性能，为理解和改进这类模型提供了新视角。

Abstract: Masked Diffusion Language Models have recently emerged as a powerful generative paradigm, yet their generalization properties remain understudied compared to their auto-regressive counterparts. In this work, we investigate these properties within the setting of the $k$-parity problem (computing the XOR sum of $k$ relevant bits), where neural networks typically exhibit grokking -- a prolonged plateau of chance-level performance followed by sudden generalization. We theoretically decompose the Masked Diffusion (MD) objective into a Signal regime which drives feature learning, and a Noise regime which serves as an implicit regularizer. By training nanoGPT using MD objective on the $k$-parity problem, we demonstrate that MD objective fundamentally alters the learning landscape, enabling rapid and simultaneous generalization without experiencing grokking. Furthermore, we leverage our theoretical insights to optimize the distribution of the mask probability in the MD objective. Our method significantly improves perplexity for 50M-parameter models and achieves superior results across both pre-training from scratch and supervised fine-tuning. Specifically, we observe performance gains peaking at $8.8\%$ and $5.8\%$, respectively, on 8B-parameter models, confirming the scalability and effectiveness of our framework in large-scale masked diffusion language model regimes.

</details>


### [63] [Temporal Graph Pattern Machine](https://arxiv.org/abs/2601.22454)
*Yijun Ma,Zehong Wang,Weixiang Sun,Yanfang Ye*

Main category: cs.LG

TL;DR: TGPM是一个时间图基础框架，通过交互补丁和Transformer架构学习可迁移的演化模式，在链接预测任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有时间图学习方法存在任务中心化、依赖短期依赖、静态邻域语义等限制性假设，阻碍了可迁移时间演化机制的发现。

Method: 提出TGPM框架：1) 将每个交互视为通过时间偏置随机游走合成的交互补丁，捕获多尺度结构语义和长程依赖；2) 使用Transformer主干捕获全局时间规律并适应上下文交互动态；3) 引入掩码标记建模和下一时间预测等自监督预训练任务。

Result: 在转导和归纳链接预测任务中均达到最先进性能，展现出卓越的跨领域可迁移性。

Conclusion: TGPM通过直接学习广义演化模式，突破了现有方法的限制性假设，为时间图学习提供了可迁移的基础框架。

Abstract: Temporal graph learning is pivotal for deciphering dynamic systems, where the core challenge lies in explicitly modeling the underlying evolving patterns that govern network transformation. However, prevailing methods are predominantly task-centric and rely on restrictive assumptions -- such as short-term dependency modeling, static neighborhood semantics, and retrospective time usage. These constraints hinder the discovery of transferable temporal evolution mechanisms. To address this, we propose the Temporal Graph Pattern Machine (TGPM), a foundation framework that shifts the focus toward directly learning generalized evolving patterns. TGPM conceptualizes each interaction as an interaction patch synthesized via temporally-biased random walks, thereby capturing multi-scale structural semantics and long-range dependencies that extend beyond immediate neighborhoods. These patches are processed by a Transformer-based backbone designed to capture global temporal regularities while adapting to context-specific interaction dynamics. To further empower the model, we introduce a suite of self-supervised pre-training tasks -- specifically masked token modeling and next-time prediction -- to explicitly encode the fundamental laws of network evolution. Extensive experiments show that TGPM consistently achieves state-of-the-art performance in both transductive and inductive link prediction, demonstrating exceptional cross-domain transferability.

</details>


### [64] [Machine Unlearning in Low-Dimensional Feature Subspace](https://arxiv.org/abs/2601.22456)
*Kun Fang,Qinghua Tao,Junxu Liu,Yaxin Xiao,Qingqing Ye,Jian Sun,Haibo Hu*

Main category: cs.LG

TL;DR: LOFT是一种基于低维特征子空间的机器遗忘方法，通过优化投影矩阵在保留数据信息的同时消除遗忘数据的影响，显著降低了计算开销并提升了遗忘性能。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法存在两个关键问题：1) 从大量数据重新加载带来的隐私泄露风险；2) 对整个预训练模型进行更新的低效率。需要一种更高效、更安全的遗忘方法。

Method: 提出LOFT方法，在预训练模型的低维特征子空间中进行遗忘操作。通过主投影优化投影矩阵，最大化保留数据信息同时最小化遗忘数据信息。只需一次性获取特征，无需重复访问原始数据，仅优化小型投影矩阵。

Result: 大量实验验证了LOFT在多种模型、数据集、任务和应用中具有显著更低的计算开销和优越的遗忘性能。

Conclusion: LOFT通过低维特征子空间方法有效解决了机器遗忘中的隐私泄露和效率问题，为机器遗忘提供了新的视角和实用解决方案。

Abstract: Machine Unlearning (MU) aims at removing the influence of specific data from a pretrained model while preserving performance on the remaining data. In this work, a novel perspective for MU is presented upon low-dimensional feature subspaces, which gives rise to the potentials of separating the remaining and forgetting data herein. This separability motivates our LOFT, a method that proceeds unlearning in a LOw-dimensional FeaTure subspace from the pretrained model skithrough principal projections, which are optimized to maximally capture the information of the remaining data and meanwhile diminish that of the forgetting data. In training, LOFT simply optimizes a small-size projection matrix flexibly plugged into the pretrained model, and only requires one-shot feature fetching from the pretrained backbone instead of repetitively accessing the raw data. Hence, LOFT mitigates two critical issues in mainstream MU methods, i.e., the privacy leakage risk from massive data reload and the inefficiency of updates to the entire pretrained model. Extensive experiments validate the significantly lower computational overhead and superior unlearning performance of LOFT across diverse models, datasets, tasks, and applications. Code is anonymously available at https://anonymous.4open.science/r/4352/.

</details>


### [65] [EvoEGF-Mol: Evolving Exponential Geodesic Flow for Structure-based Drug Design](https://arxiv.org/abs/2601.22466)
*Yaowei Jin,Junjie Wang,Cheng Cao,Penglei Wang,Duo An,Qian Shi*

Main category: cs.LG

TL;DR: EvoEGF-Mol：基于信息几何的SBDD方法，通过指数测地流生成分子，避免传统方法在欧几里得和概率空间中的不匹配问题


<details>
  <summary>Details</summary>
Motivation: 传统基于结构的药物设计方法在欧几里得空间和概率空间中分别构建概率路径，与底层统计流形不匹配。需要从信息几何角度解决这一问题。

Method: 将分子建模为复合指数族分布，在Fisher-Rao度量下沿指数测地线定义生成流。为避免直接以狄拉克分布为目标的瞬时轨迹崩溃，提出EvoEGF-Mol方法，用动态集中分布替代静态狄拉克目标，通过渐进参数精化架构确保稳定训练。

Result: 在CrossDock上达到93.4%的PoseBusters通过率，展现出卓越的几何精度和相互作用保真度。在MolGenBench任务中优于基线方法，能够恢复生物活性支架并生成符合MedChem过滤标准的候选分子。

Conclusion: 从信息几何角度建模分子生成，提出的EvoEGF-Mol方法解决了传统SBDD中的流形不匹配问题，在几何精度和化学合理性方面表现出色，为药物发现提供了有效工具。

Abstract: Structure-Based Drug Design (SBDD) aims to discover bioactive ligands. Conventional approaches construct probability paths separately in Euclidean and probabilistic spaces for continuous atomic coordinates and discrete chemical categories, leading to a mismatch with the underlying statistical manifolds. We address this issue from an information-geometric perspective by modeling molecules as composite exponential-family distributions and defining generative flows along exponential geodesics under the Fisher-Rao metric. To avoid the instantaneous trajectory collapse induced by geodesics directly targeting Dirac distributions, we propose Evolving Exponential Geodesic Flow for SBDD (EvoEGF-Mol), which replaces static Dirac targets with dynamically concentrating distributions, ensuring stable training via a progressive-parameter-refinement architecture. Our model approaches a reference-level PoseBusters passing rate (93.4%) on CrossDock, demonstrating remarkable geometric precision and interaction fidelity, while outperforming baselines on real-world MolGenBench tasks by recovering bioactive scaffolds and generating candidates that meet established MedChem filters.

</details>


### [66] [Unrewarded Exploration in Large Language Models Reveals Latent Learning from Psychology](https://arxiv.org/abs/2601.22474)
*Jian Xiong,Jingbo Zhou,Zihan Zhou,Yixiong Xiao,Le Zhang,Jingyong Ye,Rui Qian,Yang Zhou,Dejing Dou*

Main category: cs.LG

TL;DR: LLMs在无奖励探索阶段表现出潜在学习动态，这种两阶段训练方式比纯奖励强化学习效果更好


<details>
  <summary>Details</summary>
Motivation: 心理学中的潜在学习现象在生物智能中已被证实，但当前LLMs主要依赖奖励驱动的强化学习范式，限制了模型的灵活性和泛化能力。需要探索潜在学习是否能在LLMs中实现

Method: 采用两阶段训练方法：第一阶段进行无奖励探索，让LLMs组织任务相关知识而不受奖励偏差约束；第二阶段引入奖励进行训练。在多个模型家族和多样化任务领域进行广泛实验

Result: LLMs确实表现出潜在学习动态：无奖励探索阶段带来适度性能提升，引入奖励后性能进一步增强。采用这种两阶段训练方式的LLMs最终比纯奖励强化学习训练的模型获得更高能力

Conclusion: LLMs能够展现类似生物智能的潜在学习现象，无奖励探索有助于模型组织知识而不受奖励偏差约束，为改进LLMs训练范式提供了新视角

Abstract: Latent learning, classically theorized by Tolman, shows that biological agents (e.g., rats) can acquire internal representations of their environment without rewards, enabling rapid adaptation once rewards are introduced. In contrast, from a cognitive science perspective, reward learning remains overly dependent on external feedback, limiting flexibility and generalization. Although recent advances in the reasoning capabilities of large language models (LLMs), such as OpenAI-o1 and DeepSeek-R1, mark a significant breakthrough, these models still rely primarily on reward-centric reinforcement learning paradigms. Whether and how the well-established phenomenon of latent learning in psychology can inform or emerge within LLMs' training remains largely unexplored. In this work, we present novel findings from our experiments that LLMs also exhibit the latent learning dynamics. During an initial phase of unrewarded exploration, LLMs display modest performance improvements, as this phase allows LLMs to organize task-relevant knowledge without being constrained by reward-driven biases, and performance is further enhanced once rewards are introduced. LLMs post-trained under this two-stage exploration regime ultimately achieve higher competence than those post-trained with reward-based reinforcement learning throughout. Beyond these empirical observations, we also provide theoretical analyses for our experiments explaining why unrewarded exploration yields performance gains, offering a mechanistic account of these dynamics. Specifically, we conducted extensive experiments across multiple model families and diverse task domains to establish the existence of the latent learning dynamics in LLMs.

</details>


### [67] [Continual Policy Distillation from Distributed Reinforcement Learning Teachers](https://arxiv.org/abs/2601.22475)
*Yuxuan Li,Qijun He,Mingqi Yuan,Wen-Tse Chen,Jeff Schneider,Jiayu Chen*

Main category: cs.LG

TL;DR: 提出教师-学生框架将持续强化学习解耦为两个独立过程：通过分布式RL训练单任务教师模型，并持续蒸馏到中央通用模型中，结合MoE架构和回放方法增强可塑性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 持续强化学习需要在多样任务中持续学习同时减轻灾难性遗忘，传统方法直接在序列任务流上应用RL难以实现可扩展性能，需要更有效的解决方案。

Method: 1) 使用分布式RL训练单任务专家教师模型；2) 通过持续策略蒸馏将教师知识转移到中央通用学生模型；3) 采用混合专家架构增强可塑性；4) 使用回放方法提升稳定性。

Result: 在Meta-World基准测试中，框架能恢复超过85%的教师性能，同时将任务级遗忘控制在10%以内，实现了高效的持续强化学习。

Conclusion: 通过解耦RL训练和策略蒸馏，结合MoE和回放技术，提出的教师-学生框架有效解决了持续强化学习的稳定性-可塑性困境，为大规模持续学习提供了可行方案。

Abstract: Continual Reinforcement Learning (CRL) aims to develop lifelong learning agents to continuously acquire knowledge across diverse tasks while mitigating catastrophic forgetting. This requires efficiently managing the stability-plasticity dilemma and leveraging prior experience to rapidly generalize to novel tasks. While various enhancement strategies for both aspects have been proposed, achieving scalable performance by directly applying RL to sequential task streams remains challenging. In this paper, we propose a novel teacher-student framework that decouples CRL into two independent processes: training single-task teacher models through distributed RL and continually distilling them into a central generalist model. This design is motivated by the observation that RL excels at solving single tasks, while policy distillation -- a relatively stable supervised learning process -- is well aligned with large foundation models and multi-task learning. Moreover, a mixture-of-experts (MoE) architecture and a replay-based approach are employed to enhance the plasticity and stability of the continual policy distillation process. Extensive experiments on the Meta-World benchmark demonstrate that our framework enables efficient continual RL, recovering over 85% of teacher performance while constraining task-wise forgetting to within 10%.

</details>


### [68] [Transform-Augmented GRPO Improves Pass@k](https://arxiv.org/abs/2601.22478)
*Khiem Le,Youssef Mroueh,Phuc Nguyen,Chi-Heng Lin,Shangqian Gao,Ting Hua,Nitesh V. Chawla*

Main category: cs.LG

TL;DR: TA-GRPO通过生成语义等价的变体问题并跨组池化奖励，解决了GRPO中的多样性崩溃和梯度消失问题，在数学推理基准上显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于下一个词预测的大语言模型本质上是模式匹配器，对表面表达变化敏感。GRPO旨在改进推理，但实际恶化了两种情况：多样性崩溃（训练放大单一解题策略）和梯度消失（大量问题产生零梯度）。

Method: 提出TA-GRPO（变换增强的GRPO），为每个问题生成语义等价的变换变体（通过改写、变量重命名和格式变化），并通过在整个组中池化奖励来计算优势。这种池化计算确保即使原始问题太简单或太难时也有混合奖励，同时在多样化表达上训练促进多种解题策略。

Result: 在数学推理基准上实验显示一致的Pass@k改进，在竞赛数学（AMC12, AIME24）上提升高达9.84分，在分布外科学推理（GPQA-Diamond）上提升5.05分。

Conclusion: TA-GRPO通过减少零梯度概率和降低训练-测试分布偏移来改善泛化，为语言模型推理训练提供了更稳健的方法。

Abstract: Large language models trained via next-token prediction are fundamentally pattern-matchers: sensitive to superficial phrasing variations even when the underlying problem is identical. Group Relative Policy Optimization (GRPO) was designed to improve reasoning, but in fact it worsens this situation through two failure modes: diversity collapse, where training amplifies a single solution strategy while ignoring alternatives of gradient signal, and gradient diminishing, where a large portion of questions yield zero gradients because all rollouts receive identical rewards. We propose TA-GRPO (Transform-Augmented GRPO), which generates semantically equivalent transformed variants of each question (via paraphrasing, variable renaming, and format changes) and computes advantages by pooling rewards across the entire group. This pooled computation ensures mixed rewards even when the original question is too easy or too hard, while training on diverse phrasings promotes multiple solution strategies. We provide theoretical justification showing that TA-GRPO reduces zero-gradient probability and improves generalization via reduced train-test distribution shift. Experiments on mathematical reasoning benchmarks show consistent Pass@k improvements, with gains up to 9.84 points on competition math (AMC12, AIME24) and 5.05 points on out-of-distribution scientific reasoning (GPQA-Diamond).

</details>


### [69] [Mitigating Cognitive Inertia in Large Reasoning Models via Latent Spike Steering](https://arxiv.org/abs/2601.22484)
*Seojin Lee,ByeongJeong Kim,Hwanhee Lee*

Main category: cs.LG

TL;DR: STARS框架通过监测隐藏状态的L2距离峰值来检测推理过程中的认知转折点，并注入状态感知的语言提示来实时纠正大型推理模型的认知惯性问题。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在扩展测试时计算时经常遭受认知惯性问题，表现为过度思考或推理僵化。现有的检测方法通常依赖表面文本启发式（如自我纠正标记），无法捕捉模型未表达的内部冲突。

Method: 提出STARS（峰值触发自适应推理引导）框架：1）通过检测隐藏状态的L2距离峰值来识别认知转折点；2）使用几何轨迹分析诊断转换的结构性质；3）注入状态感知的语言提示实时引导模型。这是一个无需训练的方法。

Result: 在多样化基准测试中，STARS有效减少了冗余循环，同时通过自适应纠正错误轨迹提高了准确性。该框架为优化大型推理模型的推理过程提供了鲁棒的、无监督的机制。

Conclusion: STARS提供了一种无需额外微调的框架，能够通过监测潜在动态来纠正认知惯性，优化大型推理模型的推理过程，解决了现有方法无法捕捉模型内部冲突的局限性。

Abstract: While Large Reasoning Models (LRMs) have achieved remarkable performance by scaling test-time compute, they frequently suffer from Cognitive Inertia, a failure pattern manifesting as either overthinking (inertia of motion) or reasoning rigidity (inertia of direction). Existing detection methods, typically relying on superficial textual heuristics like self-correction tokens, often fail to capture the model's unvoiced internal conflicts. To address this, we propose STARS (Spike-Triggered Adaptive Reasoning Steering), a training-free framework designed to rectify cognitive inertia by monitoring latent dynamics. STARS identifies Cognitive Pivots-critical moments of reasoning transition-by detecting distinct L2 distance spikes in the hidden states. Upon detection, the framework employs geometric trajectory analysis to diagnose the structural nature of the transition and injects state-aware language cues to steer the model in real-time. Our experiments across diverse benchmarks confirm that STARS efficiently curtails redundant loops while improving accuracy through the adaptive correction of erroneous trajectories. STARS offers a robust, unsupervised mechanism to optimize the reasoning process of LRMs without requiring additional fine-tuning.

</details>


### [70] [Elastic Spectral State Space Models for Budgeted Inference](https://arxiv.org/abs/2601.22488)
*Dachuan Song,Xuan Wang*

Main category: cs.LG

TL;DR: ES-SSM：只需一次全容量训练，即可通过截断实现任意规模的运行时推理，无需重新训练


<details>
  <summary>Details</summary>
Motivation: 基础模型通常在固定计算能力下训练，但实际应用需要在不同资源约束的平台部署。现有方法需要训练多个模型变体或进行模型蒸馏，支持预选尺寸而非运行时细粒度适配

Method: 基于状态空间模型的Hankel谱滤波，结合轻量级输入自适应门控，在随机谱预算下训练。使用共享掩码归一化规则，使预测能力集中在低索引组件

Result: 单个ES-SSM模型训练一次后，截断后能在类似参数规模下与Transformer和SSM基线竞争。在各种运行时预算下，截断水平范围内观察到平滑稳定的预算-性能曲线

Conclusion: ES-SSM通过谱分解实现弹性缩放，只需一次训练即可支持运行时任意规模部署，为资源受限场景提供高效解决方案

Abstract: Foundation models are typically trained at a fixed computational capacity, while real-world applications require deployment across platforms with different resource constraints. Current approaches usually rely on training families of model variants or model distillation, which requires additional training and supports only a pre-selected set of sizes rather than fine-grained adaptation at runtime. In this paper, we propose Elastic Spectral State Space Models (ES-SSM), which require only one-time training at full capacity, but can be directly truncated into arbitrary scales for budgeted, runtime inference without retraining. Our ES-SSM builds on Hankel spectral filtering over a state space model (SSM), coupled with a lightweight input-adaptive gate trained under randomized spectral budgets. Using a shared masked normalization rule over the ordered spectral channels, we encourage predictive capability to concentrate in low-index components, while higher-index components act primarily as refinement. We test our algorithm across long-sequence benchmarks spanning text, logic, retrieval, vision, and audio. We demonstrate that a single ES-SSM model trained once can be truncated to provide competitive performance compared with modern Transformer and SSM baselines at similar parameter scales. Furthermore, by testing under various runtime budgets, we observe smooth and stable budget-performance curves over a wide range of truncation levels.

</details>


### [71] [Gradual Fine-Tuning for Flow Matching Models](https://arxiv.org/abs/2601.22495)
*Gudrun Thorkelsdottir,Arindam Banerjee*

Main category: cs.LG

TL;DR: 提出GFT框架，通过温度控制中间目标平滑过渡预训练和目标分布，改善流匹配模型在有限数据下的微调效果


<details>
  <summary>Details</summary>
Motivation: 在数据有限、分布变化或效率要求严格的情况下，传统微调方法会损害预训练获得的准确性和效率增益。现有奖励微调方法对漂移结构或训练技术有限制。

Method: 提出渐进微调(GFT)框架，为随机流定义温度控制的中间目标序列，平滑插值预训练和目标漂移，温度趋近零时接近真实目标。

Result: 证明了边缘和条件GFT目标的收敛性，支持使用最优传输等耦合方法。实验显示GFT提高收敛稳定性、缩短概率路径、加速推理，同时保持生成质量。

Conclusion: GFT为流匹配模型在分布偏移下的可扩展适应提供了理论基础和实践有效的替代方案。

Abstract: Fine-tuning flow matching models is a central challenge in settings with limited data, evolving distributions, or strict efficiency demands, where unconstrained fine-tuning can erode the accuracy and efficiency gains learned during pretraining. Prior work has produced theoretical guarantees and empirical advances for reward-based fine-tuning formulations, but these methods often impose restrictions on permissible drift structure or training techniques. In this work, we propose Gradual Fine-Tuning (GFT), a principled framework for fine-tuning flow-based generative models when samples from the target distribution are available. For stochastic flows, GFT defines a temperature-controlled sequence of intermediate objectives that smoothly interpolate between the pretrained and target drifts, approaching the true target as the temperature approaches zero. We prove convergence results for both marginal and conditional GFT objectives, enabling the use of suitable (e.g., optimal transport) couplings during GFT while preserving correctness. Empirically, GFT improves convergence stability and shortens probability paths, resulting in faster inference, while maintaining generation quality comparable to standard fine-tuning. Our results position GFT as a theoretically grounded and practically effective alternative for scalable adaptation of flow matching models under distribution shift.

</details>


### [72] [Action-Sufficient Goal Representations](https://arxiv.org/abs/2601.22496)
*Jinu Hyeon,Woobin Park,Hongjoon Ahn,Taesup Moon*

Main category: cs.LG

TL;DR: 离线目标条件强化学习中，通过信息论框架提出动作充分性概念，证明价值充分性不足以保证最优控制，实验显示基于演员策略学习的表示优于基于价值估计的表示。


<details>
  <summary>Details</summary>
Motivation: 现有分层策略在离线目标条件强化学习中通常从价值函数学习目标表示，隐含假设保留价值估计所需信息就足以实现最优控制。但作者发现即使价值估计准确，这种表示也可能无法区分需要不同动作的目标状态。

Method: 引入信息论框架定义动作充分性条件，证明价值充分性不蕴含动作充分性。提出通过低层策略的标准对数损失训练自然诱导动作充分表示，并在离散环境中验证该方法。

Result: 实验证明动作充分性与控制成功更相关，基于演员策略学习的表示在流行基准测试中持续优于基于价值估计学习的表示。

Conclusion: 在分层目标条件强化学习中，动作充分性是比价值充分性更关键的目标表示条件，通过低层策略训练获得的表示能更好地支持最优控制。

Abstract: Hierarchical policies in offline goal-conditioned reinforcement learning (GCRL) addresses long-horizon tasks by decomposing control into high-level subgoal planning and low-level action execution. A critical design choice in such architectures is the goal representation-the compressed encoding of goals that serves as the interface between these levels. Existing approaches commonly derive goal representations while learning value functions, implicitly assuming that preserving information sufficient for value estimation is adequate for optimal control. We show that this assumption can fail, even when the value estimation is exact, as such representations may collapse goal states that need to be differentiated for action learning. To address this, we introduce an information-theoretic framework that defines action sufficiency, a condition on goal representations necessary for optimal action selection. We prove that value sufficiency does not imply action sufficiency and empirically verify that the latter is more strongly associated with control success in a discrete environment. We further demonstrate that standard log-loss training of low-level policies naturally induces action-sufficient representations. Our experimental results a popular benchmark demonstrate that our actor-derived representations consistently outperform representations learned via value estimation.

</details>


### [73] [Keep Rehearsing and Refining: Lifelong Learning Vehicle Routing under Continually Drifting Tasks](https://arxiv.org/abs/2601.22509)
*Jiyuan Pei,Yi Mei,Jialin Liu,Mengjie Zhang,Xin Yao*

Main category: cs.LG

TL;DR: 提出DREE框架，用于解决VRP神经求解器在持续漂移任务下的终身学习问题，通过双重回放和经验增强提高学习效率并减轻灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有VRP神经求解器的训练方式（一次性固定任务或顺序任务充分训练）忽略了现实世界中问题模式持续漂移的特性：任务持续产生但每个任务的训练资源有限。

Method: 提出DREE（Dual Replay with Experience Enhancement）框架，通过双重回放和经验增强机制，在持续漂移的任务序列中高效学习新任务并保留先验知识。

Result: 实验表明DREE在持续漂移环境下能有效学习新任务、保留先验知识、提高对未见任务的泛化能力，并可应用于多种现有神经求解器。

Conclusion: DREE为解决VRP神经求解器在持续漂移任务下的终身学习问题提供了一个有效的通用框架。

Abstract: Existing neural solvers for vehicle routing problems (VRPs) are typically trained either in a one-off manner on a fixed set of pre-defined tasks or in a lifelong manner on several tasks arriving sequentially, assuming sufficient training on each task. Both settings overlook a common real-world property: problem patterns may drift continually over time, yielding massive tasks sequentially arising while offering only limited training resources per task. In this paper, we study a novel lifelong learning paradigm for neural VRP solvers under continually drifting tasks over learning time steps, where sufficient training for any given task at any time is not available. We propose Dual Replay with Experience Enhancement (DREE), a general framework to improve learning efficiency and mitigate catastrophic forgetting under such drift. Extensive experiments show that, under such continual drift, DREE effectively learns new tasks, preserves prior knowledge, improves generalization to unseen tasks, and can be applied to diverse existing neural solvers.

</details>


### [74] [Shattered Compositionality: Counterintuitive Learning Dynamics of Transformers for Arithmetic](https://arxiv.org/abs/2601.22510)
*Xingyu Zhao,Darsh Sharma,Rheeya Uppaal,Yiqiao Zhong*

Main category: cs.LG

TL;DR: 研究发现Transformer模型学习技能组合时遵循"破碎组合性"模式，而非人类顺序规则，导致分布迁移时出现意外错误，且模型规模扩大或推理链方法无法缓解此问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型常出现意外错误，现有研究揭示LLM与人类在技能组合上的差异，但技能组合的学习动态及其非人类行为的根本原因尚不清楚。

Method: 在合成算术任务上训练Transformer模型，通过大量消融实验和细粒度诊断指标分析学习动态，并与现代LLM进行对比。

Result: Transformer不按人类顺序规则构建技能组合，常以反向或并行方式获取技能，导致分布迁移时出现混合错误（破碎组合性）。证据表明这是训练数据相关性匹配而非因果或程序性组合的结果，且此现象在现代LLM中持续存在，无法通过模型规模扩大或推理链方法缓解。

Conclusion: 模型学习行为与期望技能组合存在根本性不匹配，这对推理可靠性、分布外鲁棒性和对齐具有重要影响。

Abstract: Large language models (LLMs) often exhibit unexpected errors or unintended behavior, even at scale. While recent work reveals the discrepancy between LLMs and humans in skill compositions, the learning dynamics of skill compositions and the underlying cause of non-human behavior remain elusive. In this study, we investigate the mechanism of learning dynamics by training transformers on synthetic arithmetic tasks. Through extensive ablations and fine-grained diagnostic metrics, we discover that transformers do not reliably build skill compositions according to human-like sequential rules. Instead, they often acquire skills in reverse order or in parallel, which leads to unexpected mixing errors especially under distribution shifts--a phenomenon we refer to as shattered compositionality. To explain these behaviors, we provide evidence that correlational matching to the training data, rather than causal or procedural composition, shapes learning dynamics. We further show that shattered compositionality persists in modern LLMs and is not mitigated by pure model scaling or scratchpad-based reasoning. Our results reveal a fundamental mismatch between a model's learning behavior and desired skill compositions, with implications for reasoning reliability, out-of-distribution robustness, and alignment.

</details>


### [75] [DRL-Enabled Trajectory Planing for UAV-Assisted VLC: Optimal Altitude and Reward Design](https://arxiv.org/abs/2601.22512)
*Tian-Tian Lin,Yi Liu,Xiao-Wei Tang,Yunmei Shi,Yi Huang,Zhongxiang Wei,Qingqing Wu,Yuhan Dong*

Main category: cs.LG

TL;DR: 该论文提出了一种无人机辅助可见光通信系统中的三维轨迹规划框架，通过优化飞行高度和水平轨迹来最小化飞行距离，从而提高数据收集效率。


<details>
  <summary>Details</summary>
Motivation: 无人机与可见光通信技术的结合为灵活通信和高效照明提供了有前景的解决方案。然而，在无人机辅助的VLC系统中，如何规划高效的三维轨迹以最小化飞行距离并最大化数据收集效率是一个具有挑战性的问题。

Method: 首先推导了在特定VLC信道增益阈值下的闭式最优飞行高度。然后，通过将新型信息素驱动奖励机制与双延迟深度确定性策略梯度算法相结合，优化无人机的水平轨迹，使其能够在复杂环境中实现自适应运动策略。

Result: 仿真结果表明，推导的最优高度相比基线方法可将飞行距离减少高达35%。此外，提出的奖励机制显著缩短了约50%的收敛步数，在无人机辅助VLC数据收集方面表现出显著的效率提升。

Conclusion: 该研究成功开发了一个有效的三维轨迹规划框架，通过优化飞行高度和水平轨迹，显著提高了无人机辅助VLC系统的数据收集效率，为未来智能通信系统提供了有价值的解决方案。

Abstract: Recently, the integration of unmanned aerial vehicle (UAV) and visible light communication (VLC) technologies has emerged as a promising solution to offer flexible communication and efficient lighting. This letter investigates the three-dimensional trajectory planning in a UAV-assisted VLC system, where a UAV is dispatched to collect data from ground users (GUs). The core objective is to develop a trajectory planning framework that minimizes UAV flight distance, which is equivalent to maximizing the data collection efficiency. This issue is formulated as a challenging mixed-integer non-convex optimization problem. To tackle it, we first derive a closed-form optimal flight altitude under specific VLC channel gain threshold. Subsequently, we optimize the UAV horizontal trajectory by integrating a novel pheromone-driven reward mechanism with the twin delayed deep deterministic policy gradient algorithm, which enables adaptive UAV motion strategy in complex environments. Simulation results validate that the derived optimal altitude effectively reduces the flight distance by up to 35% compared to baseline methods. Additionally, the proposed reward mechanism significantly shortens the convergence steps by approximately 50%, demonstrating notable efficiency gains in the context of UAV-assisted VLC data collection.

</details>


### [76] [SCOPE-PD: Explainable AI on Subjective and Clinical Objective Measurements of Parkinson's Disease for Precision Decision-Making](https://arxiv.org/abs/2601.22516)
*Md Mezbahul Islam,John Michael Templeton,Masrur Sobhan,Christian Poellabauer,Ananda Mohan Mondal*

Main category: cs.LG

TL;DR: SCOPE-PD是一个可解释的AI预测框架，通过整合主观和客观评估来预测帕金森病，使用随机森林算法达到98.66%的准确率。


<details>
  <summary>Details</summary>
Motivation: 帕金森病预测面临传统诊断方法主观性强、诊断延迟的问题，现有机器学习方法缺乏可解释性和个性化风险评估。

Method: 提出SCOPE-PD框架，整合主观和客观临床评估数据，应用多种机器学习技术，使用SHAP分析进行模型解释。

Result: 随机森林算法在结合主观和客观特征时达到最高98.66%准确率，识别出震颤、运动迟缓和面部表情为最重要的预测特征。

Conclusion: SCOPE-PD框架通过整合多模态数据和可解释AI，为帕金森病早期预测提供了个性化、可解释的解决方案。

Abstract: Parkinson's disease (PD) is a chronic and complex neurodegenerative disorder influenced by genetic, clinical, and lifestyle factors. Predicting this disease early is challenging because it depends on traditional diagnostic methods that face issues of subjectivity, which commonly delay diagnosis. Several objective analyses are currently in practice to help overcome the challenges of subjectivity; however, a proper explanation of these analyses is still lacking. While machine learning (ML) has demonstrated potential in supporting PD diagnosis, existing approaches often rely on subjective reports only and lack interpretability for individualized risk estimation. This study proposes SCOPE-PD, an explainable AI-based prediction framework, by integrating subjective and objective assessments to provide personalized health decisions. Subjective and objective clinical assessment data are collected from the Parkinson's Progression Markers Initiative (PPMI) study to construct a multimodal prediction framework. Several ML techniques are applied to these data, and the best ML model is selected to interpret the results. Model interpretability is examined using SHAP-based analysis. The Random Forest algorithm achieves the highest accuracy of 98.66 percent using combined features from both subjective and objective test data. Tremor, bradykinesia, and facial expression are identified as the top three contributing features from the MDS-UPDRS test in the prediction of PD.

</details>


### [77] [Variational Bayesian Flow Network for Graph Generation](https://arxiv.org/abs/2601.22524)
*Yida Xiong,Jiameng Chen,Xiuwen Gong,Jia Wu,Shirui Pan,Wenbin Hu*

Main category: cs.LG

TL;DR: 提出Variational Bayesian Flow Network (VBFN)，通过变分提升到结构化精度的高斯变分信念族，在单步融合中实现节点和边的耦合更新，提升图生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有图生成方法（如扩散模型和流匹配）通常采用因子化的噪声添加或坐标插值，节点-边耦合关系未在生成几何中编码，需要在离散解码后隐式恢复，这可能导致脆弱性。传统贝叶斯流网络依赖因子化信念和独立通道，限制了几何证据融合。

Method: 提出VBFN，通过变分提升到可处理的联合高斯变分信念族，使用结构化精度矩阵。贝叶斯更新简化为求解对称正定线性系统，实现节点和边的单步耦合更新。从表示诱导的依赖图构建样本无关的稀疏精度矩阵，避免标签泄漏并强制节点-边一致性。

Result: 在合成和分子图数据集上，VBFN在保真度和多样性方面表现优于基线方法。

Conclusion: VBFN通过结构化精度矩阵和变分提升，有效解决了图生成中节点-边耦合更新的问题，提升了生成质量。

Abstract: Graph generation aims to sample discrete node and edge attributes while satisfying coupled structural constraints. Diffusion models for graphs often adopt largely factorized forward-noising, and many flow-matching methods start from factorized reference noise and coordinate-wise interpolation, so node-edge coupling is not encoded by the generative geometry and must be recovered implicitly by the core network, which can be brittle after discrete decoding. Bayesian Flow Networks (BFNs) evolve distribution parameters and naturally support discrete generation. But classical BFNs typically rely on factorized beliefs and independent channels, which limit geometric evidence fusion. We propose Variational Bayesian Flow Network (VBFN), which performs a variational lifting to a tractable joint Gaussian variational belief family governed by structured precisions. Each Bayesian update reduces to solving a symmetric positive definite linear system, enabling coupled node and edge updates within a single fusion step. We construct sample-agnostic sparse precisions from a representation-induced dependency graph, thereby avoiding label leakage while enforcing node-edge consistency. On synthetic and molecular graph datasets, VBFN improves fidelity and diversity, and surpasses baseline methods.

</details>


### [78] [Learn from A Rationalist: Distilling Intermediate Interpretable Rationales](https://arxiv.org/abs/2601.22531)
*Jiayi Dai,Randy Goebel*

Main category: cs.LG

TL;DR: 提出REKD方法，通过知识蒸馏让小型学生RE模型从教师模型的预测和rationale中学习，提升预测性能


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在关键领域应用广泛，但可解释性需求日益重要。Rationale extraction（RE）通过select-predict架构提供可解释性设计，但小型神经网络搜索特征组合空间计算困难，预测性能受限。

Method: 提出REKD（Rationale Extraction with Knowledge Distillation）方法，让学生RE模型不仅从最终任务预测学习，还从教师模型（rationalist）的rationale和预测中学习。该方法与神经网络模型无关，任何黑盒神经网络都可作为骨干模型。

Result: 在语言和视觉分类数据集（IMDB电影评论、CIFAR 10和CIFAR 100）上实验，使用BERT和ViT变体模型，REKD显著提升了学生RE模型的预测性能。

Conclusion: REKD通过知识蒸馏有效提升了小型RE模型的预测能力，同时保持了可解释性设计框架，为深度神经网络的可解释性研究提供了新思路。

Abstract: Because of the pervasive use of deep neural networks (DNNs), especially in high-stakes domains, the interpretability of DNNs has received increased attention. The general idea of rationale extraction (RE) is to provide an interpretable-by-design framework for DNNs via a select-predict architecture where two neural networks learn jointly to perform feature selection and prediction, respectively. Given only the remote supervision from the final task prediction, the process of learning to select subsets of features (or \emph{rationales}) requires searching in the space of all possible feature combinations, which is computationally challenging and even harder when the base neural networks are not sufficiently capable. To improve the predictive performance of RE models that are based on less capable or smaller neural networks (i.e., the students), we propose \textbf{REKD} (\textbf{R}ationale \textbf{E}xtraction with \textbf{K}nowledge \textbf{D}istillation) where a student RE model learns from the rationales and predictions of a teacher (i.e., a \emph{rationalist}) in addition to the student's own RE optimization. This structural adjustment to RE aligns well with how humans could learn effectively from interpretable and verifiable knowledge. Because of the neural-model agnostic nature of the method, any black-box neural network could be integrated as a backbone model. To demonstrate the viability of REKD, we conduct experiments with multiple variants of BERT and vision transformer (ViT) models. Our experiments across language and vision classification datasets (i.e., IMDB movie reviews, CIFAR 10 and CIFAR 100) show that REKD significantly improves the predictive performance of the student RE models.

</details>


### [79] [Demystifying Design Choices of Reinforcement Fine-tuning: A Batched Contextual Bandit Learning Perspective](https://arxiv.org/abs/2601.22532)
*Hong Xie,Xiao Hu,Tao Tan,Haoran Gu,Xin Li,Jianyu Han,Defu Lian,Enhong Chen*

Main category: cs.LG

TL;DR: 本文系统分析了强化学习微调中的设计选择，通过构建最小化基线来解耦各因素影响，识别关键设计要素及其对学习和泛化的作用。


<details>
  <summary>Details</summary>
Motivation: 强化学习微调领域涌现大量论文，但设计选择优化常导致不一致结论，缺乏对两个基本问题的原则性回答：1) 每个设计选择的作用是什么？2) 哪些是关键设计选择？

Method: 构建最小化基线（每轮查询一次rollout、使用结果奖励作为训练信号、无优势技巧、批量大小32），将其与批量上下文赌博机学习关联，设计实验流程，分析优势、rollout数量等因素的边际增益。

Result: 在三个基础模型和两个数据集上的实验揭示了各种设计选择对学习和泛化动态的新理解，并识别出值得更多努力的关键设计选择。

Conclusion: 通过系统解耦设计选择因素，本文为强化学习微调提供了更清晰的设计原则，识别出关键影响因素，有助于该领域的更稳健发展。

Abstract: The reinforcement fine-tuning area is undergoing an explosion papers largely on optimizing design choices. Though performance gains are often claimed, inconsistent conclusions also arise from time to time, making the progress illusive. Reflecting on this illusion, we still lack principled answers to two fundamental questions: 1) what is the role of each design choice? 2) which ones are critical? This paper aims to shed light on them. The underlying challenge is that design choices are entangled together, making their contribution to learning and generalization difficult to attribute. To address this challenge, we first construct a minimalist baseline for disentangling factors: one rollout per query in each round, the outcome reward serving as the training signal without any advantage trick, and a batch size of thirty-two. This baseline connects to batched contextual bandit learning, which facilitates experimental analysis. Centering around this baseline, we design an experiment pipeline, examining the marginal gains of factors like advantage, number of rollouts, etc. Experiments on three base models and two datasets, not only reveal new understanding on the role of various design choices on learning and generalization dynamics, but also identify critical ones that deserve more effort.

</details>


### [80] [Learning to Defer in Non-Stationary Time Series via Switching State-Space Models](https://arxiv.org/abs/2601.22538)
*Yannis Montreuil,Letian Yu,Axel Carlier,Lai Xing Ng,Wei Tsang Ooi*

Main category: cs.LG

TL;DR: 提出L2D-SLDS模型处理非平稳时间序列的延迟学习问题，通过切换线性高斯状态空间模型建模专家残差，包含共享全局因子和专家特定状态，支持专家动态注册，并提出基于信息导向的路径选择规则。


<details>
  <summary>Details</summary>
Motivation: 解决非平稳时间序列中部分反馈和专家可用性随时间变化情况下的延迟学习问题，传统方法难以处理专家动态变化和跨专家信息共享。

Method: 提出L2D-SLDS模型：因子化切换线性高斯状态空间模型，包含上下文相关的机制转换、共享全局因子（实现跨专家信息传递）、专家特定状态，支持专家动态注册和剪枝，并提出基于一步预测信念的IDS启发路径选择规则。

Result: 实验表明该方法优于上下文多臂老虎机基线和无共享因子消融模型，在非平稳时间序列延迟学习任务中表现更好。

Conclusion: L2D-SLDS模型能有效处理非平稳时间序列中的延迟学习问题，通过共享全局因子实现跨专家信息传递，动态专家注册机制适应专家可用性变化，信息导向路径规则平衡成本与信息获取。

Abstract: We study Learning to Defer for non-stationary time series with partial feedback and time-varying expert availability. At each time step, the router selects an available expert, observes the target, and sees only the queried expert's prediction. We model signed expert residuals using L2D-SLDS, a factorized switching linear-Gaussian state-space model with context-dependent regime transitions, a shared global factor enabling cross-expert information transfer, and per-expert idiosyncratic states. The model supports expert entry and pruning via a dynamic registry. Using one-step-ahead predictive beliefs, we propose an IDS-inspired routing rule that trades off predicted cost against information gained about the latent regime and shared factor. Experiments show improvements over contextual-bandit baselines and a no-shared-factor ablation.

</details>


### [81] [Neural-Inspired Posterior Approximation (NIPA)](https://arxiv.org/abs/2601.22539)
*Babak Shahbaba,Zahra Moslemi*

Main category: cs.LG

TL;DR: 提出一种受人类多系统学习启发的贝叶斯采样算法，包含模型导向、无模型和情景控制三个模块，用于大规模统计机器学习中的不确定性量化


<details>
  <summary>Details</summary>
Motivation: 人类通过多个相互作用的神经系统的协同工作实现高效学习，包括模型导向规划、无模型习惯性响应和情景记忆学习。作者希望借鉴这种生物效率的计算原理，开发可扩展的贝叶斯推断采样算法

Method: 提出包含三个组件的采样算法：1) 模型导向模块：使用目标分布进行引导但计算较慢的采样；2) 无模型模块：利用先前样本学习参数空间模式，实现快速反射式采样而无需直接评估昂贵的目标分布；3) 情景控制模块：通过回忆特定过去事件（样本）支持快速采样

Result: 该方法推进了贝叶斯方法的发展，并促进其在大规模统计机器学习问题中的应用，特别是在贝叶斯深度学习中进行适当和原则性的不确定性量化

Conclusion: 受人类多系统学习启发的三模块采样算法为大规模贝叶斯推断提供了一种高效的计算框架，特别适用于需要不确定性量化的深度学习应用

Abstract: Humans learn efficiently from their environment by engaging multiple interacting neural systems that support distinct yet complementary forms of control, including model-based (goal-directed) planning, model-free (habitual) responding, and episodic memory-based learning. Model-based mechanisms compute prospective action values using an internal model of the environment, supporting flexible but computationally costly planning; model-free mechanisms cache value estimates and build heuristics that enable fast, efficient habitual responding; and memory-based mechanisms allow rapid adaptation from individual experience. In this work, we aim to elucidate the computational principles underlying this biological efficiency and translate them into a sampling algorithm for scalable Bayesian inference through effective exploration of the posterior distribution. More specifically, our proposed algorithm comprises three components: a model-based module that uses the target distribution for guided but computationally slow sampling; a model-free module that uses previous samples to learn patterns in the parameter space, enabling fast, reflexive sampling without directly evaluating the expensive target distribution; and an episodic-control module that supports rapid sampling by recalling specific past events (i.e., samples). We show that this approach advances Bayesian methods and facilitates their application to large-scale statistical machine learning problems. In particular, we apply our proposed framework to Bayesian deep learning, with an emphasis on proper and principled uncertainty quantification.

</details>


### [82] [Benchmarking Long Roll-outs of Auto-regressive Neural Operators for the Compressible Navier-Stokes Equations with Conserved Quantity Correction](https://arxiv.org/abs/2601.22541)
*Sean Current,Chandan Kumar,Datta Gaitonde,Srinivasan Parthasarathy*

Main category: cs.LG

TL;DR: 提出一种模型无关的守恒量校正技术，通过融入物理守恒准则来改善神经算子在长期预测中的稳定性问题


<details>
  <summary>Details</summary>
Motivation: 深度学习用于PDE数值解时，在长期预测中因自回归误差累积和无法守恒物理量而表现不佳，需要解决这些稳定性问题

Method: 提出守恒量校正技术，这是一种模型无关的方法，将物理守恒准则融入深度学习模型中

Result: 该方法能一致改善自回归神经算子模型的长期稳定性，与模型架构无关；同时从谱域分析发现现有架构在高频分量处理上存在显著局限

Conclusion: 需要设计能特别关注高频分量的架构，这对理解和建模湍流等复杂流动至关重要

Abstract: Deep learning has been proposed as an efficient alternative for the numerical approximation of PDE solutions, offering fast, iterative simulation of PDEs through the approximation of solution operators. However, deep learning solutions have struggle to perform well over long prediction durations due to the accumulation of auto-regressive error, which is compounded by the inability of models to conserve physical quantities. In this work, we present conserved quantity correction, a model-agnostic technique for incorporation physical conservation criteria within deep learning models. Our results demonstrate consistent improvement in the long-term stability of auto-regressive neural operator models, regardless of the model architecture. Furthermore, we analyze the performance of neural operators from the spectral domain, highlighting significant limitations of present architectures. These results highlight the need for future work to consider architectures that place specific emphasis on high frequency components, which are integral to the understanding and modeling of turbulent flows.

</details>


### [83] [EUGens: Efficient, Unified, and General Dense Layers](https://arxiv.org/abs/2601.22563)
*Sang Min Kim,Byeongchan Kim,Arijit Sehanobish,Somnath Basu Roy Chowdhury,Rahul Kidambi,Dongseok Shim,Avinava Dubey,Snigdha Chaturvedi,Min-hwan Oh,Krzysztof Choromanski*

Main category: cs.LG

TL;DR: 提出EUGens层，一种高效、统一、通用的密集层，通过随机特征和输入范数依赖，将全连接层复杂度从二次降到线性，同时保持表达能力。


<details>
  <summary>Details</summary>
Motivation: 全连接前馈层在神经网络中引入计算和参数瓶颈，限制了模型在实时应用和资源受限环境中的可扩展性。需要更高效的层设计来减少计算开销和参数数量。

Method: 提出EUGens层，利用随机特征近似标准全连接层，并在计算中引入输入范数的直接依赖。该方法统一了现有高效全连接层扩展，并提出了无需反向传播的层间知识迁移技术。

Result: 在Transformer和MLP中集成EUGens层，在图像分类、语言模型预训练和3D场景重建等任务中，推理速度提升高达27%，内存效率提升高达30%。

Conclusion: EUGens层通过将计算复杂度从二次降到线性，显著提升了神经网络的推理效率和内存效率，为大模型在真实场景中的可扩展部署提供了潜力。

Abstract: Efficient neural networks are essential for scaling machine learning models to real-time applications and resource-constrained environments. Fully-connected feedforward layers (FFLs) introduce computation and parameter count bottlenecks within neural network architectures. To address this challenge, in this work, we propose a new class of dense layers that generalize standard fully-connected feedforward layers, \textbf{E}fficient, \textbf{U}nified and \textbf{Gen}eral dense layers (EUGens). EUGens leverage random features to approximate standard FFLs and go beyond them by incorporating a direct dependence on the input norms in their computations. The proposed layers unify existing efficient FFL extensions and improve efficiency by reducing inference complexity from quadratic to linear time. They also lead to \textbf{the first} unbiased algorithms approximating FFLs with arbitrary polynomial activation functions. Furthermore, EuGens reduce the parameter count and computational overhead while preserving the expressive power and adaptability of FFLs. We also present a layer-wise knowledge transfer technique that bypasses backpropagation, enabling efficient adaptation of EUGens to pre-trained models. Empirically, we observe that integrating EUGens into Transformers and MLPs yields substantial improvements in inference speed (up to \textbf{27}\%) and memory efficiency (up to \textbf{30}\%) across a range of tasks, including image classification, language model pre-training, and 3D scene reconstruction. Overall, our results highlight the potential of EUGens for the scalable deployment of large-scale neural networks in real-world scenarios.

</details>


### [84] [FedDis: A Causal Disentanglement Framework for Federated Traffic Prediction](https://arxiv.org/abs/2601.22578)
*Chengyang Zhou,Zijian Zhang,Chunxu Zhang,Hao Miao,Yulin Zhang,Kedi Lyu,Juncheng Hu*

Main category: cs.LG

TL;DR: FedDis：首个利用因果解耦进行联邦时空预测的框架，通过双分支设计分离客户端特定因素和全局模式，解决非IID数据挑战


<details>
  <summary>Details</summary>
Motivation: 联邦学习在隐私保护的交通预测中面临非独立同分布数据的挑战，现有方法难以区分全局共享模式和客户端特定动态，导致性能受限

Method: 提出FedDis框架，采用双分支设计：个性化银行捕获客户端特定因素，全局模式银行提取共同知识；使用互信息最小化目标确保分支间的信息正交性

Result: 在四个真实世界基准数据集上的实验表明，FedDis持续实现最先进性能，具有高效性和优越的可扩展性

Conclusion: FedDis通过因果解耦有效解决了联邦时空预测中的数据异质性问题，在保持隐私的同时实现了更好的知识迁移和本地适应性

Abstract: Federated learning offers a promising paradigm for privacy-preserving traffic prediction, yet its performance is often challenged by the non-identically and independently distributed (non-IID) nature of decentralized traffic data. Existing federated methods frequently struggle with this data heterogeneity, typically entangling globally shared patterns with client-specific local dynamics within a single representation. In this work, we postulate that this heterogeneity stems from the entanglement of two distinct generative sources: client-specific localized dynamics and cross-client global spatial-temporal patterns. Motivated by this perspective, we introduce FedDis, a novel framework that, to the best of our knowledge, is the first to leverage causal disentanglement for federated spatial-temporal prediction. Architecturally, FedDis comprises a dual-branch design wherein a Personalized Bank learns to capture client-specific factors, while a Global Pattern Bank distills common knowledge. This separation enables robust cross-client knowledge transfer while preserving high adaptability to unique local environments. Crucially, a mutual information minimization objective is employed to enforce informational orthogonality between the two branches, thereby ensuring effective disentanglement. Comprehensive experiments conducted on four real-world benchmark datasets demonstrate that FedDis consistently achieves state-of-the-art performance, promising efficiency, and superior expandability.

</details>


### [85] [Non-Intrusive Graph-Based Bot Detection for E-Commerce Using Inductive Graph Neural Networks](https://arxiv.org/abs/2601.22579)
*Sichen Zhao,Zhiming Xue,Yalun Qi,Xianling Zeng,Zihan Yu*

Main category: cs.LG

TL;DR: 提出基于图神经网络的非侵入式电商恶意机器人检测框架，通过建模用户会话行为图，准确识别自动化活动，优于传统方法且部署友好。


<details>
  <summary>Details</summary>
Motivation: 电商平台面临恶意机器人日益严重的威胁（数据爬取、库存囤积、欺诈），传统IP黑名单和验证码等方法效果有限且侵入性强，现代机器人使用代理、僵尸网络和AI规避策略。

Method: 提出非侵入式图基机器人检测框架：将用户会话行为建模为图表示，应用归纳式图神经网络进行分类，捕捉关系结构和行为语义，无需客户端检测工具。

Result: 在真实电商流量实验中，提出的归纳图模型在AUC和F1分数上优于会话级多层感知机基线；对抗扰动和冷启动模拟显示模型对适度图修改具有鲁棒性，能有效泛化到未见过的会话和URL。

Conclusion: 该框架部署友好，可与现有系统集成而不需客户端检测，支持实时推理和增量更新，适合实际电商安全部署，能有效检测规避传统方法的自动化活动。

Abstract: Malicious bots pose a growing threat to e-commerce platforms by scraping data, hoarding inventory, and perpetrating fraud. Traditional bot mitigation techniques, including IP blacklists and CAPTCHA-based challenges, are increasingly ineffective or intrusive, as modern bots leverage proxies, botnets, and AI-assisted evasion strategies. This work proposes a non-intrusive graph-based bot detection framework for e-commerce that models user session behavior through a graph representation and applies an inductive graph neural network for classification. The approach captures both relational structure and behavioral semantics, enabling accurate identification of subtle automated activity that evades feature-based methods. Experiments on real-world e-commerce traffic demonstrate that the proposed inductive graph model outperforms a strong session-level multilayer perceptron baseline in terms of AUC and F1 score. Additional adversarial perturbation and cold-start simulations show that the model remains robust under moderate graph modifications and generalizes effectively to previously unseen sessions and URLs. The proposed framework is deployment-friendly, integrates with existing systems without client-side instrumentation, and supports real-time inference and incremental updates, making it suitable for practical e-commerce security deployments.

</details>


### [86] [MC-GRPO: Median-Centered Group Relative Policy Optimization for Small-Rollout Reinforcement Learning](https://arxiv.org/abs/2601.22582)
*Youngeun Kim*

Main category: cs.LG

TL;DR: 提出MC-GRPO方法，用中位数基线替代均值基线，解决小样本训练中优势值符号翻转问题，提升资源受限场景下的训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的小样本训练场景中，基于群体相对策略优化的方法常因基线噪声导致优势值符号翻转，使某些样本获得错误更新方向，导致性能下降。

Method: 提出中位数中心化群体相对策略优化(MC-GRPO)：用中位数基线替代均值基线，中位数对异常奖励更不敏感；生成G+1个样本用于中位数参考，排除中位数样本的反向传播，保持与标准G样本训练相同的计算成本。

Result: 在各种GRPO系列方法和不同规模模型上，中位数中心化训练在小样本场景下显著提升稳定性和最终准确率，将G=2和G=8之间的性能差距缩小到1%以内。

Conclusion: MC-GRPO通过简单有效的中位数基线替换，解决了小样本训练中的优势值符号翻转问题，在保持计算成本不变的情况下显著提升了训练稳定性和性能。

Abstract: Group-relative policy optimization methods train language models by generating multiple rollouts per prompt and normalizing rewards with a shared mean reward baseline. In resource-constrained settings where the rollout budget is small, accuracy often degrades. We find that noise in the shared baseline induces advantage sign flips, where some rollouts receive an incorrect advantage sign, and the update direction is reversed. To address this, we propose Median-Centered Group Relative Policy Optimization (MC-GRPO), a simple and effective solution for small-rollout training. Our main idea is to replace the mean baseline with a median baseline: the median is far less sensitive to outlier rewards than the mean, mitigating the sign flips under small rollout size (G). We generate one additional rollout for median reference (G+1), and compute advantages by using the group median. With an odd-sized group, exactly one completion is the median and receives zero advantage, we exclude this pivot rollout from backpropagation so the number of gradient-contributing samples per prompt remains G, preserving the core update cost of standard G-rollout training. Across various GRPO-family methods and a wide range of models and scales, this median-centered training consistently improves stability and final accuracy in the low-rollout regime, reducing the gap between G=2 and G=8 to within 1%. Code is available at https://github.com/lotusroot-kim/MC-GRPO

</details>


### [87] [FedCARE: Federated Unlearning with Conflict-Aware Projection and Relearning-Resistant Recovery](https://arxiv.org/abs/2601.22589)
*Yue Li,Mingmin Chu,Xilei Yang,Da Xiao,Ziqi Xu,Wei Shao,Qipeng Song,Hui Li*

Main category: cs.LG

TL;DR: FedCARE是一个联邦遗忘学习框架，通过梯度上升和模型反演实现高效遗忘，同时防止重新学习，支持客户端、实例和类别级别的遗忘。


<details>
  <summary>Details</summary>
Motivation: 联邦学习需要遵守"被遗忘权"等隐私法规，但现有联邦遗忘方法存在遗忘开销大、效用下降、知识纠缠和重新学习等问题。

Method: 使用梯度上升进行高效遗忘，通过数据无关的模型反演构建类别级知识代理，结合伪样本生成器、冲突感知投影梯度上升和抑制回滚的恢复策略。

Result: 在多个数据集和模型架构上，FedCARE在IID和非IID设置下都能实现有效遗忘、更好的效用保持和更低的重新学习风险。

Conclusion: FedCARE提供了一个统一且低开销的联邦遗忘框架，支持多粒度遗忘，在遗忘效果和模型效用之间取得了良好平衡。

Abstract: Federated learning (FL) enables collaborative model training without centralizing raw data, but privacy regulations such as the right to be forgotten require FL systems to remove the influence of previously used training data upon request. Retraining a federated model from scratch is prohibitively expensive, motivating federated unlearning (FU). However, existing FU methods suffer from high unlearning overhead, utility degradation caused by entangled knowledge, and unintended relearning during post-unlearning recovery. In this paper, we propose FedCARE, a unified and low overhead FU framework that enables conflict-aware unlearning and relearning-resistant recovery. FedCARE leverages gradient ascent for efficient forgetting when target data are locally available and employs data free model inversion to construct class level proxies of shared knowledge. Based on these insights, FedCARE integrates a pseudo-sample generator, conflict-aware projected gradient ascent for utility preserving unlearning, and a recovery strategy that suppresses rollback toward the pre-unlearning model. FedCARE supports client, instance, and class level unlearning with modest overhead. Extensive experiments on multiple datasets and model architectures under both IID and non-IID settings show that FedCARE achieves effective forgetting, improved utility retention, and reduced relearning risk compared to state of the art FU baselines.

</details>


### [88] [Heterogeneous Graph Alignment for Joint Reasoning and Interpretability](https://arxiv.org/abs/2601.22593)
*Zahra Moslemi,Ziyi Liang,Norbert Fortin,Babak Shahbaba*

Main category: cs.LG

TL;DR: MGMT是一个统一、可扩展、可解释的多图学习框架，通过图Transformer编码器将不同图映射到共享潜在空间，构建元图连接功能对齐的超节点，在合成和真实神经科学数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多图学习需要从异构图集合中提取有意义信号，但有效整合具有不同拓扑、规模和语义的图信息（通常缺乏共享节点标识）仍然是一个重大挑战。

Method: MGMT首先对每个图应用图Transformer编码器，将结构和属性映射到共享潜在空间；然后通过注意力选择任务相关的超节点，使用潜在空间相似性构建连接跨图功能对齐超节点的元图；最后在元图上应用额外的图Transformer层进行联合推理。

Result: 在合成数据集和真实世界神经科学应用中，MGMT在图级预测任务中始终优于现有最先进模型，同时提供可解释的表示，促进科学发现。

Conclusion: MGMT作为结构化多图学习的统一框架，在图数据起核心作用的领域中推进了表示技术，提供了内置的可解释性（超节点和超边突出显示有影响力的子结构和跨图对齐）。

Abstract: Multi-graph learning is crucial for extracting meaningful signals from collections of heterogeneous graphs. However, effectively integrating information across graphs with differing topologies, scales, and semantics, often in the absence of shared node identities, remains a significant challenge. We present the Multi-Graph Meta-Transformer (MGMT), a unified, scalable, and interpretable framework for cross-graph learning. MGMT first applies Graph Transformer encoders to each graph, mapping structure and attributes into a shared latent space. It then selects task-relevant supernodes via attention and builds a meta-graph that connects functionally aligned supernodes across graphs using similarity in the latent space. Additional Graph Transformer layers on this meta-graph enable joint reasoning over intra- and inter-graph structure. The meta-graph provides built-in interpretability: supernodes and superedges highlight influential substructures and cross-graph alignments. Evaluating MGMT on both synthetic datasets and real-world neuroscience applications, we show that MGMT consistently outperforms existing state-of-the-art models in graph-level prediction tasks while offering interpretable representations that facilitate scientific discoveries. Our work establishes MGMT as a unified framework for structured multi-graph learning, advancing representation techniques in domains where graph-based data plays a central role.

</details>


### [89] [Lethe:Adapter-Augmented Dual-Stream Update for Persistent Knowledge Erasure in Federated Unlearning](https://arxiv.org/abs/2601.22601)
*Hanwei Tan,Wentai Hu,Ligang He,Yijun Quan*

Main category: cs.LG

TL;DR: Lethe提出了一种联邦遗忘方法，通过解耦待遗忘与待保留知识，解决持续训练中知识重现问题，实现持久遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有联邦遗忘研究通常假设遗忘操作后协作结束，忽视了后续持续训练的情况。研究发现持续训练会重新激活已遗忘知识，导致知识重现问题。

Method: Lethe采用Reshape-Rectify-Restore流程：1) 在遗忘数据上训练临时适配器获得放大更新；2) 使用修正信号对剩余更新进行分层校正；3) 移除适配器并在保留数据上进行短期恢复。

Result: Lethe能统一支持联邦系统中所有级别的遗忘，在多数情况下保持极低的知识重现率（<1%），即使在多轮后续训练后仍能保持优越的持久性。

Conclusion: Lethe解决了联邦遗忘中的知识重现问题，通过解耦知识实现了持久遗忘，为联邦学习系统提供了可靠的遗忘机制。

Abstract: Federated unlearning (FU) aims to erase designated client-level, class-level, or sample-level knowledge from a global model. Existing studies commonly assume that the collaboration ends up with the unlearning operation, overlooking the follow-up situation where the federated training continues over the remaining data.We identify a critical failure mode, termed Knowledge resurfacing, by revealing that continued training can re-activate unlearned knowledge and cause the removed influence to resurface in the global model. To address this, we propose Lethe, a novel federated unlearning method that de-correlates knowledge to be unlearned from knowledge to be retained, ensuring persistent erasure during continued training.Lethe follows a Reshape--Rectify--Restore pipeline: a temporary adapter is first trained with gradient ascent on the unlearning data to obtain magnified updates, which is then used as corrective signals to diverge layer-wise rectification on the remaining updates in two streams. Finally, the adapter is removed and a short recovery stage is performed on the retained data. Our experiments show that Lethe supports unlearning in the federated system at all levels in a unified manner and maintains superior persistence (Resurfacing Rate <1% in most cases) even after numerous rounds of follow-up training.

</details>


### [90] [Local-Global Multimodal Contrastive Learning for Molecular Property Prediction](https://arxiv.org/abs/2601.22610)
*Xiayu Liu,Zhengyi Lu,Yunhong Liao,Chan Fan,Hou-biao Li*

Main category: cs.LG

TL;DR: LGM-CL是一个局部-全局多模态对比学习框架，通过联合建模分子图和文本表示来提升分子性质预测精度，在MoleculeNet基准测试中取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 准确的分子性质预测需要整合分子结构和化学语义的互补信息。当前方法往往未能充分利用局部功能基团信息、全局分子拓扑结构以及化学语义的协同作用。

Method: 提出LGM-CL框架：1) 使用AttentiveFP和Graph Transformer分别编码局部功能基团和全局分子拓扑；2) 通过自监督对比学习对齐局部和全局表示；3) 将化学增强的文本描述与原始SMILES进行对比，融入物理化学语义；4) 微调时通过双交叉注意力多模态融合整合分子指纹。

Result: 在MoleculeNet基准测试的广泛实验中，LGM-CL在分类和回归任务上都取得了一致且具有竞争力的性能，验证了统一局部-全局和多模态表示学习的有效性。

Conclusion: LGM-CL通过整合局部功能基团、全局分子拓扑和化学语义的多模态表示，为分子性质预测提供了一个有效的统一框架，证明了多模态对比学习在化学信息学中的潜力。

Abstract: Accurate molecular property prediction requires integrating complementary information from molecular structure and chemical semantics. In this work, we propose LGM-CL, a local-global multimodal contrastive learning framework that jointly models molecular graphs and textual representations derived from SMILES and chemistry-aware augmented texts. Local functional group information and global molecular topology are captured using AttentiveFP and Graph Transformer encoders, respectively, and aligned through self-supervised contrastive learning. In addition, chemically enriched textual descriptions are contrasted with original SMILES to incorporate physicochemical semantics in a task-agnostic manner. During fine-tuning, molecular fingerprints are further integrated via Dual Cross-attention multimodal fusion. Extensive experiments on MoleculeNet benchmarks demonstrate that LGM-CL achieves consistent and competitive performance across both classification and regression tasks, validating the effectiveness of unified local-global and multimodal representation learning.

</details>


### [91] [Stabilizing Transformer Training Through Consensus](https://arxiv.org/abs/2601.22614)
*Shyam Venkatasubramanian,Sean Moushegian,Michael Lin,Mir Park,Ankit Singhal,Connor Lee*

Main category: cs.LG

TL;DR: 共识机制作为注意力替代方案，可提升Transformer训练稳定性，扩大有效学习率范围


<details>
  <summary>Details</summary>
Motivation: 标准注意力Transformer在训练时对学习率过指定表现出不稳定性，特别是在高学习率下。虽然已有方法通过修改优化过程来提高对此类过指定的鲁棒性，但根本性的架构创新仍未被充分探索。

Method: 提出共识机制作为注意力的即插即用替代方案，将其形式化为图模型。进一步提出混合共识-注意力框架，在保持性能的同时提高稳定性。提供理论分析来表征共识机制的特性。

Result: 在文本、DNA和蛋白质模态的学习率扫描实验中，共识机制展现出改进的稳定性。混合共识-注意力框架在保持性能的同时提高了稳定性。

Conclusion: 共识机制能够稳定Transformer训练，扩大有效学习率范围，为注意力机制提供了有前景的替代方案。

Abstract: Standard attention-based transformers are known to exhibit instability under learning rate overspecification during training, particularly at high learning rates. While various methods have been proposed to improve resilience to such overspecification by modifying the optimization procedure, fundamental architectural innovations to this end remain underexplored. In this work, we illustrate that the consensus mechanism, a drop-in replacement for attention, stabilizes transformer training across a wider effective range of learning rates. We formulate consensus as a graphical model and provide extensive empirical analysis demonstrating improved stability across learning rate sweeps on text, DNA, and protein modalities. We further propose a hybrid consensus-attention framework that preserves performance while improving stability. We provide theoretical analysis characterizing the properties of consensus.

</details>


### [92] [TTCS: Test-Time Curriculum Synthesis for Self-Evolving](https://arxiv.org/abs/2601.22628)
*Chengyi Yang,Zhishang Xiang,Yunbo Tang,Zongpei Teng,Chengsong Huang,Fei Long,Yuhan Liu,Jinsong Su*

Main category: cs.LG

TL;DR: TTCS是一个协同进化的测试时训练框架，通过问题合成器和推理求解器的迭代优化，为LLMs构建动态测试时课程，提升在困难推理问题上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有测试时训练方法在困难推理问题上表现不佳，因为原始测试问题太难无法产生高质量伪标签，且测试集规模有限导致连续在线更新不稳定。

Method: TTCS从同一预训练模型初始化两个策略：问题合成器和推理求解器。通过迭代优化，合成器基于测试问题生成渐进挑战性的变体，构建结构化课程；求解器使用自一致性奖励在原始测试和合成问题上更新。两者相互指导，合成器根据求解器反馈生成适合其当前能力的问题。

Result: 实验表明TTCS在具有挑战性的数学基准上持续增强推理能力，并能迁移到不同LLM骨干网络的一般领域任务，展示了为自进化构建动态测试时课程的可扩展路径。

Conclusion: TTCS通过协同进化的测试时训练框架，有效解决了现有方法在困难推理问题上的局限性，为LLMs的动态自我进化提供了有前景的方向。

Abstract: Test-Time Training offers a promising way to improve the reasoning ability of large language models (LLMs) by adapting the model using only the test questions. However, existing methods struggle with difficult reasoning problems for two reasons: raw test questions are often too difficult to yield high-quality pseudo-labels, and the limited size of test sets makes continuous online updates prone to instability. To address these limitations, we propose TTCS, a co-evolving test-time training framework. Specifically, TTCS initializes two policies from the same pretrained model: a question synthesizer and a reasoning solver. These policies evolve through iterative optimization: the synthesizer generates progressively challenging question variants conditioned on the test questions, creating a structured curriculum tailored to the solver's current capability, while the solver updates itself using self-consistency rewards computed from multiple sampled responses on both original test and synthetic questions. Crucially, the solver's feedback guides the synthesizer to generate questions aligned with the model's current capability, and the generated question variants in turn stabilize the solver's test-time training. Experiments show that TTCS consistently strengthens the reasoning ability on challenging mathematical benchmarks and transfers to general-domain tasks across different LLM backbones, highlighting a scalable path towards dynamically constructing test-time curricula for self-evolving. Our code and implementation details are available at https://github.com/XMUDeepLIT/TTCS.

</details>


### [93] [PEFT-MuTS: A Multivariate Parameter-Efficient Fine-Tuning Framework for Remaining Useful Life Prediction based on Cross-domain Time Series Representation Model](https://arxiv.org/abs/2601.22631)
*En Fu,Yanyan Hu,Changhua Hu,Zengwang Jin,Kaixiang Peng*

Main category: cs.LG

TL;DR: 提出PEFT-MuTS框架，通过跨域预训练的时间序列表示模型进行少样本剩余使用寿命预测，显著减少对目标设备数据的需求。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的RUL预测长期受限于需要大量退化数据。现有方法如域适应和元学习仍需要大量相同或相似设备的历史数据，这在实践中限制很大。

Method: 开发了独立特征调优网络和基于元变量的低秩多元融合机制，使预训练的单变量时间序列表示模型能充分利用退化数据中的多元关系。引入零初始化回归器以稳定少样本条件下的微调过程。

Result: 在航空发动机和工业轴承数据集上的实验表明，即使使用目标设备少于1%的样本，也能实现有效的RUL预测。显著优于传统监督和少样本方法，同时大幅减少实现高预测精度所需的数据量。

Conclusion: 通过跨域预训练时间序列模型，可以在设备间实现有效的知识迁移，突破传统认为只能在相似设备间进行知识转移的限制，为少样本RUL预测提供了有效解决方案。

Abstract: The application of data-driven remaining useful life (RUL) prediction has long been constrained by the availability of large amount of degradation data. Mainstream solutions such as domain adaptation and meta-learning still rely on large amounts of historical degradation data from equipment that is identical or similar to the target, which imposes significant limitations in practical applications. This study investigates PEFT-MuTS, a Parameter-Efficient Fine-Tuning framework for few-shot RUL prediction, built on cross-domain pre-trained time-series representation models. Contrary to the widely held view that knowledge transfer in RUL prediction can only occur within similar devices, we demonstrate that substantial benefits can be achieved through pre-training process with large-scale cross-domain time series datasets. A independent feature tuning network and a meta-variable-based low rank multivariate fusion mechanism are developed to enable the pre-trained univariate time-series representation backbone model to fully exploit the multivariate relationships in degradation data for downstream RUL prediction task. Additionally, we introduce a zero-initialized regressor that stabilizes the fine-tuning process under few-shot conditions. Experiments on aero-engine and industrial bearing datasets demonstrate that our method can achieve effective RUL prediction even when less than 1\% of samples of target equipment are used. Meanwhile, it substantially outperforms conventional supervised and few-shot approaches while markedly reducing the data required to achieve high predictive accuracy. Our code is available at https://github.com/fuen1590/PEFT-MuTS.

</details>


### [94] [Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification](https://arxiv.org/abs/2601.22642)
*Chuxue Cao,Jinluan Yang,Haoran Li,Kunhao Pan,Zijian Zhao,Zhengyu Chen,Yuchen Tian,Lijun Wu,Conghui He,Sirui Han,Yike Guo*

Main category: cs.LG

TL;DR: 提出一种结合形式逻辑验证与LLM生成过程的框架，通过实时反馈检测和纠正错误，显著提升推理性能


<details>
  <summary>Details</summary>
Motivation: LLM的随机性导致逻辑不一致和奖励黑客问题，而形式符号系统可以避免这些问题。需要桥接神经符号方法的差距，超越被动事后验证的限制

Method: 1. 形式逻辑验证引导框架：在自然语言生成过程中动态交织形式符号验证，提供实时反馈
2. 两阶段训练管道：结合形式逻辑验证引导的监督微调和策略优化
3. 主动惩罚推理链中的中间谬误

Result: 在6个数学、逻辑和一般推理基准测试中，7B和14B模型分别以平均10.4%和14.2%的优势超越最先进基线

Conclusion: 形式验证可以作为可扩展机制，显著推进先进LLM推理的性能边界

Abstract: Large Language Models (LLMs) show remarkable capabilities, yet their stochastic next-token prediction creates logical inconsistencies and reward hacking that formal symbolic systems avoid. To bridge this gap, we introduce a formal logic verification-guided framework that dynamically interleaves formal symbolic verification with the natural language generation process, providing real-time feedback to detect and rectify errors as they occur. Distinguished from previous neuro-symbolic methods limited by passive post-hoc validation, our approach actively penalizes intermediate fallacies during the reasoning chain. We operationalize this framework via a novel two-stage training pipeline that synergizes formal logic verification-guided supervised fine-tuning and policy optimization. Extensive evaluation on six benchmarks spanning mathematical, logical, and general reasoning demonstrates that our 7B and 14B models outperform state-of-the-art baselines by average margins of 10.4% and 14.2%, respectively. These results validate that formal verification can serve as a scalable mechanism to significantly push the performance boundaries of advanced LLM reasoning.

</details>


### [95] [GUDA: Counterfactual Group-wise Training Data Attribution for Diffusion Models via Unlearning](https://arxiv.org/abs/2601.22651)
*Naoki Murata,Yuhta Takida,Chieh-Hsin Lai,Toshimitsu Uesaka,Bac Nguyen,Stefano Ermon,Yuki Mitsufuji*

Main category: cs.LG

TL;DR: GUDA是一种用于扩散模型的群体级训练数据归因方法，通过机器遗忘技术近似反事实模型，相比重新训练实现100倍加速，能更可靠地识别主要贡献群体。


<details>
  <summary>Details</summary>
Motivation: 现有训练数据归因方法主要针对单个样本评分，但实际应用中需要群体层面的答案（如艺术风格或对象类别）。群体归因是反事实的：如果某个群体从训练数据中移除，模型在生成样本上的行为会如何变化？虽然留一组出（LOGO）重新训练是自然的实现方式，但随着群体数量增加，计算成本变得过高。

Method: 提出GUDA（基于群体遗忘的数据归因）方法，通过机器遗忘技术对共享的全数据模型应用遗忘操作来近似每个反事实模型，而不是从头训练。使用基于似然的评分规则（ELBO）在全模型和每个遗忘反事实模型之间的差异来量化群体影响。

Result: 在CIFAR-10和Stable Diffusion艺术风格归因实验中，GUDA比语义相似性、基于梯度的归因和实例级遗忘方法更可靠地识别主要贡献群体，同时在CIFAR-10上相比LOGO重新训练实现了100倍的加速。

Conclusion: GUDA为扩散模型提供了一种高效且有效的群体级训练数据归因方法，通过机器遗忘技术近似反事实模型，显著降低了计算成本，同时保持了归因的可靠性。

Abstract: Training-data attribution for vision generative models aims to identify which training data influenced a given output. While most methods score individual examples, practitioners often need group-level answers (e.g., artistic styles or object classes). Group-wise attribution is counterfactual: how would a model's behavior on a generated sample change if a group were absent from training? A natural realization of this counterfactual is Leave-One-Group-Out (LOGO) retraining, which retrains the model with each group removed; however, it becomes computationally prohibitive as the number of groups grows. We propose GUDA (Group Unlearning-based Data Attribution) for diffusion models, which approximates each counterfactual model by applying machine unlearning to a shared full-data model instead of training from scratch. GUDA quantifies group influence using differences in a likelihood-based scoring rule (ELBO) between the full model and each unlearned counterfactual. Experiments on CIFAR-10 and artistic style attribution with Stable Diffusion show that GUDA identifies primary contributing groups more reliably than semantic similarity, gradient-based attribution, and instance-level unlearning approaches, while achieving x100 speedup on CIFAR-10 over LOGO retraining.

</details>


### [96] [Layerwise Progressive Freezing Enables STE-Free Training of Deep Binary Neural Networks](https://arxiv.org/abs/2601.22660)
*Evan Gibson Smith,Bashima Islam*

Main category: cs.LG

TL;DR: 本文提出StoMPP方法，使用层级随机掩码渐进式冻结来训练二进制神经网络，避免了直通估计器的问题，在多个数据集和网络深度上显著提升准确率。


<details>
  <summary>Details</summary>
Motivation: 研究渐进式冻结作为直通估计器的替代方案来训练二进制神经网络，发现全局渐进式冻结在二进制权重网络中有效，但在全二进制神经网络中因激活引起的梯度阻塞而失败。

Method: 提出StoMPP方法：使用层级随机掩码渐进式替换可微分的裁剪权重/激活为硬二进制阶跃函数，仅通过未冻结（裁剪）子集进行反向传播，不使用直通估计器。

Result: 在匹配的最小训练方案下，StoMPP相比BinaryConnect风格的STE基线显著提升准确率，增益随深度增加而增大（如ResNet-50 BNN：CIFAR-10 +18.0，CIFAR-100 +13.5，ImageNet +3.8）。二进制权重网络在CIFAR-10上达到91.2%，CIFAR-100上达到69.5%。

Conclusion: StoMPP方法有效解决了全二进制神经网络训练中的梯度阻塞问题，通过渐进式冻结分析揭示了非单调收敛特性和在二值化约束下改进的深度缩放能力。

Abstract: We investigate progressive freezing as an alternative to straight-through estimators (STE) for training binary networks from scratch. Under controlled training conditions, we find that while global progressive freezing works for binary-weight networks, it fails for full binary neural networks due to activation-induced gradient blockades. We introduce StoMPP (Stochastic Masked Partial Progressive Binarization), which uses layerwise stochastic masking to progressively replace differentiable clipped weights/activations with hard binary step functions, while only backpropagating through the unfrozen (clipped) subset (i.e., no straight-through estimator). Under a matched minimal training recipe, StoMPP improves accuracy over a BinaryConnect-style STE baseline, with gains that increase with depth (e.g., for ResNet-50 BNN: +18.0 on CIFAR-10, +13.5 on CIFAR-100, and +3.8 on ImageNet; for ResNet-18: +3.1, +4.7, and +1.3). For binary-weight networks, StoMPP achieves 91.2\% accuracy on CIFAR-10 and 69.5\% on CIFAR-100 with ResNet-50. We analyze training dynamics under progressive freezing, revealing non-monotonic convergence and improved depth scaling under binarization constraints.

</details>


### [97] [Beyond Fixed Rounds: Data-Free Early Stopping for Practical Federated Learning](https://arxiv.org/abs/2601.22669)
*Youngjoon Lee,Hyukjoon Lee,Seungrok Jung,Andy Luo,Jinu Gong,Yang Cao,Joonhyuk Kang*

Main category: cs.LG

TL;DR: 提出一种无需验证数据的联邦学习早期停止框架，通过监控任务向量增长率确定最优停止点，仅使用服务器端参数，在皮肤病变/血细胞分类任务上表现优于基于验证数据的早期停止方法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然支持去中心化协作学习，但依赖固定全局轮次或验证数据进行超参数调优会导致高计算成本和隐私风险，阻碍实际部署。

Method: 提出数据无关的早期停止框架，通过监控任务向量的增长率来确定最优停止点，仅使用服务器端参数，无需任何验证数据。

Result: 在皮肤病变和血细胞分类任务上，该框架平均仅需47/20轮即可达到比基于验证数据的早期停止方法高12.5%/10.3%的性能，且与各种最先进FL方法的验证基早期停止效果相当。

Conclusion: 这是首个无需验证数据的联邦学习早期停止框架，通过监控服务器端任务向量增长率有效解决了计算成本和隐私问题，为FL的实际部署提供了实用解决方案。

Abstract: Federated Learning (FL) facilitates decentralized collaborative learning without transmitting raw data. However, reliance on fixed global rounds or validation data for hyperparameter tuning hinders practical deployment by incurring high computational costs and privacy risks. To address this, we propose a data-free early stopping framework that determines the optimal stopping point by monitoring the task vector's growth rate using solely server-side parameters. The numerical results on skin lesion/blood cell classification demonstrate that our approach is comparable to validation-based early stopping across various state-of-the-art FL methods. In particular, the proposed framework spends an average of 47/20 (skin lesion/blood cell) rounds to achieve over 12.5%/10.3% higher performance than early stopping based on validation data. To the best of our knowledge, this is the first work to propose an early stopping framework for FL methods without using any validation data.

</details>


### [98] [Full-Graph vs. Mini-Batch Training: Comprehensive Analysis from a Batch Size and Fan-Out Size Perspective](https://arxiv.org/abs/2601.22678)
*Mengfan Liu,Da Zheng,Junwei Su,Chuan Wu*

Main category: cs.LG

TL;DR: 本文系统比较了全图训练与mini-batch训练在图神经网络中的性能差异，通过批大小和扇出大小的视角，揭示了这两个超参数对收敛和泛化的非各向同性影响。


<details>
  <summary>Details</summary>
Motivation: 全图训练和mini-batch训练在图神经网络中具有不同的系统设计需求，但缺乏对这两种训练方法在模型性能（收敛和泛化）和计算效率方面的系统比较。传统深度神经网络中批大小是重要分析维度，但GNNs引入了扇出大小这一新维度，其影响尚未充分探索。

Method: 通过实证和理论分析，从批大小和扇出大小的角度系统比较全图与mini-batch训练。使用Wasserstein距离进行泛化分析以研究图结构（特别是扇出大小）的影响，并分析批大小和扇出大小在GNN收敛和泛化中的非各向同性效应。

Result: 研究发现批大小和扇出大小对GNN收敛和泛化具有非各向同性影响，为在资源约束下调整这些超参数提供了实用指导。全图训练并不总是比调优良好的较小mini-batch设置产生更好的模型性能或计算效率。

Conclusion: 全图训练并非总是最优选择，通过适当调整批大小和扇出大小，mini-batch训练可以在模型性能和计算效率方面达到甚至超越全图训练的效果。这为GNN训练策略选择提供了新的理论依据和实践指导。

Abstract: Full-graph and mini-batch Graph Neural Network (GNN) training approaches have distinct system design demands, making it crucial to choose the appropriate approach to develop. A core challenge in comparing these two GNN training approaches lies in characterizing their model performance (i.e., convergence and generalization) and computational efficiency. While a batch size has been an effective lens in analyzing such behaviors in deep neural networks (DNNs), GNNs extend this lens by introducing a fan-out size, as full-graph training can be viewed as mini-batch training with the largest possible batch size and fan-out size. However, the impact of the batch and fan-out size for GNNs remains insufficiently explored. To this end, this paper systematically compares full-graph vs. mini-batch training of GNNs through empirical and theoretical analyses from the view points of the batch size and fan-out size. Our key contributions include: 1) We provide a novel generalization analysis using the Wasserstein distance to study the impact of the graph structure, especially the fan-out size. 2) We uncover the non-isotropic effects of the batch size and the fan-out size in GNN convergence and generalization, providing practical guidance for tuning these hyperparameters under resource constraints. Finally, full-graph training does not always yield better model performance or computational efficiency than well-tuned smaller mini-batch settings. The implementation can be found in the github link: https://github.com/LIUMENGFAN-gif/GNN_fullgraph_minibatch_training.

</details>


### [99] [Stabilizing Consistency Training: A Flow Map Analysis and Self-Distillation](https://arxiv.org/abs/2601.22679)
*Youngjoong Kim,Duhoe Kim,Woosung Kim,Jaesik Park*

Main category: cs.LG

TL;DR: 本文从流映射角度理论分析一致性模型，揭示训练不稳定和收敛问题的根源，提出改进的自蒸馏方法，并扩展到扩散策略学习


<details>
  <summary>Details</summary>
Motivation: 一致性模型虽然能实现快速生成建模，但在从头训练时存在不稳定性和可复现性差的问题。现有研究对此的解释较为零散，理论关系不清晰，需要系统性的理论分析

Method: 从流映射角度对一致性模型进行理论分析，揭示训练稳定性和收敛行为的机制。基于此重新审视自蒸馏方法，改进其形式以避免梯度爆炸，实现稳定优化

Result: 理论分析阐明了导致次优收敛的退化解产生机制。改进的自蒸馏方法能有效稳定训练，且该方法可扩展到图像生成之外的扩散策略学习领域，无需预训练扩散模型初始化

Conclusion: 通过流映射视角的系统理论分析，澄清了一致性模型的训练稳定性问题，提出的改进自蒸馏方法具有更广泛的适用性，为快速生成建模提供了更稳健的解决方案

Abstract: Consistency models have been proposed for fast generative modeling, achieving results competitive with diffusion and flow models. However, these methods exhibit inherent instability and limited reproducibility when training from scratch, motivating subsequent work to explain and stabilize these issues. While these efforts have provided valuable insights, the explanations remain fragmented, and the theoretical relationships remain unclear. In this work, we provide a theoretical examination of consistency models by analyzing them from a flow map-based perspective. This joint analysis clarifies how training stability and convergence behavior can give rise to degenerate solutions. Building on these insights, we revisit self-distillation as a practical remedy for certain forms of suboptimal convergence and reformulate it to avoid excessive gradient norms for stable optimization. We further demonstrate that our strategy extends beyond image generation to diffusion-based policy learning, without reliance on a pretrained diffusion model for initialization, thereby illustrating its broader applicability.

</details>


### [100] [Do Transformers Have the Ability for Periodicity Generalization?](https://arxiv.org/abs/2601.22690)
*Huanyu Liu,Ge Li,Yihong Dong,Sihan Wu,Peixu Wang,Sihao Cheng,Taozhi Chen,Kechi Zhang,Hao Zhu,Tongxuan Liu*

Main category: cs.LG

TL;DR: Transformer模型在周期性OOD泛化上存在局限，能记忆训练数据但无法泛化到未见复合周期性模式


<details>
  <summary>Details</summary>
Motivation: 当前基于Transformer的大语言模型在分布外泛化能力上仍显著落后于人类，需要研究周期性这一基本OOD场景来理解这一差距

Method: 从抽象代数和推理角度统一解释周期性（包括单一和复合周期性），构建Coper基准测试（包含Hollow和Extrapolation两种OOD设置）

Result: 实验表明Transformer在周期性泛化上能力有限，模型能记忆训练数据中的周期性模式，但无法泛化到未见复合周期性

Conclusion: Transformer在周期性OOD泛化方面存在根本性局限，需要进一步研究来提升模型在分布外场景的泛化能力

Abstract: Large language models (LLMs) based on the Transformer have demonstrated strong performance across diverse tasks. However, current models still exhibit substantial limitations in out-of-distribution (OOD) generalization compared with humans. We investigate this gap through periodicity, one of the basic OOD scenarios. Periodicity captures invariance amid variation. Periodicity generalization represents a model's ability to extract periodic patterns from training data and generalize to OOD scenarios. We introduce a unified interpretation of periodicity from the perspective of abstract algebra and reasoning, including both single and composite periodicity, to explain why Transformers struggle to generalize periodicity. Then we construct Coper about composite periodicity, a controllable generative benchmark with two OOD settings, Hollow and Extrapolation. Experiments reveal that periodicity generalization in Transformers is limited, where models can memorize periodic data during training, but cannot generalize to unseen composite periodicity. We release the source code to support future research.

</details>


### [101] [Metric Hub: A metric library and practical selection workflow for use-case-driven data quality assessment in medical AI](https://arxiv.org/abs/2601.22702)
*Katinka Becker,Maximilian P. Oppelt,Tobias S. Zech,Martin Seyferth,Sandie Cabon,Vanja Miskovic,Ivan Cimrak,Michal Kozubek,Giuseppe D'Avenio,Ilaria Campioni,Jana Fehr,Kanjar De,Ismail Mahmoudi,Emilio Dolgener Cantu,Laurenz Ottmann,Andreas Klaß,Galaad Altares,Jackie Ma,Alireza Salehi M.,Nadine R. Lang-Richter,Tobias Schaeffter,Daniel Schwabe*

Main category: cs.LG

TL;DR: 论文提出了METRIC框架的实践化实现——一个用于评估医疗AI数据质量的度量库，包含详细的度量卡片和决策树，并在ECG数据集上进行了验证。


<details>
  <summary>Details</summary>
Motivation: 医疗AI从研究走向实际应用，需要建立可信赖性。数据质量评估是可信AI发展的关键因素，需要系统化的实践方法来评估数据是否适合特定医疗任务。

Method: 将理论框架METRIC操作化为数据质量度量库，为每个度量提供包含定义、适用性、示例、陷阱和建议的度量卡片，并提供决策树帮助选择适合特定用例的度量集合。

Result: 在PTB-XL心电图数据集上展示了该方法的实际影响，验证了框架的有效性，为实践中评估训练和测试数据的适用性提供了工具。

Conclusion: 这是实现医疗领域可信AI的重要第一步，通过系统化的数据质量评估方法，为医疗AI的临床接受、监管批准和有效采用奠定了基础。

Abstract: Machine learning (ML) in medicine has transitioned from research to concrete applications aimed at supporting several medical purposes like therapy selection, monitoring and treatment. Acceptance and effective adoption by clinicians and patients, as well as regulatory approval, require evidence of trustworthiness. A major factor for the development of trustworthy AI is the quantification of data quality for AI model training and testing. We have recently proposed the METRIC-framework for systematically evaluating the suitability (fit-for-purpose) of data for medical ML for a given task. Here, we operationalize this theoretical framework by introducing a collection of data quality metrics - the metric library - for practically measuring data quality dimensions. For each metric, we provide a metric card with the most important information, including definition, applicability, examples, pitfalls and recommendations, to support the understanding and implementation of these metrics. Furthermore, we discuss strategies and provide decision trees for choosing an appropriate set of data quality metrics from the metric library given specific use cases. We demonstrate the impact of our approach exemplarily on the PTB-XL ECG-dataset. This is a first step to enable fit-for-purpose evaluation of training and test data in practice as the base for establishing trustworthy AI in medicine.

</details>


### [102] [Deep Learning-Based Early-Stage IR-Drop Estimation via CNN Surrogate Modeling](https://arxiv.org/abs/2601.22707)
*Ritesh Bhadana*

Main category: cs.LG

TL;DR: 提出基于深度学习的IR-drop早期估计方法，使用CNN将物理版图特征映射到IR-drop热力图，实现毫秒级推理，作为传统物理签核工具的补充


<details>
  <summary>Details</summary>
Motivation: 传统IR-drop分析依赖物理签核工具，计算成本高且需要接近最终版图信息，不适合早期快速设计探索。需要一种快速、准确的早期IR-drop估计方法

Method: 采用U-Net编码器-解码器架构，将IR-drop估计建模为密集像素级回归问题。使用物理启发的合成数据集训练，包含电源网格结构、单元密度分布和开关活动等关键物理因素

Result: 模型能够准确预测IR-drop分布，推理时间达到毫秒级别，支持快速预签核筛选和迭代设计优化。通过MSE和PSNR等标准回归指标评估性能

Conclusion: 提出的深度学习框架可作为早期分析工具，在昂贵的签核分析之前为设计者提供快速IR-drop洞察。代码、数据集生成脚本和交互式推理应用已开源

Abstract: IR-drop is a critical power integrity challenge in modern VLSI designs that can cause timing degradation, reliability issues, and functional failures if not detected early in the design flow. Conventional IR-drop analysis relies on physics-based signoff tools, which provide high accuracy but incur significant computational cost and require near-final layout information, making them unsuitable for rapid early-stage design exploration. In this work, we propose a deep learning-based surrogate modeling approach for early-stage IR-drop estimation using a CNN. The task is formulated as a dense pixel-wise regression problem, where spatial physical layout features are mapped directly to IR-drop heatmaps. A U-Net-based encoder-decoder architecture with skip connections is employed to effectively capture both local and global spatial dependencies within the layout. The model is trained on a physics-inspired synthetic dataset generated by us, which incorporates key physical factors including power grid structure, cell density distribution, and switching activity. Model performance is evaluated using standard regression metrics such as Mean Squared Error (MSE) and Peak Signal-to-Noise Ratio (PSNR). Experimental results demonstrate that the proposed approach can accurately predict IR-drop distributions with millisecond-level inference time, enabling fast pre-signoff screening and iterative design optimization. The proposed framework is intended as a complementary early-stage analysis tool, providing designers with rapid IR-drop insight prior to expensive signoff analysis. The implementation, dataset generation scripts, and the interactive inference application are publicly available at: https://github.com/riteshbhadana/IR-Drop-Predictor. The live application can be accessed at: https://ir-drop-predictor.streamlit.app/.

</details>


### [103] [A Unified Study of LoRA Variants: Taxonomy, Review, Codebase, and Empirical Evaluation](https://arxiv.org/abs/2601.22708)
*Haonan He,Jingqi Ye,Minglei Li,Zhengbo Wang,Tao Chen,Lei Bai,Peng Ye*

Main category: cs.LG

TL;DR: 本文对LoRA变体进行了首次统一研究，提出了系统分类、理论框架、统一代码库和标准化评估，发现LoRA对学习率敏感，且通过适当超参数配置可匹敌或超越多数变体。


<details>
  <summary>Details</summary>
Motivation: LoRA作为参数高效微调方法已被广泛应用，但其众多变体导致方法、理论、代码和评估碎片化，缺乏统一研究框架。

Method: 1) 从四个主要维度对LoRA变体分类：秩、优化动态、初始化和与MoE集成；2) 在低秩更新动态框架下统一理论分析；3) 开发模块化代码库LoRAFactory；4) 在NLG、NLU和图像分类任务上进行大规模评估。

Result: 研究发现：1) LoRA及其变体对学习率选择特别敏感；2) 通过适当超参数配置，标准LoRA可匹敌或超越大多数变体性能；3) 提供了系统化的评估结果和洞见。

Conclusion: 本文首次统一研究LoRA变体，建立了系统分类和评估框架，揭示了LoRA的核心特性，为未来研究提供了标准化基础和实践指导。

Abstract: Low-Rank Adaptation (LoRA) is a fundamental parameter-efficient fine-tuning method that balances efficiency and performance in large-scale neural networks. However, the proliferation of LoRA variants has led to fragmentation in methodology, theory, code, and evaluation. To this end, this work presents the first unified study of LoRA variants, offering a systematic taxonomy, unified theoretical review, structured codebase, and standardized empirical assessment. First, we categorize LoRA variants along four principal axes: rank, optimization dynamics, initialization, and integration with Mixture-of-Experts. Then, we review their relationships and evolution within a common theoretical framework focused on low-rank update dynamics. Further, we introduce LoRAFactory, a modular codebase that implements variants through a unified interface, supporting plug-and-play experimentation and fine-grained analysis. Last, using this codebase, we conduct a large-scale evaluation across natural language generation, natural language understanding, and image classification tasks, systematically exploring key hyperparameters. Our results uncover several findings, notably: LoRA and its variants exhibit pronounced sensitivity to the choices of learning rate compared to other hyperparameters; moreover, with proper hyperparameter configurations, LoRA consistently matches or surpasses the performance of most of its variants.

</details>


### [104] [SQUAD: Scalable Quorum Adaptive Decisions via ensemble of early exit neural networks](https://arxiv.org/abs/2601.22711)
*Matteo Gambella,Fabrizio Pittorino,Giuliano Casale,Manuel Roveri*

Main category: cs.LG

TL;DR: SQUAD将早期退出机制与分布式集成学习结合，通过基于法定人数的停止准则和优化的层次多样性选择，提高不确定性估计并减少推理延迟。


<details>
  <summary>Details</summary>
Motivation: 传统早期退出神经网络依赖单一模型的置信度阈值，但由于校准问题常常不可靠，需要更鲁棒的停止准则和更好的不确定性估计方法。

Method: 提出SQUAD框架，使用基于法定人数的停止准则，按计算复杂度顺序收集中间预测直到达成共识；引入QUEST神经架构搜索方法，选择具有优化层次多样性的早期退出学习器。

Result: 相比最先进的动态解决方案，测试准确率提升高达5.95%，推理延迟相比静态集成减少高达70.60%，同时保持良好准确率。

Conclusion: SQUAD通过共识驱动的早期退出方法，在保持计算成本相当的情况下，显著提高了准确率和推理效率，为动态推理提供了更可靠的解决方案。

Abstract: Early-exit neural networks have become popular for reducing inference latency by allowing intermediate predictions when sufficient confidence is achieved. However, standard approaches typically rely on single-model confidence thresholds, which are frequently unreliable due to inherent calibration issues. To address this, we introduce SQUAD (Scalable Quorum Adaptive Decisions), the first inference scheme that integrates early-exit mechanisms with distributed ensemble learning, improving uncertainty estimation while reducing the inference time. Unlike traditional methods that depend on individual confidence scores, SQUAD employs a quorum-based stopping criterion on early-exit learners by collecting intermediate predictions incrementally in order of computational complexity until a consensus is reached and halting the computation at that exit if the consensus is statistically significant. To maximize the efficacy of this voting mechanism, we also introduce QUEST (Quorum Search Technique), a Neural Architecture Search method to select early-exit learners with optimized hierarchical diversity, ensuring learners are complementary at every intermediate layer. This consensus-driven approach yields statistically robust early exits, improving the test accuracy up to 5.95% compared to state-of-the-art dynamic solutions with a comparable computational cost and reducing the inference latency up to 70.60% compared to static ensembles while maintaining a good accuracy.

</details>


### [105] [Vision-Language Models Unlock Task-Centric Latent Actions](https://arxiv.org/abs/2601.22714)
*Alexander Nikulin,Ilya Zisman,Albina Klepach,Denis Tarasov,Alexander Derevyagin,Andrei Polubarov,Lyubaykin Nikita,Vladislav Kurenkov*

Main category: cs.LG

TL;DR: 利用视觉语言模型的常识推理能力，为潜在动作模型提供可提示的表征，有效分离可控变化与噪声，显著提升下游任务成功率


<details>
  <summary>Details</summary>
Motivation: 现有潜在动作模型在处理包含动作相关干扰物的观测时会失败，而人类却能轻松区分任务相关运动与无关细节。本文旨在利用视觉语言模型的常识推理能力来解决这一问题。

Method: 提出使用视觉语言模型提供可提示的表征，以无监督方式分离可控变化与噪声，并将这些表征作为潜在动作模型训练的目标。对多种流行的视觉语言模型进行了基准测试。

Result: 研究发现不同视觉语言模型提供的可提示表征质量存在显著差异，且较新的模型可能表现更差。通过简单要求视觉语言模型忽略干扰物，可以大幅提升潜在动作质量，在Distracting MetaWorld上实现下游成功率最高六倍的增长。

Conclusion: 利用视觉语言模型的常识推理能力可以有效提升潜在动作模型在存在干扰物环境中的性能，为视觉-语言-动作模型的预训练提供了重要改进方向。

Abstract: Latent Action Models (LAMs) have rapidly gained traction as an important component in the pre-training pipelines of leading Vision-Language-Action models. However, they fail when observations contain action-correlated distractors, often encoding noise instead of meaningful latent actions. Humans, on the other hand, can effortlessly distinguish task-relevant motions from irrelevant details in any video given only a brief task description. In this work, we propose to utilize the common-sense reasoning abilities of Vision-Language Models (VLMs) to provide promptable representations, effectively separating controllable changes from the noise in unsupervised way. We use these representations as targets during LAM training and benchmark a wide variety of popular VLMs, revealing substantial variation in the quality of promptable representations as well as their robustness to different prompts and hyperparameters. Interestingly, we find that more recent VLMs may perform worse than older ones. Finally, we show that simply asking VLMs to ignore distractors can substantially improve latent action quality, yielding up to a six-fold increase in downstream success rates on Distracting MetaWorld.

</details>


### [106] [Breaking the Blocks: Continuous Low-Rank Decomposed Scaling for Unified LLM Quantization and Adaptation](https://arxiv.org/abs/2601.22716)
*Pingzhi Tang,Ruijie Zhou,Fanxu Meng,Wenjie Pei,Muhan Zhang*

Main category: cs.LG

TL;DR: LoRDS提出了一种基于低秩分解的元素级量化方法，通过连续低秩矩阵建模缩放流形，在保持效率的同时提供更强的表达能力，在量化精度和下游微调任务中均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM量化方法主要依赖块级结构来保持效率，但这限制了表示的灵活性。需要一种既能保持效率又能提供更强表达能力的量化方法。

Method: 提出LoRDS框架，将缩放流形建模为连续低秩矩阵(S=BA)，打破空间约束的块结构。通过低秩分解实现高效的元素级量化，支持PTQ初始化、权重和缩放因子的联合QAT，以及高秩乘法PEFT适配。

Result: 在Llama3-8B上，3位量化比NormalFloat精度提升27.0%，NVIDIA RTX 4090上推理速度提升1.5倍，下游任务上4位QLoRA的PEFT性能提升9.6%。在各种模型家族中均优于现有基线方法。

Conclusion: LoRDS通过低秩分解实现了高效的元素级量化，提供了统一的压缩和适配解决方案，在保持推理效率的同时显著提升了量化精度和下游任务性能。

Abstract: Current quantization methods for LLMs predominantly rely on block-wise structures to maintain efficiency, often at the cost of representational flexibility. In this work, we demonstrate that element-wise quantization can be made as efficient as block-wise scaling while providing strictly superior expressive power by modeling the scaling manifold as continuous low-rank matrices ($S = BA$). We propose Low-Rank Decomposed Scaling (LoRDS), a unified framework that rethinks quantization granularity through this low-rank decomposition. By "breaking the blocks" of spatial constraints, LoRDS establishes a seamless efficiency lifecycle: it provides high-fidelity PTQ initialization refined via iterative optimization, enables joint QAT of weights and scaling factors, and facilitates high-rank multiplicative PEFT adaptation. Unlike additive PEFT approaches such as QLoRA, LoRDS enables high-rank weight updates within a low-rank budget while incurring no additional inference overhead. Supported by highly optimized Triton kernels, LoRDS consistently outperforms state-of-the-art baselines across various model families in both quantization and downstream fine-tuning tasks. Notably, on Llama3-8B, our method achieves up to a 27.0% accuracy improvement at 3 bits over NormalFloat quantization and delivers a 1.5x inference speedup on NVIDIA RTX 4090 while enhancing PEFT performance by 9.6% on downstream tasks over 4bit QLoRA, offering a robust and integrated solution for unified compression and adaptation of LLMs.

</details>


### [107] [Local Intrinsic Dimension of Representations Predicts Alignment and Generalization in AI Models and Human Brain](https://arxiv.org/abs/2601.22722)
*Junjie Yu,Wenxiao Ma,Chen Wei,Jianyu Zhang,Haotian Deng,Zihan Deng,Quanying Liu*

Main category: cs.LG

TL;DR: 研究发现：神经网络泛化能力越强，其表征与人类神经活动越对齐；泛化性能、模型间对齐度和模型-大脑对齐度三者显著相关，这些关系可由表征的局部本征维度解释。


<details>
  <summary>Details</summary>
Motivation: 探索神经网络泛化能力、模型间表征对齐以及模型与人类大脑神经活动对齐之间的关系，寻找能够统一描述这些现象的基础几何特性。

Method: 通过分析不同架构和训练范式的神经网络，研究其泛化性能、模型间对齐度、模型-大脑对齐度的相关性，并引入局部本征维度作为表征的几何描述符。

Result: 发现：1) 泛化能力强的模型与人类神经活动对齐度更高；2) 泛化性能、模型间对齐、模型-大脑对齐三者显著相关；3) 局部本征维度越低，对齐度和泛化能力越强；4) 增加模型容量和训练数据会降低局部本征维度。

Conclusion: 局部本征维度是描述人工和生物系统中表征收敛的统一几何描述符，为理解模型缩放的好处提供了几何解释。

Abstract: Recent work has found that neural networks with stronger generalization tend to exhibit higher representational alignment with one another across architectures and training paradigms. In this work, we show that models with stronger generalization also align more strongly with human neural activity. Moreover, generalization performance, model--model alignment, and model--brain alignment are all significantly correlated with each other. We further show that these relationships can be explained by a single geometric property of learned representations: the local intrinsic dimension of embeddings. Lower local dimension is consistently associated with stronger model--model alignment, stronger model--brain alignment, and better generalization, whereas global dimension measures fail to capture these effects. Finally, we find that increasing model capacity and training data scale systematically reduces local intrinsic dimension, providing a geometric account of the benefits of scaling. Together, our results identify local intrinsic dimension as a unifying descriptor of representational convergence in artificial and biological systems.

</details>


### [108] [Decomposing Epistemic Uncertainty for Causal Decision Making](https://arxiv.org/abs/2601.22736)
*Md Musfiqur Rahman,Ziwei Jiang,Hilaf Hasson,Murat Kocaoglu*

Main category: cs.LG

TL;DR: 提出新框架区分因果效应边界中的样本不确定性与不可识别不确定性，通过置信集方法指导数据收集决策


<details>
  <summary>Details</summary>
Motivation: 现有神经因果方法可能过拟合且无法区分因果效应边界宽度来源：根本不可识别性 vs 有限样本限制，需要系统方法指导实践决策

Method: 构建观测分布置信集，求取置信集中所有分布的因果效应边界交集，通过神经因果模型求解min-max和max-min问题，区分样本不确定性与不可识别不确定性

Result: 实验证明算法能确定何时收集更多样本无助于确定最佳行动，指导实践者收集更多变量或转向随机研究

Conclusion: 提出系统框架区分因果效应边界不确定性来源，为实践决策提供指导，帮助在收集更多样本与变量之间做出明智选择

Abstract: Causal inference from observational data provides strong evidence for the best action in decision-making without performing expensive randomized trials. The effect of an action is usually not identifiable under unobserved confounding, even with an infinite amount of data. Recent work uses neural networks to obtain practical bounds to such causal effects, which is often an intractable problem. However, these approaches may overfit to the dataset and be overconfident in their causal effect estimates. Moreover, there is currently no systematic approach to disentangle how much of the width of causal effect bounds is due to fundamental non-identifiability versus how much is due to finite-sample limitations. We propose a novel framework to address this problem by considering a confidence set around the empirical observational distribution and obtaining the intersection of causal effect bounds for all distributions in this confidence set. This allows us to distinguish the part of the interval that can be reduced by collecting more samples, which we call sample uncertainty, from the part that can only be reduced by observing more variables, such as latent confounders or instrumental variables, but not with more data, which we call non-ID uncertainty. The upper and lower bounds to this intersection are obtained by solving min-max and max-min problems with neural causal models by searching over all distributions that the dataset might have been sampled from, and all SCMs that entail the corresponding distribution. We demonstrate via extensive experiments on synthetic and real-world datasets that our algorithm can determine when collecting more samples will not help determine the best action. This can guide practitioners to collect more variables or lean towards a randomized study for best action identification.

</details>


### [109] [Is Softmax Loss All You Need? A Principled Analysis of Softmax-family Loss](https://arxiv.org/abs/2601.22745)
*Yuanhao Pu,Defu Lian,Enhong Chen*

Main category: cs.LG

TL;DR: 该论文系统研究了Softmax系列损失函数，分析了不同替代损失在分类和排序任务中的一致性、梯度动态和收敛行为，提出了近似方法的偏差-方差分解，并提供了收敛保证和复杂度分析，为大规模类别机器学习应用中的损失选择提供了理论基础和实践指导。


<details>
  <summary>Details</summary>
Motivation: Softmax损失是分类和排序任务中最广泛使用的替代目标之一，但现有研究存在两个分离的视角：一是Fenchel-Young框架从理论角度分析其性质，二是大规模类别场景下的近似方法研究。本文旨在整合这两个视角，系统研究Softmax系列损失的理论属性和实际性能。

Method: 1. 在Fenchel-Young框架下分析不同替代损失的一致性（与分类和排序指标的一致性）；2. 分析梯度动态以揭示不同的收敛行为；3. 为近似方法提出系统的偏差-方差分解，提供收敛保证；4. 推导每轮迭代的复杂度分析，展示效果与效率之间的权衡。

Result: 实验结果表明，一致性、收敛性和经验性能之间存在强烈的对应关系。不同替代损失在分类和排序任务中表现出不同的理论性质和实际效果，近似方法在效率和准确性之间存在明确的权衡。

Conclusion: 该研究为Softmax系列损失建立了系统的理论基础，揭示了不同损失函数在一致性、收敛性和效率方面的特性，为大规模类别机器学习应用中的损失选择提供了原则性指导和实践建议。

Abstract: The Softmax loss is one of the most widely employed surrogate objectives for classification and ranking tasks. To elucidate its theoretical properties, the Fenchel-Young framework situates it as a canonical instance within a broad family of surrogates. Concurrently, another line of research has addressed scalability when the number of classes is exceedingly large, in which numerous approximations have been proposed to retain the benefits of the exact objective while improving efficiency. Building on these two perspectives, we present a principled investigation of the Softmax-family losses. We examine whether different surrogates achieve consistency with classification and ranking metrics, and analyze their gradient dynamics to reveal distinct convergence behaviors. We also introduce a systematic bias-variance decomposition for approximate methods that provides convergence guarantees, and further derive a per-epoch complexity analysis, showing explicit trade-offs between effectiveness and efficiency. Extensive experiments on a representative task demonstrate a strong alignment between consistency, convergence, and empirical performance. Together, these results establish a principled foundation and offer practical guidance for loss selections in large-class machine learning applications.

</details>


### [110] [Discovering Scaling Exponents with Physics-Informed Müntz-Szász Networks](https://arxiv.org/abs/2601.22751)
*Gnankan Landry Regis N'guessan,Bum Jun Kim*

Main category: cs.LG

TL;DR: 提出MSN-PINN网络，将幂律标度指数作为可训练参数，能同时学习解及其标度结构，在奇异点、界面和临界点附近物理系统中实现高精度指数恢复。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络在处理奇异点、界面和临界点附近的物理系统时，无法显式捕捉幂律标度指数，而这些指数对理解物理现象至关重要。需要一种既能保持神经网络表达能力又能提供物理解释性的方法。

Method: 提出物理信息Müntz-Szász网络（MSN-PINN），采用幂律基函数网络，将标度指数作为可训练参数。模型同时输出解及其标度结构，并证明了可识别性（唯一恢复性）。引入约束感知训练来编码物理要求（如边界条件兼容性）。

Result: 在噪声和稀疏采样下实现1-5%误差的单指数恢复；二维拉普拉斯方程角奇点指数恢复误差0.009%；奇异泊松问题中强迫诱导指数恢复误差0.03%和0.05%；40配置楔形基准测试达到100%成功率，平均误差0.022%；约束感知训练比朴素训练精度提高三个数量级。

Conclusion: MSN-PINN成功结合了神经网络的表达能力和渐近分析的可解释性，产生的学习参数具有直接物理意义，为物理系统奇异点分析提供了有效工具。

Abstract: Physical systems near singularities, interfaces, and critical points exhibit power-law scaling, yet standard neural networks leave the governing exponents implicit. We introduce physics-informed M"untz-Sz'asz Networks (MSN-PINN), a power-law basis network that treats scaling exponents as trainable parameters. The model outputs both the solution and its scaling structure. We prove identifiability, or unique recovery, and show that, under these conditions, the squared error between learned and true exponents scales as $O(|μ- α|^2)$. Across experiments, MSN-PINN achieves single-exponent recovery with 1--5% error under noise and sparse sampling. It recovers corner singularity exponents for the two-dimensional Laplace equation with 0.009% error, matches the classical result of Kondrat'ev (1967), and recovers forcing-induced exponents in singular Poisson problems with 0.03% and 0.05% errors. On a 40-configuration wedge benchmark, it reaches a 100% success rate with 0.022% mean error. Constraint-aware training encodes physical requirements such as boundary condition compatibility and improves accuracy by three orders of magnitude over naive training. By combining the expressiveness of neural networks with the interpretability of asymptotic analysis, MSN-PINN produces learned parameters with direct physical meaning.

</details>


### [111] [OSNIP: Breaking the Privacy-Utility-Efficiency Trilemma in LLM Inference via Obfuscated Semantic Null Space](https://arxiv.org/abs/2601.22752)
*Zhiyuan Cao,Zeyu Ma,Chenhao Yang,Han Zheng,Mingang Chen*

Main category: cs.LG

TL;DR: OSNIP是一种轻量级客户端加密框架，通过向LLM潜在空间注入扰动来保护隐私，在保持语义保真度的同时实现隐私保护


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理中的隐私保护需求日益增长，需要一种既能保护用户隐私又不影响模型效用的轻量级解决方案

Method: 提出"混淆语义零空间"概念，将线性核的几何直觉推广到LLM高维潜在空间，通过注入扰动将原始嵌入投影到该空间，并采用密钥相关的随机映射生成个性化扰动轨迹

Result: 在12个生成和分类基准测试中达到最先进性能，显著降低攻击成功率，同时在严格安全约束下保持强大的模型效用

Conclusion: OSNIP提供了一种无需后处理的隐私保护LLM推理框架，在隐私保护和模型效用之间取得了良好平衡

Abstract: We propose Obfuscated Semantic Null space Injection for Privacy (OSNIP), a lightweight client-side encryption framework for privacy-preserving LLM inference. Generalizing the geometric intuition of linear kernels to the high-dimensional latent space of LLMs, we formally define the ``Obfuscated Semantic Null Space'', a high-dimensional regime that preserves semantic fidelity while enforcing near-orthogonality to the original embedding. By injecting perturbations that project the original embedding into this space, OSNIP ensures privacy without any post-processing. Furthermore, OSNIP employs a key-dependent stochastic mapping that synthesizes individualized perturbation trajectories unique to each user. Evaluations on 12 generative and classification benchmarks show that OSNIP achieves state-of-the-art performance, sharply reducing attack success rates while maintaining strong model utility under strict security constraints.

</details>


### [112] [Understanding Generalization from Embedding Dimension and Distributional Convergence](https://arxiv.org/abs/2601.22756)
*Junjie Yu,Zhuoli Ouyang,Haotian Deng,Chen Wei,Wenxiao Ma,Jianyu Zhang,Zihan Deng,Quanying Liu*

Main category: cs.LG

TL;DR: 论文提出从表示中心视角分析深度神经网络泛化能力，发现泛化误差可由嵌入分布的内在维度和下游映射的敏感性共同界定，不依赖参数数量，解释了嵌入维度与泛化性能的强相关性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在严重过参数化情况下仍能良好泛化，这挑战了传统的基于参数的分析方法。作者希望从表示学习的角度理解泛化现象，研究学习到的嵌入几何如何控制预测性能。

Method: 采用表示中心视角，分析嵌入分布的几何特性。提出泛化误差由两个因素界定：(1)嵌入分布的内在维度，决定经验嵌入分布到总体分布在水斯坦距离下的收敛速率；(2)从嵌入到预测的下游映射敏感性，用Lipschitz常数表征。在最终嵌入层，架构敏感性消失，误差界由嵌入维度主导。

Result: 理论分析表明，总体风险可由嵌入分布的内在维度和下游映射的Lipschitz常数界定，不依赖参数数量或假设类复杂度。在最终嵌入层，误差界由嵌入维度主导，解释了嵌入维度与泛化性能的强经验相关性。跨架构和数据集实验验证了理论并展示了基于嵌入的诊断工具的实用性。

Conclusion: 从表示中心视角分析泛化能力提供了新的理论框架，解释了深度神经网络在过参数化情况下的泛化现象。嵌入分布的几何特性（特别是内在维度）是理解泛化性能的关键因素，为模型诊断和设计提供了新工具。

Abstract: Deep neural networks often generalize well despite heavy over-parameterization, challenging classical parameter-based analyses. We study generalization from a representation-centric perspective and analyze how the geometry of learned embeddings controls predictive performance for a fixed trained model. We show that population risk can be bounded by two factors: (i) the intrinsic dimension of the embedding distribution, which determines the convergence rate of empirical embedding distribution to the population distribution in Wasserstein distance, and (ii) the sensitivity of the downstream mapping from embeddings to predictions, characterized by Lipschitz constants. Together, these yield an embedding-dependent error bound that does not rely on parameter counts or hypothesis class complexity. At the final embedding layer, architectural sensitivity vanishes and the bound is dominated by embedding dimension, explaining its strong empirical correlation with generalization performance. Experiments across architectures and datasets validate the theory and demonstrate the utility of embedding-based diagnostics.

</details>


### [113] [Unveiling Scaling Behaviors in Molecular Language Models: Effects of Model Size, Data, and Representation](https://arxiv.org/abs/2601.22757)
*Dong Xu,Qihua Pan,Sisi Yuan,Jianqiang Li,Zexuan Zhu,Junkai Ji*

Main category: cs.LG

TL;DR: 该研究系统探索了分子语言模型的缩放规律，通过300个模型和10,000多次实验，揭示了在固定计算预算下模型大小、训练数据和分子表示对性能的影响，并发布了最大的分子语言模型库。


<details>
  <summary>Details</summary>
Motivation: 分子生成模型（通常基于GPT风格的分子字符串表示）在大规模数据和模型上表现出潜力，但尚不清楚它们是否遵循可预测的缩放规律。这对于在模型大小、数据量和分子表示之间优化资源分配至关重要。

Method: 训练了300个模型并进行10,000多次实验，严格控制计算预算，同时独立改变模型大小、训练token数量和分子表示，系统研究分子语言模型在预训练和下游任务中的缩放行为。

Result: 结果表明分子模型在预训练和下游迁移中都存在清晰的缩放规律；分子表示对性能有显著影响；解释了先前观察到的分子生成缩放行为不一致性；发布了迄今为止最大的分子语言模型库。

Conclusion: 分子语言模型确实遵循可预测的缩放规律，分子表示是影响性能的关键因素，该研究为优化分子生成模型的资源分配提供了重要指导，并公开了模型库促进未来研究。

Abstract: Molecular generative models, often employing GPT-style language modeling on molecular string representations, have shown promising capabilities when scaled to large datasets and model sizes. However, it remains unclear and subject to debate whether these models adhere to predictable scaling laws under fixed computational budgets, which is a crucial understanding for optimally allocating resources between model size, data volume, and molecular representation. In this study, we systematically investigate the scaling behavior of molecular language models across both pretraining and downstream tasks. We train 300 models and conduct over 10,000 experiments, rigorously controlling compute budgets while independently varying model size, number of training tokens, and molecular representation. Our results demonstrate clear scaling laws in molecular models for both pretraining and downstream transfer, reveal the substantial impact of molecular representation on performance, and explain previously observed inconsistencies in scaling behavior for molecular generation. Additionally, we publicly release the largest library of molecular language models to date to facilitate future research and development. Code and models are available at https://github.com/SZU-ADDG/MLM-Scaling.

</details>


### [114] [Sparse Attention as Compact Kernel Regression](https://arxiv.org/abs/2601.22766)
*Saul Santos,Nuno Gonçalves,Daniel C. McNamee,André F. T Martins*

Main category: cs.LG

TL;DR: 论文建立了稀疏注意力机制与紧支撑核函数之间的理论对应关系，揭示了ReLU和sparsemax注意力分别对应固定和自适应归一化的Epanechnikov核回归，为稀疏注意力提供了核理论基础。


<details>
  <summary>Details</summary>
Motivation: 现有研究揭示了transformer自注意力机制与Nadaraya-Watson核回归之间的联系，标准softmax注意力对应高斯核。然而，稀疏注意力机制的核理论理解目前缺失，需要建立稀疏注意力与紧支撑核函数之间的形式化对应关系。

Method: 通过理论分析建立稀疏注意力机制与紧支撑核函数的对应关系，证明归一化ReLU和sparsemax注意力分别来自固定和自适应归一化的Epanechnikov核回归。更一般地，展示非参数密度估计中广泛使用的核函数（Epanechnikov、biweight、triweight）对应α-entmax注意力，而softmax/高斯关系在极限情况下出现。

Result: 建立了稀疏注意力与紧支撑核函数的理论对应，解释了稀疏性如何从核设计中自然产生。基于核回归的transformer变体Memory Mosaics在语言建模、上下文学习和长度泛化任务上取得竞争性性能，为设计注意力机制提供了原则性框架。

Conclusion: 论文为稀疏注意力机制提供了统一的核理论视角，解释了稀疏性源于核设计，为替代启发式top-k注意力和其他关联记忆机制提供了原则性方案，为设计注意力机制建立了理论基础。

Abstract: Recent work has revealed a link between self-attention mechanisms in transformers and test-time kernel regression via the Nadaraya-Watson estimator, with standard softmax attention corresponding to a Gaussian kernel. However, a kernel-theoretic understanding of sparse attention mechanisms is currently missing. In this paper, we establish a formal correspondence between sparse attention and compact (bounded support) kernels. We show that normalized ReLU and sparsemax attention arise from Epanechnikov kernel regression under fixed and adaptive normalizations, respectively. More generally, we demonstrate that widely used kernels in nonparametric density estimation -- including Epanechnikov, biweight, and triweight -- correspond to $α$-entmax attention with $α= 1 + \frac{1}{n}$ for $n \in \mathbb{N}$, while the softmax/Gaussian relationship emerges in the limit $n \to \infty$. This unified perspective explains how sparsity naturally emerges from kernel design and provides principled alternatives to heuristic top-$k$ attention and other associative memory mechanisms. Experiments with a kernel-regression-based variant of transformers -- Memory Mosaics -- show that kernel-based sparse attention achieves competitive performance on language modeling, in-context learning, and length generalization tasks, offering a principled framework for designing attention mechanisms.

</details>


### [115] [Float8@2bits: Entropy Coding Enables Data-Free Model Compression](https://arxiv.org/abs/2601.22787)
*Patrick Putzky,Martin Genzel,Mattes Mollenhauer,Sebastian Schulze,Thomas Wollmann,Stefan Dietzel*

Main category: cs.LG

TL;DR: EntQuant是一种新的后训练压缩框架，通过熵编码将数值精度与存储成本解耦，在极端压缩（低于4位）下实现数据依赖方法的性能和数据无关方法的速度。


<details>
  <summary>Details</summary>
Motivation: 当前后训练压缩存在两个对立范式：快速、数据无关、模型无关的方法（如NF4、HQQ）在极端比特率（低于4位）下会出现功能崩溃；而依赖校准数据或恢复训练的方法虽然保真度高，但计算成本高且对数据分布变化鲁棒性不确定。需要结合两者优势。

Method: EntQuant通过熵编码将数值精度与存储成本解耦，使用数据无关技术实现快速压缩（70B参数模型在30分钟内完成），同时达到数据依赖方法的性能水平。

Result: EntQuant在标准评估集和模型上达到最先进结果，在指令调优模型的复杂基准测试中保持功能性能，且推理开销适中。

Conclusion: EntQuant首次统一了后训练压缩的两个对立范式，在极端压缩下实现了实用价值，平衡了性能、速度和通用性。

Abstract: Post-training compression is currently divided into two contrasting regimes. On the one hand, fast, data-free, and model-agnostic methods (e.g., NF4 or HQQ) offer maximum accessibility but suffer from functional collapse at extreme bit-rates below 4 bits. On the other hand, techniques leveraging calibration data or extensive recovery training achieve superior fidelity but impose high computational constraints and face uncertain robustness under data distribution shifts. We introduce EntQuant, the first framework to unite the advantages of these distinct paradigms. By matching the performance of data-dependent methods with the speed and universality of data-free techniques, EntQuant enables practical utility in the extreme compression regime. Our method decouples numerical precision from storage cost via entropy coding, compressing a 70B parameter model in less than 30 minutes. We demonstrate that EntQuant does not only achieve state-of-the-art results on standard evaluation sets and models, but also retains functional performance on more complex benchmarks with instruction-tuned models, all at modest inference overhead.

</details>


### [116] [Clipping-Free Policy Optimization for Large Language Models](https://arxiv.org/abs/2601.22801)
*Ömer Veysel Çağatan,Barış Akgün,Gözde Gül Şahin,Xuandong Zhao*

Main category: cs.LG

TL;DR: CFPO提出了一种无裁剪策略优化方法，用凸二次惩罚替代启发式裁剪，解决大规模LLM后训练中的优化问题


<details>
  <summary>Details</summary>
Motivation: 当前强化学习在大型语言模型后训练中占据核心地位，但主流算法依赖裁剪机制，这在大规模应用中引入了零梯度区域、奖励黑客和训练不稳定等优化问题

Method: CFPO用基于总变差散度约束的凸二次惩罚替代启发式裁剪，产生处处可微的目标函数，无需硬边界即可强制执行稳定的策略更新

Result: 在推理任务中，CFPO与裁剪方法在下游基准测试中表现相当，同时扩展了稳定训练范围；在对齐任务中，CFPO缓解了冗长利用问题，减少了能力退化，同时实现了有竞争力的指令跟随性能

Conclusion: CFPO只需一行代码更改且无需额外超参数，是LLM后训练中裁剪方法的有前景的直接替代方案

Abstract: Reinforcement learning has become central to post-training large language models, yet dominant algorithms rely on clipping mechanisms that introduce optimization issues at scale, including zero-gradient regions, reward hacking, and training instability. We propose Clipping-Free Policy Optimization (CFPO), which replaces heuristic clipping with a convex quadratic penalty derived from Total Variation divergence constraints, yielding an everywhere-differentiable objective that enforces stable policy updates without hard boundaries. We evaluate CFPO across both reasoning and alignment settings. In reasoning, CFPO matches clipping-based methods on downstream benchmarks while extending the stable training regime. In alignment, CFPO mitigates verbosity exploitation and reduces capability degradation, while achieving competitive instruction-following performance. CFPO requires only a one-line code change and no additional hyperparameters. Our results suggest that CFPO is a promising drop-in alternative to clipping-based methods for LLM post-training.

</details>


### [117] [SOMBRERO: Measuring and Steering Boundary Placement in End-to-End Hierarchical Sequence Models](https://arxiv.org/abs/2601.22805)
*Pit Neitemeier,Alessio Serra,Jiaze Li,Sascha Wirges,Lukas Balles,Jan Hendrik Metzen*

Main category: cs.LG

TL;DR: Sombrero是一种改进分层序列模型中边界学习的方法，通过边界富集度指标和置信度对齐损失，使边界更集中在预测困难的位置，提升效率-准确率权衡。


<details>
  <summary>Details</summary>
Motivation: 现有分层序列模型虽然能学习有意义的边界，但难以定量评估和系统性地控制计算资源分配。边界放置对模型效率有重要影响，需要更好的方法来评估边界质量并引导边界学习。

Method: 提出边界富集度B指标来量化边界质量，衡量分块起始位置在预测困难位置（高下一个字节惊奇度）的集中程度。基于此提出Sombrero方法，通过置信度对齐边界损失引导边界放置在预测困难位置，并在输入级别应用置信度加权平滑来稳定边界学习。

Result: 在1B规模上，在涵盖英文和德文文本、代码和数学内容的UTF-8语料上，Sombrero改善了准确率-效率权衡，产生的边界更一致地将计算资源与难以预测的位置对齐。

Conclusion: 边界富集度B是一个有效的边界质量评估指标，Sombrero方法通过引导边界学习向预测困难位置，能够系统性地改善分层序列模型的效率-准确率权衡。

Abstract: Hierarchical sequence models replace fixed tokenization with learned segmentations that compress long byte sequences for efficient autoregressive modeling. While recent end-to-end methods can learn meaningful boundaries from the language-modeling objective alone, it remains difficult to quantitatively assess and systematically steer where compute is spent. We introduce a router-agnostic metric of boundary quality, boundary enrichment B, which measures how strongly chunk starts concentrate on positions with high next-byte surprisal. Guided by this metric, we propose Sombrero, which steers boundary placement toward predictive difficulty via a confidence-alignment boundary loss and stabilizes boundary learning by applying confidence-weighted smoothing at the input level rather than on realized chunks. On 1B scale, across UTF-8 corpora covering English and German text as well as code and mathematical content, Sombrero improves the accuracy-efficiency trade-off and yields boundaries that more consistently align compute with hard-to-predict positions.

</details>


### [118] [Quartet II: Accurate LLM Pre-Training in NVFP4 by Improved Unbiased Gradient Estimation](https://arxiv.org/abs/2601.22813)
*Andrei Panferov,Erik Schultheis,Soroush Tabesh,Dan Alistarh*

Main category: cs.LG

TL;DR: 提出MS-EDEN量化方法和Quartet II方案，在NVFP4格式下实现更精确的量化训练，相比现有方法降低2倍量化误差，在1.9B参数LLM训练中验证效果，提供Blackwell GPU内核实现4.2倍加速。


<details>
  <summary>Details</summary>
Motivation: NVFP4格式首次支持端到端全量化预训练大规模模型，但现有量化方法为获得无偏梯度估计而牺牲表示能力，导致精度损失。需要改进NVFP4量化训练方法以接近FP16/FP8训练精度。

Method: 提出MS-EDEN无偏量化方法，比随机舍入降低2倍以上量化误差；设计Quartet II全NVFP4量化方案，优化线性层的前向和反向传播中的矩阵乘法梯度估计；与NVFP4特定训练改进技术协同。

Result: Quartet II在所有主要矩阵乘法中实现更优梯度估计；在1.9B参数、38B tokens的LLM端到端训练中验证有效性；提供Blackwell GPU内核实现4.2倍加速（相比BF16）；代码已开源。

Conclusion: MS-EDEN和Quartet II显著提升NVFP4量化训练精度，接近FP16/FP8训练水平，同时保持硬件加速优势，为大模型全量化训练提供实用解决方案。

Abstract: The NVFP4 lower-precision format, supported in hardware by NVIDIA Blackwell GPUs, promises to allow, for the first time, end-to-end fully-quantized pre-training of massive models such as LLMs. Yet, existing quantized training methods still sacrifice some of the representation capacity of this format in favor of more accurate unbiased quantized gradient estimation by stochastic rounding (SR), losing noticeable accuracy relative to standard FP16 and FP8 training. In this paper, improve the state of the art for quantized training in NVFP4 via a novel unbiased quantization routine for micro-scaled formats, called MS-EDEN, that has more than 2x lower quantization error than SR. We integrate it into a novel fully-NVFP4 quantization scheme for linear layers, called Quartet II. We show analytically that Quartet II achieves consistently better gradient estimation across all major matrix multiplications, both on the forward and on the backward passes. In addition, our proposal synergizes well with recent training improvements aimed specifically at NVFP4. We further validate Quartet II on end-to-end LLM training with up to 1.9B parameters on 38B tokens. We provide kernels for execution on NVIDIA Blackwell GPUs with up to 4.2x speedup over BF16. Our code is available at https://github.com/IST-DASLab/Quartet-II .

</details>


### [119] [Cascaded Flow Matching for Heterogeneous Tabular Data with Mixed-Type Features](https://arxiv.org/abs/2601.22816)
*Markus Mueller,Kathrin Gruber,Dennis Fok*

Main category: cs.LG

TL;DR: 提出一种级联扩散模型用于表格数据生成，通过先生成低分辨率版本（分类特征和数值特征的粗粒度表示），再利用这些信息指导高分辨率流匹配模型，从而更好地处理混合类型特征。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型在处理表格数据时，特别是混合类型特征（同时包含离散状态和连续分布的特征）方面仍然存在挑战。现有方法难以准确生成这类复杂特征。

Method: 采用级联方法：1）首先生成低分辨率版本，包括纯分类特征和数值特征的粗粒度分类表示；2）通过新颖的引导条件概率路径和数据依赖耦合，利用低分辨率信息指导高分辨率流匹配模型。

Result: 模型能更准确地生成混合类型特征，检测分数提高40%。正式证明该级联方法能收紧传输成本界限，生成的样本更真实，分布细节捕捉更准确。

Conclusion: 提出的级联扩散模型在表格数据生成方面取得了显著进展，特别是在处理混合类型特征时表现出色，为表格数据生成提供了更有效的解决方案。

Abstract: Advances in generative modeling have recently been adapted to tabular data containing discrete and continuous features. However, generating mixed-type features that combine discrete states with an otherwise continuous distribution in a single feature remains challenging. We advance the state-of-the-art in diffusion models for tabular data with a cascaded approach. We first generate a low-resolution version of a tabular data row, that is, the collection of the purely categorical features and a coarse categorical representation of numerical features. Next, this information is leveraged in the high-resolution flow matching model via a novel guided conditional probability path and data-dependent coupling. The low-resolution representation of numerical features explicitly accounts for discrete outcomes, such as missing or inflated values, and therewith enables a more faithful generation of mixed-type features. We formally prove that this cascade tightens the transport cost bound. The results indicate that our model generates significantly more realistic samples and captures distributional details more accurately, for example, the detection score increases by 40%.

</details>


### [120] [User-Adaptive Meta-Learning for Cold-Start Medication Recommendation with Uncertainty Filtering](https://arxiv.org/abs/2601.22820)
*Arya Hadizadeh Moghaddam,Mohsen Nayebi Kerdabadi,Dongjie Wang,Mei Liu,Zijun Yao*

Main category: cs.LG

TL;DR: MetaDrug是一个针对药物推荐中患者冷启动问题的元学习框架，通过两级元适应机制和不确定性量化模块，在新患者缺乏处方历史的情况下提供个性化药物推荐。


<details>
  <summary>Details</summary>
Motivation: 现有药物推荐方法面临患者冷启动问题，即新患者因缺乏足够的处方历史而难以获得可靠的推荐。虽然已有研究使用医学知识图谱连接药物概念，但这些方法主要解决项目冷启动问题，无法提供适应个体患者特征的个性化推荐。元学习在处理推荐系统中的新用户稀疏交互方面表现出潜力，但在电子健康记录（EHR）中的应用仍不足，因为EHR数据具有独特的序列结构。

Method: 提出MetaDrug框架，包含：1）两级元适应机制：自我适应（使用患者自身医疗事件作为支持集捕捉时间依赖性）和同伴适应（使用相似患者的就诊记录丰富新患者表示）；2）不确定性量化模块：对支持就诊进行排序并过滤无关信息以确保适应一致性。

Result: 在MIMIC-III和急性肾损伤（AKI）数据集上的实验结果表明，MetaDrug在冷启动患者上持续优于最先进的药物推荐方法。

Conclusion: MetaDrug通过创新的元学习框架有效解决了药物推荐中的患者冷启动问题，结合自我适应、同伴适应和不确定性量化，为新患者提供了更可靠的个性化药物推荐。

Abstract: Large-scale Electronic Health Record (EHR) databases have become indispensable in supporting clinical decision-making through data-driven treatment recommendations. However, existing medication recommender methods often struggle with a user (i.e., patient) cold-start problem, where recommendations for new patients are usually unreliable due to the lack of sufficient prescription history for patient profiling. While prior studies have utilized medical knowledge graphs to connect medication concepts through pharmacological or chemical relationships, these methods primarily focus on mitigating the item cold-start issue and fall short in providing personalized recommendations that adapt to individual patient characteristics. Meta-learning has shown promise in handling new users with sparse interactions in recommender systems. However, its application to EHRs remains underexplored due to the unique sequential structure of EHR data. To tackle these challenges, we propose MetaDrug, a multi-level, uncertainty-aware meta-learning framework designed to address the patient cold-start problem in medication recommendation. MetaDrug proposes a novel two-level meta-adaptation mechanism, including self-adaptation, which adapts the model to new patients using their own medical events as support sets to capture temporal dependencies; and peer-adaptation, which adapts the model using similar visits from peer patients to enrich new patient representations. Meanwhile, to further improve meta-adaptation outcomes, we introduce an uncertainty quantification module that ranks the support visits and filters out the unrelated information for adaptation consistency. We evaluate our approach on the MIMIC-III and Acute Kidney Injury (AKI) datasets. Experimental results on both datasets demonstrate that MetaDrug consistently outperforms state-of-the-art medication recommendation methods on cold-start patients.

</details>


### [121] [Offline Reinforcement Learning of High-Quality Behaviors Under Robust Style Alignment](https://arxiv.org/abs/2601.22823)
*Mathieu Petitbois,Rémy Portelas,Sylvain Lamprier*

Main category: cs.LG

TL;DR: 提出Style-Conditioned Implicit Q-Learning (SCIQL)，通过显式风格监督和门控优势加权回归机制，在离线强化学习中同时优化任务性能和风格对齐。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习中，风格条件策略面临分布偏移和风格与奖励固有冲突的挑战，现有方法难以有效协调这两个目标。

Method: 提出统一的行为风格定义，并基于此构建SCIQL框架，结合离线目标条件RL技术（如后见重标记和值学习）和新的门控优势加权回归机制。

Result: 实验表明SCIQL在任务性能和风格对齐两方面都优于现有离线方法。

Conclusion: SCIQL通过统一风格定义和有效优化机制，成功解决了离线强化学习中风格与任务性能的协调问题。

Abstract: We study offline reinforcement learning of style-conditioned policies using explicit style supervision via subtrajectory labeling functions. In this setting, aligning style with high task performance is particularly challenging due to distribution shift and inherent conflicts between style and reward. Existing methods, despite introducing numerous definitions of style, often fail to reconcile these objectives effectively. To address these challenges, we propose a unified definition of behavior style and instantiate it into a practical framework. Building on this, we introduce Style-Conditioned Implicit Q-Learning (SCIQL), which leverages offline goal-conditioned RL techniques, such as hindsight relabeling and value learning, and combine it with a new Gated Advantage Weighted Regression mechanism to efficiently optimize task performance while preserving style alignment. Experiments demonstrate that SCIQL achieves superior performance on both objectives compared to prior offline methods. Code, datasets and visuals are available in: https://sciql-iclr-2026.github.io/.

</details>


### [122] [Decomposing and Composing: Towards Efficient Vision-Language Continual Learning via Rank-1 Expert Pool in a Single LoRA](https://arxiv.org/abs/2601.22828)
*Zhan Fa,Yue Duan,Jian Zhang,Lei Qi,Wanqi Yang,Yinghuan Shi*

Main category: cs.LG

TL;DR: 提出一种基于LoRA分解为Rank-1专家池的持续学习框架，通过动态组合稀疏任务特定更新和正交化损失，在减少96.7%参数的同时实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型的持续学习面临任务适应和灾难性遗忘的挑战。现有方法通常有沉重的推理负担或依赖外部知识，而LoRA虽能减少参数但直接用于缓解遗忘问题并不简单。

Method: 将单个LoRA模块重构为可分解的Rank-1专家池，基于[CLS]令牌语义动态选择专家组成稀疏任务特定更新，并提出激活引导正交(AGO)损失来正交化关键LoRA权重。

Result: 在多个设置下实现所有指标的SOTA结果，超越零样本泛化上界，相比基线方法减少96.7%可训练参数，无需外部数据集或任务ID判别器，合并的LoRA权重更少且无推理延迟。

Conclusion: 通过稀疏组合和正交化实现领域感知学习，最小化任务间干扰并保持下游任务性能，提供了一种计算轻量的持续学习解决方案。

Abstract: Continual learning (CL) in vision-language models (VLMs) faces significant challenges in improving task adaptation and avoiding catastrophic forgetting. Existing methods usually have heavy inference burden or rely on external knowledge, while Low-Rank Adaptation (LoRA) has shown potential in reducing these issues by enabling parameter-efficient tuning. However, considering directly using LoRA to alleviate the catastrophic forgetting problem is non-trivial, we introduce a novel framework that restructures a single LoRA module as a decomposable Rank-1 Expert Pool. Our method learns to dynamically compose a sparse, task-specific update by selecting from this expert pool, guided by the semantics of the [CLS] token. In addition, we propose an Activation-Guided Orthogonal (AGO) loss that orthogonalizes critical parts of LoRA weights across tasks. This sparse composition and orthogonalization enable fewer parameter updates, resulting in domain-aware learning while minimizing inter-task interference and maintaining downstream task performance. Extensive experiments across multiple settings demonstrate state-of-the-art results in all metrics, surpassing zero-shot upper bounds in generalization. Notably, it reduces trainable parameters by 96.7% compared to the baseline method, eliminating reliance on external datasets or task-ID discriminators. The merged LoRAs retain less weights and incur no inference latency, making our method computationally lightweight.

</details>


### [123] [Unconditional flow-based time series generation with equivariance-regularised latent spaces](https://arxiv.org/abs/2601.22848)
*Camilo Carvajal Reyes,Felipe Tobar*

Main category: cs.LG

TL;DR: 提出一种通过正则化预训练自编码器来增强潜在空间等变性的流匹配框架，用于时间序列生成，在保持高效采样的同时提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前基于流模型的时间序列生成方法通常在低维潜在空间中实现高效采样，但如何设计具有理想等变性质的潜在表示仍未被充分探索。等变性对于时间序列生成很重要，因为时间序列通常具有平移、振幅缩放等基本变换的对称性。

Method: 提出潜在流匹配框架，通过简单的正则化方法在预训练自编码器上显式鼓励等变性。引入等变性损失函数，强制变换后的信号与其重构之间的一致性，并使用该损失对基本时间序列变换（如平移和振幅缩放）的潜在空间进行微调。

Result: 在多个真实世界数据集上的实验表明，该方法在标准时间序列生成指标上一致优于现有的基于扩散的基线方法，同时实现了数量级更快的采样速度。等变性正则化的潜在空间提高了生成质量，同时保持了潜在流模型的计算优势。

Conclusion: 将几何归纳偏置纳入时间序列的潜在生成模型中具有实际益处。等变性正则化的潜在空间能够改善生成质量，同时保持高效采样，为时间序列生成提供了有前景的方向。

Abstract: Flow-based models have proven successful for time-series generation, particularly when defined in lower-dimensional latent spaces that enable efficient sampling. However, how to design latent representations with desirable equivariance properties for time-series generative modelling remains underexplored. In this work, we propose a latent flow-matching framework in which equivariance is explicitly encouraged through a simple regularisation of a pre-trained autoencoder. Specifically, we introduce an equivariance loss that enforces consistency between transformed signals and their reconstructions, and use it to fine-tune latent spaces with respect to basic time-series transformations such as translation and amplitude scaling. We show that these equivariance-regularised latent spaces improve generation quality while preserving the computational advantages of latent flow models. Experiments on multiple real-world datasets demonstrate that our approach consistently outperforms existing diffusion-based baselines in standard time-series generation metrics, while achieving orders-of-magnitude faster sampling. These results highlight the practical benefits of incorporating geometric inductive biases into latent generative models for time series.

</details>


### [124] [Hierarchical Shift Mixing -- Beyond Dense Attention in Transformers](https://arxiv.org/abs/2601.22852)
*Robert Forchheimer*

Main category: cs.LG

TL;DR: HSM是一种分层混合框架，通过将token交互分布在Transformer各层中，实现线性时间复杂度的token混合，性能接近softmax注意力，且能与注意力机制结合降低计算成本。


<details>
  <summary>Details</summary>
Motivation: Transformer中的softmax注意力层存在二次方时间复杂度问题，现有替代方法大多以性能下降为代价。需要一种既能降低计算复杂度又能保持性能的token混合方法。

Method: 提出分层混合（HSM）框架，将成对token交互分布在Transformer各层中而非每层密集计算，保持线性时间复杂度，且对具体混合函数保持不可知性。

Result: 简单HSM变体性能接近softmax注意力，HSM与softmax注意力结合的混合架构在训练和推理中均能超越GPT风格Transformer基线，同时降低计算成本。

Conclusion: HSM提供了一种有效的线性时间复杂度token混合框架，既能保持性能又能降低计算复杂度，为Transformer架构优化提供了新方向。

Abstract: Since the introduction of the Transformer architecture for large language models, the softmax-based attention layer has faced increasing scrutinity due to its quadratic-time computational complexity. Attempts have been made to replace it with less complex methods, at the cost of reduced performance in most cases. We introduce Hierarchical Shift Mixing (HSM), a general framework for token mixing that distributes pairwise token interactions across Transformer layers rather than computing them densely within each layer. HSM enables linear-time complexity while remaining agnostic to the specific mixing function. We show that even simple HSM variants achieve performance close to softmax attention, and that hybrid architectures combining HSM with softmax attention can outperform a GPT-style Transformer baseline while reducing computational cost during both training and inference.

</details>


### [125] [OptiMAG: Structure-Semantic Alignment via Unbalanced Optimal Transport](https://arxiv.org/abs/2601.22856)
*Yilong Zuo,Xunkai Li,Zhihan Zhang,Qiangqiang Dai,Ronghua Li,Guoren Wang*

Main category: cs.LG

TL;DR: OptiMAG提出了一种基于不平衡最优运输的正则化框架，通过Fused Gromov-Wasserstein距离解决多模态属性图中显式图结构与隐式语义结构之间的不一致问题。


<details>
  <summary>Details</summary>
Motivation: 多模态属性图中存在显式图结构与不同模态嵌入诱导的隐式语义结构之间的不一致问题。现有方法在固定的显式图结构上进行消息传递，会聚合不相似的特征，引入模态特定噪声，阻碍有效的节点表示学习。

Method: 提出OptiMAG框架，使用Fused Gromov-Wasserstein距离显式指导局部邻域内的跨模态结构一致性，并通过KL散度惩罚自适应处理跨模态不一致性。该框架可作为即插即用的正则器集成到现有多模态图模型中。

Result: 实验表明OptiMAG在多种任务上持续优于基线方法，包括图中心任务（节点分类、链接预测）和多模态中心生成任务（图到文本、图到图像生成）。

Conclusion: OptiMAG通过最优运输理论有效缓解多模态属性图中的结构-语义冲突，提升现有模型的性能，为多模态图学习提供了有效的正则化解决方案。

Abstract: Multimodal Attributed Graphs (MAGs) have been widely adopted for modeling complex systems by integrating multi-modal information, such as text and images, on nodes. However, we identify a discrepancy between the implicit semantic structure induced by different modality embeddings and the explicit graph structure. For instance, neighbors in the explicit graph structure may be close in one modality but distant in another. Since existing methods typically perform message passing over the fixed explicit graph structure, they inadvertently aggregate dissimilar features, introducing modality-specific noise and impeding effective node representation learning. To address this, we propose OptiMAG, an Unbalanced Optimal Transport-based regularization framework. OptiMAG employs the Fused Gromov-Wasserstein distance to explicitly guide cross-modal structural consistency within local neighborhoods, effectively mitigating structural-semantic conflicts. Moreover, a KL divergence penalty enables adaptive handling of cross-modal inconsistencies. This framework can be seamlessly integrated into existing multimodal graph models, acting as an effective drop-in regularizer. Experiments demonstrate that OptiMAG consistently outperforms baselines across multiple tasks, ranging from graph-centric tasks (e.g., node classification, link prediction) to multimodal-centric generation tasks (e.g., graph2text, graph2image). The source code will be available upon acceptance.

</details>


### [126] [Matterhorn: Efficient Analog Sparse Spiking Transformer Architecture with Masked Time-To-First-Spike Encoding](https://arxiv.org/abs/2601.22876)
*Zhanglu Yan,Kaiwen Tang,Zixuan Zhu,Zhenyu Bai,Qianhui Liu,Weng-Fai Wong*

Main category: cs.LG

TL;DR: Matterhorn是一个尖峰变压器，采用掩码时间到首次尖峰编码和忆阻突触单元，减少尖峰移动和权重访问开销，在GLUE基准上实现SOTA精度和2.31倍能效提升。


<details>
  <summary>Details</summary>
Motivation: 当前SNN的能效评估主要关注计算操作，忽略了数据移动等实际硬件成本（占总能耗近80%）。需要开发能减少尖峰移动和权重访问开销的SNN架构。

Method: 提出Matterhorn尖峰变压器：1) M-TTFS编码方法，通过掩码策略将零能量静默状态重新分配给最频繁的膜电位而非最低值，与数据分布对齐；2) "死区"策略，将给定范围内的所有值映射到静默状态以最大化稀疏性；3) MSU忆阻突触单元，利用存内计算技术在内存内直接进行模拟积分，消除权重访问开销。

Result: 在GLUE基准测试中，Matterhorn建立了新的最先进水平，平均准确率超过现有SNN 1.42%，同时能效提升2.31倍。

Conclusion: Matterhorn通过创新的编码方法和硬件设计，有效解决了SNN中数据移动和权重访问的能耗问题，在保持高精度的同时显著提升了能效，为节能LLM推理提供了有前景的解决方案。

Abstract: Spiking neural networks (SNNs) have emerged as a promising candidate for energy-efficient LLM inference. However, current energy evaluations for SNNs primarily focus on counting accumulate operations, and fail to account for real-world hardware costs such as data movement, which can consume nearly 80% of the total energy. In this paper, we propose Matterhorn, a spiking transformer that integrates a novel masked time-to-first-spike (M-TTFS) encoding method to reduce spike movement and a memristive synapse unit (MSU) to eliminate weight access overhead. M-TTFS employs a masking strategy that reassigns the zero-energy silent state (a spike train of all 0s) to the most frequent membrane potential rather than the lowest. This aligns the coding scheme with the data distribution, minimizing spike movement energy without information loss. We further propose a `dead zone' strategy that maximizes sparsity by mapping all values within a given range to the silent state. At the hardware level, the MSU utilizes compute-in-memory (CIM) technology to perform analog integration directly within memory, effectively removing weight access costs. On the GLUE benchmark, Matterhorn establishes a new state-of-the-art, surpassing existing SNNs by 1.42% in average accuracy while delivering a 2.31 times improvement in energy efficiency.

</details>


### [127] [Synthetic Time Series Generation via Complex Networks](https://arxiv.org/abs/2601.22879)
*Jaime Vale,Vanessa Freitas Silva,Maria Eduarda Silva,Fernando Silva*

Main category: cs.LG

TL;DR: 提出基于分位数图映射的时间序列生成框架，通过将时间序列转换为复杂网络再重构，生成保持原始统计和结构特性的合成数据


<details>
  <summary>Details</summary>
Motivation: 高质量时间序列数据获取受限（隐私、成本、标注问题），需要有效的合成数据生成方法

Method: 将时间序列转换为分位数图，再通过逆映射重构生成合成时间序列，并与GAN方法对比

Result: 基于分位数图的方法在模拟和真实数据集上都能保持原始数据的统计和结构特性，与最先进的GAN方法相比具有竞争力

Conclusion: 分位数图方法为时间序列合成提供了一种可解释且有效的替代方案

Abstract: Time series data are essential for a wide range of applications, particularly in developing robust machine learning models. However, access to high-quality datasets is often limited due to privacy concerns, acquisition costs, and labeling challenges. Synthetic time series generation has emerged as a promising solution to address these constraints. In this work, we present a framework for generating synthetic time series by leveraging complex networks mappings. Specifically, we investigate whether time series transformed into Quantile Graphs (QG) -- and then reconstructed via inverse mapping -- can produce synthetic data that preserve the statistical and structural properties of the original. We evaluate the fidelity and utility of the generated data using both simulated and real-world datasets, and compare our approach against state-of-the-art Generative Adversarial Network (GAN) methods. Results indicate that our quantile graph-based methodology offers a competitive and interpretable alternative for synthetic time series generation.

</details>


### [128] [MoVE: Mixture of Value Embeddings -- A New Axis for Scaling Parametric Memory in Autoregressive Models](https://arxiv.org/abs/2601.22887)
*Yangyan Li*

Main category: cs.LG

TL;DR: MoVE (Mixture of Value Embeddings) 是一种新的自回归建模机制，通过引入全局可学习的值嵌入库和解耦内存与计算，使模型容量可以独立于网络深度进行扩展。


<details>
  <summary>Details</summary>
Motivation: 传统自回归序列建模存在模型容量与计算成本的刚性耦合问题：增加模型参数内存（存储事实知识或视觉模式）通常需要加深或加宽网络，这会导致计算成本成比例增加。需要打破这种耦合，建立新的容量扩展维度。

Method: MoVE 引入了一个全局可学习的值嵌入库，在所有注意力层之间共享。对于序列中的每个步骤，模型使用可微分的软门控机制从该库中动态混合检索到的概念到标准值投影中。通过增加嵌入槽的数量，可以独立于网络深度扩展参数内存。

Result: 在文本生成和图像生成两个代表性自回归建模应用中，MoVE 相比标准和分层内存基线都取得了持续的性能改进。MoVE 能够构建"内存密集型"模型，在相似计算预算下实现比密集对应模型更低的困惑度和更高的保真度。

Conclusion: MoVE 成功打破了模型容量与计算成本之间的刚性耦合，为自回归建模提供了新的容量扩展维度，在保持计算效率的同时显著提升了模型性能。

Abstract: Autoregressive sequence modeling stands as the cornerstone of modern Generative AI, powering results across diverse modalities ranging from text generation to image generation. However, a fundamental limitation of this paradigm is the rigid structural coupling of model capacity to computational cost: expanding a model's parametric memory -- its repository of factual knowledge or visual patterns -- traditionally requires deepening or widening the network, which incurs a proportional rise in active FLOPs. In this work, we introduce $\textbf{MoVE (Mixture of Value Embeddings)}$, a mechanism that breaks this coupling and establishes a new axis for scaling capacity. MoVE decouples memory from compute by introducing a global bank of learnable value embeddings shared across all attention layers. For every step in the sequence, the model employs a differentiable soft gating mechanism to dynamically mix retrieved concepts from this bank into the standard value projection. This architecture allows parametric memory to be scaled independently of network depth by simply increasing the number of embedding slots. We validate MoVE through strictly controlled experiments on two representative applications of autoregressive modeling: Text Generation and Image Generation. In both domains, MoVE yields consistent performance improvements over standard and layer-wise memory baselines, enabling the construction of "memory-dense" models that achieve lower perplexity and higher fidelity than their dense counterparts at comparable compute budgets.

</details>


### [129] [PlatoLTL: Learning to Generalize Across Symbols in LTL Instructions for Multi-Task RL](https://arxiv.org/abs/2601.22891)
*Jacques Cloete,Mathias Jackermeier,Ioannis Havoutis,Alessandro Abate*

Main category: cs.LG

TL;DR: PlatoLTL：一种多任务强化学习方法，能够零样本泛化到未见过的命题词汇表，通过将命题视为参数化谓词而非离散符号来实现


<details>
  <summary>Details</summary>
Motivation: 现有基于线性时序逻辑（LTL）的多任务强化学习方法虽然能在LTL公式结构上实现泛化，但无法泛化到未见过的高层事件命题词汇表，这限制了策略的通用性

Method: 提出PlatoLTL方法，将命题视为参数化谓词而非离散符号，学习相关命题间的共享结构；设计新颖架构来嵌入和组合谓词以表示LTL规范

Result: 在多个挑战性环境中成功实现了对新颖命题和任务的零样本泛化，证明了方法的有效性

Conclusion: PlatoLTL通过参数化谓词表示实现了跨命题词汇表的泛化，扩展了LTL引导的多任务强化学习的泛化能力，为构建更通用的智能体提供了新途径

Abstract: A central challenge in multi-task reinforcement learning (RL) is to train generalist policies capable of performing tasks not seen during training. To facilitate such generalization, linear temporal logic (LTL) has recently emerged as a powerful formalism for specifying structured, temporally extended tasks to RL agents. While existing approaches to LTL-guided multi-task RL demonstrate successful generalization across LTL specifications, they are unable to generalize to unseen vocabularies of propositions (or "symbols"), which describe high-level events in LTL. We present PlatoLTL, a novel approach that enables policies to zero-shot generalize not only compositionally across LTL formula structures, but also parametrically across propositions. We achieve this by treating propositions as instances of parameterized predicates rather than discrete symbols, allowing policies to learn shared structure across related propositions. We propose a novel architecture that embeds and composes predicates to represent LTL specifications, and demonstrate successful zero-shot generalization to novel propositions and tasks across challenging environments.

</details>


### [130] [Calibrated Multivariate Distributional Regression with Pre-Rank Regularization](https://arxiv.org/abs/2601.22895)
*Aya Laajil,Elnura Zhalieva,Naomi Desobry,Souhaib Ben Taieb*

Main category: cs.LG

TL;DR: 提出基于预秩函数的正则化校准方法，用于多变量分布回归模型训练中强制多变量校准，并引入基于PCA的新预秩函数


<details>
  <summary>Details</summary>
Motivation: 尽管单变量概率预测已取得进展，但实现多变量校准仍然具有挑战性。现有预秩函数主要用于事后评估，缺乏在训练过程中强制多变量校准的方法

Method: 提出基于正则化的校准方法，在训练多变量分布回归模型时使用预秩函数强制多变量校准；引入基于PCA的新预秩函数，将预测投影到预测分布的主方向上

Result: 在模拟研究和18个真实世界多输出回归数据集上的实验表明，该方法显著改善了多变量预秩校准，且不损害预测准确性；PCA预秩能检测出现有预秩无法发现的依赖结构误设

Conclusion: 该方法为多变量概率预测提供了有效的训练时校准机制，PCA预秩函数能更好地揭示预测分布的依赖结构问题

Abstract: The goal of probabilistic prediction is to issue predictive distributions that are as informative as possible, subject to being calibrated. Despite substantial progress in the univariate setting, achieving multivariate calibration remains challenging. Recent work has introduced pre-rank functions, scalar projections of multivariate forecasts and observations, as flexible diagnostics for assessing specific aspects of multivariate calibration, but their use has largely been limited to post-hoc evaluation. We propose a regularization-based calibration method that enforces multivariate calibration during training of multivariate distributional regression models using pre-rank functions. We further introduce a novel PCA-based pre-rank that projects predictions onto principal directions of the predictive distribution. Through simulation studies and experiments on 18 real-world multi-output regression datasets, we show that the proposed approach substantially improves multivariate pre-rank calibration without compromising predictive accuracy, and that the PCA pre-rank reveals dependence-structure misspecifications that are not detected by existing pre-ranks.

</details>


### [131] [Uncertainty-Aware Extrapolation in Bayesian Oblique Trees](https://arxiv.org/abs/2601.22899)
*Viktor Andonovikj,Sašo Džeroski,Pavle Boškoski*

Main category: cs.LG

TL;DR: 提出一种结合决策树与高斯过程的贝叶斯模型，通过GP叶节点实现可靠外推和不确定性校准


<details>
  <summary>Details</summary>
Motivation: 传统决策树在回归任务中外推能力差且不确定性校准不足，叶节点预测受限于训练目标范围，在分布偏移下容易过度自信

Method: 扩展VSPYCT模型，为每个叶节点配备GP预测器；使用贝叶斯斜分割进行不确定性感知的空间划分；GP叶节点建模局部函数行为；提出高效推理预测方案，结合分割参数后验采样与GP后验预测；设计门控机制在输入超出叶节点训练支持时激活基于GP的外推

Result: 在基准回归任务中相比标准变分斜分割树有预测性能提升，在外推场景中获得显著性能增益

Conclusion: 提出的贝叶斯树模型通过GP叶节点有效解决了决策树在外推和不确定性校准方面的局限性，实现了更好的预测性能和外推能力

Abstract: Decision trees are widely used due to their interpretability and efficiency, but they struggle in regression tasks that require reliable extrapolation and well-calibrated uncertainty. Piecewise-constant leaf predictions are bounded by the training targets and often become overconfident under distribution shift. We propose a single-tree Bayesian model that extends VSPYCT by equipping each leaf with a GP predictor. Bayesian oblique splits provide uncertainty-aware partitioning of the input space, while GP leaves model local functional behaviour and enable principled extrapolation beyond the observed target range. We present an efficient inference and prediction scheme that combines posterior sampling of split parameters with \gls{gp} posterior predictions, and a gating mechanism that activates GP-based extrapolation when inputs fall outside the training support of a leaf. Experiments on benchmark regression tasks show improvements in the predictive performance compared to standard variational oblique trees, and substantial performance gains in extrapolation scenarios.

</details>


### [132] [FlexLoRA: Entropy-Guided Flexible Low-Rank Adaptation](https://arxiv.org/abs/2601.22905)
*Muqing Liu,Chongjie Si,Yuheng Jia*

Main category: cs.LG

TL;DR: FlexLoRA：一种基于熵引导的灵活低秩适应框架，通过谱能量熵评估矩阵重要性，支持在全局预算下进行秩的剪枝和扩展，使用零影响初始化确保稳定性，在参数高效微调中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型预训练模型虽然在各领域表现出色，但完全微调的计算和内存成本过高。参数高效微调（PEFT）成为主流范式，其中LoRA通过引入可训练低秩矩阵表现良好，但其固定秩设计限制了灵活性。现有的动态秩分配方法依赖启发式元素级指标，缺乏矩阵级区分，且没有机制在需要额外适应的层中扩展容量。

Method: 提出FlexLoRA框架：1）通过谱能量熵评估矩阵重要性；2）在全局预算下支持秩的剪枝和扩展；3）对新添加的奇异方向使用零影响初始化以确保稳定性。该方法解决了现有方法在粒度、灵活性和稳定性方面的限制。

Result: 大量实验表明，FlexLoRA在各种基准测试中始终优于最先进的基线方法。

Conclusion: FlexLoRA通过熵引导的灵活低秩适应，为参数高效微调提供了更原则性的解决方案，解决了现有方法的局限性，并在性能上取得了显著提升。

Abstract: Large pre-trained models achieve remarkable success across diverse domains, yet fully fine-tuning incurs prohibitive computational and memory costs. Parameter-efficient fine-tuning (PEFT) has thus become a mainstream paradigm. Among them, Low-Rank Adaptation (LoRA) introduces trainable low-rank matrices and shows strong performance, nevertheless, its fixed-rank design limits flexibility. Dynamic rank allocation methods mitigate this issue by pruning redundant directions; however, they often rely on heuristic, element-level metrics that globally sort rank directions without matrix-wise distinction, and they lack mechanisms to expand capacity in layers requiring additional adaptation. To overcome these limitations, we propose FlexLoRA, an entropy-guided flexible low-rank adaptation framework that (i) evaluates matrix importance via spectral energy entropy, (ii) supports rank pruning and expansion under a global budget, and (iii) employs zero-impact initialization for newly added singular directions to ensure stability. By addressing granularity, flexibility, and stability limitations, FlexLoRA provides a more principled solution for PEFT. Extensive experiments show that FlexLoRA consistently outperforms state-of-the-art baselines across benchmarks. Codes are available at https://github.com/Chongjie-Si/Subspace-Tuning.

</details>


### [133] [DC-LA: Difference-of-Convex Langevin Algorithm](https://arxiv.org/abs/2601.22932)
*Hoang Phuc Hau Luu,Zhongjian Wang*

Main category: cs.LG

TL;DR: 本文提出DC-LA算法，用于采样目标分布π∝exp(-f-r)，其中r是非光滑的DC函数。通过利用DC结构平滑正则项，将凹部分重新分配到数据保真项，并分析其对应的近端Langevin算法。


<details>
  <summary>Details</summary>
Motivation: 研究具有非光滑DC正则项的目标分布采样问题。传统方法在处理非光滑DC函数时面临挑战，需要开发能够利用DC结构特性的采样算法。

Method: 利用DC结构，通过Moreau包络分别平滑r1和r2。基于DC规划思想，将正则项的凹部分重新分配到数据保真项，提出DC-LA（近端Langevin算法）。

Result: 在V是距离耗散的假设下，DC-LA在q-Wasserstein距离下收敛到目标分布π（考虑离散化和平滑误差），改进了先前非对数凹采样的结果。数值实验显示DC-LA在合成设置中产生准确分布，在CT应用中可靠提供不确定性量化。

Conclusion: DC-LA算法能够有效处理具有非光滑DC正则项的采样问题，在更一般的框架和假设下改进了非对数凹采样结果，并在实际应用中表现出良好性能。

Abstract: We study a sampling problem whose target distribution is $π\propto \exp(-f-r)$ where the data fidelity term $f$ is Lipschitz smooth while the regularizer term $r=r_1-r_2$ is a non-smooth difference-of-convex (DC) function, i.e., $r_1,r_2$ are convex. By leveraging the DC structure of $r$, we can smooth out $r$ by applying Moreau envelopes to $r_1$ and $r_2$ separately. In line of DC programming, we then redistribute the concave part of the regularizer to the data fidelity and study its corresponding proximal Langevin algorithm (termed DC-LA). We establish convergence of DC-LA to the target distribution $π$, up to discretization and smoothing errors, in the $q$-Wasserstein distance for all $q \in \mathbb{N}^*$, under the assumption that $V$ is distant dissipative. Our results improve previous work on non-log-concave sampling in terms of a more general framework and assumptions. Numerical experiments show that DC-LA produces accurate distributions in synthetic settings and reliably provides uncertainty quantification in a real-world Computed Tomography application.

</details>


### [134] [Scalable Topology-Preserving Graph Coarsening with Graph Collapse](https://arxiv.org/abs/2601.22943)
*Xiang Wu,Rong-Hua Li,Xunkai Li,Kangfei Zhao,Hongchao Qin,Guoren Wang*

Main category: cs.LG

TL;DR: STPGC提出可扩展的拓扑保持图粗化方法，通过图强塌缩和边塌缩概念，在保持拓扑特征的同时降低计算复杂度，加速GNN训练。


<details>
  <summary>Details</summary>
Motivation: 现有图粗化方法要么保持谱特征要么保持空间特征，而保持拓扑特征的方法虽然能维持GNN预测性能，但存在指数级时间复杂度过高的问题。

Method: 提出STPGC方法，引入代数拓扑中的图强塌缩和图边塌缩概念，开发了GStrongCollapse、GEdgeCollapse和NeighborhoodConing三个新算法，消除支配节点和边的同时严格保持拓扑特征。

Result: 实验证明STPGC在节点分类任务中高效有效，能够保持GNN感受野，并通过近似算法加速GNN训练。

Conclusion: STPGC解决了拓扑保持图粗化的可扩展性问题，在保持拓扑特征的同时显著降低计算复杂度，为大规模图神经网络训练提供了实用解决方案。

Abstract: Graph coarsening reduces the size of a graph while preserving certain properties. Most existing methods preserve either spectral or spatial characteristics. Recent research has shown that preserving topological features helps maintain the predictive performance of graph neural networks (GNNs) trained on the coarsened graph but suffers from exponential time complexity. To address these problems, we propose Scalable Topology-Preserving Graph Coarsening (STPGC) by introducing the concepts of graph strong collapse and graph edge collapse extended from algebraic topology. STPGC comprises three new algorithms, GStrongCollapse, GEdgeCollapse, and NeighborhoodConing based on these two concepts, which eliminate dominated nodes and edges while rigorously preserving topological features. We further prove that STPGC preserves the GNN receptive field and develop approximate algorithms to accelerate GNN training. Experiments on node classification with GNNs demonstrate the efficiency and effectiveness of STPGC.

</details>


### [135] [Environment-Conditioned Tail Reweighting for Total Variation Invariant Risk Minimization](https://arxiv.org/abs/2601.22944)
*Wang Yuanchao,Lai Zhao-Rong,Zhong Tianqi,Li Fengnan*

Main category: cs.LG

TL;DR: ECTR提出了一种统一框架，通过环境条件尾部重加权增强基于总变差的IRM，同时处理环境级相关偏移和样本级多样性偏移，提升混合分布偏移下的OOD泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有IRM方法主要处理环境级的虚假相关性，但忽略了环境内部的样本级异质性（如罕见或困难样本），这会影响OOD性能。需要同时处理环境级相关偏移和样本级多样性偏移。

Method: 提出环境条件尾部重加权总变差不变风险最小化（ECTR）框架：1）基于总变差的不变学习处理环境级相关偏移；2）环境条件尾部重加权处理环境内样本级异质性；3）通过极小极大公式推断潜在环境，扩展到无环境标注场景。

Result: 在回归、表格数据、时间序列和图像分类基准测试中，在混合分布偏移下，最差环境和平均OOD性能均获得一致提升。

Conclusion: ECTR通过统一框架同时处理环境级和样本级分布偏移，使两种机制在混合分布偏移下互补，显著提升OOD泛化性能，并能扩展到无环境标注场景。

Abstract: Out-of-distribution (OOD) generalization remains challenging when models simultaneously encounter correlation shifts across environments and diversity shifts driven by rare or hard samples. Existing invariant risk minimization (IRM) methods primarily address spurious correlations at the environment level, but often overlook sample-level heterogeneity within environments, which can critically impact OOD performance. In this work, we propose \emph{Environment-Conditioned Tail Reweighting for Total Variation Invariant Risk Minimization} (ECTR), a unified framework that augments TV-based invariant learning with environment-conditioned tail reweighting to jointly address both types of distribution shift. By integrating environment-level invariance with within-environment robustness, the proposed approach makes these two mechanisms complementary under mixed distribution shifts. We further extend the framework to scenarios without explicit environment annotations by inferring latent environments through a minimax formulation. Experiments across regression, tabular, time-series, and image classification benchmarks under mixed distribution shifts demonstrate consistent improvements in both worst-environment and average OOD performance.

</details>


### [136] [Perplexity Cannot Always Tell Right from Wrong](https://arxiv.org/abs/2601.22950)
*Petar Veličković,Federico Barbero,Christos Perivolaropoulos,Simon Osindero,Razvan Pascanu*

Main category: cs.LG

TL;DR: 本文通过理论分析证明困惑度可能不适合作为模型选择指标，因为即使模型在某些序列上预测准确，也会存在其他序列具有很低困惑度但模型预测错误。


<details>
  <summary>Details</summary>
Motivation: 困惑度作为衡量模型质量的简单计算指标近年来得到广泛应用，但先前研究已指出其局限性。本文旨在通过严谨的理论分析证明困惑度可能不适合作为模型选择的标准。

Method: 利用Transformer连续性的最新结果进行理论证明：1) 证明如果紧凑的解码器Transformer模型能准确预测某个序列，则必然存在另一个序列具有很低困惑度但模型预测错误；2) 通过分析等困惑度图，研究困惑度与模型选择的关系。

Result: 理论证明表明：1) 任何能准确预测某些序列的紧凑Transformer模型，必然存在其他序列具有极低困惑度但模型预测错误；2) 困惑度并不总是选择更准确的模型，只有当模型置信度增加的同时准确率也相应提高时，新模型才会被选中。

Conclusion: 困惑度作为模型选择指标存在根本性缺陷，可能选择在特定序列上表现不佳的模型。模型置信度的增加必须伴随准确率的相应提高，困惑度才能有效选择更好的模型。

Abstract: Perplexity -- a function measuring a model's overall level of "surprise" when encountering a particular output -- has gained significant traction in recent years, both as a loss function and as a simple-to-compute metric of model quality. Prior studies have pointed out several limitations of perplexity, often from an empirical manner. Here we leverage recent results on Transformer continuity to show in a rigorous manner how perplexity may be an unsuitable metric for model selection. Specifically, we prove that, if there is any sequence that a compact decoder-only Transformer model predicts accurately and confidently -- a necessary pre-requisite for strong generalisation -- it must imply existence of another sequence with very low perplexity, but not predicted correctly by that same model. Further, by analytically studying iso-perplexity plots, we find that perplexity will not always select for the more accurate model -- rather, any increase in model confidence must be accompanied by a commensurate rise in accuracy for the new model to be selected.

</details>


### [137] [Improved Algorithms for Nash Welfare in Linear Bandits](https://arxiv.org/abs/2601.22969)
*Dhruv Sarkar,Nishant Pandey,Sayak Ray Chowdhury*

Main category: cs.LG

TL;DR: 本文解决了线性bandit中Nash regret的次优性问题，提出了新的分析工具获得最优界，并首次研究了p-means regret框架，提出了通用的FairLinBandit算法框架。


<details>
  <summary>Details</summary>
Motivation: 现有线性bandit中的Nash regret结果存在次优性，源于依赖限制性的集中不等式。需要新的分析工具来解决这个开放问题，并扩展研究更一般的p-means regret框架。

Method: 提出了新的分析工具来解决Nash regret的次优性问题，并提出了通用的FairLinBandit算法框架，该框架可作为元算法运行在任何线性bandit策略之上。具体实例化了Phased Elimination和Upper Confidence Bound两种算法。

Result: 获得了线性bandit中Nash regret的最优界，证明了两种算法实例都能在整个p值范围内实现次线性的p-means regret。在真实数据集生成的线性bandit实例上的实验表明，方法始终优于现有最先进基线。

Conclusion: 本文解决了线性bandit中Nash regret的开放问题，提出了新的分析工具和通用的FairLinBandit框架，首次研究了p-means regret，为公平性和效用目标提供了统一的框架。

Abstract: Nash regret has recently emerged as a principled fairness-aware performance metric for stochastic multi-armed bandits, motivated by the Nash Social Welfare objective. Although this notion has been extended to linear bandits, existing results suffer from suboptimality in ambient dimension $d$, stemming from proof techniques that rely on restrictive concentration inequalities. In this work, we resolve this open problem by introducing new analytical tools that yield an order-optimal Nash regret bound in linear bandits. Beyond Nash regret, we initiate the study of $p$-means regret in linear bandits, a unifying framework that interpolates between fairness and utility objectives and strictly generalizes Nash regret. We propose a generic algorithmic framework, FairLinBandit, that works as a meta-algorithm on top of any linear bandit strategy. We instantiate this framework using two bandit algorithms: Phased Elimination and Upper Confidence Bound, and prove that both achieve sublinear $p$-means regret for the entire range of $p$. Extensive experiments on linear bandit instances generated from real-world datasets demonstrate that our methods consistently outperform the existing state-of-the-art baseline.

</details>


### [138] [Stabilizing the Q-Gradient Field for Policy Smoothness in Actor-Critic](https://arxiv.org/abs/2601.22970)
*Jeong Woon Lee,Kyoleen Kwak,Daeho Kim,Hyoseok Hwang*

Main category: cs.LG

TL;DR: PAVE：通过正则化critic的几何结构来稳定策略，避免直接正则化策略输出，解决了actor-critic方法中策略振荡的问题。


<details>
  <summary>Details</summary>
Motivation: 连续actor-critic方法学习的策略经常表现出不稳定的高频振荡，不适合物理部署。现有方法直接正则化策略输出，但这只是治标不治本。

Method: 理论证明策略平滑性由critic的微分几何决定，提出PAVE框架，将critic视为标量场，通过最小化Q梯度波动同时保持局部曲率来稳定其诱导的动作梯度场。

Result: PAVE在实现平滑性和鲁棒性方面与策略侧平滑正则化方法相当，同时保持竞争力的任务性能，且无需修改actor。

Conclusion: 策略非平滑性根本上由critic的微分几何控制，通过正则化critic而非策略本身，可以更有效地获得平滑策略。

Abstract: Policies learned via continuous actor-critic methods often exhibit erratic, high-frequency oscillations, making them unsuitable for physical deployment. Current approaches attempt to enforce smoothness by directly regularizing the policy's output. We argue that this approach treats the symptom rather than the cause. In this work, we theoretically establish that policy non-smoothness is fundamentally governed by the differential geometry of the critic. By applying implicit differentiation to the actor-critic objective, we prove that the sensitivity of the optimal policy is bounded by the ratio of the Q-function's mixed-partial derivative (noise sensitivity) to its action-space curvature (signal distinctness). To empirically validate this theoretical insight, we introduce PAVE (Policy-Aware Value-field Equalization), a critic-centric regularization framework that treats the critic as a scalar field and stabilizes its induced action-gradient field. PAVE rectifies the learning signal by minimizing the Q-gradient volatility while preserving local curvature. Experimental results demonstrate that PAVE achieves smoothness and robustness comparable to policy-side smoothness regularization methods, while maintaining competitive task performance, without modifying the actor.

</details>


### [139] [Learnable Permutation for Structured Sparsity on Transformer Models](https://arxiv.org/abs/2601.22980)
*Zekai Li,Ji Liu,Guanchen Li,Yixing Xu,Ziqiong Liu,Xuanwu Yin,Dong Li,Emad Barsoum*

Main category: cs.LG

TL;DR: 提出一种可端到端学习的权重置换框架，通过可学习的置换成本矩阵、可微二分图匹配求解器和稀疏优化损失函数，优化Transformer模型的结构化稀疏性。


<details>
  <summary>Details</summary>
Motivation: 结构化稀疏已成为流行的模型剪枝技术，权重置换可以重新排序模型权重以获得更好的剪枝模式。然而，Transformer架构中置换搜索空间随规模指数增长，现有方法依赖贪婪或启发式算法，限制了重新排序的效果。

Method: 提出端到端可学习的置换框架：1) 引入可学习的置换成本矩阵量化交换任意两个输入通道的成本；2) 使用可微二分图匹配求解器获取给定成本矩阵下的最优二进制置换矩阵；3) 设计稀疏优化损失函数直接优化置换算子。

Result: 在视觉和语言Transformer上广泛验证，该方法在结构化稀疏方面实现了最先进的置换结果。

Conclusion: 提出的端到端可学习置换框架能够有效优化Transformer模型的权重排列，为结构化稀疏剪枝提供了更优的解决方案。

Abstract: Structured sparsity has emerged as a popular model pruning technique, widely adopted in various architectures, including CNNs, Transformer models, and especially large language models (LLMs) in recent years. A promising direction to further improve post-pruning performance is weight permutation, which reorders model weights into patterns more amenable to pruning. However, the exponential growth of the permutation search space with the scale of Transformer architectures forces most methods to rely on greedy or heuristic algorithms, limiting the effectiveness of reordering.
  In this work, we propose a novel end-to-end learnable permutation framework. Our method introduces a learnable permutation cost matrix to quantify the cost of swapping any two input channels of a given weight matrix, a differentiable bipartite matching solver to obtain the optimal binary permutation matrix given a cost matrix, and a sparsity optimization loss function to directly optimize the permutation operator. We extensively validate our approach on vision and language Transformers, demonstrating that our method achieves state-of-the-art permutation results for structured sparsity.

</details>


### [140] [dgMARK: Decoding-Guided Watermarking for Diffusion Language Models](https://arxiv.org/abs/2601.22985)
*Pyo Min Hong,Albert No*

Main category: cs.LG

TL;DR: dgMARK是一种针对离散扩散语言模型的解码引导水印方法，通过引导解掩码顺序来实现水印嵌入，无需显式重新加权模型概率。


<details>
  <summary>Details</summary>
Motivation: 离散扩散语言模型（dLLMs）可以按任意顺序生成token，而实际模型对解掩码顺序表现出强烈敏感性，这为水印技术创造了新的通道。

Method: dgMARK引导解掩码顺序朝向那些高奖励候选token满足由二进制哈希诱导的简单奇偶约束的位置，无需显式重新加权模型学习到的概率。方法可与常见解码策略（如置信度、熵和基于边界的排序）即插即用，并可通过一步前瞻变体加强。

Result: 水印通过提升的奇偶匹配统计量检测，滑动窗口检测器确保在插入、删除、替换和改写等后编辑操作下的鲁棒性。

Conclusion: dgMARK为离散扩散语言模型提供了一种有效的解码引导水印方法，利用模型对解掩码顺序的敏感性，实现了鲁棒且实用的水印嵌入和检测机制。

Abstract: We propose dgMARK, a decoding-guided watermarking method for discrete diffusion language models (dLLMs). Unlike autoregressive models, dLLMs can generate tokens in arbitrary order. While an ideal conditional predictor would be invariant to this order, practical dLLMs exhibit strong sensitivity to the unmasking order, creating a new channel for watermarking. dgMARK steers the unmasking order toward positions whose high-reward candidate tokens satisfy a simple parity constraint induced by a binary hash, without explicitly reweighting the model's learned probabilities. The method is plug-and-play with common decoding strategies (e.g., confidence, entropy, and margin-based ordering) and can be strengthened with a one-step lookahead variant. Watermarks are detected via elevated parity-matching statistics, and a sliding-window detector ensures robustness under post-editing operations including insertion, deletion, substitution, and paraphrasing.

</details>


### [141] [Value-at-Risk Constrained Policy Optimization](https://arxiv.org/abs/2601.22993)
*Rohan Tangri,Jan-Peter Calliess*

Main category: cs.LG

TL;DR: 提出VaR-CPO算法，直接优化风险价值约束，实现安全探索和零约束违反


<details>
  <summary>Details</summary>
Motivation: 现有方法无法在训练过程中保证零约束违反，需要一种能够直接优化VaR约束的安全强化学习方法

Method: 使用单边切比雪夫不等式处理VaR约束的非可微性，基于成本回报的前两矩获得可处理的替代约束；扩展CPO的信任区域框架

Result: VaR-CPO能够在可行环境中实现零约束违反训练，提供策略改进和约束违反的最坏情况理论保证

Conclusion: VaR-CPO是一种样本高效且保守的方法，能够直接优化VaR约束，在安全强化学习中具有重要应用价值

Abstract: We introduce the Value-at-Risk Constrained Policy Optimization algorithm (VaR-CPO), a sample efficient and conservative method designed to optimize Value-at-Risk (VaR) constraints directly. Empirically, we demonstrate that VaR-CPO is capable of safe exploration, achieving zero constraint violations during training in feasible environments, a critical property that baseline methods fail to uphold. To overcome the inherent non-differentiability of the VaR constraint, we employ the one-sided Chebyshev inequality to obtain a tractable surrogate based on the first two moments of the cost return. Additionally, by extending the trust-region framework of the Constrained Policy Optimization (CPO) method, we provide rigorous worst-case bounds for both policy improvement and constraint violation during the training process.

</details>


### [142] [Mano: Restriking Manifold Optimization for LLM Training](https://arxiv.org/abs/2601.23000)
*Yufei Gu,Zeke Xie*

Main category: cs.LG

TL;DR: 提出名为Mano的新型优化器，通过将动量投影到参数切空间并约束在旋转斜流形上，首次弥合了流形优化与现代优化器之间的性能差距，在LLaMA和Qwen3模型上优于AdamW和Muon，且内存和计算成本更低。


<details>
  <summary>Details</summary>
Motivation: 当前主流优化器存在局限性：AdamW依赖对角曲率估计而忽略结构特性，Muon使用全局谱归一化但丢失曲率信息。传统流形优化方法在大规模模型优化中表现不佳，需要开发能结合两者优势的新方法。

Method: 提出Mano优化器，创新性地将动量投影到模型参数的切空间，并将其约束在旋转斜流形上，从而有效结合曲率信息和结构特性，同时保持计算效率。

Result: 在LLaMA和Qwen3模型上的实验表明，Mano在性能上持续显著优于AdamW和Muon，同时分别减少了内存消耗和计算复杂度，扩展了时空效率的帕累托前沿。

Conclusion: Mano是首个成功弥合流形优化与现代优化器性能差距的方法，为大规模语言模型训练提供了更高效、更强大的优化方案，在保持性能优势的同时降低了资源需求。

Abstract: While large language models (LLMs) have emerged as a significant advancement in artificial intelligence, the hardware and computational costs for training LLMs are also significantly burdensome. Among the state-of-the-art optimizers, AdamW relies on diagonal curvature estimates and ignores structural properties, while Muon applies global spectral normalization at the expense of losing curvature information. In this study, we restriked manifold optimization methods for training LLMs, which may address both optimizers' limitations, while conventional manifold optimization methods have been largely overlooked due to the poor performance in large-scale model optimization. By innovatively projecting the momentum onto the tangent space of model parameters and constraining it on a rotational Oblique manifold, we propose a novel, powerful, and efficient optimizer **Mano** that is the first to bridge the performance gap between manifold optimization and modern optimizers. Extensive experiments on the LLaMA and Qwen3 models demonstrate that Mano consistently and significantly outperforms AdamW and Muon even with less memory consumption and computational complexity, respectively, suggesting an expanded Pareto frontier in terms of space and time efficiency.

</details>


### [143] [Automatic Constraint Policy Optimization based on Continuous Constraint Interpolation Framework for Offline Reinforcement Learning](https://arxiv.org/abs/2601.23010)
*Xinchen Han,Qiuyang Fang,Hossam Afifi,Michel Marot*

Main category: cs.LG

TL;DR: 提出Continuous Constraint Interpolation (CCI)框架，统一离线强化学习中的三种约束类型，并开发ACPO算法自适应调整约束参数，在多个基准上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有离线强化学习方法通常只采用单一类型的策略约束（如加权行为克隆、密度正则化或支持约束），缺乏统一的框架来解释它们之间的联系和权衡。

Method: 提出CCI框架，通过单个插值参数实现三种约束类型的平滑过渡和组合；基于此开发ACPO算法，使用原始-对偶方法自适应调整插值参数。

Result: 在D4RL和NeoRL2基准测试中表现出色，实现了稳健的性能提升，整体达到最先进的性能水平。

Conclusion: CCI框架为离线强化学习的约束方法提供了统一的理论基础，ACPO算法通过自适应约束插值实现了优越的性能，为离线RL的约束设计提供了新思路。

Abstract: Offline Reinforcement Learning (RL) relies on policy constraints to mitigate extrapolation error, where both the constraint form and constraint strength critically shape performance. However, most existing methods commit to a single constraint family: weighted behavior cloning, density regularization, or support constraints, without a unified principle that explains their connections or trade-offs. In this work, we propose Continuous Constraint Interpolation (CCI), a unified optimization framework in which these three constraint families arise as special cases along a common constraint spectrum. The CCI framework introduces a single interpolation parameter that enables smooth transitions and principled combinations across constraint types. Building on CCI, we develop Automatic Constraint Policy Optimization (ACPO), a practical primal--dual algorithm that adapts the interpolation parameter via a Lagrangian dual update. Moreover, we establish a maximum-entropy performance difference lemma and derive performance lower bounds for both the closed-form optimal policy and its parametric projection. Experiments on D4RL and NeoRL2 demonstrate robust gains across diverse domains, achieving state-of-the-art performance overall.

</details>


### [144] [Leveraging Convolutional Sparse Autoencoders for Robust Movement Classification from Low-Density sEMG](https://arxiv.org/abs/2601.23011)
*Blagoj Hristov,Zoran Hadzi-Velkov,Katerina Hadzi-Velkova Saneva,Gorjan Nadzinski,Vesna Ojleska Latkoska*

Main category: cs.LG

TL;DR: 提出一个仅使用两个表面肌电通道的深度学习框架，通过卷积稀疏自编码器提取特征，实现高精度手势识别，并支持少样本迁移学习和增量学习。


<details>
  <summary>Details</summary>
Motivation: 传统肌电假肢控制面临受试者间差异大和高密度传感器阵列临床不实用的挑战，需要开发更简单、适应性强的解决方案。

Method: 使用卷积稀疏自编码器直接从原始信号提取时间特征，避免启发式特征工程；采用少样本迁移学习处理受试者差异；通过增量学习策略支持功能扩展。

Result: 在6类手势集上获得94.3%±0.3%的多受试者F1分数；少样本迁移学习将未见受试者性能从35.1%±3.1%提升至92.3%±0.9%；增量学习扩展到10类手势集保持90.0%±0.2%的F1分数。

Conclusion: 该框架结合高精度、低计算和传感器开销，为下一代经济、自适应的假肢系统提供了可扩展且高效的解决方案。

Abstract: Reliable control of myoelectric prostheses is often hindered by high inter-subject variability and the clinical impracticality of high-density sensor arrays. This study proposes a deep learning framework for accurate gesture recognition using only two surface electromyography (sEMG) channels. The method employs a Convolutional Sparse Autoencoder (CSAE) to extract temporal feature representations directly from raw signals, eliminating the need for heuristic feature engineering. On a 6-class gesture set, our model achieved a multi-subject F1-score of 94.3% $\pm$ 0.3%. To address subject-specific differences, we present a few-shot transfer learning protocol that improved performance on unseen subjects from a baseline of 35.1% $\pm$ 3.1% to 92.3% $\pm$ 0.9% with minimal calibration data. Furthermore, the system supports functional extensibility through an incremental learning strategy, allowing for expansion to a 10-class set with a 90.0% $\pm$ 0.2% F1-score without full model retraining. By combining high precision with minimal computational and sensor overhead, this framework provides a scalable and efficient approach for the next generation of affordable and adaptive prosthetic systems.

</details>


### [145] [Mem-T: Densifying Rewards for Long-Horizon Memory Agents](https://arxiv.org/abs/2601.23014)
*Yanwei Yue,Guibin Zhang,Boci Peng,Xuanbo Fan,Jiaxin Guo,Qiankun Li,Yan Zhang*

Main category: cs.LG

TL;DR: Mem-T是一个自主记忆代理，通过层次化内存数据库和树引导强化学习框架，实现动态内存管理和检索的端到端优化，在性能和效率上均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有记忆代理训练范式受限，需要在长序列记忆操作后才能获得稀疏延迟奖励，难以实现内存管理策略的真正端到端优化。

Method: 提出Mem-T自主记忆代理，结合轻量级层次化内存数据库进行动态更新和多轮检索；引入MoT-GRPO树引导强化学习框架，通过内存操作树反向传播和事后信用分配将稀疏终端反馈转化为密集的步进监督。

Result: Mem-T性能优于A-Mem和Mem0等框架达14.92%；在准确率-效率帕累托前沿表现优异，相比GAM减少约24.45%的推理token消耗而不损失性能。

Conclusion: Mem-T通过端到端优化内存构建和检索，实现了高性能且经济的自主记忆管理，为长时程记忆操作训练提供了有效解决方案。

Abstract: Memory agents, which depart from predefined memory-processing pipelines by endogenously managing the processing, storage, and retrieval of memories, have garnered increasing attention for their autonomy and adaptability. However, existing training paradigms remain constrained: agents often traverse long-horizon sequences of memory operations before receiving sparse and delayed rewards, which hinders truly end-to-end optimization of memory management policies. To address this limitation, we introduce Mem-T, an autonomous memory agent that interfaces with a lightweight hierarchical memory database to perform dynamic updates and multi-turn retrieval over streaming inputs. To effectively train long-horizon memory management capabilities, we further propose MoT-GRPO, a tree-guided reinforcement learning framework that transforms sparse terminal feedback into dense, step-wise supervision via memory operation tree backpropagation and hindsight credit assignment, thereby enabling the joint optimization of memory construction and retrieval. Extensive experiments demonstrate that Mem-T is (1) high-performing, surpassing frameworks such as A-Mem and Mem0 by up to $14.92\%$, and (2) economical, operating on a favorable accuracy-efficiency Pareto frontier and reducing inference tokens per query by $\sim24.45\%$ relative to GAM without sacrificing performance.

</details>


### [146] [Causal Characterization of Measurement and Mechanistic Anomalies](https://arxiv.org/abs/2601.23026)
*Hendrik Suhr,David Kaltenpoth,Jilles Vreeken*

Main category: cs.LG

TL;DR: 提出一个因果模型，将异常分为测量误差和机制偏移两种类型，通过潜在干预建模实现可识别性，并在根因定位和异常分类上表现优异


<details>
  <summary>Details</summary>
Motivation: 现有异常根因分析方法忽略了异常可能源于两种根本不同的过程：测量误差（数据正常生成但记录错误）和机制偏移（数据生成过程本身发生变化）。这两种异常需要不同的处理方式，测量误差可以安全修正，而机制异常需要仔细考虑。

Method: 定义了一个因果模型，通过将异常视为对潜在变量（"真实"变量）和观测变量（"测量"变量）的潜在干预来显式捕获两种异常类型。证明了这两种异常类型的可识别性，并提出了最大似然估计方法来实现这一模型。

Result: 实验表明，该方法在根因定位方面达到了最先进的性能，同时还能准确分类异常类型，即使在因果DAG未知的情况下也能保持鲁棒性。

Conclusion: 通过显式建模测量误差和机制偏移两种异常类型，提出的因果模型不仅能有效定位根因，还能区分异常类型，为异常分析提供了更全面的框架。

Abstract: Root cause analysis of anomalies aims to identify those features that cause the deviation from the normal process. Existing methods ignore, however, that anomalies can arise through two fundamentally different processes: measurement errors, where data was generated normally but one or more values were recorded incorrectly, and mechanism shifts, where the causal process generating the data changed. While measurement errors can often be safely corrected, mechanistic anomalies require careful consideration. We define a causal model that explicitly captures both types by treating outliers as latent interventions on latent ("true") and observed ("measured") variables. We show that they are identifiable, and propose a maximum likelihood estimation approach to put this to practice. Experiments show that our method matches state-of-the-art performance in root cause localization, while it additionally enables accurate classification of anomaly types, and remains robust even when the causal DAG is unknown.

</details>


### [147] [Divide-and-Conquer CoT: RL for Reducing Latency via Parallel Reasoning](https://arxiv.org/abs/2601.23027)
*Arvind Mahankali,Kaiyue Wen,Tengyu Ma*

Main category: cs.LG

TL;DR: 提出DC-CoT方法，通过并行推理减少长思维链的延迟，在保持准确率的同时将最长路径长度降低35-40%


<details>
  <summary>Details</summary>
Motivation: 长思维链推理导致LLM生成延迟高，因为生成过程高度顺序化。需要降低推理延迟同时保持高准确率

Method: 训练Divide-and-Conquer CoT模型，让模型作为指导者识别可并行执行的子任务，然后生成工作线程执行。采用多阶段RL算法，结合数据过滤策略，从SFT初始化开始恢复准确率

Result: 在AIME 2024和HMMT 2025等基准测试中，DC-CoT达到与DeepScaleR-1.5B-Preview相似的准确率，同时将最长路径长度降低35-40%

Conclusion: DC-CoT能有效降低长思维链推理的延迟，为低延迟并行推理提供了可行方案

Abstract: Long chain-of-thought reasoning (Long CoT) is now fundamental to state-of-the-art LLMs, especially in mathematical reasoning. However, LLM generation is highly sequential, and long CoTs lead to a high latency. We propose to train Divide-and-Conquer CoT (DC-CoT) to reduce the latency. With DC-CoT, the model can act as a director that identifies distinct subtasks that can be performed in parallel in its reasoning process, and then spawns workers to execute the subtasks. Our goal is to achieve high accuracy, with a low longest path length, which is a theoretical measure of the latency needed for the response. We start with a long CoT base model (DeepScaleR-1.5B-Preview), and first use SFT with a small curated demonstration set to initialize its ability to spawn workers in a certain format. Because SFT degrades the accuracy significantly, we design a multi-stage RL algorithm, with various data filtering strategies, to recover the accuracy while decreasing the longest path length. Across several benchmarks including AIME 2024 and HMMT 2025, DC-CoT achieves similar accuracy as DeepScaleR-1.5B-Preview while decreasing longest path length by 35-40%. Our code, SFT dataset and models are publicly available at https://github.com/amahankali10/DC_CoT_RL_for_Low_Latency_CoT_with_Parallel_Reasoning.

</details>


### [148] [Avoiding Premature Collapse: Adaptive Annealing for Entropy-Regularized Structural Inference](https://arxiv.org/abs/2601.23039)
*Yizhi Liu*

Main category: cs.LG

TL;DR: 论文分析了可微匹配层中通过退火恢复离散排列的不稳定性问题，提出了"过早模式崩溃"机制和热力学速度限制理论，并开发了自适应调度算法Efficient PH-ASC来解决该问题。


<details>
  <summary>Details</summary>
Motivation: 可微匹配层（通常通过熵正则化最优传输实现）是结构化预测中的关键近似推理机制，但通过退火ε→0恢复离散排列的过程极不稳定。论文旨在解决这一根本问题。

Method: 通过分析Sinkhorn不动点映射的非正规动力学，揭示了热力学速度限制理论。提出了Efficient PH-ASC自适应调度算法，通过监控推理过程的稳定性并强制执行线性稳定性定律，将昂贵的谱诊断与训练循环解耦。

Result: 提出的Efficient PH-ASC算法将计算开销从O(N³)降低到摊销O(1)，解决了过早模式崩溃问题。提供了开源实现和交互式演示。

Conclusion: 论文揭示了可微匹配层中退火不稳定的根本机制，提出了理论框架和高效算法，为结构化预测中的稳定推理提供了解决方案。

Abstract: Differentiable matching layers, often implemented via entropy-regularized Optimal Transport, serve as a critical approximate inference mechanism in structural prediction. However, recovering discrete permutations via annealing $ε\to 0$ is notoriously unstable. We identify a fundamental mechanism for this failure: \textbf{Premature Mode Collapse}. By analyzing the non-normal dynamics of the Sinkhorn fixed-point map, we reveal a theoretical \textbf{thermodynamic speed limit}. Under standard exponential cooling, the shift in the target posterior ($O(1)$) outpaces the contraction rate of the inference operator, which degrades as $O(1/ε)$. This mismatch inevitably forces the inference trajectory into spurious local basins. To address this, we propose \textbf{Efficient PH-ASC}, an adaptive scheduling algorithm that monitors the stability of the inference process. By enforcing a linear stability law, we decouple expensive spectral diagnostics from the training loop, reducing overhead from $O(N^3)$ to amortized $O(1)$. Our implementation and interactive demo are available at https://github.com/xxx0438/torch-sinkhorn-asc and https://huggingface.co/spaces/leon0923/torch-sinkhorn-asc-demo. bounded away from zero in generic training dynamics unless the feature extractor converges unrealistically fast.

</details>


### [149] [Adaptive Edge Learning for Density-Aware Graph Generation](https://arxiv.org/abs/2601.23052)
*Seyedeh Ava Razi Razavi,James Sargant,Sheridan Houghten,Renata Dividino*

Main category: cs.LG

TL;DR: 提出基于Wasserstein GAN的密度感知条件图生成框架，用可学习的距离边预测器替代随机采样，能生成结构连贯、类别一致的图数据


<details>
  <summary>Details</summary>
Motivation: 传统图生成方法通常依赖固定概率的随机边采样，难以捕捉节点间复杂的结构依赖关系，限制了生成图的真实性和类别特异性

Method: 使用WGAN-GP框架，将节点嵌入到潜在空间，通过可微边预测器从节点嵌入直接确定边关系，密度感知选择机制自适应控制边密度以匹配真实图的稀疏分布

Result: 在基准数据集上优于现有基线，生成图具有更好的结构连贯性和类别一致性，边预测器能捕捉复杂关系模式，密度和拓扑更接近真实分布

Conclusion: 该方法实现了更稳定的训练和可控的合成，为真实图生成和数据增强提供了有效框架，代码已开源

Abstract: Generating realistic graph-structured data is challenging due to discrete structures, variable sizes, and class-specific connectivity patterns that resist conventional generative modelling. While recent graph generation methods employ generative adversarial network (GAN) frameworks to handle permutation invariance and irregular topologies, they typically rely on random edge sampling with fixed probabilities, limiting their capacity to capture complex structural dependencies between nodes. We propose a density-aware conditional graph generation framework using Wasserstein GANs (WGAN) that replaces random sampling with a learnable distance-based edge predictor. Our approach embeds nodes into a latent space where proximity correlates with edge likelihood, enabling the generator to learn meaningful connectivity patterns. A differentiable edge predictor determines pairwise relationships directly from node embeddings, while a density-aware selection mechanism adaptively controls edge density to match class-specific sparsity distributions observed in real graphs. We train the model using a WGAN with gradient penalty, employing a GCN-based critic to ensure generated graphs exhibit realistic topology and align with target class distributions. Experiments on benchmark datasets demonstrate that our method produces graphs with superior structural coherence and class-consistent connectivity compared to existing baselines. The learned edge predictor captures complex relational patterns beyond simple heuristics, generating graphs whose density and topology closely match real structural distributions. Our results show improved training stability and controllable synthesis, making the framework effective for realistic graph generation and data augmentation. Source code is publicly available at https://github.com/ava-12/Density_Aware_WGAN.git.

</details>


### [150] [From Absolute to Relative: Rethinking Reward Shaping in Group-Based Reinforcement Learning](https://arxiv.org/abs/2601.23058)
*Wenzhe Niu,Wei He,Zongxia Xie,Jinpeng Ou,Huichuan Fan,Yuchen Ge,Yanru Sun,Ziyin Wang,Yizhao Sun,Chengshun Shi,Jiuchong Gao,Jinghua Hao,Renqing He*

Main category: cs.LG

TL;DR: 提出RLRR框架，将强化学习中的绝对奖励转换为相对排名奖励，解决现有基于组的强化学习方法中奖励稀疏和模型评分不稳定的问题


<details>
  <summary>Details</summary>
Motivation: 现有基于组的强化学习方法（如GRPO）依赖绝对数值奖励存在固有局限：在可验证任务中，相同组评估导致监督稀疏；在开放式任务中，奖励模型评分范围不稳定，影响基于组均值的优势估计

Method: 提出RLRR框架，将奖励塑造从绝对评分转向相对排名；引入Ranking Reward Model，这是一个为基于组优化定制的列表偏好模型，可直接生成相对排名；将原始评估转换为稳健的相对信号

Result: 实验结果表明，RLRR在推理基准测试和开放式生成任务中，相比标准的基于组基线方法，能带来一致的性能提升

Conclusion: 通过将绝对奖励转换为相对排名，RLRR有效缓解了信号稀疏和奖励不稳定的问题，为基于组的强化学习方法提供了更稳健的优化框架

Abstract: Reinforcement learning has become a cornerstone for enhancing the reasoning capabilities of Large Language Models, where group-based approaches such as GRPO have emerged as efficient paradigms that optimize policies by leveraging intra-group performance differences. However, these methods typically rely on absolute numerical rewards, introducing intrinsic limitations. In verifiable tasks, identical group evaluations often result in sparse supervision, while in open-ended scenarios, the score range instability of reward models undermines advantage estimation based on group means. To address these limitations, we propose Reinforcement Learning with Relative Rewards (RLRR), a framework that shifts reward shaping from absolute scoring to relative ranking. Complementing this framework, we introduce the Ranking Reward Model, a listwise preference model tailored for group-based optimization to directly generate relative rankings. By transforming raw evaluations into robust relative signals, RLRR effectively mitigates signal sparsity and reward instability. Experimental results demonstrate that RLRR yields consistent performance improvements over standard group-based baselines across reasoning benchmarks and open-ended generation tasks.

</details>


### [151] [ExplainerPFN: Towards tabular foundation models for model-free zero-shot feature importance estimations](https://arxiv.org/abs/2601.23068)
*Joao Fonseca,Julia Stoyanovich*

Main category: cs.LG

TL;DR: ExplainerPFN：首个零样本Shapley值估计方法，无需访问底层模型或参考解释，通过预训练在合成数据上的表格基础模型实现


<details>
  <summary>Details</summary>
Motivation: Shapley值广泛用于模型解释，但需要访问底层模型且计算成本高，现实部署中常无法满足这些条件，需要零样本解释方法

Method: 基于TabPFN构建ExplainerPFN表格基础模型，在随机结构因果模型生成的合成数据集上预训练，使用精确或近似Shapley值监督学习，训练后无需模型访问即可预测特征归因

Result: 实验表明：1）少样本学习解释能达到高保真度；2）ExplainerPFN性能与依赖2-10个SHAP示例的少样本替代解释器相当；3）开源实现可用

Conclusion: ExplainerPFN是首个零样本Shapley值估计方法，无需模型访问即可提供有意义的特征重要性解释，为模型可解释性提供了实用解决方案

Abstract: Computing the importance of features in supervised classification tasks is critical for model interpretability. Shapley values are a widely used approach for explaining model predictions, but require direct access to the underlying model, an assumption frequently violated in real-world deployments. Further, even when model access is possible, their exact computation may be prohibitively expensive. We investigate whether meaningful Shapley value estimations can be obtained in a zero-shot setting, using only the input data distribution and no evaluations of the target model. To this end, we introduce ExplainerPFN, a tabular foundation model built on TabPFN that is pretrained on synthetic datasets generated from random structural causal models and supervised using exact or near-exact Shapley values. Once trained, ExplainerPFN predicts feature attributions for unseen tabular datasets without model access, gradients, or example explanations.
  Our contributions are fourfold: (1) we show that few-shot learning-based explanations can achieve high fidelity to SHAP values with as few as two reference observations; (2) we propose ExplainerPFN, the first zero-shot method for estimating Shapley values without access to the underlying model or reference explanations; (3) we provide an open-source implementation of ExplainerPFN, including the full training pipeline and synthetic data generator; and (4) through extensive experiments on real and synthetic datasets, we show that ExplainerPFN achieves performance competitive with few-shot surrogate explainers that rely on 2-10 SHAP examples.

</details>


### [152] [SplineFlow: Flow Matching for Dynamical Systems with B-Spline Interpolants](https://arxiv.org/abs/2601.23072)
*Santanu Subhash Rathod,Pietro Liò,Xiao Zhang*

Main category: cs.LG

TL;DR: SplineFlow是一种基于B样条插值的流匹配算法，专门用于建模动态系统，相比现有方法能更好地处理不规则采样观测数据并学习高阶动态。


<details>
  <summary>Details</summary>
Motivation: 现有流匹配方法不适合建模动态系统，因为它们使用线性插值构建条件路径，无法捕捉底层状态演化，特别是在从不规则采样观测中学习高阶动态时。构建满足多边际约束的统一路径具有挑战性，因为朴素的高阶多项式往往不稳定且振荡。

Method: 提出SplineFlow算法，通过B样条插值联合建模观测间的条件路径。利用B样条基函数的平滑性和稳定性，以结构化方式学习复杂底层动态，同时确保满足多边际要求。

Result: 在各种复杂度的确定性和随机动态系统以及细胞轨迹推断任务上的综合实验表明，SplineFlow相比现有基线方法有显著改进。

Conclusion: SplineFlow是一种理论基础的流匹配算法，通过B样条插值有效解决了动态系统建模中的路径构建问题，在多个应用场景中表现出优越性能。

Abstract: Flow matching is a scalable generative framework for characterizing continuous normalizing flows with wide-range applications. However, current state-of-the-art methods are not well-suited for modeling dynamical systems, as they construct conditional paths using linear interpolants that may not capture the underlying state evolution, especially when learning higher-order dynamics from irregular sampled observations. Constructing unified paths that satisfy multi-marginal constraints across observations is challenging, since naïve higher-order polynomials tend to be unstable and oscillatory. We introduce SplineFlow, a theoretically grounded flow matching algorithm that jointly models conditional paths across observations via B-spline interpolation. Specifically, SplineFlow exploits the smoothness and stability of B-spline bases to learn the complex underlying dynamics in a structured manner while ensuring the multi-marginal requirements are met. Comprehensive experiments across various deterministic and stochastic dynamical systems of varying complexity, as well as on cellular trajectory inference tasks, demonstrate the strong improvement of SplineFlow over existing baselines. Our code is available at: https://github.com/santanurathod/SplineFlow.

</details>


### [153] [RN-D: Discretized Categorical Actors with Regularized Networks for On-Policy Reinforcement Learning](https://arxiv.org/abs/2601.23075)
*Yuexin Bian,Jie Feng,Tao Wang,Yijiang Li,Sicun Gao,Yuanyuan Shi*

Main category: cs.LG

TL;DR: 本文提出用离散化分类actor替代传统高斯actor，通过正则化actor网络提升on-policy强化学习的性能，在连续控制任务中达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前on-policy深度强化学习主要依赖高斯actor和浅层MLP策略，在梯度噪声大、策略更新保守时优化脆弱。需要重新思考策略表示作为on-policy优化的首要设计选择。

Method: 提出离散化分类actor，将每个动作维度表示为多个bin上的分布，策略目标类似于交叉熵损失。基于监督学习的架构进展，进一步提出正则化actor网络，同时保持critic设计不变。

Result: 仅用离散化正则化actor替换标准actor网络就能带来一致性能提升，在多样连续控制基准测试中达到最先进性能。

Conclusion: 策略表示是on-policy优化的重要设计维度，离散化分类actor结合正则化架构能显著提升连续控制任务的性能，为强化学习策略设计提供了新方向。

Abstract: On-policy deep reinforcement learning remains a dominant paradigm for continuous control, yet standard implementations rely on Gaussian actors and relatively shallow MLP policies, often leading to brittle optimization when gradients are noisy and policy updates must be conservative. In this paper, we revisit policy representation as a first-class design choice for on-policy optimization. We study discretized categorical actors that represent each action dimension with a distribution over bins, yielding a policy objective that resembles a cross-entropy loss. Building on architectural advances from supervised learning, we further propose regularized actor networks, while keeping critic design fixed. Our results show that simply replacing the standard actor network with our discretized regularized actor yields consistent gains and achieve the state-of-the-art performance across diverse continuous-control benchmarks.

</details>


### [154] [CATTO: Balancing Preferences and Confidence in Language Models](https://arxiv.org/abs/2601.23096)
*Nisarg Parikh,Kunjal Panchal,Ananya Sai,Pannaga Shivaswamy,Andrew Lan*

Main category: cs.LG

TL;DR: 提出CATTO校准感知训练目标，解决LLM预测置信度与正确性不匹配问题，结合偏好优化提升校准性能，同时保持任务准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然能做出准确的下一词预测，但其预测置信度校准不佳：高置信度预测经常错误，低置信度预测反而可能正确。基于偏好的对齐方法进一步破坏了预测概率与正确性之间的联系。

Method: 引入CATTO（校准感知词级训练目标），该目标将预测置信度与经验预测正确性对齐，可与原始偏好优化目标结合使用。同时提出Confidence@k，一种利用校准后词概率进行贝叶斯最优输出词选择的测试时缩放机制。

Result: 相比直接偏好优化（DPO），CATTO将预期校准误差（ECE）降低2.22%-7.61%（分布内）和1.46%-10.44%（分布外）；相比最强DPO基线降低0.22%-1.24%（分布内）和1.23%-5.07%（分布外）。置信度提升的同时不损失任务准确性，在五个数据集上保持或略微提升多项选择题回答准确率。

Conclusion: CATTO能有效改善LLM的置信度校准问题，在不牺牲任务性能的前提下提升预测可靠性，为更可信的LLM部署提供解决方案。

Abstract: Large language models (LLMs) often make accurate next token predictions but their confidence in these predictions can be poorly calibrated: high-confidence predictions are frequently wrong, and low-confidence predictions may be correct. This miscalibration is exacerbated by preference-based alignment methods breaking the link between predictive probability and correctness. We introduce a Calibration Aware Token-level Training Objective (CATTO), a calibration-aware objective that aligns predicted confidence with empirical prediction correctness, which can be combined with the original preference optimization objectives. Empirically, CATTO reduces Expected Calibration Error (ECE) by 2.22%-7.61% in-distribution and 1.46%-10.44% out-of-distribution compared to direct preference optimization (DPO), and by 0.22%-1.24% in-distribution and 1.23%-5.07% out-of-distribution compared to the strongest DPO baseline. This improvement in confidence does not come at a cost of losing task accuracy, where CATTO maintains or slightly improves multiple-choice question-answering accuracy on five datasets. We also introduce Confidence@k, a test-time scaling mechanism leveraging calibrated token probabilities for Bayes-optimal selection of output tokens.

</details>


### [155] [To See Far, Look Close: Evolutionary Forecasting for Long-term Time Series](https://arxiv.org/abs/2601.23114)
*Jiaming Ma,Siyuan Mu,Ruilin Tang,Haofeng Ma,Qihe Huang,Zhengyang Zhou,Pengkun Wang,Binwu Wang,Yang Wang*

Main category: cs.LG

TL;DR: 本文提出进化预测（EF）范式，解决直接预测（DF）在长期时间序列预测中的优化问题，通过短时训练结合EF显著超越长时直接训练，实现单一模型适应多预测范围


<details>
  <summary>Details</summary>
Motivation: 现有直接预测范式在长期时间序列预测中需要为每个目标范围重新训练模型，计算成本高，且存在优化异常：长时训练模型性能反而不如短时训练模型

Method: 提出进化预测（EF）范式，作为统一的生成框架，将直接预测视为其退化特例。通过短时训练模型结合EF推理，避免远距离未来梯度冲突对局部动态学习的干扰

Result: 单一EF模型在标准基准测试中超越任务特定的DF集成模型，在极端外推中表现出鲁棒的渐近稳定性，显著提升预测性能

Conclusion: EF范式推动LTSF从被动静态映射向自主进化推理的范式转变，解决了直接预测的优化病理问题，为长期预测提供了更高效稳定的解决方案

Abstract: The prevailing Direct Forecasting (DF) paradigm dominates Long-term Time Series Forecasting (LTSF) by forcing models to predict the entire future horizon in a single forward pass. While efficient, this rigid coupling of output and evaluation horizons necessitates computationally prohibitive re-training for every target horizon. In this work, we uncover a counter-intuitive optimization anomaly: models trained on short horizons-when coupled with our proposed Evolutionary Forecasting (EF) paradigm-significantly outperform those trained directly on long horizons. We attribute this success to the mitigation of a fundamental optimization pathology inherent in DF, where conflicting gradients from distant futures cripple the learning of local dynamics. We establish EF as a unified generative framework, proving that DF is merely a degenerate special case of EF. Extensive experiments demonstrate that a singular EF model surpasses task-specific DF ensembles across standard benchmarks and exhibits robust asymptotic stability in extreme extrapolation. This work propels a paradigm shift in LTSF: moving from passive Static Mapping to autonomous Evolutionary Reasoning.

</details>


### [156] [Distribution-informed Efficient Conformal Prediction for Full Ranking](https://arxiv.org/abs/2601.23128)
*Wenbo Liao,Huipeng Huang,Chen Jia,Huajun Xi,Hao Zeng,Hongxin Wei*

Main category: cs.LG

TL;DR: 提出DCR方法，通过精确推导非一致性分数的分布来构建高效的排序预测集，相比基线方法减少平均预测集大小达36%，同时保持有效覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有基于保形预测的排序方法过于保守，导致预测集过大。需要更高效的方法来量化排序模型的不确定性，以便在实际应用中安全部署。

Method: 提出分布感知保形排序(DCR)，通过推导校准项目绝对排名的负超几何分布，精确计算非一致性分数的分布，从而确定更优的保形阈值。

Result: DCR在保持有效覆盖率的同时，将平均预测集大小减少高达36%，显著优于基线方法。

Conclusion: DCR通过利用非一致性分数的精确分布，实现了更高效的排序不确定性量化，为排序模型的安全部署提供了改进方案。

Abstract: Quantifying uncertainty is critical for the safe deployment of ranking models in real-world applications. Recent work offers a rigorous solution using conformal prediction in a full ranking scenario, which aims to construct prediction sets for the absolute ranks of test items based on the relative ranks of calibration items. However, relying on upper bounds of non-conformity scores renders the method overly conservative, resulting in substantially large prediction sets. To address this, we propose Distribution-informed Conformal Ranking (DCR), which produces efficient prediction sets by deriving the exact distribution of non-conformity scores. In particular, we find that the absolute ranks of calibration items follow Negative Hypergeometric distributions, conditional on their relative ranks. DCR thus uses the rank distribution to derive non-conformity score distribution and determine conformal thresholds. We provide theoretical guarantees that DCR achieves improved efficiency over the baseline while ensuring valid coverage under mild assumptions. Extensive experiments demonstrate the superiority of DCR, reducing average prediction set size by up to 36%, while maintaining valid coverage.

</details>


### [157] [Regularisation in neural networks: a survey and empirical analysis of approaches](https://arxiv.org/abs/2601.23131)
*Christiaan P. Opperman,Anna S. Bosman,Katherine M. Malan*

Main category: cs.LG

TL;DR: 该研究系统回顾了神经网络正则化技术，提出四类分类法，并通过实证研究发现正则化效果具有数据集依赖性，挑战了"正则化总能提升性能"的普遍假设。


<details>
  <summary>Details</summary>
Motivation: 尽管神经网络在许多任务上取得了巨大成功，但它们在泛化到未见数据方面仍存在困难。虽然已有多种正则化技术被提出并作为常规实践使用，但人们普遍假设任何正则化都会带来性能提升。本研究旨在验证这一假设是否在实践中成立。

Method: 1. 对正则化技术进行广泛回顾，包括现代理论如双下降现象；2. 提出四类分类法：基于数据的策略、架构策略、训练策略和损失函数策略；3. 在10个数值和图像数据集上对多层感知机和卷积神经网络进行各种正则化技术的实证比较。

Result: 实证结果表明正则化的有效性具有数据集依赖性：正则化项仅能提升数值数据集的性能，而批量归一化仅能提升图像数据集的性能。不同方法之间存在矛盾和对立关系。

Conclusion: 泛化对机器学习至关重要，理解正则化技术的效果及其之间的联系对于在实践中恰当使用这些方法至关重要。正则化并非总是有效，其效果取决于具体的数据集特征。

Abstract: Despite huge successes on a wide range of tasks, neural networks are known to sometimes struggle to generalise to unseen data. Many approaches have been proposed over the years to promote the generalisation ability of neural networks, collectively known as regularisation techniques. These are used as common practice under the assumption that any regularisation added to the pipeline would result in a performance improvement. In this study, we investigate whether this assumption holds in practice. First, we provide a broad review of regularisation techniques, including modern theories such as double descent. We propose a taxonomy of methods under four broad categories, namely: (1) data-based strategies, (2) architecture strategies, (3) training strategies, and (4) loss function strategies. Notably, we highlight the contradictions and correspondences between the approaches in these broad classes. Further, we perform an empirical comparison of the various regularisation techniques on classification tasks for ten numerical and image datasets applied to the multi-layer perceptron and convolutional neural network architectures. Results show that the efficacy of regularisation is dataset-dependent. For example, the use of a regularisation term only improved performance on numeric datasets, whereas batch normalisation improved performance on image datasets only. Generalisation is crucial to machine learning; thus, understanding the effects of applying regularisation techniques, and considering the connections between them is essential to the appropriate use of these methods in practice.

</details>


### [158] [Why GRPO Needs Normalization: A Local-Curvature Perspective on Adaptive Gradients](https://arxiv.org/abs/2601.23135)
*Cheng Ge,Caitlyn Heqi Yin,Hao Liang,Jiawei Zhang*

Main category: cs.LG

TL;DR: GRPO通过标准差归一化实现自适应梯度，在特定条件下比未归一化的REINFORCE有更快的收敛速度，其效果受特征正交性和奖励方差相互作用的影响。


<details>
  <summary>Details</summary>
Motivation: GRPO作为语言模型推理中的标准RL算法，使用每提示基线和方差归一化避免了批评者的需求，但为什么以及何时这种归一化有帮助仍然不清楚。

Method: 从序列级策略梯度的局部曲率角度解释GRPO：标准差归一化实现了自适应梯度。理论上分析收敛速度，实证上在GSM8K和MATH基准上分析训练阶段。

Result: 理论证明在温和条件下，GRPO比未归一化的REINFORCE有严格改进的收敛速度。实证分析揭示了三个训练阶段：早期加速阶段、相对稳定的过渡阶段和后期增益受限阶段。

Conclusion: 研究为GRPO中标准差归一化何时有帮助提供了原则性解释，并为无批评者RL算法的设计提供了更广泛的见解。

Abstract: Reinforcement learning (RL) has become a key driver of language model reasoning. Among RL algorithms, Group Relative Policy Optimization (GRPO) is the de facto standard, avoiding the need for a critic by using per-prompt baselines and variance normalization. Yet why and when this normalization helps remains unclear. In this work, we provide an explanation through the lens of local curvature of the sequence-level policy gradient: standard deviation normalization implements an adaptive gradient. Theoretically, under mild conditions, GRPO enjoys a strictly improved convergence rate over unnormalized REINFORCE, with gains characterized by the average within-prompt reward standard deviation across prompts and iterations. Empirically, our analysis on GSM8K and MATH benchmarks reveals three distinct training phases governed by the interplay between feature orthogonality and reward variance: (I) an early acceleration phase where high variance and orthogonality favor adaptive scaling; (II) a relatively stable transition phase; and (III) a late-stage regime where the loss of orthogonality limits further gains. Together, these results provide a principled account of when std normalization helps in GRPO, and offer broader insights into the design of critic-free RL algorithms.

</details>


### [159] [Securing Time in Energy IoT: A Clock-Dynamics-Aware Spatio-Temporal Graph Attention Network for Clock Drift Attacks and Y2K38 Failures](https://arxiv.org/abs/2601.23147)
*Saeid Jamshidi,Omar Abdul Wahab,Rolando Herrero,Foutse Khomh*

Main category: cs.LG

TL;DR: STGAT框架通过时空图注意力网络检测能源物联网设备中的时间异常（时钟漂移、同步偏移、Y2K38溢出），结合漂移感知时间嵌入和图注意力机制，在时序扰动数据上达到95.7%准确率。


<details>
  <summary>Details</summary>
Motivation: 分布式物联网设备的时间完整性对智能电网等能源网络至关重要，但面临时钟漂移、时间同步操纵和Y2K38溢出等威胁，传统基于可靠时间戳的异常检测方法无法捕捉这些时间不一致性。

Method: 提出STGAT框架：1) 漂移感知时间嵌入和时间自注意力捕捉单个设备的时间演化；2) 图注意力建模时间错误的空间传播；3) 曲率正则化潜在表示几何分离正常时钟演化和异常。

Result: 在受控时序扰动的能源物联网遥测数据上，STGAT达到95.7%准确率，显著优于循环、Transformer和图基线方法（d > 1.8, p < 0.001），检测延迟减少26%（2.3个时间步），在溢出、漂移和物理不一致场景下保持稳定性能。

Conclusion: STGAT能有效检测能源物联网中的时间异常，通过建模时间扭曲和设备间一致性，为智能电网等关键基础设施提供可靠的时间完整性保障。

Abstract: The integrity of time in distributed Internet of Things (IoT) devices is crucial for reliable operation in energy cyber-physical systems, such as smart grids and microgrids. However, IoT systems are vulnerable to clock drift, time-synchronization manipulation, and timestamp discontinuities, such as the Year 2038 (Y2K38) Unix overflow, all of which disrupt temporal ordering. Conventional anomaly-detection models, which assume reliable timestamps, fail to capture temporal inconsistencies. This paper introduces STGAT (Spatio-Temporal Graph Attention Network), a framework that models both temporal distortion and inter-device consistency in energy IoT systems. STGAT combines drift-aware temporal embeddings and temporal self-attention to capture corrupted time evolution at individual devices, and uses graph attention to model spatial propagation of timing errors. A curvature-regularized latent representation geometrically separates normal clock evolution from anomalies caused by drift, synchronization offsets, and overflow events. Experimental results on energy IoT telemetry with controlled timing perturbations show that STGAT achieves 95.7% accuracy, outperforming recurrent, transformer, and graph-based baselines with significant improvements (d > 1.8, p < 0.001). Additionally, STGAT reduces detection delay by 26%, achieving a 2.3-time-step delay while maintaining stable performance under overflow, drift, and physical inconsistencies.

</details>


### [160] [Manifold-Aware Perturbations for Constrained Generative Modeling](https://arxiv.org/abs/2601.23151)
*Katherine Keegan,Lars Ruthotto*

Main category: cs.LG

TL;DR: 提出一种约束感知的数据扰动方法，解决生成模型在等式约束分布建模中的数学限制问题


<details>
  <summary>Details</summary>
Motivation: 生成模型在科学领域经常遇到样本受等式约束的分布建模问题，存在固有的数学限制，需要开发灵活且计算廉价的方法来解决这些缺陷

Method: 提出约束感知的数据扰动方法，通过扰动数据分布使其支持匹配环境空间维度，同时隐式包含底层流形几何结构

Result: 该方法在多个代表性任务中能够一致地实现数据分布恢复和稳定采样，适用于扩散模型和标准化流模型

Conclusion: 提出的约束感知扰动方法在数学上有充分依据，计算廉价且高度灵活，能够有效解决等式约束生成模型的已知缺陷

Abstract: Generative models have enjoyed widespread success in a variety of applications. However, they encounter inherent mathematical limitations in modeling distributions where samples are constrained by equalities, as is frequently the setting in scientific domains. In this work, we develop a computationally cheap, mathematically justified, and highly flexible distributional modification for combating known pitfalls in equality-constrained generative models. We propose perturbing the data distribution in a constraint-aware way such that the new distribution has support matching the ambient space dimension while still implicitly incorporating underlying manifold geometry. Through theoretical analyses and empirical evidence on several representative tasks, we illustrate that our approach consistently enables data distribution recovery and stable sampling with both diffusion models and normalizing flows.

</details>


### [161] [Behemoth: Benchmarking Unlearning in LLMs Using Fully Synthetic Data](https://arxiv.org/abs/2601.23153)
*Eugenia Iofinova,Dan Alistarh*

Main category: cs.LG

TL;DR: 论文提出了Behemoth，一个完全合成的数据生成框架，用于研究模型编辑与训练数据分布之间的关系，通过在简单表格数据上实验发现了一些与真实世界结果相似的有趣发现。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在实际应用中的部署增加，模型编辑（调整模型权重以修改特定事实的输出）变得重要。然而，当前模型编辑方法存在脆弱性和不完整性问题，且编辑效果严重依赖于训练数据分布。由于真实世界数据复杂，难以理解数据分布与模型存储方式的关系，因此需要合成数据框架来系统研究。

Method: 提出了Behemoth框架，这是一个完全合成的数据生成框架。为了展示该框架的实际价值，作者在简单的表格数据场景下探索模型编辑，通过控制实验环境来研究训练数据分布与模型编辑效果的关系。

Result: 在简单表格数据上的实验显示了一些令人惊讶的发现，这些发现在某些情况下与真实世界结果相似。例如，在某些情况下，限制更新秩（update rank）反而能产生更有效的更新效果。

Conclusion: Behemoth合成数据框架为理解训练数据分布与模型编辑效果之间的关系提供了可控的实验环境。该框架有助于更可靠地执行模型编辑，并揭示了在简单数据设置中也能观察到与复杂真实世界相似的模式。

Abstract: As artificial neural networks, and specifically large language models, have improved rapidly in capabilities and quality, they have increasingly been deployed in real-world applications, from customer service to Google search, despite the fact that they frequently make factually incorrect or undesirable statements. This trend has inspired practical and academic interest in model editing, that is, in adjusting the weights of the model to modify its likely outputs for queries relating to a specific fact or set of facts. This may be done either to amend a fact or set of facts, for instance, to fix a frequent error in the training data, or to suppress a fact or set of facts entirely, for instance, in case of dangerous knowledge. Multiple methods have been proposed to do such edits. However, at the same time, it has been shown that such model editing can be brittle and incomplete. Moreover the effectiveness of any model editing method necessarily depends on the data on which the model is trained, and, therefore, a good understanding of the interaction of the training data distribution and the way it is stored in the network is necessary and helpful to reliably perform model editing. However, working with large language models trained on real-world data does not allow us to understand this relationship or fully measure the effects of model editing. We therefore propose Behemoth, a fully synthetic data generation framework. To demonstrate the practical insights from the framework, we explore model editing in the context of simple tabular data, demonstrating surprising findings that, in some cases, echo real-world results, for instance, that in some cases restricting the update rank results in a more effective update. The code is available at https://github.com/IST-DASLab/behemoth.git.

</details>


### [162] [On Safer Reinforcement Learning Policies for Sedation and Analgesia in Intensive Care](https://arxiv.org/abs/2601.23154)
*Joel Romero-Hernandez,Oscar Camara*

Main category: cs.LG

TL;DR: 该研究使用深度强化学习框架，基于MIMIC-IV数据库中47,144次ICU住院数据，开发了两种镇痛药物剂量策略：一种仅以减少疼痛为目标，另一种同时考虑减少疼痛和死亡率。研究发现，仅关注短期疼痛缓解的策略与死亡率正相关，而考虑长期结果的策略与死亡率负相关。


<details>
  <summary>Details</summary>
Motivation: ICU疼痛管理面临治疗目标与患者安全之间的复杂权衡，传统方法在镇痛不足和过度治疗之间难以平衡。现有强化学习研究存在两个问题：1) 优化目标不重视患者生存率；2) 算法不适合不完全信息环境。本研究旨在探讨这些设计选择的风险。

Method: 开发了一个深度强化学习框架，在部分可观测环境下提供每小时药物剂量建议。使用MIMIC-IV数据库中47,144次ICU住院数据，训练两种策略：策略1仅以减少疼痛为目标；策略2同时减少疼痛和死亡率。药物包括阿片类药物、丙泊酚、苯二氮䓬类和右美托咪定。

Result: 两种策略都能有效降低疼痛，但策略1（仅关注疼痛）的行动与死亡率呈正相关，而策略2（同时考虑疼痛和死亡率）的行动与死亡率呈负相关。这表明即使短期目标是主要关注点，重视长期结果对于制定更安全的治疗策略至关重要。

Conclusion: 在ICU疼痛管理的强化学习策略设计中，仅优化短期目标（如疼痛缓解）可能导致不良长期结果（如死亡率增加）。将长期生存率纳入优化目标可以产生更安全的治疗策略，这强调了在医疗决策中考虑长期结果的重要性。

Abstract: Pain management in intensive care usually involves complex trade-offs between therapeutic goals and patient safety, since both inadequate and excessive treatment may induce serious sequelae. Reinforcement learning can help address this challenge by learning medication dosing policies from retrospective data. However, prior work on sedation and analgesia has optimized for objectives that do not value patient survival while relying on algorithms unsuitable for imperfect information settings. We investigated the risks of these design choices by implementing a deep reinforcement learning framework to suggest hourly medication doses under partial observability. Using data from 47,144 ICU stays in the MIMIC-IV database, we trained policies to prescribe opioids, propofol, benzodiazepines, and dexmedetomidine according to two goals: reduce pain or jointly reduce pain and mortality. We found that, although the two policies were associated with lower pain, actions from the first policy were positively correlated with mortality, while those proposed by the second policy were negatively correlated. This suggests that valuing long-term outcomes could be critical for safer treatment policies, even if a short-term goal remains the primary objective.

</details>


### [163] [SPICE: Submodular Penalized Information-Conflict Selection for Efficient Large Language Model Training](https://arxiv.org/abs/2601.23155)
*Powei Chang,Jinpeng Zhang,Bowen Chen,Chenyu Wang,Chenlu Guo,Yixing Zhang,Yukang Gao,JianXiang Xiang,Yue Gao,Chaoqun Sun,Yiyi Chen,Dongying Kong*

Main category: cs.LG

TL;DR: SPICE是一种冲突感知的数据选择方法，通过最大化Fisher信息的同时惩罚梯度冲突，仅用10%数据就能在指令调优中达到或超过全数据调优的性能。


<details>
  <summary>Details</summary>
Motivation: 基于信息的数据选择在指令调优中很有吸引力，但实践中发现梯度冲突（样本间梯度不对齐）会减缓边际对数行列式信息增益的衰减，阻碍信息选择效果。需要量化这种偏离理想子模性的程度。

Method: 提出ε-分解来量化梯度冲突导致的子模性偏差，基于此设计SPICE选择器：最大化Fisher信息的同时惩罚梯度不对齐，支持早停和代理模型以提高效率。

Result: SPICE选择的子集比原始标准具有更高的对数行列式信息，在8个基准测试中，仅使用10%数据就能匹配或超过包括全数据调优在内的6种方法，显著降低训练成本。

Conclusion: 梯度冲突是信息基数据选择的关键限制因素，SPICE通过冲突感知的选择策略实现了高效的数据选择，为指令调优提供了实用的数据选择方案。

Abstract: Information-based data selection for instruction tuning is compelling: maximizing the log-determinant of the Fisher information yields a monotone submodular objective, enabling greedy algorithms to achieve a $(1-1/e)$ approximation under a cardinality budget. In practice, however, we identify alleviating gradient conflicts, misalignment between per-sample gradients, is a key factor that slows down the decay of marginal log-determinant information gains, thereby preventing significant loss of information. We formalize this via an $\varepsilon$-decomposition that quantifies the deviation from ideal submodularity as a function of conflict statistics, yielding data-dependent approximation factors that tighten as conflicts diminish. Guided by this analysis, we propose SPICE, a conflict-aware selector that maximizes information while penalizing misalignment, and that supports early stopping and proxy models for efficiency. Empirically, SPICE selects subsets with higher log-determinant information than original criteria, and these informational gains translate into performance improvements: across 8 benchmarks with LLaMA2-7B and Qwen2-7B, SPICE uses only 10% of the data, yet matches or exceeds 6 methods including full-data tuning. This achieves performance improvements with substantially lower training cost.

</details>


### [164] [Unsupervised Hierarchical Skill Discovery](https://arxiv.org/abs/2601.23156)
*Damion Harvey,Geraud Nangue Tasse,Branden Ingram,Benjamin Rosman,Steven James*

Main category: cs.LG

TL;DR: 提出一种基于语法的无监督技能分割与层次结构发现方法，可在像素级环境中自动发现可重用技能并构建语义层次结构


<details>
  <summary>Details</summary>
Motivation: 现有技能分割方法大多依赖动作标签、奖励或人工标注，限制了应用范围。需要一种完全无监督的方法来自动发现轨迹中的技能并构建层次结构

Method: 使用基于语法的方法，将无标签轨迹分割成技能，并诱导出层次结构。该方法能捕捉低级行为及其组合成高级技能的过程

Result: 在Craftax和完整版Minecraft等高维像素环境中评估，在技能分割、重用和层次质量指标上优于现有基线，产生更结构化、语义更清晰的层次

Conclusion: 该方法能有效发现无监督技能层次结构，且发现的层次结构能加速和稳定下游强化学习任务的学习过程

Abstract: We consider the problem of unsupervised skill segmentation and hierarchical structure discovery in reinforcement learning. While recent approaches have sought to segment trajectories into reusable skills or options, most rely on action labels, rewards, or handcrafted annotations, limiting their applicability. We propose a method that segments unlabelled trajectories into skills and induces a hierarchical structure over them using a grammar-based approach. The resulting hierarchy captures both low-level behaviours and their composition into higher-level skills. We evaluate our approach in high-dimensional, pixel-based environments, including Craftax and the full, unmodified version of Minecraft. Using metrics for skill segmentation, reuse, and hierarchy quality, we find that our method consistently produces more structured and semantically meaningful hierarchies than existing baselines. Furthermore, as a proof of concept for utility, we demonstrate that these discovered hierarchies accelerate and stabilise learning on downstream reinforcement learning tasks.

</details>


### [165] [Probing the Trajectories of Reasoning Traces in Large Language Models](https://arxiv.org/abs/2601.23163)
*Marthe Ballon,Brecht Verbeken,Vincent Ginis,Andres Algaba*

Main category: cs.LG

TL;DR: 该研究提出了一种系统探测LLM推理轨迹的协议，通过截断推理轨迹并测量答案分布变化，发现准确率和决策确定性随推理token比例增加而提升，主要受相关内容而非长度或风格影响。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM通过生成"推理轨迹"来解决复杂问题，但尚不清楚准确率和决策承诺如何沿推理轨迹演变，以及中间轨迹片段是否提供超出通用长度或风格效应的答案相关信息。

Method: 提出三步协议：1)生成模型的推理轨迹；2)在固定token百分位处截断；3)将每个部分轨迹注入模型（或不同模型），通过下一个token概率测量诱导的答案选择分布。应用于Qwen3和gpt-oss模型在GPQA Diamond和MMLU-Pro基准测试。

Result: 准确率和决策承诺随提供的推理token比例增加而一致提升。这些增益主要由模型生成的相关内容驱动，而非上下文长度或通用"推理风格"效应。更强模型常能从错误的部分轨迹成功回溯，但即时答案常锚定在较弱模型的错误响应中。

Conclusion: 轨迹探测为推理模型的高效和安全部署提供诊断工具，测量结果可为实用的轨迹处理和监控策略提供信息，从而提高可靠性，无需假设中间token是固有的忠实解释。

Abstract: Large language models (LLMs) increasingly solve difficult problems by producing "reasoning traces" before emitting a final response. However, it remains unclear how accuracy and decision commitment evolve along a reasoning trajectory, and whether intermediate trace segments provide answer-relevant information beyond generic length or stylistic effects. Here, we propose a protocol to systematically probe the trajectories of reasoning traces in LLMs by 1) generating a model's reasoning trace, 2) truncating it at fixed token-percentiles, and 3) injecting each partial trace back into the model (or a different model) to measure the induced distribution over answer choices via next-token probabilities. We apply this protocol to the open-source Qwen3-4B/-8B/-14B and gpt-oss-20b/-120b models across the multiple-choice GPQA Diamond and MMLU-Pro benchmarks. We find that accuracy and decision commitment consistently increase as the percentage of provided reasoning tokens grows. These gains are primarily driven by relevant content in the model generation rather than context length or generic "reasoning style" effects. Stronger models often backtrack successfully from incorrect partial traces, but immediate answers often remain anchored in the weaker model's incorrect response. More broadly, we show that trajectory probing provides diagnostics for efficient and safer deployment of reasoning models as the measurements can inform practical trace-handling and monitoring policies that improve reliability without assuming intermediate tokens are inherently faithful explanations.

</details>


### [166] [Stochastic Linear Bandits with Parameter Noise](https://arxiv.org/abs/2601.23164)
*Daniel Ezer,Alon Peled-Cohen,Yishay Mansour*

Main category: cs.LG

TL;DR: 本文研究了带有参数噪声的随机线性赌博机问题，提出了针对一般动作集和特定ℓ_p球动作集的紧致遗憾界，并展示了简单的探索-利用算法可以达到最优性能。


<details>
  <summary>Details</summary>
Motivation: 传统线性赌博机通常假设奖励噪声是加性的，但实际中系统参数本身可能存在随机性。本文研究参数噪声模型，其中奖励参数θ是随机采样的，这更符合某些现实场景如推荐系统、动态定价等。

Method: 针对一般动作集，作者提出了遗憾上界分析；针对ℓ_p单位球（p≤2）动作集，作者展示了最优遗憾界可以通过简单的探索-利用算法实现，该算法交替进行探索和利用阶段。

Result: 1. 一般动作集：遗憾上界为Õ(√(dT log(K/δ)σ²_max))，下界为Õ(d√(Tσ²_max))，当log(K)≈d时紧致
2. ℓ_p球动作集：极小极大遗憾为Õ(√(dTσ²_q))，其中σ²_q≤4，远优于经典加性噪声模型的d√T遗憾
3. 简单探索-利用算法可以达到最优遗憾界

Conclusion: 参数噪声模型下的线性赌博机问题具有与经典加性噪声模型不同的理论性质，在某些动作集上可以获得更好的遗憾界。简单的探索-利用算法足以达到最优性能，这为实际应用提供了高效且理论保证的解决方案。

Abstract: We study the stochastic linear bandits with parameter noise model, in which the reward of action $a$ is $a^\top θ$ where $θ$ is sampled i.i.d. We show a regret upper bound of $\widetilde{O} (\sqrt{d T \log (K/δ) σ^2_{\max})}$ for a horizon $T$, general action set of size $K$ of dimension $d$, and where $σ^2_{\max}$ is the maximal variance of the reward for any action. We further provide a lower bound of $\widetildeΩ (d \sqrt{T σ^2_{\max}})$ which is tight (up to logarithmic factors) whenever $\log (K) \approx d$. For more specific action sets, $\ell_p$ unit balls with $p \leq 2$ and dual norm $q$, we show that the minimax regret is $\widetildeΘ (\sqrt{dT σ^2_q)}$, where $σ^2_q$ is a variance-dependent quantity that is always at most $4$. This is in contrast to the minimax regret attainable for such sets in the classic additive noise model, where the regret is of order $d \sqrt{T}$. Surprisingly, we show that this optimal (up to logarithmic factors) regret bound is attainable using a very simple explore-exploit algorithm.

</details>


### [167] [Names Don't Matter: Symbol-Invariant Transformer for Open-Vocabulary Learning](https://arxiv.org/abs/2601.23169)
*İlker Işık,Wenchao Li*

Main category: cs.LG

TL;DR: 提出一种对可互换令牌具有重命名不变性的Transformer机制，通过并行嵌入流和聚合注意力实现，在开放词汇任务上显著提升性能


<details>
  <summary>Details</summary>
Motivation: 当前神经网络架构缺乏处理可互换令牌（如绑定变量）的原则性方法，导致模型在固定词汇表上训练后难以泛化到未见符号，即使底层语义保持不变

Method: 提出基于Transformer的新机制，使用并行嵌入流隔离每个可互换令牌的贡献，结合聚合注意力机制实现跨流的结构化信息共享，理论上保证对可互换令牌重命名的不变性

Result: 实验结果证实了该方法的理论保证，在需要泛化到新符号的开放词汇任务上表现出显著的性能提升

Conclusion: 该方法为解决神经网络处理可互换令牌的泛化问题提供了原则性解决方案，通过理论保证的架构设计实现了对符号重命名的不变性

Abstract: Current neural architectures lack a principled way to handle interchangeable tokens, i.e., symbols that are semantically equivalent yet distinguishable, such as bound variables. As a result, models trained on fixed vocabularies often struggle to generalize to unseen symbols, even when the underlying semantics remain unchanged. We propose a novel Transformer-based mechanism that is provably invariant to the renaming of interchangeable tokens. Our approach employs parallel embedding streams to isolate the contribution of each interchangeable token in the input, combined with an aggregated attention mechanism that enables structured information sharing across streams. Experimental results confirm the theoretical guarantees of our method and demonstrate substantial performance gains on open-vocabulary tasks that require generalization to novel symbols.

</details>


### [168] [Beyond Fixed Frames: Dynamic Character-Aligned Speech Tokenization](https://arxiv.org/abs/2601.23174)
*Luca Della Libera,Cem Subakan,Mirco Ravanelli*

Main category: cs.LG

TL;DR: DyCAST是一种动态字符对齐的语音分词器，通过软字符级对齐和显式时长建模实现可变帧率分词，相比固定帧率编解码器显著减少token数量


<details>
  <summary>Details</summary>
Motivation: 现有神经音频编解码器通常以固定帧率运行，在时间上均匀分配token，产生不必要的长序列，需要更高效的语音表示方法

Method: DyCAST通过软字符级对齐和显式时长建模实现可变帧率分词，训练时学习将token与字符级语言单元关联，解码时支持无对齐推理和直接时长控制，并引入检索增强解码机制提高低帧率下的语音重建质量

Result: DyCAST在保持竞争力的语音重建质量和下游任务性能的同时，相比固定帧率编解码器显著减少了token使用量

Conclusion: DyCAST提供了一种高效的动态语音分词方法，通过字符对齐和可变帧率机制实现了更紧凑的语音表示，为LLM处理语音数据提供了更好的token化方案

Abstract: Neural audio codecs are at the core of modern conversational speech technologies, converting continuous speech into sequences of discrete tokens that can be processed by LLMs. However, existing codecs typically operate at fixed frame rates, allocating tokens uniformly in time and producing unnecessarily long sequences. In this work, we introduce DyCAST, a Dynamic Character-Aligned Speech Tokenizer that enables variable-frame-rate tokenization through soft character-level alignment and explicit duration modeling. DyCAST learns to associate tokens with character-level linguistic units during training and supports alignment-free inference with direct control over token durations at decoding time. To improve speech resynthesis quality at low frame rates, we further introduce a retrieval-augmented decoding mechanism that enhances reconstruction fidelity without increasing bitrate. Experiments show that DyCAST achieves competitive speech resynthesis quality and downstream performance while using significantly fewer tokens than fixed-frame-rate codecs.

</details>


### [169] [MeshGraphNet-Transformer: Scalable Mesh-based Learned Simulation for Solid Mechanics](https://arxiv.org/abs/2601.23177)
*Mikel M. Iparraguirre,Iciar Alfaro,David Gonzalez,Elias Cueto*

Main category: cs.LG

TL;DR: MeshGraphNet-Transformer (MGN-T) 结合Transformer的全局建模能力和MeshGraphNets的几何归纳偏置，通过物理注意力Transformer解决标准MGN在大规模高分辨率网格上长距离信息传播效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 标准MeshGraphNets在处理大规模高分辨率网格时存在长距离信息传播效率低的问题，这限制了其在工业规模应用中的效果。需要一种既能保持网格图表示优势，又能高效处理长距离物理相互作用的架构。

Method: 提出MeshGraphNet-Transformer (MGN-T)架构，结合物理注意力Transformer作为全局处理器，同时更新所有节点状态并显式保留节点和边属性。该方法直接捕获长距离物理相互作用，无需深度消息传递堆栈或分层粗化网格。

Result: MGN-T成功处理工业规模的冲击动力学网格，准确建模自接触、塑性和多变量输出。在经典基准测试中优于最先进方法，实现更高精度，同时保持实际效率，仅使用竞争基线所需参数的一小部分。

Conclusion: MGN-T通过结合Transformer的全局建模能力和MeshGraphNets的几何归纳偏置，有效解决了大规模高分辨率网格上的长距离信息传播问题，为工业规模物理模拟提供了高效准确的解决方案。

Abstract: We present MeshGraphNet-Transformer (MGN-T), a novel architecture that combines the global modeling capabilities of Transformers with the geometric inductive bias of MeshGraphNets, while preserving a mesh-based graph representation. MGN-T overcomes a key limitation of standard MGN, the inefficient long-range information propagation caused by iterative message passing on large, high-resolution meshes. A physics-attention Transformer serves as a global processor, updating all nodal states simultaneously while explicitly retaining node and edge attributes. By directly capturing long-range physical interactions, MGN-T eliminates the need for deep message-passing stacks or hierarchical, coarsened meshes, enabling efficient learning on high-resolution meshes with varying geometries, topologies, and boundary conditions at an industrial scale.
  We demonstrate that MGN-T successfully handles industrial-scale meshes for impact dynamics, a setting in which standard MGN fails due message-passing under-reaching. The method accurately models self-contact, plasticity, and multivariate outputs, including internal, phenomenological plastic variables. Moreover, MGN-T outperforms state-of-the-art approaches on classical benchmarks, achieving higher accuracy while maintaining practical efficiency, using only a fraction of the parameters required by competing baselines.

</details>


### [170] [TriSpec: Ternary Speculative Decoding via Lightweight Proxy Verification](https://arxiv.org/abs/2601.23180)
*Haoyun Jiang,Junqi He,Feng Hong,Xinlong Yang,Jianwei Zhang,Zheng Li,Zhengyang Zhuge,Zhiyong Chen,Bo Han,Junyang Lin,Jiangchao Yao*

Main category: cs.LG

TL;DR: TriSpec提出了一种三元推测解码框架，通过引入轻量级代理来减少验证成本，仅在遇到不确定token时才调用完整目标模型，相比标准推测解码实现了最高35%的加速。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的推理效率受限于其串行自回归生成，尤其是在推理成为关键能力且响应序列变长时。推测解码虽然提供了加速方案，但现有工作几乎饱和了草案有效性和效率的改进，本文从验证成本这一新视角推进推测解码。

Method: 提出TriSpec三元推测解码框架，核心是引入轻量级代理来显著降低计算成本：通过批准易于验证的草案序列，仅在遇到不确定token时才调用完整目标模型。TriSpec可以与EAGLE-3等最先进的SD方法集成，进一步降低验证成本。

Result: 在Qwen3和DeepSeek-R1-Distill-Qwen/LLaMA系列上的大量实验表明，TriSpec相比标准推测解码实现了最高35%的加速，目标模型调用次数减少了50%，同时保持了相当的准确性。

Conclusion: TriSpec通过从验证成本角度优化推测解码，显著提高了大语言模型的推理效率，为推测解码提供了新的优化方向，能够与现有先进方法集成获得更大加速。

Abstract: Inference efficiency in Large Language Models (LLMs) is fundamentally limited by their serial, autoregressive generation, especially as reasoning becomes a key capability and response sequences grow longer. Speculative decoding (SD) offers a powerful solution, providing significant speed-ups through its lightweight drafting and parallel verification mechanism. While existing work has nearly saturated improvements in draft effectiveness and efficiency, this paper advances SD from a new yet critical perspective: the verification cost. We propose TriSpec, a novel ternary SD framework that, at its core, introduces a lightweight proxy to significantly reduce computational cost by approving easily verifiable draft sequences and engaging the full target model only when encountering uncertain tokens. TriSpec can be integrated with state-of-the-art SD methods like EAGLE-3 to further reduce verification costs, achieving greater acceleration. Extensive experiments on the Qwen3 and DeepSeek-R1-Distill-Qwen/LLaMA families show that TriSpec achieves up to 35\% speedup over standard SD, with up to 50\% fewer target model invocations while maintaining comparable accuracy.

</details>


### [171] [Ensuring Semantics in Weights of Implicit Neural Representations through the Implicit Function Theorem](https://arxiv.org/abs/2601.23181)
*Tianming Qiu,Christos Sonis,Hao Shen*

Main category: cs.LG

TL;DR: 该论文使用隐函数定理建立了数据空间与权重表示空间之间的严格映射，为理解神经网络权重如何编码数据语义提供了理论解释。


<details>
  <summary>Details</summary>
Motivation: 权重空间学习是一个新兴领域，但缺乏对网络权重如何编码数据语义的精确理论解释。特别是隐式神经表示作为方便的测试平台，需要理论支撑来解释权重与数据之间的关系。

Method: 采用隐函数定理建立数据空间与权重表示空间之间的严格映射。通过共享超网络将实例特定嵌入映射到INR权重，构建理论框架。

Result: 在2D和3D数据集的下游分类任务中，该方法与现有基线方法表现相当，为网络权重的理论研究提供了新的视角。

Conclusion: 该工作为权重空间学习提供了理论基础，通过隐函数定理建立了数据与权重之间的映射关系，为未来网络权重研究提供了理论框架。

Abstract: Weight Space Learning (WSL), which frames neural network weights as a data modality, is an emerging field with potential for tasks like meta-learning or transfer learning. Particularly, Implicit Neural Representations (INRs) provide a convenient testbed, where each set of weights determines the corresponding individual data sample as a mapping from coordinates to contextual values. So far, a precise theoretical explanation for the mechanism of encoding semantics of data into network weights is still missing. In this work, we deploy the Implicit Function Theorem (IFT) to establish a rigorous mapping between the data space and its latent weight representation space. We analyze a framework that maps instance-specific embeddings to INR weights via a shared hypernetwork, achieving performance competitive with existing baselines on downstream classification tasks across 2D and 3D datasets. These findings offer a theoretical lens for future investigations into network weights.

</details>


### [172] [Learning to Execute Graph Algorithms Exactly with Graph Neural Networks](https://arxiv.org/abs/2601.23207)
*Muhammad Fetrat Qharabagh,Artur Back de Luca,George Giapitzakis,Kimon Fountoulakis*

Main category: cs.LG

TL;DR: 论文证明了在有界度和有限精度约束下，图神经网络能够精确学习图算法，通过训练MLP集合执行节点本地指令，并在推理时将其作为GNN更新函数，利用NTK理论保证算法无错误执行。


<details>
  <summary>Details</summary>
Motivation: 理解图神经网络的学习能力，特别是它们执行算法的能力，是一个核心理论挑战。本文旨在证明图神经网络在特定约束下能够精确学习图算法。

Method: 采用两步法：1) 训练多层感知机(MLP)集合来执行单个节点的本地指令；2) 在推理时，将训练好的MLP集合作为图神经网络(GNN)的更新函数。利用神经正切核(NTK)理论分析学习能力。

Result: 证明了在有界度和有限精度约束下，图算法可以从小的训练集中学习，使得完整图算法在推理时能够无错误、高概率地执行。特别针对LOCAL分布式计算模型建立了严格的可学习性结果，并对消息洪泛、广度优先搜索、深度优先搜索和Bellman-Ford等算法展示了积极的可学习性结果。

Conclusion: 图神经网络能够在理论保证下学习并执行图算法，为理解GNN的计算能力提供了理论框架，并展示了其在分布式计算模型中的实际应用潜力。

Abstract: Understanding what graph neural networks can learn, especially their ability to learn to execute algorithms, remains a central theoretical challenge. In this work, we prove exact learnability results for graph algorithms under bounded-degree and finite-precision constraints. Our approach follows a two-step process. First, we train an ensemble of multi-layer perceptrons (MLPs) to execute the local instructions of a single node. Second, during inference, we use the trained MLP ensemble as the update function within a graph neural network (GNN). Leveraging Neural Tangent Kernel (NTK) theory, we show that local instructions can be learned from a small training set, enabling the complete graph algorithm to be executed during inference without error and with high probability. To illustrate the learning power of our setting, we establish a rigorous learnability result for the LOCAL model of distributed computation. We further demonstrate positive learnability results for widely studied algorithms such as message flooding, breadth-first and depth-first search, and Bellman-Ford.

</details>


### [173] [Tackling air quality with SAPIENS](https://arxiv.org/abs/2601.23215)
*Marcella Bona,Nathan Heatley,Jia-Chen Hua,Adriana Lara,Valeria Legaria-Santiago,Alberto Luviano Juarez,Fernando Moreno-Gomez,Jocelyn Richardson,Natan Vilchis,Xiwen Shirley Zheng*

Main category: cs.LG

TL;DR: 该研究通过分析墨西哥城的交通数据与空气质量监测数据，开发了一种基于彩色交通地图的同心环描述方法，使用偏最小二乘回归预测污染水平，提供超本地化的动态空气质量预报。


<details>
  <summary>Details</summary>
Motivation: 城市空气污染是全球性慢性问题，交通是主要污染源。现有空气质量监测和预报在时空上粒度较粗，而实时交通数据通常更精细且公开可用。研究旨在利用精细交通数据提供超本地化、动态的空气质量预报。

Method: 1. 将彩色编码的交通地图转换为同心环描述，改进交通条件表征；2. 使用偏最小二乘回归（PLSR）基于新定义的交通强度预测污染水平；3. 通过不同训练样本优化模型性能；4. 分析污染物与交通的关系。

Result: 开发了创新的交通强度表示方法，建立了有效的污染预测模型，工作流程简单且可适应其他城市环境，为超本地化空气质量预报提供了可行方案。

Conclusion: 通过结合精细交通数据和空气质量监测，可以建立有效的污染预测模型，提供超本地化、动态的空气质量预报。该方法具有普适性，可推广到其他城市环境。

Abstract: Air pollution is a chronic problem in large cities worldwide and awareness is rising as the long-term health implications become clearer. Vehicular traffic has been identified as a major contributor to poor air quality. In a lot of cities the publicly available air quality measurements and forecasts are coarse-grained both in space and time. However, in general, real-time traffic intensity data is openly available in various forms and is fine-grained. In this paper, we present an in-depth study of pollution sensor measurements combined with traffic data from Mexico City. We analyse and model the relationship between traffic intensity and air quality with the aim to provide hyper-local, dynamic air quality forecasts. We developed an innovative method to represent traffic intensities by transforming simple colour-coded traffic maps into concentric ring-based descriptions, enabling improved characterisation of traffic conditions. Using Partial Least Squares Regression, we predict pollution levels based on these newly defined traffic intensities. The model was optimised with various training samples to achieve the best predictive performance and gain insights into the relationship between pollutants and traffic. The workflow we have designed is straightforward and adaptable to other contexts, like other cities beyond the specifics of our dataset.

</details>


### [174] [Optimal Fair Aggregation of Crowdsourced Noisy Labels using Demographic Parity Constraints](https://arxiv.org/abs/2601.23221)
*Gabriel Singer,Samuel Gruffaz,Olivier Vo Van,Nicolas Vayatis,Argyris Kalogeratos*

Main category: cs.LG

TL;DR: 该论文研究众包标注聚合中的公平性问题，分析了多数投票和贝叶斯最优聚合的公平性差距，提出了收敛性理论保证，并将多类别公平后处理算法扩展到离散设置。


<details>
  <summary>Details</summary>
Motivation: 获取可靠的真实标签通常成本高昂或不可行，因此众包和聚合噪声人工标注成为常用方法。然而，聚合主观标签可能放大个体偏见，特别是在敏感特征方面，引发公平性担忧。目前众包聚合中的公平性问题尚未充分探索，缺乏收敛性保证，且只有有限的后处理方法来强制执行ε公平性。

Method: 1. 在ε公平性框架下分析多数投票和最优贝叶斯聚合的公平性差距；2. 在小众包规模下推导多数投票公平性差距的上界；3. 证明聚合共识的公平性差距在可解释条件下以指数速度收敛到真实标签的公平性差距；4. 将最先进的多类别公平后处理算法从连续设置推广到离散设置，强制执行严格的人口统计奇偶约束。

Result: 1. 理论分析表明聚合共识的公平性差距以指数速度收敛到真实标签的公平性差距；2. 提出的后处理算法能够对任何聚合规则强制执行严格的人口统计奇偶约束；3. 在合成和真实数据集上的实验验证了方法的有效性，并证实了理论见解。

Conclusion: 该研究填补了众包聚合中公平性分析的空白，提供了理论收敛性保证，并提出了有效的后处理方法来确保公平性。即使在真实标签本身可能存在不公平的情况下，通过推广的公平后处理算法仍能强制执行严格的人口统计奇偶约束。

Abstract: As acquiring reliable ground-truth labels is usually costly, or infeasible, crowdsourcing and aggregation of noisy human annotations is the typical resort. Aggregating subjective labels, though, may amplify individual biases, particularly regarding sensitive features, raising fairness concerns. Nonetheless, fairness in crowdsourced aggregation remains largely unexplored, with no existing convergence guarantees and only limited post-processing approaches for enforcing $\varepsilon$-fairness under demographic parity. We address this gap by analyzing the fairness s of crowdsourced aggregation methods within the $\varepsilon$-fairness framework, for Majority Vote and Optimal Bayesian aggregation. In the small-crowd regime, we derive an upper bound on the fairness gap of Majority Vote in terms of the fairness gaps of the individual annotators. We further show that the fairness gap of the aggregated consensus converges exponentially fast to that of the ground-truth under interpretable conditions. Since ground-truth itself may still be unfair, we generalize a state-of-the-art multiclass fairness post-processing algorithm from the continuous to the discrete setting, which enforces strict demographic parity constraints to any aggregation rule. Experiments on synthetic and real datasets demonstrate the effectiveness of our approach and corroborate the theoretical insights.

</details>


### [175] [Agile Reinforcement Learning through Separable Neural Architecture](https://arxiv.org/abs/2601.23225)
*Rajib Mostakim,Reza T. Batley,Sourav Saha*

Main category: cs.LG

TL;DR: SPAN是一种基于样条的自适应网络，通过可学习的预处理层和可分离张量积B样条基，在资源受限环境中实现了比传统MLP更好的样本效率和策略学习性能。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习在资源受限环境中部署时，传统的多层感知机（MLP）存在参数效率低下的问题，因为它们对许多值函数的平滑结构具有不完美的归纳偏置。这种不匹配会降低样本效率并减慢策略学习。现有的模型压缩技术是事后处理，无法改善学习效率。

Method: SPAN（基于样条的自适应网络）通过整合可学习的预处理层和可分离张量积B样条基，改进了低秩KHRONOS框架。该方法在离散（PPO）和高维连续（SAC）控制任务以及离线设置（Minari/D4RL）中进行评估。

Result: 实验结果显示，SPAN相比MLP基线实现了30-50%的样本效率提升，在基准测试中获得了1.3-9倍更高的成功率。此外，SPAN表现出更优的随时性能和超参数变化的鲁棒性。

Conclusion: SPAN作为一种可行的高性能替代方案，能够在资源受限环境中学习内在高效策略，为深度强化学习中的函数逼近提供了更有效的解决方案。

Abstract: Deep reinforcement learning (RL) is increasingly deployed in resource-constrained environments, yet the go-to function approximators - multilayer perceptrons (MLPs) - are often parameter-inefficient due to an imperfect inductive bias for the smooth structure of many value functions. This mismatch can also hinder sample efficiency and slow policy learning in this capacity-limited regime. Although model compression techniques exist, they operate post-hoc and do not improve learning efficiency. Recent spline-based separable architectures - such as Kolmogorov-Arnold Networks (KANs) - have been shown to offer parameter efficiency but are widely reported to exhibit significant computational overhead, especially at scale.
  In seeking to address these limitations, this work introduces SPAN (SPline-based Adaptive Networks), a novel function approximation approach to RL. SPAN adapts the low rank KHRONOS framework by integrating a learnable preprocessing layer with a separable tensor product B-spline basis. SPAN is evaluated across discrete (PPO) and high-dimensional continuous (SAC) control tasks, as well as offline settings (Minari/D4RL). Empirical results demonstrate that SPAN achieves a 30-50% improvement in sample efficiency and 1.3-9 times higher success rates across benchmarks compared to MLP baselines. Furthermore, SPAN demonstrates superior anytime performance and robustness to hyperparameter variations, suggesting it as a viable, high performance alternative for learning intrinsically efficient policies in resource-limited settings.

</details>


### [176] [Sequence Diffusion Model for Temporal Link Prediction in Continuous-Time Dynamic Graph](https://arxiv.org/abs/2601.23233)
*Nguyen Minh Duc,Viet Cuong Ta*

Main category: cs.LG

TL;DR: SDG：一种新颖的序列级扩散框架，将动态图学习与生成式去噪统一，通过条件去噪过程重建所有交互嵌入，在时序链接预测任务中实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有的时序图神经网络主要关注学习历史交互表示，这些模型虽然性能强，但仍然是纯判别式的，只能对未来链接进行点估计，缺乏明确机制来捕捉未来时序交互的不确定性和序列结构。

Method: 提出SDG框架：1）将噪声注入整个历史交互序列；2）通过条件去噪过程联合重建所有交互嵌入；3）使用交叉注意力去噪解码器指导目标序列重建；4）以端到端方式优化模型。

Result: 在多个时序图基准测试上的广泛实验表明，SDG在时序链接预测任务中始终实现最先进的性能。

Conclusion: SDG通过将动态图学习与生成式去噪统一，能够捕捉更全面的交互分布，有效解决了现有时序图神经网络在捕捉未来交互不确定性和序列结构方面的不足。

Abstract: Temporal link prediction in dynamic graphs is a fundamental problem in many real-world systems. Existing temporal graph neural networks mainly focus on learning representations of historical interactions. Despite their strong performance, these models are still purely discriminative, producing point estimates for future links and lacking an explicit mechanism to capture the uncertainty and sequential structure of future temporal interactions. In this paper, we propose SDG, a novel sequence-level diffusion framework that unifies dynamic graph learning with generative denoising. Specifically, SDG injects noise into the entire historical interaction sequence and jointly reconstructs all interaction embeddings through a conditional denoising process, thereby enabling the model to capture more comprehensive interaction distributions. To align the generative process with temporal link prediction, we employ a cross-attention denoising decoder to guide the reconstruction of the destination sequence and optimize the model in an end-to-end manner. Extensive experiments on various temporal graph benchmarks show that SDG consistently achieves state-of-the-art performance in the temporal link prediction task.

</details>


### [177] [YuriiFormer: A Suite of Nesterov-Accelerated Transformers](https://arxiv.org/abs/2601.23236)
*Aleksandr Zimin,Yury Polyanskiy,Philippe Rigollet*

Main category: cs.LG

TL;DR: 将transformer层解释为优化算法迭代，自注意力对应交互能量梯度步，MLP对应势能梯度步，标准GPT是复合目标的梯度下降


<details>
  <summary>Details</summary>
Motivation: 为transformer架构提供优化理论解释，使架构设计有理论依据，将经典优化思想应用于神经网络设计

Method: 提出变分框架，将transformer层视为token嵌入优化算法的迭代，自注意力实现交互能量的梯度步，MLP层实现势能的梯度步，使用Lie-Trotter分裂方法

Result: 基于Nesterov加速思想设计了加速transformer，在TinyStories和OpenWebText上一致优于nanoGPT基线

Conclusion: 优化理论视角能为transformer架构提供原则性设计指导，并能转化为实际性能提升

Abstract: We propose a variational framework that interprets transformer layers as iterations of an optimization algorithm acting on token embeddings. In this view, self-attention implements a gradient step of an interaction energy, while MLP layers correspond to gradient updates of a potential energy. Standard GPT-style transformers emerge as vanilla gradient descent on the resulting composite objective, implemented via Lie--Trotter splitting between these two energy functionals. This perspective enables principled architectural design using classical optimization ideas. As a proof of concept, we introduce a Nesterov-style accelerated transformer that preserves the same attention and MLP oracles. The resulting architecture consistently outperforms a nanoGPT baseline on TinyStories and OpenWebText, demonstrating that optimization-theoretic insights can translate into practical gains.

</details>


### [178] [How well do generative models solve inverse problems? A benchmark study](https://arxiv.org/abs/2601.23238)
*Patrick Krüger,Patrick Materne,Werner Krebs,Hanno Gottschalk*

Main category: cs.LG

TL;DR: 比较传统贝叶斯逆问题方法与三种生成式学习模型在燃气轮机燃烧室设计中的应用，发现条件流匹配方法表现最佳


<details>
  <summary>Details</summary>
Motivation: 生成式学习能够基于低维条件生成高维数据，适合解决贝叶斯逆问题。本文旨在比较传统方法与现代生成式模型在工程逆设计问题中的性能

Method: 比较四种方法：1) 基于前向回归模型和MCMC采样的传统贝叶斯方法；2) 条件生成对抗网络；3) 可逆神经网络；4) 条件流匹配。应用于燃气轮机燃烧室设计，将6个设计参数映射到3个性能指标

Result: 提出多个评估指标来衡量生成设计的准确性和多样性，并研究训练数据集大小对性能的影响。条件流匹配方法在所有比较方法中表现最佳，一致优于其他方法

Conclusion: 条件流匹配是解决燃气轮机燃烧室逆设计问题的最有效方法，在准确性和多样性方面均优于传统贝叶斯方法和现代生成式模型

Abstract: Generative learning generates high dimensional data based on low dimensional conditions, also called prompts. Therefore, generative learning algorithms are eligible for solving (Bayesian) inverse problems. In this article we compare a traditional Bayesian inverse approach based on a forward regression model and a prior sampled with the Markov Chain Monte Carlo method with three state of the art generative learning models, namely conditional Generative Adversarial Networks, Invertible Neural Networks and Conditional Flow Matching. We apply them to a problem of gas turbine combustor design where we map six independent design parameters to three performance labels. We propose several metrics for the evaluation of this inverse design approaches and measure the accuracy of the labels of the generated designs along with the diversity. We also study the performance as a function of the training dataset size. Our benchmark has a clear winner, as Conditional Flow Matching consistently outperforms all competing approaches.

</details>


### [179] [Agnostic Language Identification and Generation](https://arxiv.org/abs/2601.23258)
*Mikael Møller Høgsgaard,Chirag Pabbaraju*

Main category: cs.LG

TL;DR: 本文研究语言识别和生成任务，在完全放松可实现性假设的情况下，提出了新的目标函数，并获得了新颖的特征描述和接近紧致的收敛速率。


<details>
  <summary>Details</summary>
Motivation: 现有语言识别和生成研究通常基于可实现性假设，即输入数据来自某个未知分布，且该分布必然支持给定语言集合中的某个语言。本文旨在放松这一强假设，研究在更一般的"不可知"设置下的语言识别和生成问题。

Method: 提出在完全放松可实现性假设的情况下，研究语言识别和生成的新目标函数。不对输入数据的分布施加任何限制，在更一般的"不可知"设置下进行分析。

Result: 在两个问题中都获得了新颖有趣的特征描述和接近紧致的收敛速率，为无可实现性假设的语言处理任务提供了理论保证。

Conclusion: 本文成功地将语言识别和生成任务从可实现性假设中解放出来，在更一般的不可知设置下建立了理论框架，为实际应用中更广泛的数据分布情况提供了理论基础。

Abstract: Recent works on language identification and generation have established tight statistical rates at which these tasks can be achieved. These works typically operate under a strong realizability assumption: that the input data is drawn from an unknown distribution necessarily supported on some language in a given collection. In this work, we relax this assumption of realizability entirely, and impose no restrictions on the distribution of the input data. We propose objectives to study both language identification and generation in this more general "agnostic" setup. Across both problems, we obtain novel interesting characterizations and nearly tight rates.

</details>


### [180] [TEON: Tensorized Orthonormalization Beyond Layer-Wise Muon for Large Language Model Pre-Training](https://arxiv.org/abs/2601.23261)
*Ruijie Zhang,Yequan Zhao,Ziyue Liu,Zhengyang Wang,Dongyang Li,Yupeng Su,Sijia Liu,Zheng Zhang*

Main category: cs.LG

TL;DR: TEON是Muon优化器的推广，通过将神经网络梯度建模为结构化高阶张量，实现跨层正交化，在GPT和LLaMA模型上表现出更好的训练效果。


<details>
  <summary>Details</summary>
Motivation: Muon优化器在每层独立进行矩阵级梯度正交化，在大型语言模型预训练中表现出色。但作者认为可以进一步推广正交化到跨层维度，通过张量建模实现更优的优化效果。

Method: 将神经网络梯度建模为结构化高阶张量，提出TEON方法作为Muon的推广，实现跨层正交化。基于理论分析开发了实用实现，并在GPT和LLaMA架构上进行评估。

Result: 在130M到774M参数的GPT模型和60M到1B参数的LLaMA模型上，TEON持续改善了训练和验证困惑度，并在各种近似SVD方案下表现出强鲁棒性。

Conclusion: TEON通过张量建模实现跨层梯度正交化，相比层级Muon有更好的收敛保证和实际性能，为神经网络优化提供了新的有效方法。

Abstract: The Muon optimizer has demonstrated strong empirical performance in pre-training large language models by performing matrix-level gradient (or momentum) orthogonalization in each layer independently. In this work, we propose TEON, a principled generalization of Muon that extends orthogonalization beyond individual layers by modeling the gradients of a neural network as a structured higher-order tensor. We present TEON's improved convergence guarantee over layer-wise Muon, and further develop a practical instantiation of TEON based on the theoretical analysis with corresponding ablation. We evaluate our approach on two widely adopted architectures: GPT-style models, ranging from 130M to 774M parameters, and LLaMA-style models, ranging from 60M to 1B parameters. Experimental results show that TEON consistently improves training and validation perplexity across model scales and exhibits strong robustness under various approximate SVD schemes.

</details>


### [181] [Particle-Guided Diffusion Models for Partial Differential Equations](https://arxiv.org/abs/2601.23262)
*Andrew Millard,Fredrik Lindsten,Zheng Zhao*

Main category: cs.LG

TL;DR: 提出一种结合扩散模型与物理约束的随机采样方法，通过PDE残差和观测约束确保生成样本的物理合理性，并嵌入SMC框架构建可扩展的生成式PDE求解器。


<details>
  <summary>Details</summary>
Motivation: 现有生成方法在求解偏微分方程时可能产生物理上不可行的解，需要确保生成样本满足物理约束和观测数据。

Method: 提出引导随机采样方法，将基于PDE残差的物理引导和观测约束融入扩散模型采样过程，并将该方法嵌入序列蒙特卡洛框架中。

Result: 在多个基准PDE系统以及多物理场和相互作用PDE系统中，该方法产生的解场比现有最先进生成方法具有更低的数值误差。

Conclusion: 该方法成功将物理约束融入生成模型，构建了可扩展的生成式PDE求解器，在保持物理合理性的同时提高了求解精度。

Abstract: We introduce a guided stochastic sampling method that augments sampling from diffusion models with physics-based guidance derived from partial differential equation (PDE) residuals and observational constraints, ensuring generated samples remain physically admissible. We embed this sampling procedure within a new Sequential Monte Carlo (SMC) framework, yielding a scalable generative PDE solver. Across multiple benchmark PDE systems as well as multiphysics and interacting PDE systems, our method produces solution fields with lower numerical error than existing state-of-the-art generative methods.

</details>


### [182] [FOCUS: DLLMs Know How to Tame Their Compute Bound](https://arxiv.org/abs/2601.23278)
*Kaihua Liang,Xin Tan,An Zhong,Hong Xu,Marco Canini*

Main category: cs.LG

TL;DR: FOCUS是一个针对扩散大语言模型的推理系统，通过动态聚焦计算于可解码token并实时淘汰不可解码token，实现了最高3.52倍的吞吐量提升，同时保持或提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型（DLLMs）相比自回归模型具有优势，但其部署受到高解码成本的限制。研究发现DLLM解码存在关键低效问题：计算在token块上并行化，但每个扩散步骤只有少量token可解码，导致大部分计算浪费在不可解码token上。

Method: 观察到注意力机制衍生的token重要性与token级解码概率之间存在强相关性。基于此洞察，提出FOCUS系统，通过动态聚焦计算于可解码token并实时淘汰不可解码token，增加有效批处理大小，缓解计算限制并实现可扩展吞吐量。

Result: 实证评估表明，FOCUS相比生产级引擎LMDeploy实现了最高3.52倍的吞吐量提升，同时在多个基准测试中保持或改善了生成质量。

Conclusion: FOCUS系统有效解决了DLLM解码的计算效率问题，通过智能token选择机制显著提升推理性能，为扩散大语言模型的实用部署提供了高效解决方案。系统已在GitHub开源。

Abstract: Diffusion Large Language Models (DLLMs) offer a compelling alternative to Auto-Regressive models, but their deployment is constrained by high decoding cost. In this work, we identify a key inefficiency in DLLM decoding: while computation is parallelized over token blocks, only a small subset of tokens is decodable at each diffusion step, causing most compute to be wasted on non-decodable tokens. We further observe a strong correlation between attention-derived token importance and token-wise decoding probability. Based on this insight, we propose FOCUS -- an inference system designed for DLLMs. By dynamically focusing computation on decodable tokens and evicting non-decodable ones on-the-fly, FOCUS increases the effective batch size, alleviating compute limitations and enabling scalable throughput. Empirical evaluations demonstrate that FOCUS achieves up to 3.52$\times$ throughput improvement over the production-grade engine LMDeploy, while preserving or improving generation quality across multiple benchmarks. The FOCUS system is publicly available on GitHub: https://github.com/sands-lab/FOCUS.

</details>


### [183] [Decoupled Diffusion Sampling for Inverse Problems on Function Spaces](https://arxiv.org/abs/2601.23280)
*Thomas Y. L. Lin,Jiachen Yao,Lufang Chiang,Julius Berner,Anima Anandkumar*

Main category: cs.LG

TL;DR: 提出DDIS框架，通过解耦设计实现数据高效的物理感知生成，用于PDE逆问题求解


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散后验采样的方法需要大量配对监督数据，且将物理关系隐式建模在联合系数-解模型中，数据效率低

Method: DDIS采用解耦设计：无条件扩散学习系数先验，神经算子显式建模前向PDE提供指导；支持DAPS避免DPS中的过平滑问题

Result: 理论证明DDIS避免联合模型在数据稀缺时的指导衰减问题；实验显示在稀疏观测下平均提升l2误差11%、谱误差54%；数据仅1%时比联合模型保持40%的l2误差优势

Conclusion: DDIS通过解耦设计实现了数据高效、物理感知的PDE逆问题求解，在数据稀缺和稀疏观测场景下显著优于现有方法

Abstract: We propose a data-efficient, physics-aware generative framework in function space for inverse PDE problems. Existing plug-and-play diffusion posterior samplers represent physics implicitly through joint coefficient-solution modeling, requiring substantial paired supervision. In contrast, our Decoupled Diffusion Inverse Solver (DDIS) employs a decoupled design: an unconditional diffusion learns the coefficient prior, while a neural operator explicitly models the forward PDE for guidance. This decoupling enables superior data efficiency and effective physics-informed learning, while naturally supporting Decoupled Annealing Posterior Sampling (DAPS) to avoid over-smoothing in Diffusion Posterior Sampling (DPS). Theoretically, we prove that DDIS avoids the guidance attenuation failure of joint models when training data is scarce. Empirically, DDIS achieves state-of-the-art performance under sparse observation, improving $l_2$ error by 11% and spectral error by 54% on average; when data is limited to 1%, DDIS maintains accuracy with 40% advantage in $l_2$ error compared to joint models.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [184] [Practical Evaluation of Quantum Kernel Methods for Radar Micro-Doppler Classification on Noisy Intermediate-Scale Quantum (NISQ) Hardware](https://arxiv.org/abs/2601.22194)
*Vikas Agnihotri,Jasleen Kaur,Sarvagya Kaushik*

Main category: quant-ph

TL;DR: 本文研究量子支持向量机(QSVM)在雷达空中目标分类中的应用，使用微多普勒特征，通过PCA降维后量子编码，在量子模拟器和实际量子硬件上验证性能。


<details>
  <summary>Details</summary>
Motivation: 探索量子机器学习在雷达信号处理中的实际应用，特别是利用量子支持向量机处理微多普勒特征进行分类，评估在NISQ时代量子硬件上的可行性。

Method: 提取经典特征后通过PCA降维，使用完全纠缠的ZZFeatureMap将特征向量嵌入量子核诱导的特征空间，采用基于核的QSVM进行分类。先在量子模拟器评估，后在IBM Torino和Fez量子处理器上验证。

Result: QSVM在显著降低特征维度的情况下，实现了与经典SVM基线相当的分类性能。硬件实验揭示了噪声、退相干和测量次数对量子核估计的影响，并在新的Heron r2架构上显示出改进的稳定性和保真度。

Conclusion: 该研究系统比较了模拟器和硬件上的QSVM实现，突出了量子核方法在实际雷达信号分类任务中的可行性和当前局限性，为量子机器学习在雷达应用中的部署提供了实证基础。

Abstract: This paper examines the application of a Quantum Support Vector Machine (QSVM) for radarbased aerial target classification using micro-Doppler signatures. Classical features are extracted and reduced via Principal Component Analysis (PCA) to enable efficient quantum encoding. The reduced feature vectors are embedded into a quantum kernel-induced feature space using a fully entangled ZZFeatureMap and classified using a kernel based QSVM. Performance is first evaluated on a quantum simulator and subsequently validated on NISQ-era superconducting quantum hardware, specifically the IBM Torino (133-qubit) and IBM Fez (156-qubit) processors. Experimental results demonstrate that the QSVM achieves competitive classification performance relative to classical SVM baselines while operating on substantially reduced feature dimensionality. Hardware experiments reveal the impact of noise and decoherence and measurement shot count on quantum kernel estimation, and further show improved stability and fidelity on newer Heron r2 architecture. This study provides a systematic comparison between simulator-based and hardware-based QSVM implementations and highlights both the feasibility and current limitations of deploying quantum kernel methods for practical radar signal classification tasks.

</details>


### [185] [Probing Entanglement and Symmetries in Random States Using a Superconducting Quantum Processor](https://arxiv.org/abs/2601.22224)
*Jia-Nan Yang,Lata Kh Joshi,Filiberto Ares,Yihang Han,Pengfei Zhang,Pasquale Calabrese*

Main category: quant-ph

TL;DR: 实验研究随机量子多体态的纠缠和对称性，通过演化简单乘积态生成随机态，验证其符合Haar随机态预测，包括观测Page曲线、子系统对称性和纠缠相


<details>
  <summary>Details</summary>
Motivation: 量子多体系统具有高度复杂性，但许多特征是普适的，只取决于对称性等基本物理特性而非微观细节。核心挑战是从模型特定特征中提取这些普适特性。均匀分布的随机量子态（Haar测度）为捕捉这种典型性提供了强大框架。

Method: 通过演化简单乘积态在遍历Floquet模型下生成随机多体量子态，实验测量：1）Rényi-2纠缠熵随子系统尺寸变化（观测Page曲线）；2）子系统对称性（纠缠不对称性）；3）部分转置约化密度矩阵的矩，揭示不同纠缠相

Result: 实验发现生成的随机态与Haar随机态系综的预测高度一致：成功观测到Page曲线，验证了子系统对称性特征，并通过部分转置密度矩阵的矩揭示了不同的纠缠相

Conclusion: 研究结果为多体量子系统的典型纠缠和对称性提供了实验视角，验证了随机态系综在描述量子多体系统普适特性方面的有效性

Abstract: Quantum many-body systems display an extraordinary degree of complexity, yet many of their features are universal: they depend not on microscopic details, but on a few fundamental physical aspects such as symmetries. A central challenge is to distill these universal characteristics from model-specific ones. Random quantum states sampled from a uniform distribution, the Haar measure, provide a powerful framework for capturing this typicality. Here, we experimentally study the entanglement and symmetries of random many-body quantum states generated by evolving simple product states under ergodic Floquet models. We find excellent agreement with the predictions from the Haar-random state ensemble. First, we measure the Rényi-2 entanglement entropy as a function of the subsystem size, observing the Page curve. Second, we probe the subsystem symmetries using entanglement asymmetry. Finally, we measure the moments of partially transposed reduced density matrices obtained by tracing out part of the system in the generated ensembles, thereby revealing distinct entanglement phases. Our results offer an experimental perspective on the typical entanglement and symmetries of many-body quantum systems.

</details>


### [186] [The Photonic Foundation of Temperature: Mechanisms of Thermal Equilibrium and Entropy Production](https://arxiv.org/abs/2601.22247)
*David Vaknin*

Main category: quant-ph

TL;DR: 论文提出光子是建立和维持物质热平衡的基本媒介，从量子电动力学角度为经典热力学提供微观基础，将温度解释为光子介导能量交换的涌现集体性质。


<details>
  <summary>Details</summary>
Motivation: 经典热力学虽然成功描述了平衡态现象，但缺乏实现热分布的微观机制。作者旨在从量子电动力学角度为温度概念提供物理基础，阐明热平衡的微观实现机制。

Method: 采用光子作为基本媒介的框架，从最小微分标度假设推导玻尔兹曼分布，分析维持热平衡所需的光子交换能量，研究非弹性光子散射如何产生熵增。

Result: 发现维持热平衡需要连续光子交换，平均能量为2.701Ec；熵增源于高能光子转化为多个低能量子；建立了区分真实热平衡与形式温度分配的物理标准。

Conclusion: 光子框架为现象学热力学提供了微观基础，澄清了温度作为光子介导能量交换涌现集体性质的物理意义，经典无限热库是光子浴动态维持层次结构中的有效理想化。

Abstract: I examine the physical foundations of temperature and thermal equilibrium by identifying photons as the fundamental agents that establish and maintain the characteristic energy scale $E_c = k_B T$ in ordinary matter. While classical thermodynamics successfully describes equilibrium phenomenologically, the realization of thermal distributions requires concrete microscopic mechanisms provided by quantum electrodynamics. We derive the Boltzmann distribution from a minimal differential scaling postulate and show that sustaining thermal equilibrium demands continuous photon exchange with average energy $\langle hν\rangle = 2.701\,E_c$, quantifying the energetic throughput necessary to counter radiative losses. Entropy production is shown to arise naturally from inelastic photon scattering that converts high-energy photons into many lower-energy quanta, thereby increasing accessible microstates and driving irreversible evolution toward equilibrium. We establish physical criteria distinguishing genuine thermal equilibrium from purely formal temperature assignments and demonstrate that the classical notion of an infinite thermal reservoir emerges as an effective idealization within a hierarchy of dynamically maintained photon baths. This photonic framework complements phenomenological thermodynamics by providing its microscopic foundation and clarifies the physical meaning of temperature as an emergent collective property of photon-mediated energy exchange.

</details>


### [187] [Entanglement and discord classification via deep learning](https://arxiv.org/abs/2601.22253)
*Katherine Muñoz-Mellado,Daniel Uzcátegui-Contreras,Antonio Guerra,Aldo Delgado,Dardo Goyeneche*

Main category: quant-ph

TL;DR: 使用卷积自编码器进行量子纠缠和量子不和谐的深度学习分类，能够识别束缚纠缠态并生成样本


<details>
  <summary>Details</summary>
Motivation: 量子纠缠和量子不和谐是量子信息处理中的核心概念，但束缚纠缠态（最罕见的纠缠形式）难以通过解析方法构造和识别

Method: 采用卷积自编码器深度学习架构，训练模型区分纠缠态和可分态，适用于d×d系统（d=2-7），并分别训练模型检测量子不和谐

Result: 模型在多种量子态家族上实现高分类准确率，能够识别束缚和自由纠缠，并能生成束缚纠缠态样本；量子不和谐检测模型也表现出高准确率且训练时间显著减少

Conclusion: 深度学习方法是识别和生成量子纠缠态（包括难以构造的束缚纠缠态）的有效工具，为量子信息处理提供了新的分析手段

Abstract: In this work, we propose a deep learning-based approach for quantum entanglement and discord classification using convolutional autoencoders. We train models to distinguish entangled from separable bipartite states for $d \times d$ systems with local dimension $d$ ranging from two to seven, which enables identification of bound and free entanglement. Through extensive numerical simulations across various quantum state families, we demonstrate that our model achieves high classification accuracy. Furthermore, we leverage the learned representations to generate samples of bound entangled states, the rarest form of entanglement and notoriously difficult to construct analytically. We separately train the same convolutional autoencoders architecture for detecting the presence of quantum discord and show that the model also exhibits high accuracy while requiring significantly less training time.

</details>


### [188] [Some properties of coherent states with singular complex matrix argument](https://arxiv.org/abs/2601.22258)
*Dušan Popov*

Main category: quant-ph

TL;DR: 提出了一种基于特殊奇异2x2矩阵的新型相干态，验证了其满足纯态和混合态的所有条件，并探讨了与量子比特和冯·诺依曼熵的关联


<details>
  <summary>Details</summary>
Motivation: 研究一种新型相干态的性质，该相干态的参数是两个特殊奇异2x2矩阵的线性组合，矩阵只有一个非零元素（等于1），并带有两个复变量作为展开系数

Method: 构建基于特殊奇异2x2矩阵的新型相干态，验证其满足相干态的所有条件，包括纯态和混合态（热态）的密度算符表征

Result: 证明了这种新型相干态满足相干态的所有条件，并成功建立了这些相干态与量子比特概念以及冯·诺依曼熵之间的联系

Conclusion: 成功构建并验证了一种新型相干态，展示了其在量子信息理论中的潜在应用价值，特别是在量子比特和熵分析方面

Abstract: In the paper our aim was to study the properties of a new version of coherent states whose argument is a linear combination of two special singular square 2 x 2 matrix, having a single nonzero element, equal to 1, and two labeling complex variables as developing coefficients. We have shown that this new version of coherent states satisfies all the conditions imposed on coherent states, both of pure, as well as the mixed (thermal) states characterized by the density operator. As applications, we examined the connection between these coherent states and the notions of qubits and von Neuman entropy.

</details>


### [189] [Three-dimensional squeezing of optically levitated nanospheres](https://arxiv.org/abs/2601.22283)
*Giacomo Marocco,David C. Moore,Daniel Carney*

Main category: quant-ph

TL;DR: 提出一种测量超越标准量子极限的脉冲协议，通过频率跳跃压缩机械系统状态，降低三维噪声，预测当前技术可实现约10dB压缩，实现量子增强的弱脉冲检测。


<details>
  <summary>Details</summary>
Motivation: 标准量子极限限制了脉冲测量的灵敏度，需要开发量子增强技术来检测弱脉冲，特别是在机械系统中实现超越标准量子极限的测量能力。

Method: 通过一系列谐波势频率跳跃来压缩机械系统的量子态，从而降低所有三个空间维度的噪声。协议应用于光学悬浮介电纳米粒子的实际系统。

Result: 量化了实际系统中退相干对最终灵敏度的限制，预测在当前技术条件下可实现约10dB的压缩，这足以实现量子增强的弱脉冲检测。

Conclusion: 该协议能够超越标准量子极限测量脉冲，通过频率跳跃压缩机械态，在当前技术条件下可实现显著量子增强，为弱脉冲检测提供了新途径。

Abstract: We propose a protocol to measure impulses beyond the standard quantum limit. The protocol reduces noise in all three spatial dimensions and consists of squeezing a mechanical system's state via a series of jumps in the frequency of the harmonic potential. We quantify how decoherence in a realistic system of an optically levitated, dielectric nanoparticle limits the ultimate sensitivity. We predict that $\sim$10 dB of squeezing is achievable with current technology, enabling quantum-enhanced detection of weak impulses.

</details>


### [190] [Efficient learning of logical noise from syndrome data](https://arxiv.org/abs/2601.22286)
*Han Zheng,Chia-Tung Chu,Senrui Chen,Argyris Giannisis Manes,Su-un Lee,Sisi Zhou,Liang Jiang*

Main category: quant-ph

TL;DR: 论文提出了一种基于综合征测量数据学习逻辑信道的方法，用于高效表征容错量子电路中的逻辑错误，相比直接逻辑基准测试可节省数个数量级的样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 量子电路错误表征对设备校准至关重要，但检测罕见错误事件需要大量样本。在容错纠错电路中，逻辑错误概率相对于物理噪声被抑制到更高阶，难以通过直接逻辑测量进行校准。

Method: 从统一的编码理论视角和时空编码形式出发，推导了仅从综合征数据学习逻辑信道的充要条件，明确表征了电路级泡利故障的可学习自由度。使用傅里叶分析和压缩感知技术，开发了具有可证明样本复杂度和计算成本保证的高效估计器。

Result: 在多个综合征提取电路上展示了端到端协议的性能，相比直接逻辑基准测试实现了数个数量级的样本复杂度节省。建立了基于综合征的学习作为表征容错量子设备中逻辑信道的实用方法。

Conclusion: 该工作将先前仅适用于现象学泡利噪声模型的逻辑信道推断框架扩展到现实的电路级噪声模型，为高效表征容错量子电路中的逻辑错误提供了理论基础和实用工具。

Abstract: Characterizing errors in quantum circuits is essential for device calibration, yet detecting rare error events requires a large number of samples. This challenge is particularly severe in calibrating fault-tolerant, error-corrected circuits, where logical error probabilities are suppressed to higher order relative to physical noise and are therefore difficult to calibrate through direct logical measurements. Recently, Wagner et al. [PRL 130, 200601 (2023)] showed that, for phenomenological Pauli noise models, the logical channel can instead be inferred from syndrome measurement data generated during error correction. Here, we extend this framework to realistic circuit-level noise models. From a unified code-theoretic perspective and spacetime code formalism, we derive necessary and sufficient conditions for learning the logical channel from syndrome data alone and explicitly characterize the learnable degrees of freedom of circuit-level Pauli faults. Using Fourier analysis and compressed sensing, we develop efficient estimators with provable guarantees on sample complexity and computational cost. We further present an end-to-end protocol and demonstrate its performance on several syndrome-extraction circuits, achieving orders-of-magnitude sample-complexity savings over direct logical benchmarking. Our results establish syndrome-based learning as a practical approach to characterizing the logical channel in fault-tolerant quantum devices.

</details>


### [191] [Local-oscillator-agnostic squeezing detection](https://arxiv.org/abs/2601.22291)
*Suchitra Krishnaswamy,Dhrithi Maria,Laura Ares,Lorenzo M. Procopio,Tim J. Bartley,Jan Sperling*

Main category: quant-ph

TL;DR: 提出无需已知参考信号即可测量连续变量玻色子系统非经典性的新框架，基于部分正规排序概念，适用于任意非经典本地振荡器的平衡零差检测


<details>
  <summary>Details</summary>
Motivation: 在无法获得已知相干激光参考信号的实际物理领域中，需要一种能够测量连续变量玻色子系统非经典性的方法。传统方法依赖于已知的经典参考信号，限制了在非经典参考条件下的量子特性评估。

Method: 基于部分正规排序概念构建更广泛的非经典性判据，这些判据允许研究量子现象而无需考虑选定子系统的量子性。将这种方法应用于平衡零差检测，使用任意（可能非经典）的本地振荡器状态，仅揭示被探测信号的量子性。

Result: 与标准技术相比，该方法显示出更强的鲁棒性和更高的灵敏度。框架能够有效评估光子系统的量子特征，即使在无法获得明确定义的相干激光作为参考状态的情况下。

Conclusion: 提出了一个广泛适用的框架，特别适合量子计量学和量子信息应用，能够在没有明确定义相干激光参考信号的物理领域中评估光子系统的量子特征。

Abstract: We address the problem of measuring nonclassicality in continuous-variable bosonic systems without having access to a known reference signal. To this end, we construct broader classes of criteria for nonclassicality which allow us to investigate quantum phenomena regardless of the quantumness of selected subsystems. Such witnesses are based on the notion of partial normal ordering. This approach is applied to balanced homodyne detection using arbitrary, potentially nonclassical local oscillator states, yet only revealing the probed signal's quantumness. Our framework is compared to standard techniques, and the robustness and advanced sensitivity of our approach is shown. Therefore, a widely applicable framework, well-suited for applications in quantum metrology and quantum information, is derived to assess the quantum features of a photonic system when a well-defined coherent laser as a reference state is not available in the physical domain under study.

</details>


### [192] [Quantum bootstrap product codes](https://arxiv.org/abs/2601.22363)
*Meng-Yuan Li*

Main category: quant-ph

TL;DR: 提出量子引导积（QBP）新范式，超越传统同调积方法，统一超图积码和分形码，构建叉复形结构，可生成自校正量子码并突破超图积码的码率上限。


<details>
  <summary>Details</summary>
Motivation: 传统量子CSS码的积构造主要基于同调代数方法（链复形的张量积），限制了设计灵活性。需要一种更通用的框架来统一现有重要码类（如高维超图积码、分形码）并突破现有构造的限制。

Method: 提出量子引导积（QBP）方法，通过求解"引导方程"来确定量子码结构。该方法产生具有多分量链群和边界映射的"叉复形"结构，超越了传统同调积的单一链复形框架。

Result: QBP范式统一了多种重要码类：任意维度的超图积码和分形码（如X-cube码）。叉复形结构揭示了分形码的拓扑结构（类似叶状分形序理论）。能够从具有恒定能量势垒的输入码生成自校正量子码，并突破超图积码的码率上限。

Conclusion: 量子引导积显著扩展了量子积码的范围，为设计容错量子存储器提供了通用框架，超越了传统同调积方法的限制，统一了多种重要量子码类并实现了更好的性能特性。

Abstract: Product constructions constitute a powerful method for generating quantum CSS codes, yielding celebrated examples such as toric codes and asymptotically good low-density parity check (LDPC) codes. Since a CSS code is fully described by a chain complex, existing product formalisms are predominantly homological, defined via the tensor product of the underlying chain complexes of input codes, thereby establishing a natural connection between quantum codes and topology. In this Letter, we introduce the \textit{quantum bootstrap product} (QBP), an approach that extends beyond this standard homological paradigm. Specifically, a QBP code is determined by solving a consistency condition termed the ``bootstrap equation''. We find that the QBP paradigm unifies a wide range of important codes, including general hypergraph product (HGP) codes of arbitrary dimensions and fracton codes typically represented by the X-cube code. Crucially, the solutions to the bootstrap equation yield chain complexes where the chain groups and associated boundary maps consist of multiple components. We term such structures \textit{fork complexes}. This structure elucidates the underlying topological structures of fracton codes, akin to foliated fracton order theories. Beyond conceptual insights, we demonstrate that the QBP paradigm can generate self-correcting quantum codes from input codes with constant energy barriers and surpass the code-rate upper bounds inherent to HGP codes. Our work thus substantially extends the scope of quantum product codes and provides a versatile framework for designing fault-tolerant quantum memories.

</details>


### [193] [Manjushri: A Tool for Equivalence Checking of Quantum Circuits](https://arxiv.org/abs/2601.22372)
*Xuan Du Trinh,Meghana Sistla,Nengkun Yu,Thomas Reps*

Main category: quant-ph

TL;DR: Manjushri是一个基于局部投影和加权二元决策图(WBDD)的量子电路等价性检查框架，在深度小于38时比ECMC快8-10倍，但在深度大于38时性能下降。


<details>
  <summary>Details</summary>
Motivation: 量子电路等价性验证是量子程序编译和优化的核心挑战，需要可扩展的自动化解决方案来处理大规模量子电路。

Method: 使用局部投影作为区分性电路指纹，通过加权二元决策图(WBDD)实现，提供紧凑高效的量子行为符号表示。

Result: 对于1D Clifford+T电路，Manjushri在深度30以内比ECMC快8-10倍；在32-64量子比特电路中，ECMC在深度50内几乎总是成功；Manjushri在深度38内几乎总是成功，但在128量子比特等效电路中深度48时成功率降至0%。

Conclusion: Manjushri是大规模量子电路验证的实用可扩展解决方案，在深度不超过38时是首选工具，深度超过38时ECMC表现更好。

Abstract: Verifying whether two quantum circuits are equivalent is a central challenge in the compilation and optimization of quantum programs. We introduce \textsc{Manjushri}, a new automated framework for scalable quantum-circuit equivalence checking. \textsc{Manjushri} uses local projections as discriminative circuit fingerprints, implemented with weighted binary decision diagrams (WBDDs), yielding a compact and efficient symbolic representation of quantum behavior. We present an extensive experimental evaluation that, for random 1D Clifford+$T$ circuits, explores the trade-off between \textsc{Manjushri} and \textsc{ECMC}, a tool for equivalence checking based on a much different approach. \textsc{Manjushri} is much faster up to depth 30 (with the crossover point varying from 39--49, depending on the number of qubits and whether the input circuits are equivalent or inequivalent): when inputs are equivalent, \textsc{Manjushri} is about 10$\times$ faster (or more); when inputs are inequivalent, \textsc{Manjushri} is about 8$\times$ faster (or more). For both kinds of equivalence-checking outcomes, \textsc{ECMC}'s success rate out to depth 50 is impressive on 32- and 64-qubit circuits: on such circuits, \textsc{ECMC} is almost uniformly successful. However, \textsc{ECMC} struggled on 128-qubit circuits for some depths. \textsc{Manjushri} is almost uniformly successful out to about depth 38, before tailing off to about 75\% at depth 50 (falling to 0\% at depth 48 for 128-qubit circuits that are equivalent). These results establish that \textsc{Manjushri} is a practical and scalable solution for large-scale quantum-circuit verification, and would be the preferred choice unless clients need to check equivalence of circuits of depth $>$38.

</details>


### [194] [Spectral Filtering for Learning Quantum Dynamics](https://arxiv.org/abs/2601.22400)
*Elad Hazan,Annie Marsden*

Main category: quant-ph

TL;DR: 提出量子谱滤波方法，将量子系统学习问题转化为带谱约束的复值线性动力系统学习，证明学习复杂度仅取决于有效量子维度而非环境维度。


<details>
  <summary>Details</summary>
Motivation: 高维量子系统学习面临维度灾难，传统系统辨识方法需要重构完整系统矩阵，计算成本随希尔伯特维度指数增长。需要找到更高效的学习方法。

Method: 提出量子谱滤波方法，将量子演化预测任务转化为带扇区有界特征值的复值线性动力系统学习问题。利用Slepian基的最优集中特性，将目标转向非适当动态学习。

Result: 证明此类系统的可学习性严格由有效量子维度k*决定，该维度由谱带宽和记忆时域确定。复值LDS可以在样本和计算复杂度独立于环境状态维度的情况下学习，前提是其谱有界。

Conclusion: 量子谱滤波方法突破了高维量子系统学习的维度限制，通过谱约束将学习复杂度从指数级降低到多项式级，为量子系统学习提供了新的理论框架。

Abstract: Learning high-dimensional quantum systems is a fundamental challenge that notoriously suffers from the curse of dimensionality. We formulate the task of predicting quantum evolution in the linear response regime as a specific instance of learning a Complex-Valued Linear Dynamical System (CLDS) with sector-bounded eigenvalues -- a setting that also encompasses modern Structured State Space Models (SSMs). While traditional system identification attempts to reconstruct full system matrices (incurring exponential cost in the Hilbert dimension), we propose Quantum Spectral Filtering, a method that shifts the goal to improper dynamic learning. Leveraging the optimal concentration properties of the Slepian basis, we prove that the learnability of such systems is governed strictly by an effective quantum dimension $k^*$, determined by the spectral bandwidth and memory horizon. This result establishes that complex-valued LDSs can be learned with sample and computational complexity independent of the ambient state dimension, provided their spectrum is bounded.

</details>


### [195] [On the undecidability of quantum channel capacities](https://arxiv.org/abs/2601.22471)
*Archishna Bhattacharyya,Arthur Mehta,Yuming Zhao*

Main category: quant-ph

TL;DR: 该论文研究了计算量子信道容量的计算复杂度问题，证明了量子容量的计算是QMA困难的，而最大纠缠辅助的零错误单次经典容量是不可计算的。


<details>
  <summary>Details</summary>
Motivation: 理解经典信道与量子信道容量计算能力的差异，探索量子信道容量是否可计算的问题。虽然大量证据表明量子信道容量可能不可计算，但缺乏形式化证明。

Method: 通过计算复杂性理论分析量子信道容量的计算难度，使用QMA（量子梅林-亚瑟）复杂性类来形式化量子容量的计算复杂度。

Result: 证明了对于一般量子信道，计算其量子容量是QMA困难的；同时证明了最大纠缠辅助的零错误单次经典容量是不可计算的。

Conclusion: 量子信道容量的计算具有本质上的困难性，这为理解量子信息处理的基本限制提供了新的理论依据，并揭示了量子与经典信道在计算复杂性方面的根本差异。

Abstract: An important distinction in our understanding of capacities of classical versus quantum channels is marked by the following question: is there an algorithm which can compute (or even efficiently compute) the capacity? While there is overwhelming evidence suggesting that quantum channel capacities may be uncomputable, a formal proof of any such statement is elusive. We initiate the study of the hardness of computing quantum channel capacities. We show that, for a general quantum channel, it is QMA-hard to compute its quantum capacity, and that the maximal-entanglement-assisted zero-error one-shot classical capacity is uncomputable.

</details>


### [196] [Dicke States for Accelerated Two Two-Level Atoms](https://arxiv.org/abs/2601.22479)
*Muzzamal I. Shaukat,Charles A. Wallace,Anatoly A. Svidzinsky,Marlan O. Scully*

Main category: quant-ph

TL;DR: 研究两个两能级原子在右Rindler楔形中形成Dicke态的条件，分析N原子对称态的动力学，发现多原子激发概率与单原子激发概率相关，推导出展示干涉效应的联合激发概率解析表达式。


<details>
  <summary>Details</summary>
Motivation: 探索非惯性系中量子系统的行为，特别是Dicke态（超辐射和亚辐射态）的形成条件，为相对论量子信息理论提供新见解。

Method: 研究两个两能级原子在右Rindler楔形中的系统，分析N个两能级原子形成对称态的动力学，推导联合激发概率的解析表达式。

Result: 确定了超辐射或亚辐射态形成的条件，发现N原子系统中激发任一原子的概率与激发单个原子的概率相关，联合激发概率表达式展示了干涉效应。

Conclusion: 这些发现为理解非惯性系中量子系统行为提供了新见解，有助于推动相对论量子信息理论的发展。

Abstract: We explore the formation of Dicke states. A system consisting of two two-level atoms located in the right Rindler wedge, has investigated to determine the conditions under which the superradiant or subradiant state can be formed. The dynamics of N two-level atoms forming symmetric state has also been analyzed and showed that the probability to excite any one atom of a collection of N atoms is related to the probability of exciting a single atom. We derive the analytical expression for the joint excitation probability which demonstrates the the interference effect. These findings provide new insights into the behavior of quantum systems in non-inertial frames and contribute to the broader understanding of relativistic quantum information theory.

</details>


### [197] [Structural Conditions for Native CCZ Magic-State Fountains in qLDPC Codes](https://arxiv.org/abs/2601.22489)
*Mohammad Rowshan*

Main category: quant-ph

TL;DR: 本文提出了一种结构化的方法，为CSS qLDPC码族构建常数深度的CCZ魔法态喷泉，通过识别编码理论条件来实现并行逻辑CCZ门。


<details>
  <summary>Details</summary>
Motivation: 目前没有已知的量子比特qLDPC码族能同时具备常数速率、线性距离、有界稳定子权重和原生的常数深度魔法态喷泉。本文旨在解决这一缺口，为qLDPC码实现高效的非克利福德门操作。

Method: 采用结构化方法，识别CSS qLDPC码族支持常数深度CCZ魔法态喷泉的编码理论条件。关键要素包括：(1) 定义X型逻辑算子的魔法友好三元组概念，要求两两正交且具有控制对角CCZ相位的三重重叠形式；(2) 建立物理CCZ电路的3-均匀超图模型，结合打包引理将具有有界重叠的大量三元组转化为有界度超图。

Result: 主要定理表明：如果CSS码族在n个量子比特上允许Ω(n^{1+γ})个魔法友好三元组，且其支撑具有有界的每量子比特参与度，则存在物理CCZ门的常数深度电路，可并行实现Ω(n^γ)个逻辑CCZ门，同时保持距离至常数因子。

Conclusion: 对于量子Tanner码等渐近良好的qLDPC码族，将原生CCZ魔法态喷泉的存在性简化为逻辑X空间中魔法友好三元组计数和分布的具体组合问题，为构建高效量子计算架构提供了理论基础。

Abstract: Quantum low-density parity-check (qLDPC) codes promise constant-rate, linear-distance families with bounded-weight checks, and recent work has realized transversal or constant-depth non-Clifford gates on various (often non-LDPC) codes. However, no explicit \emph{qubit} qLDPC family is known that simultaneously has constant rate, linear distance, bounded stabilizer weight, and a native \emph{magic-state fountain} that prepares many non-Clifford resource states in constant depth.
  We take a structural approach and identify coding-theoretic conditions under which a CSS qLDPC family necessarily supports a constant-depth $\CCZ$ magic-state fountain. The key ingredients are: (i) an algebraic notion of \emph{magic-friendly triples} of $X$-type logical operators, defined by pairwise orthogonality and a triple-overlap form controlling diagonal $\CCZ$ phases, and (ii) a 3-uniform hypergraph model of physical $\CCZ$ circuits combined with a packing lemma that turns large collections of such triples with bounded overlaps into bounded-degree hypergraphs.
  Our main theorem shows that if a CSS code family on $n$ qubits admits $Ω(n^{1+γ})$ magic-friendly triples whose supports have bounded per-qubit participation, then there exists a constant-depth circuit of physical $\CCZ$ gates implementing $Ω(n^γ)$ logical $\CCZ$ gates in parallel while preserving distance up to a constant factor. For asymptotically good qLDPC families such as quantum Tanner codes, this reduces the existence of a native $\CCZ$ magic-state fountain to a concrete combinatorial problem about counting and distributing magic-friendly triples in the logical $X$ space.

</details>


### [198] [Quantum-Enhanced Sensing Enabled by Scrambling-Induced Genuine Multipartite Entanglement](https://arxiv.org/abs/2601.22503)
*Guantian Hu,Wenxuan Zhang,Zhihua Chen,Liuzhu Zhong,Jingchao Zhao,Chilong Liu,Zixing Liu,Yue Xu,Yongchang Lin,Yougui Ri,Guixu Xie,Mingze Liu,Haolan Yuan,Yuxuan Zhou,Yu Zhang,Chang-Kang Hu,Song Liu,Dian Tan,Dapeng Yu*

Main category: quant-ph

TL;DR: 该论文实验实现了基于量子信息扰乱的蝴蝶计量学协议，在超导量子处理器上展示了超越标准量子极限的量子增强传感能力，系统规模达10个量子比特。


<details>
  <summary>Details</summary>
Motivation: 传统量子传感协议依赖复杂纠缠态制备和哈密顿量工程，在普适性和可扩展性上面临挑战。需要开发更实用、可扩展的量子增强传感方法。

Method: 采用蝴蝶计量学协议，利用多体量子信息扰乱机制，在超导量子处理器上实现量子增强传感。通过测量时间顺序关联函数(OTOC)来表征扰乱动力学。

Result: 实验观测到超越标准量子极限的相位编码灵敏度增强，灵敏度缩放因子接近海森堡极限的两倍。建立了增强灵敏度与OTOC动力学之间的关联，并证实扰乱诱导的真实多体纠缠是灵敏度增强的基础。

Conclusion: 该研究展示了一种可扩展且实用的量子增强传感方法，为相互作用多体量子系统中的量子计量学提供了新途径。

Abstract: Quantum sensing leverages quantum resources to surpass the standard quantum limit, yet many existing protocols rely on the preparation of complex entangled states and Hamiltonian engineering, posing challenges for universality and scalability. Here, we report an experimental realization of a universal protocol, known as Butterfly Metrology, proposed in [arXiv:2411.12794], demonstrating a scrambling-based approach for quantum-enhanced sensing on a superconducting quantum processor. By exploiting many-body information scrambling, we observe quantum-enhanced sensitivity to an encoded phase beyond the standard quantum limit, with a scaling consistent with a factor-of-two of the Heisenberg limit for system sizes of up to 10 qubits. Importantly, we experimentally establish a connection between the enhanced sensitivity and the dynamics of the out-of-time-order correlator (OTOC), and show that the buildup of scrambling-induced genuine multipartite entanglement underlies the observed sensitivity enhancement. Our results demonstrate a scalable and practical approach for quantum-enhanced sensing in interacting many-body quantum systems.

</details>


### [199] [Analysis of self-thermalization dynamics in the Bose-Hubbard model by using the pseudoclassical approach](https://arxiv.org/abs/2601.22553)
*Andrey R. Kolovsky*

Main category: quant-ph

TL;DR: 研究使用伪经典方法分析M位点Bose-Hubbard模型的自热化动力学，发现弱相互作用足以使系统混沌化但对热密度矩阵影响可忽略，这为两个初始不同热态的系统弛豫到相同热态提供了可能。通过短晶格耦合两个子系统时，数值计算的准稳态粒子流与边界驱动模型的解一致。


<details>
  <summary>Details</summary>
Motivation: 研究Bose-Hubbard模型在弱相互作用下的热化动力学，探索非相互作用可积系统如何通过弱相互作用转变为混沌系统，并研究两个不同热态子系统耦合后的弛豫过程。

Method: 使用伪经典方法计算单粒子密度矩阵，分析M位点Bose-Hubbard模型的自热化动力学。通过将两个子系统用长度L≪M的晶格耦合，数值计算准稳态的玻色粒子流。

Result: 弱相互作用足以将非相互作用玻色子的可积系统转变为混沌系统，但对玻色-爱因斯坦分布给出的热密度矩阵影响可忽略。两个初始不同热态的耦合Bose-Hubbard系统可以弛豫到相同热态。准稳态粒子流的数值计算结果与边界驱动L位点Bose-Hubbard模型的主方程解一致。

Conclusion: 弱相互作用在Bose-Hubbard模型中既能诱导混沌又对热密度矩阵影响微小，这为不同热态系统的热化弛豫提供了理论基础，且短晶格耦合下的粒子流行为与边界驱动模型一致。

Abstract: We analyze the self-thermalization dynamics of the $M$-site Bose-Hubbard model in terms of the single-particle density matrix that is calculated by using the pseudoclassical approach. It is shown that a weak inter-particle interaction, which suffices to convert the integrable system of non-interacting bosons into a chaotic system, has a negligible effect on the thermal density matrix given by the Bose-Einstein distribution. This opens the door for equilibration where the two coupled Bose-Hubbard systems, which are initially in different thermal states, relax to the same thermal state. When we couple these two subsystems by using a lattice of the length $L\ll M$, we numerically calculate the quasi-stationary current of Bose particles across the lattice and show that its magnitude is consistent with the solution of the master equation for the boundary driven $L$-site Bose-Hubbard model.

</details>


### [200] [Towards Sample Efficient Entanglement Classification for 3 and 4 Qubit Systems: A Tailored CNN-BiLSTM Approach](https://arxiv.org/abs/2601.22562)
*Qian Sun,Yuedong Sun,Yu Hu,Yihan Ma,Runqi Han,Nan Jiang*

Main category: quant-ph

TL;DR: 提出CNN-BiLSTM混合神经网络架构，用于高维量子系统的多体纠缠分类，在极少训练数据（仅100样本）下仍能保持90%以上准确率，显著减轻实验数据采集负担。


<details>
  <summary>Details</summary>
Motivation: 高维量子系统的多体纠缠准确分类对量子通信和信息处理至关重要，但传统方法资源密集，许多机器学习方法也需要大量训练数据，实验数据采集成为主要瓶颈。

Method: 提出CNN-BiLSTM混合神经网络架构：CNN提取局部特征，BiLSTM建模序列依赖关系。研究了两种融合范式：架构1（展平式）和架构2（维度变换式）。

Result: 仅用100个训练样本，架构2在3-qubit和4-qubit系统中分类准确率超过90%；在40万样本全数据条件下，两种架构准确率均超过99.97%。在低数据条件下，CNN-BiLSTM（特别是架构2）明显优于单独的CNN、BiLSTM和MLP。

Conclusion: 定制的CNN-BiLSTM融合显著减轻了实验数据采集负担，为复杂量子系统中的可扩展纠缠验证提供了实用途径。

Abstract: Accurate classification of multipartite entanglement in high-dimensional quantum systems is crucial for advancing quantum communication and information processing. However, conventional methods are resource-intensive, and even many machine-learning-based approaches necessitate large training datasets, creating a significant experimental bottleneck for data acquisition. To address this challenge, we propose a hybrid neural network architecture integrating Convolutional and Bidirectional Long Short-Term Memory networks (CNN-BiLSTM). This design leverages CNNs for local feature extraction and BiLSTMs for sequential dependency modeling, enabling robust feature learning from minimal training data. We investigate two fusion paradigms: Architecture 1 (flattening-based) and Architecture 2 (dimensionality-transforming). When trained on only 100 samples, Architecture 2 maintains classification accuracies exceeding 90% for both 3-qubit and 4-qubit systems, demonstrating rapid loss convergence within tens of epochs. Under full-data conditions (400 000 samples), both architectures achieve accuracies above 99.97%. Comparative benchmarks reveal that our CNN-BiLSTM models, especially Architecture 2, consistently outperform standalone CNNs, BiLSTMs, and MLPs in low-data regimes, albeit with increased training time. These results demonstrates that the tailored CNN-BiLSTM fusion significantly alleviates experimental data acquisition burden, offering a practical pathway toward scalable entanglement verification in complex quantum systems.

</details>


### [201] [Two-parameter bipartite entanglement measure](https://arxiv.org/abs/2601.22568)
*Chen-Ming Bai,Yu Luo*

Main category: quant-ph

TL;DR: 本文提出了一种新的双参数纠缠度量族——统一(q,s)-并发度，将标准并发度和q-并发度作为特例包含其中，并研究了其解析下界、具体表达式、单配性性质和纠缠多边形不等式。


<details>
  <summary>Details</summary>
Motivation: 纠缠并发度是量子技术中重要的二分纠缠度量。受统一熵启发，本文旨在构建一个更通用的双参数纠缠度量族，将现有度量作为特例包含，从而提供更丰富的纠缠描述框架。

Method: 结合正偏转置和重排判据，推导了任意二分混合态的统一(q,s)-并发度的解析下界；针对各向同性和Werner态，在q>1且qs≥1约束下获得了显式表达式；研究了q≥2、0≤s≤1且1≤qs≤3时在量子比特系统中的单配性性质；推导了q≥1且qs≥1时的纠缠多边形不等式。

Result: 成功构建了统一(q,s)-并发度这一新的纠缠度量族；获得了与强可分性判据相关的解析下界；得到了各向同性和Werner态的显式表达式；证明了在特定参数范围内的单配性性质；建立了适用于任意多分量量子系统的纠缠多边形不等式。

Conclusion: 统一(q,s)-并发度为纠缠度量提供了一个灵活的双参数框架，统一了现有度量，并揭示了纠缠结构的新数学关系，为量子信息处理中的纠缠分析提供了更强大的工具。

Abstract: Entanglement concurrence is an important bipartite entanglement measure that has found wide applications in quantum technologies. In this work, inspired by unified entropy, we introduce a two-parameter family of entanglement measures, referred to as the unified $(q,s)$-concurrence. Both the standard entanglement concurrence and the recently proposed $q$-concurrence emerge as special cases within this family. By combining the positive partial transposition and realignment criteria, we derive an analytical lower bound for this measure for arbitrary bipartite mixed states, revealing a connection to strong separability criteria. Explicit expressions are obtained for the unified $(q,s)$-concurrence in the cases of isotropic and Werner states under the constraint $q>1$ and $qs\geq 1$. Furthermore, we explore the monogamy properties of the unified $(q,s)$-concurrence for $q\geq 2$, $0\leq s\leq 1$ and $1\leq qs\leq 3$, in qubit systems. In addition, we derive an entanglement polygon inequality for the unified $(q,s)$-concurrence with $q\geq 1$ and $qs\geq 1$, which manifests the relationship among all the marginal entanglements in any multipartite qudit system.

</details>


### [202] [Multipartite entanglement measures based on the thermodynamic framework](https://arxiv.org/abs/2601.22583)
*Chen-Ming Bai,Yu Luo*

Main category: quant-ph

TL;DR: 提出基于热力学框架的统一多体纠缠表征方法，引入"ergotropic-gap concentratable entanglement"纠缠度量族，证明其在特定参数下满足连续性、主控单调性和单配性等关键性质。


<details>
  <summary>Details</summary>
Motivation: 需要一种统一的方法来表征和测量多体纠缠，特别是在热力学框架下建立系统的纠缠度量理论。

Method: 基于热力学框架，提出ergotropic-gap concentratable entanglement这一新的纠缠度量族，并在特定参数范围内证明其满足纠缠度量的关键数学性质。

Result: 该纠缠度量能有效区分多量子比特GHZ态和W态，并能检测四体星型量子网络态中的纠缠，证明其实际应用价值。

Conclusion: 成功建立了基于热力学的统一多体纠缠表征框架，提出的新纠缠度量具有良好数学性质和实际应用能力，为多体纠缠分析提供了新工具。

Abstract: In this work, we introduce a unified method to characterize and measure multipartite entanglement using the framework of thermodynamics. A family of the new entanglement measures is proposed: \textit{ergotropic-gap concentratable entanglement}. Furthermore, we establish that ergotropic-gap concentratable entanglement constitutes a well-defined entanglement measure within a specific parameter regime, satisfying key properties including continuity, majorization monotonicity and monogamy. We demonstrate the utility of this measure by showing it effectively distinguishes between multi-qubit Greenberger-Horne-Zeilinger states and W states. It also proves effective in detecting entanglement in specific classes of four-partite star quantum network states.

</details>


### [203] [Unconventional Distance Scaling of Casimir-Polder Force between Atomic Arrays](https://arxiv.org/abs/2601.22640)
*Qihang Ye,Qihang Ye,Bing Miao,Lei Ying*

Main category: quant-ph

TL;DR: 离散原子阵列间的Casimir-Polder力表现出反常距离标度：短距离衰减更快，远距离衰减更慢，与传统连续介质预期相反。


<details>
  <summary>Details</summary>
Motivation: 传统上，量子真空涨落介导的色散力具有普适的距离标度关系，延迟效应通常导致相互作用衰减更快。本文旨在研究离散系统是否违反这一预期。

Method: 采用微观散射方法研究两个原子阵列间的Casimir-Polder相互作用，分析离散晶格结构的影响，并将分析扩展到里德堡原子阵列。

Result: 发现离散原子阵列间的Casimir-Polder力表现出反常距离标度：短距离衰减更快，远距离衰减更慢。里德堡原子阵列表现出更强的反常标度偏离，并提出了可行的实验测量方案。

Conclusion: 离散系统的色散力行为与传统连续介质预期不同，为探索超越连续极限的色散力提供了新平台。

Abstract: Conventionally, dispersion forces mediated by quantum vacuum fluctuations are known to exhibit universal distance scalings, with retardation typically leading to a faster decay of the interaction. Here, we show that this expectation fails for intrinsically discrete systems. Using the microscopic scattering approach, we study the Casimir-Polder interaction between two atomic arrays, and uncover an unconventional distance scaling in which the force crosses over from a faster decay at short separations to a slower decay in the retarded regime. This behavior originates from the discrete lattice structure and can be consistently understood within the scattering picture. Extending our analysis to Rydberg atomic arrays, we predict an even stronger deviation from conventional scaling and propose an experimentally feasible scheme for direct measurement. Our results provide a new platform for exploring dispersion forces beyond the continuum limit.

</details>


### [204] [A complex-linear reformulation of Hamilton--Jacobi theory and the emergence of quantum structure](https://arxiv.org/abs/2601.22697)
*Yong Zhang*

Main category: quant-ph

TL;DR: 提出Hamilton-Jacobi-Schrödinger (HJS)理论，通过将经典力学中的(R,S)对嵌入单个复场，统一了经典和量子力学框架。


<details>
  <summary>Details</summary>
Motivation: 经典力学有多种等价表述（牛顿方程、拉格朗日-哈密顿框架、哈密顿-雅可比理论），但缺少与量子力学的统一数学视角。希望找到一种能自然包含经典和量子力学作为不同极限的单一底层结构。

Method: 从完全一般的复函数ansatz ψ = f(R,S)e^{ig(R,S)}出发，施加两个最小结构要求，得到唯一映射 ψ = Re^{iS/κ} 和线性HJS方程。当|κ|→0时精确重现HJ理论，当Re(κ)≠0时自然产生量子力学特征。

Result: HJS理论成功统一了经典和量子动力学：经典极限(|κ|→0)精确恢复哈密顿-雅可比理论，量子情形(Re(κ)≠0)自然产生量子力学核心特征，包括叠加原理、算符代数、对易关系、海森堡不确定性原理、玻恩规则和幺正演化。

Conclusion: HJS理论提供了统一的数学视角，其中经典和量子动力学作为单一底层结构的不同极限出现，为理解经典与量子力学的关系提供了新框架。

Abstract: Classical mechanics admits multiple equivalent formulations, from Newton's equations to the variational Lagrange-Hamilton framework and the scalar Hamilton-Jacobi (HJ) theory. In the HJ formulation, classical ensembles evolve through the continuity equation for a real density $ρ= R^{2}$ coupled to Hamilton's principal function $S$. Here we develop a complementary formulation, the Hamilton-Jacobi-Schrödinger (HJS) theory, by embedding the pair $(R,S)$ into a single complex field. Starting from a completely general complex ansatz $ψ= f(R,S) e^{i g(R,S)}$, and imposing two minimal structural requirements, we obtain a unique map $ψ= R e^{iS/κ}$ together with a linear HJS equation whose $|κ| \to 0$ limit reproduces the HJ formulation exactly. Remarkably, when $\mathrm{Re}(κ)\neq 0$, essential features of quantum mechanics, including superposition, operator algebra, commutators, the Heisenberg uncertainty principle, Born's rule, and unitary evolution, arise naturally as consistency conditions. HJS thus provides a unified mathematical viewpoint in which classical and quantum dynamics appear as different limits of a single underlying structure.

</details>


### [205] [Orders of magnitude runtime reduction in quantum error mitigation](https://arxiv.org/abs/2601.22785)
*Raam Uzdin*

Main category: quant-ph

TL;DR: 提出一种结合虚拟噪声缩放和分层缓解架构的量子误差缓解框架，相比传统零噪声外推后处理，大幅降低运行时开销，兼容动态电路并能与误差检测和量子纠错方案集成。


<details>
  <summary>Details</summary>
Motivation: 量子误差缓解（QEM）是当前量子处理器实验中常用的技术，但其采样开销大，需要长时间执行，期间设备噪声可能漂移，影响标准缓解协议的可靠性。虽然基于不可知噪声放大（ANA）的QEM策略对噪声变化具有内在弹性，但其采样成本仍是主要瓶颈。

Method: 提出一个结合虚拟噪声缩放和分层缓解架构的缓解框架。该方法兼容动态电路，可无缝集成误差检测和量子纠错方案，并能自然扩展到基于ANA的中电路测量和准备误差缓解。

Result: 相比传统零噪声外推后处理，该方法实现了数量级的运行时开销减少。在先前报告的实验数据上验证，观察到缓解效率和准确性的显著提升。

Conclusion: 该研究提出的量子误差缓解框架通过虚拟噪声缩放和分层架构，显著降低了采样开销，提高了对噪声漂移的鲁棒性，为实际量子计算应用提供了更高效的误差缓解方案。

Abstract: Quantum error mitigation (QEM) infers noiseless expectation values by combining outcomes from intentionally modified, noisy variants of a target quantum circuit. Unlike quantum error correction, QEM requires no additional hardware resources and is therefore routinely employed in experiments on contemporary quantum processors. A central limitation of QEM is its substantial sampling overhead, which necessitates long execution times where device noise may drift, potentially compromising the reliability of standard mitigation protocols. QEM strategies based on agnostic noise amplification (ANA) are intrinsically resilient to such noise variations, but their sampling cost remains a major practical bottleneck. Here we introduce a mitigation framework that combines virtual noise scaling with a layered mitigation architecture, yielding orders of magnitude reduction in runtime overhead compared to conventional zero-noise extrapolation post-processing. The proposed approach is compatible with dynamic circuits and can be seamlessly integrated with error detection and quantum error correction schemes. In addition, it naturally extends to ANA-based mitigation of mid-circuit measurements and preparation errors. We validate our post-processing approach by applying it to previously reported experimental data, where we observe a substantial improvement in mitigation efficiency and accuracy.

</details>


### [206] [Scattering of Squeezed Light by a Dielectric Slab](https://arxiv.org/abs/2601.22798)
*G. Pooseh*

Main category: quant-ph

TL;DR: 该研究建立了耗散介质板中压缩相干光散射的量子理论，分析了色散、吸收和多次反射对压缩特性的影响


<details>
  <summary>Details</summary>
Motivation: 研究压缩光在耗散介质中的传播特性，理解色散、吸收和多次反射如何影响量子压缩效应

Method: 采用格林函数量子化方法，推导场正交分量的变换，分析介质参数对压缩特性的影响

Result: 发现介质板能根据参数选择性地衰减或放大正交噪声，并给出了输出功率谱的表达式

Conclusion: 建立了耗散介质中压缩光散射的完整量子理论框架，为量子光学应用提供了理论基础

Abstract: We develop a quantum theory for the scattering of squeezed coherent light by a dissipative dielectric slab. Using the Green-function quantization approach, we derive the transformation of the field quadratures and show how dispersion, absorption, and multiple reflections distort the incident squeezing. We find that the slab can selectively attenuate or amplify quadrature noise depending on the slab parameters and provide expressions for the output power spectra.

</details>


### [207] [Steady-State Emission of Quantum-Correlated Light in the Telecom Band from a Single Atom](https://arxiv.org/abs/2601.22821)
*Alex Elliott,Takao Aoki,Scott Parkins*

Main category: quant-ph

TL;DR: 提出利用多能级原子产生电信波段量子关联光子的稳态发射方案，通过双光子跃迁和腔增强实现高效的单光子源


<details>
  <summary>Details</summary>
Motivation: 开发能够在电信波段产生量子关联光子的单原子光源，这对于量子通信和量子网络应用至关重要，因为电信波段在光纤中传输损耗最小

Method: 使用多能级原子，通过适当调谐一对激光器的频率，持续驱动双光子跃迁到原子激发态，从而在所需波长发射光子。通过谐振耦合腔模式到电信跃迁来增强发射速率，同时保持原子光源特有的反聚束计数统计特性。还探索了耦合第二个独立腔模式以增加发射速率并引入腔模式间的量子关联

Result: 建立了铯原子超精细结构模型并进行数值积分，证明了该方案在现代腔QED系统中实施的可行性。腔耦合可以增强电信发射速率，同时保持单光子特性，并且双腔耦合能进一步增加发射速率并产生腔模式间的量子关联

Conclusion: 该方案为在电信波段产生量子关联光提供了一种可行的单原子实现方法，通过腔增强和双腔耦合可以优化性能，为量子通信应用提供了有前景的单光子源解决方案

Abstract: We propose and investigate a scheme for the steady-state emission of quantum-correlated, telecom-band light from a single multilevel atom. By appropriately tuning the frequency of a pair of lasers, a two-photon transition is continually driven to an atomic excited state that emits photons at the desired wavelength. We show that resonantly coupling a cavity mode to the telecom transition can enhance the rate of emission while retaining the antibunched counting statistics that are characteristic of atomic light sources. We also explore coupling a second, independent cavity mode to the atom, which increases the telecom emission rate and introduces quantum correlations between the cavity modes. A model for the hyperfine structure of a single cesium atom is then described and numerically integrated to demonstrate the viability of implementing the scheme with a modern cavity QED system.

</details>


### [208] [Are Bell's conditions for local realism general enough?](https://arxiv.org/abs/2601.22833)
*Emilio Santos*

Main category: quant-ph

TL;DR: 本文批判性地重新审视了贝尔不等式，指出光学实验中探测器的响应模型过于理想化，提出了更物理的条件，并构建了一个违反Clauser-Horne不等式的局域实在模型，揭示了实验中存在符合时间漏洞。


<details>
  <summary>Details</summary>
Motivation: 本文的动机是重新审视贝尔不等式在光学实验中的应用，批评贝尔提出的探测器响应模型过于理想化，不符合实际物理条件。作者认为需要更物理的条件来评估局域实在性。

Method: 作者提出了更物理的探测器响应条件，构建了一个局域实在模型，并利用光学实验中存在的符合时间漏洞，使得该模型能够违反Clauser-Horne不等式。

Result: 成功构建了一个违反Clauser-Horne不等式的局域实在模型，证明了在考虑更物理的探测器响应条件和符合时间漏洞的情况下，光学实验可能无法排除局域实在性。

Conclusion: 贝尔不等式在光学实验中的应用存在局限性，特别是探测器响应模型的理想化假设和符合时间漏洞，使得实验结果可能无法完全排除局域实在性解释。

Abstract: Bell conditions for local realism are critically revisited. In particular for optical experiments I criticize Bell's proposed response of detectors to signals as extremely idealized. More physical conditions are proposed, whence a realistic local model of an optical experiment is possible which violates the Clauser-Horne (Bell) inequality. The possibility rests on the existence of a coincidence-time loophole in the experiments.

</details>


### [209] [Dynamics of states of infinite quantum systems as a cornerstone of the second law of thermodynamics](https://arxiv.org/abs/2601.22863)
*Walter F. Wreszinski*

Main category: quant-ph

TL;DR: 改进量子自旋系统的热力学第二定律确定性定理，证明绝热封闭系统中自发变化总是导致平均熵增加并达到最大值


<details>
  <summary>Details</summary>
Motivation: 改进量子自旋系统热力学第二定律的确定性定理表述，特别是在两个基本方面：一是完善第二定律的一般表述，二是研究一维系统中纯态向混合态转变的两种普适性动力学类别

Method: 采用量子自旋系统理论框架，研究绝热封闭系统的自发变化。具体分析两种一维动力学模型：指数模型和Dyson模型，后者利用Albert和Kiessling关于Cloitre函数的结果展示量子混沌的强图形证据

Result: 证明了在绝热封闭系统中，自发变化总是导致平均熵增加并最终达到最大值。通过分析两种一维动力学模型，展示了纯态向混合态转变的机制，其中Dyson模型表现出量子混沌特征

Conclusion: 成功改进了量子自旋系统的热力学第二定律确定性定理，为理解量子系统中熵增加原理提供了更精确的数学表述，并通过具体模型验证了该定理在不同动力学类别中的适用性

Abstract: We improve on our version of the second law of thermodynamics as a deterministic theorem for quantum spin systems in two basic aspects. The first concerns the general statement of the second law: spontaneous changes in an adiabatically closed system will always be in the direction of increasing mean entropy, which rises to a maximal value. Two specific examples concern the transition from pure to mixed states in two different universality classes of dynamics in one dimension, one being the exponential model, the other the Dyson model, the dynamics of the latter exhibiting strong graphical evidence of quantum chaos, as a consequence of the results of Albert and Kiessling on the Cloitre function.

</details>


### [210] [Fast magic state preparation by gauging higher-form transversal gates in parallel](https://arxiv.org/abs/2601.22939)
*Dominic J. Williamson*

Main category: quant-ph

TL;DR: 提出一种快速码手术程序，通过广义规范测量实现多个横向逻辑门的并行容错测量，用于高效并行制备魔法态。


<details>
  <summary>Details</summary>
Motivation: 随着高效量子低密度奇偶校验码和逻辑门的发展，需要寻找快速并行的逻辑魔法态制备协议以实现通用量子计算。

Method: 引入快速码手术程序，通过对支持高阶形式横向门的量子码执行广义规范测量，实现多个横向逻辑门的并行容错测量。

Result: 该程序的时间开销为常数，量子比特开销为线性，继承了基础码的容错特性和高阶形式横向门的结构。

Conclusion: 当应用于支持高阶形式Clifford门的编码时，该程序可实现快速容错的并行魔法态制备，激励了对支持此类门的好量子低密度奇偶校验码的搜索。

Abstract: Magic states are a foundational resource for universal quantum computation. To survive in a realistic noisy environment, magic states must be prepared fault-tolerantly and protected by a quantum error-correcting code. The recent discovery of highly efficient quantum low-density parity-check codes, together with efficient logic gates, lays the groundwork for low-overhead fault-tolerant quantum computation. This motivates the search for fast and parallel protocols for logical magic state preparation to enable universal quantum computation. Here, we introduce a fast code surgery procedure that performs a fault-tolerant measurement of many transversal logic gates in parallel. This is achieved by performing a generalized gauging measurement on a quantum code that supports a higher-form transversal gate. The time overhead of our procedure is constant, and the qubit overhead is linear. The procedure inherits fault-tolerance properties from the base code and the structure of the higher-form transversal gate. When applied to codes that support higher-form Clifford gates our procedure achieves fast and fault-tolerant preparation of many magic states in parallel. This motivates the search for good quantum low-density parity-check codes that support higher-form Clifford gates.

</details>


### [211] [High-resolution tunable frequency beamsplitter enabled by an integrated silicon pulse shaper](https://arxiv.org/abs/2601.23028)
*Chen-You Su,Kaiyi Wu,Lucas M. Cohen,Saleha Fatema,Navin B. Lingaraju,Hsuan-Hao Lu,Andrew M. Weiner,Joseph M. Lukens,Jason D. McKinney*

Main category: quant-ph

TL;DR: 基于集成脉冲整形器的量子频率处理器实现了高保真度、可调谐、超精细分辨率的片上频率分束器，支持2-5GHz频率间隔，保真度>0.9995，成功概率>0.9621。


<details>
  <summary>Details</summary>
Motivation: 开发可扩展、资源高效的集成频率分束器平台，用于量子信息处理中的密集并行单量子比特操作和多维门实现。

Method: 使用基于集成脉冲整形器的量子频率处理器，具有六个光谱通道，通过光谱相位或调制指数控制实现任意分束比。

Result: 实现了近乎理想的Hadamard门性能，保真度F>0.9995，修改后成功概率P>0.9621，支持2-5GHz频率间隔，最低仅需四个光谱通道。

Conclusion: 该平台为集成频率分束量子光子学提供了可扩展且资源高效的解决方案，开启了量子信息处理的新方向。

Abstract: We demonstrate high-fidelity, tunable, and ultrafine-resolution on-chip frequency beamsplitters using a quantum frequency processor based on an integrated pulse shaper with six spectral channels. Near-ideal Hadamard gate performance is achieved, with fidelity F > 0.9995 and modified success probability P > 0.9621 maintained across frequency spacings from 2-5 GHz and down to as few as four spectral pulse shaper channels. The system's support of frequency spacings as narrow as 2 GHz significantly surpasses prior bulk demonstrations and enables arbitrary splitting ratios via spectral phase or modulation index control. These results establish a scalable and resource-efficient platform for integrated frequency-bin quantum photonics, opening new directions in quantum information processing, including densely parallel single-qubit operations and multidimensional gate implementations.

</details>


### [212] [Dicke superposition probes for noise-resilient Heisenberg and super-Heisenberg Metrology](https://arxiv.org/abs/2601.23043)
*Sudha,B. N. Karthik,K. S. Akhilesh,A. R. Usha Devi*

Main category: quant-ph

TL;DR: 该论文研究在噪声环境下使用纠缠多量子比特态进行相位传感，发现特定Dicke叠加态在单体和双体相互作用下能实现近海森堡或超海森堡标度，同时相比GHZ、W叠加态和平衡Dicke态具有更强的抗退相干噪声能力。


<details>
  <summary>Details</summary>
Motivation: 噪声环境下的纠缠多量子比特态相位传感是现代量子计量学的核心主题。研究旨在寻找在单体和双体相互作用哈密顿量下，既能实现高精度（海森堡或超海森堡标度）又具有强噪声鲁棒性的量子探针。

Method: 研究Dicke态叠加探针在单体和双体相互作用哈密顿量下的量子相位传感性能。通过量子费舍尔信息分析标度行为，并评估在退相位噪声、相位阻尼、振幅阻尼和全局退极化噪声下的鲁棒性。

Result: 发现一类N量子比特Dicke叠加态在单体相互作用下能实现近海森堡标度，同时比GHZ、W叠加态和平衡Dicke态具有显著增强的抗退相位噪声能力。在双体相互作用下，某些Dicke叠加态能实现超海森堡标度，并在相位阻尼噪声下比费舍尔信息最优探针具有更好的鲁棒性。

Conclusion: 定制的近最优Dicke态叠加探针是适用于单体和双体相互作用下海森堡和超海森堡量子相位传感的多功能、噪声鲁棒性资源。

Abstract: Phase sensing with entangled multiqubit states in the presence of noise is a central theme of modern quantum metrology. The present work investigates Dicke state superposition probes for quantum phase sensing under parameter encoding generated by one- and two-body interaction Hamiltonians. A class of N-qubit Dicke superposition states that exhibit near-Heisenberg scaling, of the quantum Fisher information, while maintaining significantly enhanced robustness to dephasing noise compared to GHZ, W-superposition, and balanced Dicke states, under unitary encodings generated by one-body interaction Hamiltonians are identified. For two-body interactions, Dicke superposition probes optimizing the quantum Fisher information are identified, and their performance under phase-damping, amplitude-damping, and global depolarizing noise is explored. Within this family, certain Dicke superpositions are found to combine super-Heisenberg scaling with improved resilience to phase damping relative to Fisher information optimal probes. These results establish tailored near-optimal Dicke-state superposition probes as versatile and noise-resilient resources for Heisenberg and super-Heisenberg quantum phase sensing governed by one- and two-body interactions.

</details>


### [213] [Scalable Memory Sharing in Photonic Quantum Memristors for Reservoir Computing](https://arxiv.org/abs/2601.23044)
*Chaehyeon Lim,Hyungchul Park,Beomjoon Chae,Jeonghun Kwak,Soo-Yeon Lee,Namkyoo Park,Sunkyu Yu*

Main category: quant-ph

TL;DR: 提出可扩展的光子量子忆阻器网络，通过测量实现记忆共享，提升量子机器学习性能


<details>
  <summary>Details</summary>
Motivation: 光子虽然适合量子机器学习，但缺乏光子-光子相互作用限制了记忆功能实现。现有光子量子忆阻器记忆局限于局部元素，而生物或人工网络中记忆是跨系统共享的。

Method: 提出可扩展的光子量子忆阻器网络，每个忆阻节点使用自身和相邻量子态的历史更新内部状态，实现分布式记忆。将每个节点建模为光子量子忆晶体管。

Result: 在器件层面展示了经典和量子磁滞的显著增强，网络层面也增强了量子磁滞。作为量子储层实现时，通过增加数据可分离性提高了Fashion-MNIST分类准确率和置信度。

Conclusion: 该方法为使用与线性光学量子计算兼容的忆阻器件实现高容量量子机器学习铺平了道路。

Abstract: Although photons are robust, room-temperature carriers well suited to quantum machine learning, the absence of photon-photon interactions hinder the realization of memory functionalities that are critical for capturing long-range context. Recently, measurement-based implementations of photonic quantum memristors (PQMRs) have enabled tunable non-Markovian responses. However, their memory remains confined to local elements, in contrast to biological or artificial networks where memory is shared across the system. Here, we propose a scalable PQMR network that enables measurement-based memory sharing. Each memristive node updates its internal state using the history of its own and neighbouring quantum states, thereby realizing distributed memory. By modelling each node as a photonic quantum memtransistor, we demonstrate pronounced enhancements in both classical and quantum hysteresis at the device level, as well as enhanced network-level quantum hysteresis. Implemented as a quantum reservoir, the architecture achieves improved Fashion-MNIST classification accuracy and confidence via increased data separability. Our approach paves the way toward high-capacity quantum machine learning using memristive devices compatible with linear-optical quantum computing.

</details>


### [214] [Margin-Based Generalisation Bounds for Quantum Kernel Methods under Local Depolarising Noise](https://arxiv.org/abs/2601.23084)
*Saarisha Govender,Ilya Sinayskiy*

Main category: quant-ph

TL;DR: 该论文推导了量子核支持向量机在局部去极化噪声下的泛化边界，并通过实验验证了边界有效性，同时指出全局去极化噪声模型过于乐观。


<details>
  <summary>Details</summary>
Motivation: 当前NISQ时代的量子设备受噪声影响严重，这会降低机器学习模型的泛化性能。需要研究噪声对量子核支持向量机泛化能力的影响，并建立理论边界。

Method: 推导了量子核支持向量机在局部去极化噪声下的上下泛化边界（基于间隔的理论边界），通过数值模拟、真实量子硬件实验验证，并比较了局部与全局去极化噪声模型。

Result: 建立了噪声引起的间隔衰减理论边界，实验验证了边界的有效性，证明间隔是量子核支持向量机泛化性能的可靠指标，且全局去极化噪声模型过于乐观。

Conclusion: 局部去极化噪声模型能更准确描述NISQ时代量子设备的泛化性能衰减，间隔为基础的泛化边界为量子机器学习在噪声环境下的性能评估提供了理论工具。

Abstract: Generalisation refers to the ability of a machine learning (ML) model to successfully apply patterns learned from training data to new, unseen data. Quantum devices in the current Noisy Intermediate-Scale Quantum (NISQ) era are inherently affected by noise, which degrades generalisation performance. In this work, we derive upper and lower margin-based generalisation bounds for Quantum Kernel-Assisted Support Vector Machines (QSVMs) under local depolarising noise. These theoretical bounds characterise noise-induced margin decay and are validated via numerical simulations across multiple datasets, as well as experiments on real quantum hardware. We further justify the focus on margin-based measures by empirically establishing margins as a reliable indicator of generalisation performance for QSVMs. Additionally, we motivate the study of local depolarising noise by presenting empirical evidence demonstrating that the commonly used global depolarising noise model is overly optimistic and fails to accurately capture the degradation of generalisation performance observed in the NISQ era.

</details>


### [215] [TopoLS: Lattice Surgery Compilation via Topological Program Transformations](https://arxiv.org/abs/2601.23109)
*Junyu Zhou,Yuhao Liu,Ethan Decker,Justin Kalloor,Mathias Weiden,Kean Chen,Costin Iancu,Gushu Li*

Main category: quant-ph

TL;DR: TopoLS是一个拓扑编译器，通过结合ZX图优化和蒙特卡洛树搜索，为表面码量子计算提供可扩展的晶格手术编译方案，平均减少33%时空体积。


<details>
  <summary>Details</summary>
Motivation: 表面码量子计算需要将逻辑电路编译为晶格手术指令，现有启发式编译器资源开销大，而基于SAT求解器的编译器仅适用于小规模电路，需要可扩展且高效的编译方案。

Method: 结合ZX图优化与蒙特卡洛树搜索，采用不同的操作布局和拓扑感知电路分区，实现晶格手术结构的可扩展探索。

Result: 在多种架构的基准算法评估中，TopoLS相比先前启发式编译器平均减少33%时空体积，同时保持线性编译时间扩展性。

Conclusion: TopoLS为晶格手术编译提供了有效且可扩展的解决方案，优于仅适用于小电路的SAT求解器方法和资源开销大的启发式方法。

Abstract: Fault-tolerant quantum computing with surface codes can be achieved by compiling logical circuits into lattice-surgery instructions. To minimize space-time volume, we present TopoLS, a topological compiler that combines ZX-diagram optimizations with Monte Carlo tree search guided by different operation placements and topology-aware circuit partitioning. Our approach enables scalable exploration of lattice surgery structures and consistently reduces resource overhead. Evaluations of various benchmark algorithms across multiple architectures show that TopoLS achieves an average 33% reduction in space-time volume over prior heuristic-based compilers, while maintaining linear compilation time scaling. Compared to the SAT-solver-based compiler, which provides optimal results only for small circuits before becoming intractable, TopoLS offers an effective and scalable solution for lattice-surgery compilation.

</details>


### [216] [Free encoding capacity: A universal unit for quantum resources](https://arxiv.org/abs/2601.23116)
*Shampa Mondal,Soumajit Das,Preeti Parashar,Tamal Guha*

Main category: quant-ph

TL;DR: 论文提出了"自由编码容量"概念，作为量子资源理论中资源量的度量单位，特别在点资源理论中成为忠实资源度量。


<details>
  <summary>Details</summary>
Motivation: 研究当编码操作被限制在物理约束类别（量子资源理论中的自由操作）时，经典信息传输能力如何变化，探索这种受限容量与量子资源之间的关系。

Method: 将编码操作限制为量子资源理论中的自由操作集合，定义自由编码容量作为相应量子资源的度量单位，分析在点资源理论中的特性。

Result: 自由编码容量成为量子资源的度量单位，在点资源理论中成为忠实资源度量，并讨论了在资源理论状态转换中的意义。

Conclusion: 自由编码容量为量子资源理论提供了新的资源度量框架，特别在点资源理论中具有忠实性，为资源转换和一般资源理论的扩展提供了新视角。

Abstract: A perfect d-dimensional quantum channel can convey log d-bits of classical information by encoding messages in d-orthogonal quantum states. Alternatively, for every quantum state at the senders end, there exist d-encoding operations which produce d-orthogonal quantum states. Transmitting which via a d-level perfect quantum channel it is possible to communicate log d-bits of classical information. But what if the set of encoding operations is restricted only within a physically constrained class? Here, we consider such a class of encoding operations to be the set of free operations for any quantum resource theory and show that the constrained capacity - namely, the free encoding capacity (FEC) emerged as a unit of the corresponding quantum resource. Moreover, we show that for the pointed resource theories - a resource theory admitting only a single free state - FEC becomes a faithful resource measure also. We also discuss the implications of FEC in the question of resource-theoretic state transformations and the possibility of extending its faithfulness for general quantum resource theories.

</details>


### [217] [Compact U(1) Lattice Gauge Theory in Superconducting Circuits with Infinite-Dimensional Local Hilbert Spaces](https://arxiv.org/abs/2601.23150)
*J. M. Alcaine-Cuervo,S. Pradhan,E. Rico,Z. Shi,C. M. Wilson*

Main category: quant-ph

TL;DR: 提出了一种基于超导电路的紧凑U(1)晶格规范理论实现方案，利用相位和电荷变量的无限维希尔伯特空间，无需截断或辅助稳定器。


<details>
  <summary>Details</summary>
Motivation: 为模拟非微扰规范动力学提供一个可扩展的连续变量平台，解决传统方法中需要希尔伯特空间截断、辅助稳定器或惩罚项的问题。

Method: 使用超导电路架构，将规范场和物质场编码在转子变量的自由度中，高斯定律从局域电荷守恒自然涌现。规范-物质耦合通过约瑟夫森非线性实现，磁通量相互作用通过虚拟物质激发微扰生成。

Result: 数值对角化证实了紧凑电动力学的涌现和相干涡旋激发，表明在连续区域需要大的局域希尔伯特空间。所需电路参数在当前实验能力范围内。

Conclusion: 超导电路可作为模拟非微扰规范动力学的可扩展连续变量平台，为量子模拟规范理论提供了新途径。

Abstract: We propose a superconducting-circuit architecture that realizes a compact U(1) lattice gauge theory using the intrinsic infinite-dimensional Hilbert space of phase and charge variables. The gauge and matter fields are encoded directly in the degrees of freedom of the rotor variables associated with the circuit nodes, and Gauss's law emerges exactly from the conservation of local charge, without auxiliary stabilizers, penalty terms, or Hilbert-space truncation. A minimal gauge-matter coupling arises microscopically from Josephson nonlinearities, whereas the magnetic plaquette interaction is generated perturbatively via virtual matter excitations. Numerical diagonalization confirms the emergence of compact electrodynamics and coherent vortex excitations, underscoring the need for large local Hilbert spaces in the continuum regime. The required circuit parameters are within the current experimental capabilities. Our results establish superconducting circuits as a scalable, continuous-variable platform for analog quantum simulation of non-perturbative gauge dynamics.

</details>


### [218] [Complete Hierarchies for the Geometric Measure of Entanglement](https://arxiv.org/abs/2601.23243)
*Lisa T. Weinbrenner,Albert Rico,Kenneth Goodenough,Xiao-Dong Yu,Otfried Gühne*

Main category: quant-ph

TL;DR: 提出基于多拷贝纯态的方法计算多粒子量子态与乘积态的最大重叠，用于量化纠缠


<details>
  <summary>Details</summary>
Motivation: 量子物理中，多粒子系统的纠缠量化通常通过计算量子态与乘积态的距离来实现。该优化问题在物理和多线性代数中频繁出现，但现有方法存在计算困难

Method: 引入基于考虑纯态多个拷贝的方法，构建三种层次化近似方案，均被证明收敛到实际值

Result: 方法可用于计算几何纠缠度量，处理随机局域变换优化，为弱纠缠双粒子态寻找纠缠见证，设计混合多粒子态的强可分性测试

Conclusion: 该方法为解决纠缠量化问题提供了新途径，同时揭示了可分性测试的复杂性

Abstract: In quantum physics, multiparticle systems are described by quantum states acting on tensor products of Hilbert spaces. This product structure leads to the distinction between product states and entangled states; moreover, one can quantify entanglement by considering the distance of a quantum state to the set of product states. The underlying optimization problem occurs frequently in physics and beyond, for instance in the computation of the injective tensor norm in multilinear algebra. Here, we introduce a method to determine the maximal overlap of a pure multiparticle quantum state with product states based on considering several copies of the pure state. This leads to three types of hierarchical approximations to the problem, all of which we prove to converge to the actual value. Besides allowing for the computation of the geometric measure of entanglement, our results can be used to tackle optimizations over stochastic local transformations, to find entanglement witnesses for weakly entangled bipartite states, and to design strong separability tests for mixed multiparticle states. Finally, our approach sheds light on the complexity of separability tests.

</details>


### [219] [High-gain effects in broadband continuous-wave parametric down conversion sources and measurements with undetected photons](https://arxiv.org/abs/2601.23263)
*Martin Houde,Franz Roeder,Christine Silberhorn,Benjamin Brecht,Nicolás Quesada*

Main category: quant-ph

TL;DR: 研究高增益效应对未检测光子测量方案中可见信号光谱测量结果的影响，分析了三种干涉仪配置在不同损耗和色散条件下的性能表现。


<details>
  <summary>Details</summary>
Motivation: 研究高增益效应对未检测光子测量方案的影响，特别是当宽带光源（理想用于此类测量方案）在干涉仪中经历损耗和色散时，如何影响可见信号光谱的测量结果。

Method: 采用理论分析方法，研究三种干涉仪配置：1) SU(1,1)干涉仪；2) 诱导相干干涉仪；3) 分布式损耗配置。使用三阶色散工程集成波导参量下转换源的色散数据建模PDC光谱，分析每种配置在仅吸收、仅附加色散以及两者组合效应下的表现。

Result: 获得了不同配置在不同操作点的优势和劣势结果，揭示了高增益条件下损耗和色散对未检测光子测量方案性能的具体影响。

Conclusion: 通过系统分析三种干涉仪配置在不同操作条件下的表现，为未检测光子测量方案的设计和优化提供了理论指导，明确了各种配置的适用场景和局限性。

Abstract: We study theoretically how high-gain effects affect the measurement outcome of visible signal spectra in undetected photon measurement schemes. We consider two interferometric configurations: firstly, the SU(1,1) interferometer where the idler incurs loss and additional dispersion in between two identical, lossless, squeezers; secondly, the induced coherence interferometer where the idler incurs loss and additional dispersion in between two identical, lossless, squeezers and where the second squeezer is seeded by the idler and a vacuum ancilla mode. Furthermore, we consider a distributed loss configuration where the idler incurs loss as it propagates in the nonlinear medium. Motivated by experimental evidence and due to the fact that broadband sources are ideal for these measurement schemes, we use the dispersive data of a third-order dispersion engineered integrated waveguide parametric down conversion (PDC) source presented in New Journal of Physics 26, 123025 (2024) to model the PDC spectra in the three configurations. For each configuration we consider the case of idler-only (i) absorption, (ii) additional dispersion, and (iii) the combined effects. We obtain results which outline the strength and weaknesses of the different configurations at different operation points.

</details>


### [220] [Understanding multiscale disorder in superconducting nanowire single photon detectors](https://arxiv.org/abs/2601.23277)
*Nirjhar Sarkar,Ronan Gourgues,Yueh-Chun Wu,Chengyun Hua,Katyayani Seal,Andreas Fognini,Steven Randolph,Eugene Dumitrescu,Gabor B. Halasz,Benjamin Lawrie*

Main category: quant-ph

TL;DR: 通过氦离子辐照引入可控纳米尺度无序，结合多种测量技术区分局部不稳定过程与固有超导特性，为超导纳米线单光子探测器的无序工程提供多功能基础。


<details>
  <summary>Details</summary>
Motivation: 超导纳米线单光子探测器在量子信息科学中至关重要，但其性能受到无序和电动力学不均匀性的限制，这些效应尚未被充分理解。

Method: 结合直流传输、暗计数测量和偏置依赖的微波传输光谱，在氦离子辐照引入的可控纳米尺度无序条件下，区分局部不稳定驱动过程与固有超导去配对和动感非线性。

Result: 该方法能够系统调控单个器件中的动感、去配对电流、微波耗散和模式结构。偏置和温度依赖的共振偏移通过非线性动感量化了无序对超导态密度的修改，多个共振模式的出现揭示了电动力学上不同的超导区域形成。

Conclusion: 通过比较电流、磁场和温度下的去配对过程，分离了涡旋、准粒子和双能级系统等主要微波损耗机制，为超导纳米线探测器和谐振器的无序工程提供了稳健的多功能基础。

Abstract: Superconducting nanowire single-photon detectors are central to applications across quantum information science. Yet, their performance is limited by the effects of disorder and electrodynamic inhomogeneities that are not well understood. By combining DC transport, dark-count measurements, and bias-dependent microwave transmission spectroscopy in the presence of controlled nanoscale disorder introduced through helium-ion irradiation, we distinguish local instability-driven processes from intrinsic superconducting depairing and kinetic inductance nonlinearities. This approach enables systematic tuning of kinetic inductance, depairing currents, microwave dissipation, and mode structure within a single device. Bias- and temperature-dependent resonance shifts quantify disorder-induced modifications of the superconducting density of states through the nonlinear kinetic inductance, while the emergence of multiple resonant modes reveals the formation of electrodynamically distinct superconducting regions. Comparing depairing under current, field, and temperature isolates the dominant microwave loss mechanisms, separating vortex, quasiparticle, and two-level-system contributions, thus providing a robust multifunctional foundation for disorder engineering of superconducting nanowire detectors and resonators.

</details>


### [221] [Robust multiparameter estimation using quantum scrambling](https://arxiv.org/abs/2601.23283)
*Wenjie Gong,Bingtian Ye,Daniel Mark,Soonwon Choi*

Main category: quant-ph

TL;DR: 提出一种高效的多参数量子传感协议，可同时估计多个非对易、时变信号，利用量子混洗动力学将不同信号映射到独特的比特串测量模式，实现指数级参数估计能力。


<details>
  <summary>Details</summary>
Motivation: 传统量子传感通常专注于估计单个参数，而实际应用中需要同时监测多个非对易的时变信号。现有方法在同时估计多个参数时面临灵敏度损失和可扩展性限制，特别是在存在控制误差和读出错误的情况下。

Method: 利用量子混洗动力学将不同信号映射到独特的比特串测量模式。采用随机全局Clifford幺正变换，将信号编码到测量结果中。协议可扩展到局部随机Clifford电路、局部随机幺正电路和遍历哈密顿演化等近量子硬件中常见的动力学。

Result: 协议能够在系统尺寸上指数级地检测参数数量，同时保持灵敏度的最优标度。即使在控制不完美和读出错误存在的情况下，也能实现高效的多参数估计。数值和解析分析验证了协议的性能。

Conclusion: 该协议为多参数量子传感提供了通用且高效的框架，可应用于量子动力学的精确噪声基准测试、时变哈密顿量学习等实际应用，为近量子硬件中的多参数传感开辟了新途径。

Abstract: We propose and analyze a versatile and efficient multiparameter quantum sensing protocol, which simultaneously estimates many non-commuting and time-dependent signals that are coherently or incoherently coupled to sensing particles. Even in the presence of control imperfections and readout errors, our approach can detect exponentially many parameters in the system size while maintaining the optimal scaling of sensitivity. To accomplish this, scrambling dynamics are leveraged to map distinct signals to unique patterns of bitstring measurements, which distinguishes a large number of signals without significant sensitivity loss. Based on this principle, we develop a computationally efficient protocol utilizing random global Clifford unitaries and evaluate its performance both analytically and numerically. Our protocol naturally extends to scrambling dynamics generated by random local Clifford circuits, local random unitary circuits (RUCs), and ergodic Hamiltonian evolution--commonly realized in near-term quantum hardware--and opens the door to applications ranging from precise noise benchmarking of quantum dynamics to learning time-dependent Hamiltonians.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [222] [Revisiting the energy-momentum squared gravity](https://arxiv.org/abs/2601.22333)
*Mihai Marciu*

Main category: gr-qc

TL;DR: 重新审视能量-动量平方引力理论，考虑物质拉格朗日量对度规的二阶导数，分析修正后的宇宙学稳定性


<details>
  <summary>Details</summary>
Motivation: 传统能量-动量平方引力理论需要扩展，考虑热力学基础引入物质拉格朗日量的二阶导数修正，以更好地描述宇宙演化

Method: 1. 在能量-动量平方引力理论中引入物质拉格朗日量对度规的二阶导数修正；2. 获得修正理论的标量张量表示；3. 使用线性稳定性理论分析物理含义

Result: 对于某些特定的物质拉格朗日量，当前宇宙学系统与宇宙膨胀兼容，能够解释物质主导时代的出现，并在晚期接近德西特现象学的加速膨胀

Conclusion: 修正后的能量-动量平方引力理论能够描述宇宙从物质主导到加速膨胀的演化过程，为宇宙学现象提供理论解释

Abstract: In this paper we have revisited the energy-momentum squared gravity theory, by taking into account the second derivative of the matter Lagrangian with respect to the metric, encapsulating relations originated from thermodynamical grounds. After obtaining the scalar tensor representation of the energy-momentum squared gravity with the new corrections, we have analyzed the physical implications by relying on the linear stability theory. The results show that the current cosmological system is compatible with the expansion of the Universe for some specific matter Lagrangians, explaining the emergence of matter domination era, approaching the late time accelerated expansion era close to the de-Sitter phenomenology.

</details>


### [223] [When inflationary perturbations refuse to classicalise: the role of non-Gaussianity in Wigner negativity](https://arxiv.org/abs/2601.22219)
*Aurora Ireland,Vincent Vennin*

Main category: gr-qc

TL;DR: 该论文研究宇宙暴胀扰动中的量子印记，发现即使在大尺度上，量子干涉效应仍然显著，挑战了传统认为挤压态确保经典性的观点。


<details>
  <summary>Details</summary>
Motivation: 研究暴胀扰动的量子起源是否会在宇宙观测中留下可探测的量子印记。传统上这些扰动常被当作经典随机场处理，但作者质疑它们是否保留了量子特征，特别是量子干涉效应。

Method: 使用暴胀有效场论（EFT of inflation）计算曲率扰动的Wigner函数，考虑原初非高斯性。通过分析Wigner函数的非正定性来诊断量子干涉效应。

Result: 发现Wigner函数在超哈勃尺度上出现明显的干涉条纹，其负性在超慢滚背景下以a²增长。这表明量子效应在晚期仍然显著，挤压态本身不能确保经典性。

Conclusion: 量子效应在宇宙演化晚期可能仍然显著，传统认为挤压态确保经典性的观点需要修正。在宇宙学观测中探测宇宙起源的量子特征可能比之前认为的更有希望。

Abstract: Inflationary perturbations are quantum in origin. Yet, when computing cosmological observables, they are often treated as classical stochastic fields. Do they nevertheless retain quantum birthmarks? A hallmark of genuinely quantum behaviour is quantum interferences, arising from phase coherence between distinct branches of the wavefunction. Such interference is diagnosed by the non-positivity of the Wigner function, and according to Hudson's theorem, the only pure states with positive Wigner functions are Gaussian states. Consequently, any departure from Gaussianity necessarily implies a non-positive Wigner function, precluding a description in terms of a classical distribution. This motivates us to compute the Wigner function of curvature perturbations, accounting for primordial non-Gaussianities, using the EFT of inflation. We find that the Wigner function develops pronounced interference fringes on super-Hubble scales, and in particular, its negativity grows as $a^2$ in ultra-slow-roll backgrounds. These results demonstrate that quantum effects can remain significant at late times, and that squeezing alone does not ensure classicality, contrary to standard lore. This suggests that the prospects for detecting genuinely quantum signatures of the universe's origins in cosmological observables may be less bleak than previously thought.

</details>


### [224] [Modified Teleparallel $f(T)$ Gravity, DESI BAO and the $H_0$ Tension](https://arxiv.org/abs/2601.22225)
*Mariam Bouhmadi-López,Carlos G. Boiza,Maria Petronikolou,Emmanuel N. Saridakis*

Main category: gr-qc

TL;DR: f(T)引力模型研究显示，晚期扭率修正可以影响哈勃常数H₀的测量，部分模型能缓解H₀张力，但整体上仍不如ΛCDM模型


<details>
  <summary>Details</summary>
Motivation: 研究晚期引力修正（特别是f(T)宇宙学）是否能够缓解当前哈勃常数H₀的测量张力，探索扭率动力学如何影响宇宙背景膨胀和结构增长

Method: 采用三种代表性的f(T)参数化模型，这些模型在早期恢复为广义相对论的teleparallel等效形式，仅在晚期偏离。使用Pantheon+超新星、DESI DR2重子声学振荡、压缩Planck CMB距离先验和红移空间畸变数据进行联合约束

Result: 三个模型中，有两个部分地将H₀推断值向局部测量值偏移，而第三个模型加剧了差异。类幻影状态倾向于更高的H₀值，类quintessence状态则产生相反效果。全局统计比较显示，这些f(T)扩展模型在综合数据上不如ΛCDM模型

Conclusion: 虽然f(T)扩展模型整体上不被数据支持，但研究表明晚期扭率修正能够非平凡地重新分配当前宇宙学张力，影响背景膨胀和结构增长之间的平衡

Abstract: We investigate whether late-time modifications of gravity in the teleparallel framework can impact the current tension in the Hubble constant $H_0$, focusing on $f(T)$ cosmology as a minimal and well-controlled extension of General Relativity. We consider three representative $f(T)$ parametrisations that recover the teleparallel equivalent of General Relativity at early times and deviate from it only at late epochs. The models are confronted with unanchored Pantheon+ Type~Ia supernovae, DESI DR2 baryon acoustic oscillations, compressed Planck cosmic microwave background distance priors, and redshift-space distortion data, allowing us to jointly probe the background expansion and the growth of cosmic structures. Two of the three models partially shift the inferred value of $H_0$ towards local measurements, while the third worsens the discrepancy. This behaviour is directly linked to the effective torsional dynamics, with phantom-like regimes favouring higher $H_0$ and quintessence-like regimes producing the opposite effect. A global statistical comparison shows that the minimal $f(T)$ extensions considered here are not favoured over $Λ$CDM by the combined data. Nevertheless, our results demonstrate that late-time torsional modifications can non-trivially redistribute current cosmological tensions among the background and growth sectors.

</details>


### [225] [Scattering sections from regular black holes immersed in perfect fluid dark matter](https://arxiv.org/abs/2601.22299)
*Omar Pedraza,L. A. López,Isaac Fernández*

Main category: gr-qc

TL;DR: 研究黑洞在完美流体暗物质（PFDM）中的散射截面，分析暗物质参数对经典和半经典散射截面的影响


<details>
  <summary>Details</summary>
Motivation: 探索暗物质环绕黑洞时对散射现象的影响，特别是暗物质在黑洞附近区域可能起到的关键作用

Method: 采用经典和半经典方法计算散射截面，同时使用分波法对不同PFDM参数值的黑洞进行计算

Result: 暗物质的存在增加了经典散射截面，并改变了半经典区域干涉条纹的宽度；分波法计算结果显示出相似的定性行为

Conclusion: 黑洞周围的暗物质效应可能在黑洞现象学中扮演重要角色，特别是在黑洞附近的某些区域

Abstract: In this contribution, we investigate the scattering cross sections of black holes immersed in perfect fluid dark matter (PFDM). We present both the classical and semi-classical scattering cross sections for different values of the parameter that characterizes the PFDM contribution. Our results show that the presence of dark matter increases the classical scattering cross section and modifies the width of the interference fringes in the semi-classical regime. In addition, the scattering cross section is also computed using the partial wave method for the black holes considered, exhibiting similar qualitative behavior. These findings suggest that the effects of dark matter surrounding black holes may play an important role in black holes phenomenology, particularly in certain regions near the black hole.

</details>


### [226] [Photon-graviton polarization entanglement induced by a classical electromagnetic wave](https://arxiv.org/abs/2601.22332)
*Alessandro Ferreri*

Main category: gr-qc

TL;DR: 研究经典电磁波在闵可夫斯基时空中传播时诱导的光子-引力子对产生，发现电磁波传播可以产生光子-引力子偏振基中的贝尔态


<details>
  <summary>Details</summary>
Motivation: 探索经典电磁波与量子引力场相互作用，研究光子-引力子对产生机制及其量子纠缠特性

Method: 将电磁场分解为经典驱动波（线偏振或圆偏振）和量子涨落场，引力场用量子化引力子场描述，分析量子态的时间演化

Result: 发现电磁波传播可以产生光子-引力子偏振基中的贝尔态，即光子与引力子之间可以形成量子纠缠态

Conclusion: 经典电磁波与量子引力场相互作用可以产生光子-引力子纠缠态，讨论了在人工和自然场景中观测纠缠光子的可能性

Abstract: We study the photon-graviton pair production induced by the propagation of a classical electromagnetic (EM) wave in a Minkowskian spacetime. In our model, the gravitational field is described in terms of the quantized graviton field, whereas the electromagnetic field is split into a classical drive (a linearly or circularly polarized electromagnetic wave) and a quantum fluctuation field. We analyze the time evolution of the quantum state showing that, among other outcomes, the propagation of the EM wave can generate Bell states in the photon-graviton polarization basis. We finally discuss the possibility to observe entangled photons in artificial and natural scenarios.

</details>


### [227] [Grey-body factors of higher dimensional regular black holes in quasi-topological theories](https://arxiv.org/abs/2601.22340)
*Juan Pablo Arbelaez*

Main category: gr-qc

TL;DR: 研究准拓扑引力中高维正则黑洞的灰体因子和霍金辐射，发现辐射传输和霍金蒸发相比广义相对论中的奇异黑洞显著抑制


<details>
  <summary>Details</summary>
Motivation: 研究无限曲率修正如何影响高维正则黑洞的霍金辐射过程，探索正则黑洞与奇异黑洞在辐射特性上的差异

Method: 分析准拓扑引力中的高维正则黑洞模型，这些时空包含无限曲率修正以消除中心奇点，同时保留事件视界和明确定义的半经典描述

Result: 对于所有考虑的正则黑洞模型，辐射传输和相应的霍金蒸发相比广义相对论中的奇异黑洞解显著抑制

Conclusion: 正则黑洞的霍金辐射过程受到显著抑制，这反映了无限曲率修正对黑洞蒸发动力学的实质性影响

Abstract: We study grey-body factors and Hawking radiation of higher-dimensional regular black holes arising in quasi-topological gravity. These spacetimes incorporate infinite-curvature corrections that remove the central singularity while preserving an event horizon and a well-defined semiclassical description. We show that, for all considered regular black hole models, the transmission of radiation and the corresponding Hawking evaporation are significantly suppressed compared to the singular black hole solutions of General Relativity.

</details>


### [228] [A Maximum Entropy Conjecture for Black Hole Mergers](https://arxiv.org/abs/2601.22388)
*Monica Rincon-Ramirez,Nathan K. Johnson-McDaniel,Eugenio Bianchi,Ish Gupta,Vaishak Prasad,B. S. Sathyaprakash*

Main category: gr-qc

TL;DR: 研究发现双黑洞合并过程中，当将系统的质量和角动量映射到克尔黑洞时，对应的熵在演化过程中会出现最大值，且该最大值对应的参数与数值相对论预测的最终残骸参数惊人地接近。


<details>
  <summary>Details</summary>
Motivation: 探索是否存在一个简单的热力学原理能够决定双黑洞合并后最终残骸的状态选择，而不是仅仅依赖数值相对论的高精度预测。

Method: 使用后牛顿近似和数值相对论获得准圆形、无自旋双黑洞系统的质量M（包含结合能）和角动量J关系，将这些瞬时参数映射到假设的克尔黑洞，计算对应的熵并寻找其最大值。

Result: 熵最大值出现在M和J的特定值上，这些值与数值相对论预测的最终残骸参数惊人地接近，差异在几个百分点内。使用后牛顿近似和数值相对论结果都观察到一致的行为。

Conclusion: 提出了双黑洞合并的熵最大化猜想，暗示热力学原理可能支配着最终黑洞状态的选择，为理解黑洞合并过程提供了新的热力学视角。

Abstract: The final state of a binary black hole merger is predicted with high precision by numerical relativity, but could there be a simple thermodynamic principle within general relativity that governs the selection of the remnant? Using post-Newtonian relations between the mass M (including the binding energy) and angular momentum J of quasi-circular, nonspinning binaries, we uncover a puzzling result: When the binary's instantaneous M and J are mapped to those of a hypothetical Kerr black hole, the corresponding entropy exhibits a maximum during the evolution. This maximum occurs at values of M and J strikingly close to those of the final remnant predicted by numerical relativity. Consistent behavior is observed when using the relation between M and J obtained from numerical relativity evolution. Although this procedure is somewhat ad hoc, the agreement between the masses and spins of the final state obtained from numerical relativity and the results of this maximum entropy procedure is remarkable, with agreement to within a few percent when using either post-Newtonian or numerical relativity results for M and J. These findings allow us to propose an entropy maximization conjecture for binary black hole mergers, hinting that thermodynamic principles may govern the selection of the final black hole state.

</details>


### [229] [Thermodynamics and Stability of Ultraspinning Black Holes](https://arxiv.org/abs/2601.22565)
*Zhenbo Di*

Main category: gr-qc

TL;DR: 本文重新分析超自旋黑洞的热力学稳定性，发现与先前研究不同，超自旋黑洞在某些条件下可以存在热力学稳定区域，并首次将修正的反等周不等式应用于此类黑洞。


<details>
  <summary>Details</summary>
Motivation: 先前研究大多局限于中性情况和高温区域，认为超自旋黑洞总是热力学不稳定的。本文旨在系统分析不同系综中热容量随视界半径的变化，全面探讨超自旋黑洞的热力学稳定性。

Method: 使用Iyer-Wald形式论结合可积性条件推导守恒量和热力学第一定律；在不同系综中系统分析热容量随视界半径的变化；首次将修正的反等周不等式应用于超自旋黑洞。

Result: 发现超自旋黑洞可以存在热力学稳定区域，其存在取决于时空维度、解分支和电荷存在；修正的反等周不等式对参数空间施加了非平凡约束，包括对超自旋参数μ的上界、质量m的下界、电荷q和AdS半径l的上界。

Conclusion: 超自旋黑洞的热力学稳定性比先前认为的更复杂，存在稳定区域；修正的反等周不等式在超熵体系中仍然适用，对参数空间施加了重要约束；为超自旋黑洞的热力学描述提供了更完整的框架。

Abstract: Ultraspinning black holes have attracted considerable attention due to their super-entropic nature, and previous analyses -- mostly restricted to neutral cases and high-temperature regimes -- have suggested that such black holes are always thermodynamically unstable. In this work, we revisit the thermodynamic stability of ultraspinning black holes by performing a systematic analysis of the heat capacity in different ensembles over the full range of the horizon radius $r_H$, which were missed in earlier temperature-based analyses. We demonstrate for the first time that, contrary to earlier claims, ultraspinning black holes can admit thermodynamically stable regions, whose existence crucially depends on the spacetime dimension, the solution branch, and the presence of charge. In addition, we present the first application of the revised reverse isoperimetric inequality to ultraspinning black holes. Despite the violation of the original reverse isoperimetric inequality in this super-entropic regime, we find that the revised inequality remains applicable and imposes nontrivial constraints on the allowed parameter space, including an upper bound on the ultraspinning parameter $μ$, strengthened lower bounds on the mass $m$, and upper bounds on both the charge $q$ and the AdS radius $l$. To ensure the consistency of the thermodynamic description, the conserved charges and the first law in the ultraspinning limit are derived using the Iyer-Wald formalism together with integrability conditions.

</details>


### [230] [Holographic Dark Energy as a Source for Wormholes in Modified Gravity](https://arxiv.org/abs/2601.22577)
*G. G. L. Nashed,A. Eid*

Main category: gr-qc

TL;DR: 在f(ℛ,𝕋)引力理论中探索可穿越虫洞解，使用三种全息暗能量密度（Rényi、Moradpour、Bekenstein-Hawking熵形式）构建虫洞模型，研究参数α和β对平衡力及能量条件的影响。


<details>
  <summary>Details</summary>
Motivation: 研究f(ℛ,𝕋)引力理论中可穿越虫洞的构造，该理论是曲率-物质耦合的扩展。通过引入基于不同熵形式（Rényi、Moradpour、Bekenstein-Hawking）的全息暗能量密度，探索虫洞几何的物理性质。

Method: 在f(ℛ,𝕋)引力框架下，使用三种全息暗能量密度公式：Rényi熵密度ρ_R、Moradpour熵密度ρ_M和Bekenstein-Hawking熵密度ρ_BH。从场方程推导相应的形状函数，验证虫洞喉部和喇叭口条件。分析参数α和β变化对平衡力（TOV方程）和能量条件的影响。

Result: 获得的形状函数满足可穿越虫洞的标准喉部和喇叭口要求。研究发现零能量条件被违反，表明需要奇异物质（或有效的奇异物质部分）来支撑虫洞几何。参数α和β的变化影响平衡力分布和能量条件的满足程度。通过嵌入曲面可视化虫洞的空间结构。

Conclusion: 在f(ℛ,𝕋)引力理论中，基于不同熵形式构造的全息暗能量密度能够产生满足可穿越条件的虫洞解。零能量条件的违反表明这些虫洞需要奇异物质支持，这与经典广义相对论中的虫洞特性一致。参数α和β在调节虫洞几何和物理性质方面起重要作用。

Abstract: Traversable wormhole solutions are explored in $f(\mathcal{R},\mathbb{T})$ gravity, a curvature--matter extension in which $\mathcal{R}$ is the Ricci scalar and $\mathbb{T}$ denotes the trace of the energy--momentum tensor. To generate explicit wormhole models, we prescribe holographic dark-energy densities based on entropy formalism proposed by Rényi, Moradpour, and Bekenstein--Hawking, namely \[ ρ_{\textit{R}} = \fracα{4α_1 r^4 c^2 κ}\ln\!\left(1+πα_1 r^2\right), \qquad ρ_{\textit{M}} = \fracα{4πr^2 c^2 κ\left(πα_1 r^2 + 1\right)}, \qquad ρ_{\textit{BH}} = \fracα{4 c^2 κr^2}, \] with $α$ and $β$ carrying dimensions of $L^{-2}$. The corresponding shape functions obtained from the field equations satisfy the standard throat and flare-out requirements for traversability. We then study how varying $α$ and $β$ affects (i) the balance of forces associated with equilibrium and (ii) the status of the energy conditions. In particular, the null energy condition is found to be violated, indicating that exotic matter (or an effective exotic sector) is required to support the wormhole geometry. The spatial structure of the solutions is further visualized through embedding surfaces.

</details>


### [231] [Nonlocal Corrections to Scalar Field Effective Action in de Sitter spacetime](https://arxiv.org/abs/2601.22644)
*Will Cerne,Teruaki Suyama*

Main category: gr-qc

TL;DR: 研究FLRW背景中测试标量场的一圈有效作用量，关注二阶相互作用强度的量子修正，发现非局域记忆项和随机噪声项，并给出重整化方案。


<details>
  <summary>Details</summary>
Motivation: 研究FLRW宇宙学背景下标量场的量子修正效应，特别是理解非局域量子修正（记忆项和随机噪声项）如何影响场动力学，这对于理解早期宇宙量子涨落和结构形成具有重要意义。

Method: 采用Schwinger-Keldysh形式体系推导场期望值的运动方程，分析紫外发散结构并提供一致的重整化程序，在缓变场假设下应用局域近似分析物理效应。

Result: 发现非局域量子修正包含记忆项和随机噪声项，记忆项在局域近似下表现为漂移系数的负贡献，在φ⁴理论中这些一圈修正导致红外区域场方差相比树级结果受到抑制。

Conclusion: FLRW背景中标量场的量子修正不仅包含标准局域辐射修正，还产生重要的非局域效应（记忆和噪声），这些效应显著影响场动力学，特别是在红外区域抑制场方差。

Abstract: We investigate the one-loop effective action for a test scalar field in a general Friedmann-Lemaître-Robertson-Walker (FLRW) background, specifically focusing on quantum corrections up to the second order in the interaction strength. By employing the Schwinger-Keldysh formalism, we derive the equation of motion for the field expectation value, which incorporates not only the standard local radiative corrections but also novel nonlocal features: a memory term and a stochastic noise term. We identify all ultraviolet divergent structures within these nonlocal terms and provide a consistent renormalization procedure. To analyze the physical impact of these terms, we apply a local approximation under the assumption of slowly-varying fields, by which the memory term acts as a negative contribution to the drift coefficient. As a concrete application, we consider a massive $φ^4$ theory and show that these one-loop corrections lead to a suppression of the field variance in the infrared regime compared to the tree-level results.

</details>


### [232] [Geometric Selection Rules for Singularity Formation in Modified Gravity](https://arxiv.org/abs/2601.22739)
*Soumya Chakrabarti*

Main category: gr-qc

TL;DR: 曲率不变量的多项式简并性可作为修正引力理论中时空奇点的几何选择规则，奇点形成通常被曲率或标量诱导的各向异性所阻碍。


<details>
  <summary>Details</summary>
Motivation: 研究修正引力理论中奇点形成的普遍性问题，探讨曲率不变量代数结构如何影响奇点形成机制。

Method: 利用黎曼几何的代数结构分析曲率不变量的多项式简并性，推导这些简并对有效能量-动量张量的约束，应用于度量f(R)引力和标量-张量理论。

Result: 曲率不变量的简并对奇点形成施加了非平凡约束，奇点形成通常被曲率或标量诱导的各向异性所阻碍，奇点只能在黎曼曲率不变量选择的代数允许分支上形成。

Conclusion: 修正引力理论中的奇点形成不是普遍结果，而是受曲率不变量代数结构限制的选择性过程，这为奇点避免提供了新的几何机制。

Abstract: We argue that the polynomial degeneracies of curvature invariants can act as geometric selection rules for spacetime singularities in modified theories of gravity. The degeneracies arise purely from the algebraic structure of Riemannian geometry and impose non-trivial constraints on the effective energy-momentum tensor. We derive these constraints for metric $f(R)$ gravity and a wide class of scalar-tensor theories to show that a singularity formation is generally occluded by curvature and/or scalar-induced anisotropies. Therefore, formation of a singularity in modified theories of gravity is not always a generic outcome but can occur only along algebraically admissible branches selected by Riemannian curvature invariants.

</details>


### [233] [Cosmological Dynamics of Hyperbolic Evolution Models in $f(Q,L_m)$ Gravity](https://arxiv.org/abs/2601.22780)
*V. A. Kshirsagar,S. A. Kadam,Vishwajeet S. Goswami*

Main category: gr-qc

TL;DR: 在f(Q,ℒ_m)引力框架下研究双曲正弦和余弦演化函数，这些模型表现出与ΛCDM模型在晚期倾斜的行为，EoS参数呈现精质相并趋近于-1，支持从早期减速到晚期加速的宇宙演化。


<details>
  <summary>Details</summary>
Motivation: 在f(Q,ℒ_m)引力理论框架下探索宇宙学可行的演化模型，特别是研究双曲正弦和余弦函数形式的演化，以理解宇宙加速膨胀现象并检验这些模型与标准ΛCDM模型的一致性。

Method: 采用f(Q,ℒ_m)引力理论框架，引入双曲正弦和余弦演化函数构建宇宙学模型。通过分析状态方程参数随参数值变化的行为，并使用广泛接受的能量条件重新检验两个模型的可行性，同时通过减速参数剖面分析验证模型的加速行为。

Result: 状态方程参数呈现精质相并在晚期趋近于-1，两个模型在晚期表现出与ΛCDM模型倾斜的行为。强能量条件的违反证实了模型的加速行为，减速参数剖面分析明确支持模型能够解释从早期减速到晚期加速的宇宙演化现象。

Conclusion: 双曲正弦和余弦演化函数在f(Q,ℒ_m)引力框架下是宇宙学可行的模型，它们能够解释宇宙加速膨胀现象，与ΛCDM模型在晚期有相似行为，并通过能量条件和减速参数分析得到验证。

Abstract: This paper highlights cosmologically viable sine and cosine hyperbolic evolution functions in the framework of $f(Q,\mathcal{L}_m)$ gravity. The models have been tested to check the behavior of the equation of state (EoS) parameter under the variation of parametric values. The EoS parameter experiences a quintessence phase, and is approaching to $-1$ at late time. The models are showing inclined behaviour with the $Λ$CDM model at the late time. The viability of both the models is retested using the widely accepted energy conditions in both cases. The violation of the strong energy condition admits the accelerating behaviour of the models. The same has been explained through the analysis of the profile of deceleration parameter, which concretely supports the evidence that the models explain early deceleration to late time acceleration phenomena.

</details>


### [234] [Scalar-tensor-vector gravity theory is tested by black hole photon rings](https://arxiv.org/abs/2601.23012)
*Qiao Yue*

Main category: gr-qc

TL;DR: 研究标量-张量-矢量引力框架下Reissner-Nordström黑洞的光子环和阴影结构，发现MOG参数α和电荷Q对黑洞结构有显著影响，为测试修正黑洞模型提供了计算基础。


<details>
  <summary>Details</summary>
Motivation: 研究标量-张量-矢量引力框架下Reissner-Nordström黑洞的光子环和阴影结构，为测试修正黑洞模型和区分量子引力模型提供观测标准。

Method: 通过分析黑洞参数（MOG参数α和电荷Q）对事件视界半径、光子球半径、临界撞击参数等的影响，进行光线追踪模拟，并利用EHT对SgrA*和M87*的观测数据约束参数范围。

Result: α增加时事件视界半径、光子球半径和临界撞击参数都增加，而Q增加时这些参数减小；光子环半径由α和Q唯一确定；基于EHT数据推导了α和Q的允许范围；辐射模拟显示固定Q时较大α导致更大、非简并的光子环。

Conclusion: 研究标量-张量-矢量引力中Reissner-Nordström黑洞的光子环结构为测试修正黑洞和区分量子引力模型提供了独特的光学诊断工具，与当前EHT数据一致，未来ngEHT和多波段偏振观测可进一步验证。

Abstract: This paper investigates the photon ring and shadow structure of the Reissner-Nordström black hole in the scalar-tensor-vector gravitational framework. The black hole is characterized by the ( MOG) parameter (α) and the charge (Q). The study finds that as (α) increases, the event horizon radius (r_h), photon sphere radius (r_{ph}), and critical impact parameter (b_{ph}) all increase, while these decrease as (Q) increases. The innermost stable circular orbit radius (r_{isco}) exhibits similar monotonic behavior. Ray-tracing shows that as (Q) increases, the impact parameter (b) interval between the lensing ring and photon ring widens; (b_{\text{ph}}) is non-degenerate, and the photon ring radius is uniquely determined by (α) and (Q). Using $EHT$ constraints on (SgrA^*) and (M87^*), the bounds on (α) and (Q) are derived. For (Q = 0), (0.5), and (1), the allowed ranges are (α\in [0, 0.06]), ([0, 0.11]), and ([0.19, 0.36]), respectively. Radiative simulations show that for fixed (Q), larger (α) leads to a larger, non-degenerate photon ring. The Schwarzschild case is approached only when both (α) and (Q) are small. This provides a computational basis for testing modified black holes and offers a non-degenerate observational criterion for distinguishing quantum gravity models, consistent with current $EHT$ data. Future observations with $ngEHT$ and multi-band polarization can further test this. The results suggest that studying the photon ring structure of a Reissner-Nordström black hole in scalar-tensor-vector gravity provides a unique optical diagnostic for potential quantum-gravity tests and black hole properties.

</details>


### [235] [Towards Claiming a Detection of Gravitational Memory](https://arxiv.org/abs/2601.23019)
*Jann Zosso,Lorena Magaña Zertuche,Silvia Gasparotto,Adrien Cogez,Henri Inchauspé,Milo Jacobs*

Main category: gr-qc

TL;DR: 论文提出了一种定义和建模引力波记忆效应的理论框架，特别适用于致密双星并合，为LISA等空间探测器提供了检测引力波记忆的理论基础。


<details>
  <summary>Details</summary>
Motivation: 引力波记忆是引力辐射引起的渐近时空度规永久变化的零频效应。虽然其普遍表现是固有距离的净变化，但引力波探测器本质上对最终偏移不敏感，只能探测相关的过渡过程。检测的主要挑战在于定义物理上有意义且操作上稳健的时变信号模型，该模型应可唯一归因于引力波记忆，并能与纯振荡辐射区分开来。

Method: 1. 对引力波记忆理论进行自包含的回顾；2. 提出定义和建模引力波记忆上升的理论框架，特别适用于致密双星并合；3. 针对空间探测器（特别是LISA），分析包含记忆贡献的引力辐射响应，重点关注超大质量黑洞双星并合；4. 建立统计上定义良好的假设检验框架，用于区分无记忆和有记忆的辐射。

Result: 开发的理论框架为统计上定义良好的假设检验提供了理论基础，可用于评估引力波记忆的检测前景。超大质量黑洞双星并合为首次单事件检测提供了最有希望的观测前景。

Conclusion: 该研究结果为未来观测声称检测到引力波记忆建立了一条原则性途径，为引力波记忆的检测提供了理论框架和操作指南。

Abstract: Gravitational memory is a zero-frequency effect associated with a permanent change in the asymptotic spacetime metric induced by radiation. While its universal manifestation is a net change of proper distances, gravitational-wave detectors are intrinsically insensitive to the final offset and can only probe the associated transition. A central challenge for any claim of detection therefore lies in defining a physically meaningful and operationally robust model of this time-dependent signal, which is uniquely attributable to gravitational memory and distinguishable from purely oscillatory radiation. In this work, we propose a general solution to this challenge. Building on a self-contained review of the theory of gravitational memory, we discuss a theoretical framework for defining and modeling a gravitational memory rise, in particular applicable to compact binary coalescences. Specializing to space-based detectors, we analyze the response of LISA to gravitational radiation including a memory contribution, with particular emphasis on mergers of supermassive black hole binaries, which offer the most promising prospects for a first single-event detection. The framework developed here provides the theoretical foundation for statistically well-defined hypothesis testing between memory-free and memory-full radiation and quantitative assessments of detection prospects. As such, these results establish a principled pathway toward a future observational claim of gravitational memory.

</details>


### [236] [Causal spinfoam vertex for 4d Lorentzian quantum gravity](https://arxiv.org/abs/2601.23162)
*Eugenio Bianchi,Chaosong Chen,Mauricio Gamonal*

Main category: gr-qc

TL;DR: 提出新的4维洛伦兹量子引力因果自旋泡沫顶点，使用Toller T矩阵编码因果数据，展示其在EPRL顶点中的极点消除，大自旋极限下选择与自旋泡沫数据兼容的因果Regge几何


<details>
  <summary>Details</summary>
Motivation: 为4维洛伦兹量子引力建立包含因果结构的自旋泡沫顶点，解决现有模型中因果数据处理的不足，提供更完整的量子引力框架

Method: 引入Toller T矩阵编码因果数据（T^{(+)}+T^{(-)}=D），提供费曼iε表示，分析EPRL顶点中Toller极点的消除，研究Barrett-Crane极限下的Livine-Oriti模型，对比自旋泡沫与Regge因果数据的差异

Result: 在大自旋极限下，仅选择与自旋泡沫因果数据兼容的洛伦兹Regge几何，得到单指数exp(+i S_Regge/ℏ)，展现新的因果刚性形式

Conclusion: 成功构建了包含因果结构的自旋泡沫顶点，阐明了因果数据在量子引力中的作用机制，为大自旋极限下的半经典行为提供了新理解

Abstract: We introduce a new causal spinfoam vertex for $4$d Lorentzian quantum gravity. The causal data are encoded in Toller $T$-matrices, which add to Wigner $D$-matrices $T^{(+)}+T^{(-)}=D$, and for which we provide a Feynman $\mathrm{i}\varepsilon$ representation. We discuss how the Toller poles cancel in the EPRL vertex, how the Livine-Oriti model is obtained in the Barrett-Crane limit, and how spinfoam causal data are distinct from Regge causal data. In the large-spin limit, we show that only Lorentzian Regge geometries with causal data compatible with the spinfoam data are selected, resulting in a single exponential $\exp(+\mathrm{i}\, S_{\mathrm{Regge}}/\hbar)$ and a new form of causal rigidity.

</details>


### [237] [Detectability of Gravitational-Wave Memory with LISA: A Bayesian Approach](https://arxiv.org/abs/2601.23230)
*Adrien Cogez,Silvia Gasparotto,Jann Zosso,Henri Inchauspé,Chantal Pitte,Lorena Magaña Zertuche,Antoine Petiteau,Marc Besancon*

Main category: gr-qc

TL;DR: LISA有望探测引力波位移记忆效应，该效应是引力辐射通过后时空的永久形变，可用于检验广义相对论和替代理论


<details>
  <summary>Details</summary>
Motivation: 引力波天文学为探索宇宙开辟了新途径，LISA等未来观测站有望探测到广义相对论预测的先前无法探测的基本物理效应，特别是位移记忆效应——引力辐射通过后时空的永久形变

Method: 使用最先进的LISA仪器模拟，进行贝叶斯分析评估可探测性，对单个超大质量黑洞双星合并事件进行参数估计，探索位移记忆效应的影响并重建其振幅

Result: 建立了LISA探测位移记忆效应的一般条件，讨论了重建精度，为检验广义相对论和替代理论开辟了道路，并将分析应用于黑洞双星种群模型以估计LISA计划寿命内的观测率

Conclusion: LISA有能力观测和表征引力波位移记忆效应，这为检验广义相对论和替代理论提供了新途径，通过黑洞双星种群模型分析可预测在LISA寿命内的观测率

Abstract: Gravitational wave (GW) astronomy opens a new venue to explore the universe. Future observatories such as LISA, the Laser Interferometer Space Antenna, are expected to observe previously undetectable fundamental physics effects in signals predicted by General Relativity (GR).One particularly interesting such signal is associated to the displacement memory effect, which corresponds to a permanent deformation of spacetime due to the passage of gravitational radiation. In this work, we explore the ability of LISA to observe and characterize this effect. In order to do this, we use state-of-the-art simulations of the LISA instrument, and we perform a Bayesian analysis to assess the detectability and establish general conditions to claim detection of the displacement memory effect from individual massive black hole binary (MBHB) merger events in LISA. We perform parameter estimation both to explore the impact of the displacement memory effect and to reconstruct its amplitude. We discuss the precision at which such a reconstruction can be obtained thus opening the way to tests of GR and alternative theories. To provide astrophysical context, we apply our analysis to black hole binary populations models and estimate the rates at which the displacement memory effect could be observed within the LISA planned lifetime.

</details>


### [238] [Slow-roll approximations for Gauss-Bonnet inflation revisited](https://arxiv.org/abs/2601.23256)
*Bogdan A. Rudenko,Maria A. Skugoreva,Alexey V. Toporensky*

Main category: gr-qc

TL;DR: 研究高斯-博内特暴胀中慢滚近似的有效性，对比了耦合函数为增长函数与衰减函数的情况，发现新慢滚近似在增长函数情况下并不提高精度，有时甚至比标准近似更差。


<details>
  <summary>Details</summary>
Motivation: 检验高斯-博内特暴胀中慢滚近似的有效性，特别是当耦合函数为增长函数时，与先前研究中衰减函数的情况进行对比，探究不同耦合函数形式对近似精度的影响。

Method: 采用对比分析方法，将增长耦合函数情况与先前研究中衰减耦合函数的情况进行比较，分析不同慢滚近似（包括新慢滚近似和标准近似）在不同耦合函数下的表现。

Result: 发现当耦合函数为增长函数时，新慢滚近似并不像衰减函数情况下那样显著提高精度，在某些情况下甚至比标准近似表现更差，这与先前研究结果形成鲜明对比。

Conclusion: 高斯-博内特暴胀中慢滚近似的有效性高度依赖于耦合函数的形式，增长函数与衰减函数会导致不同的近似精度表现，需要根据具体耦合函数选择适当的近似方法。

Abstract: In our paper we consider the validity of slow-roll approximations for Gauss-Bonnet inflation introduced in [1]. In contrast to the cited paper where the coupling function before the Gauss-Bonnet term have been chosen as a decaying function of the scalar field, here we consider growing coupling functions. We have found that while in [1] new slow-roll approximations work considerably better, now they do not increase the precision. Moreover, we identify some cases where more involved approximations work worse than the standard one. Corresponding explanations of such a situation are given.

</details>
