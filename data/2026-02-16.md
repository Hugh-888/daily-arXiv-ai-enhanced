<div id=toc></div>

# Table of Contents

- [quant-ph](#quant-ph) [Total: 27]
- [physics.comp-ph](#physics.comp-ph) [Total: 7]
- [gr-qc](#gr-qc) [Total: 9]
- [cs.LG](#cs.LG) [Total: 97]


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [1] [Resource-Adaptive Teleportation Under Imperfect Entanglement: A Code-Puncturing Framework](https://arxiv.org/abs/2602.12309)
*Mahmoud Saad Abouamer,Jaron Skovsted Gundersen,Søren Pilegaard Rasmussen,Petar Popovski*

Main category: quant-ph

TL;DR: 量子隐形传态中，通过结合纠缠纯化和可调码率的穿孔量子纠错码，提高在不同纠缠条件下的传输可靠性


<details>
  <summary>Details</summary>
Motivation: 量子隐形传态的可靠性受限于共享EPR对的保真度。虽然纠缠纯化和量子纠错(QEC)都能提高可靠性，但固定码率的QEC码无法在所有纯化机制或可靠性目标下都有效，因为纯化会改变不完美EPR隐形传态引入的错误结构

Method: 在纠缠纯化的基础上补充使用穿孔QEC码，提供一系列可以根据错误通道特性和可靠性目标进行调整的码变体。穿孔码通过从单个稳定子结构中生成不同码率的变体，实现灵活适配

Result: 数值结果显示，不同的穿孔码在不同的操作机制下达到最低的逻辑错误概率，选择适当的穿孔码相比固定码率的编码隐形传态能降低逻辑错误。这降低了对初始EPR保真度或纯化的要求

Conclusion: 穿孔码技术能够在重用单个稳定子结构的同时，适应变化的纠缠条件和可靠性要求，无需硬件层面的码切换，从而更有效地提高量子隐形传态的可靠性

Abstract: Quantum teleportation is a foundational protocol for sending quantum information through entanglement distribution and classical communication. Assuming ideal classical communication, the reliability of quantum teleportation is limited by the fidelity of the shared EPR pairs. This reliability can be improved through two mechanisms: entanglement purification and quantum error correction (QEC). Using both techniques in concert requires flexible QEC rates, since purification alters the structure of errors induced by imperfect-EPR teleportation, and fixed-rate codes cannot be uniformly effective across purification regimes or reliability targets. In this work, we supplement purification with punctured QEC codes, providing a family of code variants that can be adapted to error-channel characteristics and reliability targets. Punctured codes improve teleportation reliability across a broader range of purification regimes, enabling target reliability to be met without hardware-level code switching. This is corroborated by numerical results, showing that different punctured codes achieve the lowest logical error probability in different operating regimes, and that selecting among them reduces logical error relative to fixed-rate encoded teleportation. This reduction relaxes the requirement on the initial EPR fidelity or purification needed to achieve a target reliability. Overall, puncturing enables adaptation to varying entanglement conditions and reliability requirements while reusing a single stabilizer structure.

</details>


### [2] [Reconstruction of finite Quasi-Probability and Probability from Principles: The Role of Syntactic Locality](https://arxiv.org/abs/2602.12334)
*Jacopo Surace*

Main category: quant-ph

TL;DR: 提出一个基于结构一致性要求的准概率理论框架，将准概率从计算工具提升为通用赋值的唯一可加表示，并建立条件概率和贝叶斯定理的广义理论。


<details>
  <summary>Details</summary>
Motivation: 准概率在物理学多个领域出现，但其概念基础不清晰：通常仅被视为计算工具，条件概率和贝叶斯定理等操作变得模糊。需要建立原则性框架来理解准概率的本质。

Method: 从结构一致性要求出发，通过句法局部性概念（每个宇宙可嵌入更大的环境宇宙），证明表示定理：每个可容许赋值可表示为互斥陈述上的有限可加测度（预概率）。通过正则化自由度固定，预概率简化为准概率。

Result: 建立了准概率的数学框架，将准概率提升为通用赋值的唯一可加表示；经典有限概率作为准概率在相对化下稳定的子类；建立了条件概率的相干理论和广义贝叶斯定理。

Conclusion: 该框架为准概率提供了概念基础，将其从计算工具转变为结构决定的数学对象，并建立了完整的条件概率理论，为物理应用提供了理论基础。

Abstract: Quasi-probabilities appear across diverse areas of physics, but their conceptual foundations remain unclear: they are often treated merely as computational tools, and operations like conditioning and Bayes' theorem become ambiguous. We address both issues by developing a principled framework that derives quasi-probabilities and their conditional calculus from structural consistency requirements on how statements are valued across different universes of discourse, understood as finite Boolean algebras of statements.We begin with a universal valuation that assigns definite (possibly complex) values to all statements. The central concept is Syntactic Locality: every universe can be embedded within a larger ambient one, and the universal valuation must behave coherently under such embeddings and restrictions. From a set of structural principles, we prove a representation theorem showing that every admissible valuation can be re-expressed as a finitely additive measure on mutually exclusive statements, mirroring the usual probability sum rule. We call such additive representatives pre-probabilities. This representation is unique up to an additive regraduation freedom. When this freedom can be fixed canonically, pre-probabilities reduce to finite quasi-probabilities, thereby elevating quasi-probability theory from a computational device to a uniquely determined additive representation of universal valuations. Classical finite probabilities arise as the subclass of quasi-probabilities stable under relativisation, i.e., closed under restriction to sub-universes. Finally, the same framework enables us to define a coherent theory of conditionals, yielding a well-defined generalized Bayes' theorem applicable to both pre-probabilities and quasi-probabilities. We conclude by discussing additional regularity conditions, including the role of rational versus irrational probabilities in this setting.

</details>


### [3] [Accelerating Feedback-based Algorithms for Quantum Optimization Using Gradient Descent](https://arxiv.org/abs/2602.12387)
*Masih Mozakka,Mohsen Heidari*

Main category: quant-ph

TL;DR: 提出一种结合梯度估计的混合方法，加速量子Lyapunov控制的收敛，同时保持其低训练开销和稳定性保证


<details>
  <summary>Details</summary>
Motivation: 基于反馈的方法如量子Lyapunov控制(QLC)虽然能减少QAOA的训练开销并缓解贫瘠高原问题，但可能需要较长的控制序列，导致收敛速度较慢

Method: 提出混合方法，结合每层梯度估计来加速QLC收敛，利用层间梯度信息选择接近最优的控制参数，同时保持低训练开销和稳定性保证

Result: 通过大量数值实验验证，该方法能显著加速收敛并提高鲁棒性，适用于多种问题实例和优化设置

Conclusion: 提出的混合方法成功解决了QLC收敛速度慢的问题，在保持其优势的同时实现了更快的收敛和更好的性能

Abstract: Feedback-based methods have gained significant attention as an alternative training paradigm for the Quantum Approximate Optimization Algorithm (QAOA) in solving combinatorial optimization problems such as MAX-CUT. In particular, Quantum Lyapunov Control (QLC) employs feedback-driven control laws that guarantee monotonic non-decreasing objective values, can substantially reduce the training overhead of QAOA, and mitigate barren plateaus. However, these methods might require long control sequences, leading to sub-optimal convergence rates. In this work, we propose a hybrid method that incorporates per-layer gradient estimation to accelerate the convergence of QLC while preserving its low training overhead and stability guarantees. By leveraging layer-wise gradient information, the proposed approach selects near-optimal control parameters, resulting in significantly faster convergence and improved robustness. We validate the effectiveness of the method through extensive numerical experiments across a range of problem instances and optimization settings.

</details>


### [4] [Temporal Framework for Causality-Preserving Scheduling of Measurements in Quantum Networks](https://arxiv.org/abs/2602.12459)
*Jakob Kaltoft Søndergaard,René Bødker Christensen,Petar Popovski*

Main category: quant-ph

TL;DR: 提出时间分割架构解决量子网络中测量因果顺序模糊问题，通过预分配测量时隙确保因果解释唯一性


<details>
  <summary>Details</summary>
Motivation: 分布式量子协议依赖经典前馈信息处理测量结果，但异构硬件和不确定的本地时序使得仅从到达时间推断测量因果顺序变得模糊。即使在仅进行泡利测量的简单线型网络中，终端节点也无法区分缺失结果是因测量缓慢还是经典传播延迟所致。

Method: 提出量子网络的时间分割架构，节点在预分配的时隙中执行测量，确保结果的唯一因果解释。形式化这一时间框架，推导保持测量因果性所需的前馈和邻接约束。针对简单网络拓扑，提出生成最优测量调度算法的算法。

Result: 时间分割模型为量子网络提供了一个实用的协调层，将经典网络时序与量子测量处理桥接起来，实现了可靠且可扩展的基于测量的量子网络。

Conclusion: 所提出的时间分割架构解决了量子网络中测量因果顺序的模糊性问题，通过预分配测量时隙确保因果解释的唯一性，为可靠和可扩展的测量型量子网络提供了实用的协调框架。

Abstract: Distributed quantum protocols rely on classical feedforward information to process measurement outcomes, but heterogeneous hardware and uncertain local timing can make the causal order of measurements ambiguous when inferred solely from arrival times. Even in simple line networks with only Pauli measurements, end nodes cannot distinguish whether a missing outcome is caused by slow measurement or by delayed classical propagation. To resolve this ambiguity, we propose a time-division architecture for quantum networks in which nodes perform measurements in pre-assigned slots, ensuring a unique causal interpretation of outcomes. We formalize this temporal framework and derive the feedforward and adjacency constraints required to preserve measurement causality. For simple network topologies, we present an algorithm that yields optimal measurement schedules. Overall, the proposed time-division model provides a practical coordination layer that bridges the classical network timing with quantum measurement processing, enabling reliable and scalable measurement-based quantum networking.

</details>


### [5] [Challenge-Response Quantum Reinforcement Learning with Application to Quantum-Assisted Authentication](https://arxiv.org/abs/2602.12464)
*Jawaher Kaldari,Saif Al-Kuwari*

Main category: quant-ph

TL;DR: 该论文提出了一个量子强化学习环境，其中Alice将经典比特编码到量子电路中，Bob使用训练后的强化学习代理通过有限量子态副本推断隐藏比特，研究在资源约束下的测量策略和终止决策。


<details>
  <summary>Details</summary>
Motivation: 现有量子强化学习研究多将量子代理应用于经典环境，而量子强化学习的真正优势应在具有内在量子特性的环境中探索，其中代理的观测和交互来自量子过程。

Method: 提出一个挑战-响应任务的量子强化学习环境，包含三个代理：纯经典代理、轻量级混合代理和深度混合代理。代理在资源约束下选择测量策略并决定何时终止交互。

Result: 轻量级混合代理仅需两个量子态副本即可实现可靠推断，在高度资源受限情况下优于经典基线和深度混合代理。实验分析了推理准确性与量子资源消耗之间的权衡。

Conclusion: 该量子强化学习环境展示了在资源约束下量子增强推断的可行性，轻量级混合代理在噪声环境下表现出鲁棒性，对量子辅助认证等安全应用具有相关性。

Abstract: Quantum reinforcement learning (QRL) has emerged as a promising research direction that integrates quantum information processing into reinforcement learning frameworks. While many existing QRL studies apply quantum agents to classical environments, it has been realized that the potential advantages of QRL are most naturally explored in environments that exhibit intrinsically quantum characteristics, where the agent's observations and interactions arise from quantum processes. In this work, we propose a quantum reinforcement learning environment formulated as a challenge-response task with hidden information. In the proposed environment, Alice encodes a classical bit into the parameters of a quantum circuit, while Bob, with a trained reinforcement learning agent, interacts with a limited number of quantum state copies to infer the hidden bit. The agent must select measurement strategies and decide when to terminate the interaction under explicit resource constraints. To study the solvability of the proposed environment, we consider three agents: a purely classical agent, a lightweight hybrid agent and a deep hybrid agent. Through experiments, we analyze the trade-off between inference accuracy and quantum resource consumption under varying interaction penalties. Our results show that the lightweight hybrid agent achieves reliable inference using as few as two quantum state copies, outperforming both the classical baseline and the deep hybrid agent in highly resource-constrained regimes. We further evaluate robustness under realistic quantum noise models and discuss the relevance of the proposed environment for security-oriented applications, including quantum-assisted authentication.

</details>


### [6] [Probabilistic Design of Parametrized Quantum Circuits through Local Gate Modifications](https://arxiv.org/abs/2602.12465)
*Grier M. Jones,Aviraj Newatia,Alexander Lao,Aditya K. Rao,Viki Kumar Prasad,Hans-Arno Jacobsen*

Main category: quant-ph

TL;DR: 提出一种受进化启发的启发式量子架构搜索算法（局部量子架构搜索），用于自动化发现任务特定的参数化量子电路，在合成函数拟合和量子化学回归任务中表现良好。


<details>
  <summary>Details</summary>
Motivation: 参数化量子电路的性能通常高度依赖于具体任务，手动设计电路具有挑战性。需要自动化方法来发现任务特定的量子电路架构。

Method: 提出局部量子架构搜索算法，这是一种受进化启发的启发式方法，通过对现有电路应用固定门级操作的局部概率搜索来优化参数化量子电路架构。

Result: 在两个合成函数拟合回归任务和两个量子化学回归数据集（包括BSE49键分离能数据集和基于数据驱动耦合簇方法生成的水构象数据集）上评估算法。通过状态向量模拟，算法能够识别具有良好性能指标的竞争性电路架构。

Conclusion: 局部量子架构搜索算法适用于发现具有竞争力的量子电路架构，分析了发现电路的特性，并在最先进的量子硬件上部署了最佳性能模型。

Abstract: Within quantum machine learning, parametrized quantum circuits provide flexible quantum models, but their performance is often highly task-dependent, making manual circuit design challenging. Alternatively, quantum architecture search algorithms have been proposed to automate the discovery of task-specific parametrized quantum circuits using systematic frameworks. In this work, we propose an evolution-inspired heuristic quantum architecture search algorithm, which we refer to as the local quantum architecture search. The goal of the local quantum architecture search algorithm is to optimize parametrized quantum circuit architectures through a local, probabilistic search over a fixed set of gate-level actions applied to existing circuits. We evaluate the local quantum architecture search algorithm on two synthetic function-fitting regression tasks and two quantum chemistry regression datasets, including the BSE49 dataset of bond separation energies for first- and second-row elements and a dataset of water conformers generated using the data-driven coupled-cluster approach. Using state-vector simulation, our results highlight the applicability of local quantum architecture search algorithm for identifying competitive circuit architectures with desirable performance metrics. Lastly, we analyze the properties of the discovered circuits and demonstrate the deployment of the best-performing model on state-of-the-art quantum hardware.

</details>


### [7] [Dynamic Programming Principle and Stabilization for Mean-Field Quantum Filtering Systems](https://arxiv.org/abs/2602.12472)
*Sofiane Chalal,Nina H. Amini,Hamed Amini,Mathieu Laurière*

Main category: quant-ph

TL;DR: 在量子滤波框架下，通过将状态空间嵌入希尔伯特-施密特空间，建立了无限维动态规划原理，并研究了连续监测的伊辛耦合量子比特的稳定化问题，在平均场极限下展示了量子态约简和向指定本征态的指数收敛。


<details>
  <summary>Details</summary>
Motivation: 研究量子系统的反馈控制和稳定化问题，特别是在连续监测和量子滤波的背景下，探索如何通过动态规划方法在无限维空间中实现量子态的精确控制。

Method: 采用量子滤波框架，将状态空间嵌入希尔伯特-施密特空间以建立无限维动态规划原理，然后研究连续监测的伊辛耦合量子比特系统，在平均场极限下设计合适的反馈控制律。

Result: 成功建立了无限维动态规划原理，并在平均场极限下证明了量子态约简现象，展示了在适当反馈控制下系统向指定本征态的指数收敛性。

Conclusion: 该方法为量子系统的反馈控制提供了理论框架，特别是在连续监测和平均场极限下，能够实现量子态的稳定化和精确控制，对量子信息处理和量子控制理论有重要意义。

Abstract: Working within the quantum filtering framework, we establish a dynamic programming principle in an infinite-dimensional setting by embedding the state space into the Hilbert-Schmidt space. We then study a stabilization problem for continuously monitored Ising-coupled qubits and, in the mean-field limit, demonstrate quantum state reduction together with exponential convergence toward prescribed eigenstates under suitable feedback laws.

</details>


### [8] [Compressed Sensing Shadow Tomography](https://arxiv.org/abs/2602.12518)
*Joseph Barreto,Daniel Lidar*

Main category: quant-ph

TL;DR: CSST协议结合经典影子与压缩感知，通过稀疏采样时间点和随机测量，显著减少量子模拟中估计多个泡利可观测量随时间变化所需的测量次数。


<details>
  <summary>Details</summary>
Motivation: 量子模拟和设备表征中，估计多个局部可观测量随时间的变化需要大量测量次数，成为主要瓶颈。需要开发高效方法减少所需的总测量次数。

Method: 提出压缩感知影子层析（CSST）协议：1）使用经典影子技术从相同随机快照估计多个泡利可观测量；2）利用压缩感知减少时间维度，通过稀疏采样时间点（m≪N），在酉变换基（如傅里叶基）中进行ℓ₁恢复重建信号。

Result: 对于酉变换基中精确s-稀疏信号，仅需m=O(s log²s log N)个随机时间点即可高概率重建，总测量次数节省约Õ(N/s)。对于近似稀疏信号，重建误差分解为可压缩性（尾部）项和噪声项。

Conclusion: CSST协议能显著减少量子模拟中估计泡利信号矩阵所需的测量次数，数值实验支持泡利轨迹的强傅里叶可压缩性，实现了准确重建下的显著测量减少。

Abstract: Estimating many local expectation values over time is a central measurement bottleneck in quantum simulation and device characterization. We study the task of reconstructing the Pauli-signal matrix $S_{ij}=\text{Tr}(O_i ρ(t_j))$ for a collection of $M$ low-weight Pauli observables $\{O_i\}_{i=1}^M$ over $N$ timesteps $\{t_j\}_{j=1}^N$, while minimizing the total number of device shots. We propose a Compressed Sensing Shadow Tomography (CSST) protocol that combines two complementary reductions. First, local classical shadows reduce the observable dimension by enabling many Pauli expectation values to be estimated from the same randomized snapshots at a fixed time. Second, compressed sensing reduces the time dimension by exploiting the fact that many expectation-value traces are spectrally sparse or compressible in a unitary (e.g., Fourier) transform basis. Operationally, CSST samples $m\ll N$ timesteps uniformly at random, collects shadows only at those times, and then reconstructs each length-$N$ signal via standard $\ell_1$-based recovery in the unitary transform domain. We provide end-to-end guarantees that explicitly combine shadow estimation error with compressed sensing recovery bounds. For exactly $s$-sparse signals in a unitary transform basis, we show that $m=O \left(s\log^2 s \log N\right)$ random timesteps suffice (with high probability), leading to total-shot savings scaling as $\widetildeΘ(N/s)$ (i.e., up to polylogarithmic factors) relative to collecting shadows at all $N$ timesteps. For approximately sparse signals, the reconstruction error decomposes into a compressibility (tail) term plus a noise term. We present numerical experiments on noisy many-qubit dynamics that support strong Fourier compressibility of Pauli traces and demonstrate substantial shot reductions with accurate reconstruction.

</details>


### [9] [Predicting properties of quantum thermal states from a single trajectory](https://arxiv.org/abs/2602.12539)
*Jiaqing Jiang,Jiaqi Leng,Lin Lin*

Main category: quant-ph

TL;DR: 提出一种基于单次吉布斯采样轨迹的热期望值估计方法，通过交错相干测量减少采样成本，相比传统方法显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 量子热态制备已取得进展，但通过采样估计可观测量期望值仍然昂贵。传统方法需要在测量间等待完整的混合时间以确保样本独立性，这导致采样成本过高。

Method: 使用单次吉布斯采样轨迹，在单个预热期后，交错进行满足目标吉布斯态细致平衡条件的相干测量。对于能量估计，采用对数开销的高斯滤波量子相位估计；对于一般可观测量，引入加权算子傅里叶变换技术来减轻测量引起的扰动。

Result: 该方法利用自相关时间通常显著短于混合时间的特性，大幅降低了采样成本。对于与哈密顿量对易的可观测量，测量实现仅需对数开销。

Conclusion: 通过单次吉布斯采样轨迹和交错相干测量的方法，能够显著降低热期望值估计的采样成本，为量子热态计算提供了更高效的采样策略。

Abstract: Estimating thermal expectation values of observables is a fundamental task in quantum physics, quantum chemistry, and materials science. While recent quantum algorithms have enabled efficient quantum preparation of thermal states, observable estimation via sampling remains costly: a straightforward implementation separates successive measurements by a full mixing time in order to ensure samples are approximately independent. In this work, we show that the sampling cost can be substantially reduced by using a single Gibbs-sampling trajectory. After a single burn-in period, we interleave coherent measurements that satisfy detailed balance with respect to the target Gibbs state. The efficiency of this approach rests on the fact that, in many settings, the autocorrelation time can be significantly shorter than the mixing time. For energy estimation (and more generally for observables commuting with the Hamiltonian), we implement the required measurements using Gaussian-filtered quantum phase estimation with only logarithmic overhead. We also introduce a weighted operator Fourier transform technique to mitigate measurement-induced disturbance for general observables.

</details>


### [10] [Sperner state and multipartite entanglement signals](https://arxiv.org/abs/2602.12664)
*Xin-Xiang Ju,Ya-Wen Sun,Yang Zhao*

Main category: quant-ph

TL;DR: 提出基于Sperner态和反链超图的系统化多体纠缠结构分类方案，建立多纠缠度量空间(MEMS)，通过线性组合的消失/非消失区分不同纠缠结构


<details>
  <summary>Details</summary>
Motivation: 为多体纠缠结构建立系统化的分类框架，解决现有分类方法缺乏统一理论基础的问题，提供量化纠缠模式的新方法

Method: 定义Sperner态类（表观多体纠缠可分解为较少体纠缠），每个类对应一个反链超图；引入多纠缠度量空间(MEMS)，每个Sperner类对应由特定线性组合消失定义的线性子空间

Result: 建立了超图纠缠结构与特定线性组合集合之间的双向对应关系，能够通过线性组合的消失/非消失来区分和量化纠缠模式

Conclusion: 该框架为所有多体纠缠提供了统一的分类基础，将纠缠结构分类与可测量的线性组合联系起来，实现了纠缠模式的系统化描述和量化

Abstract: We establish a systematic classification scheme for multipartite entanglement structures. We define Sperner states -- a broad class of states where apparent multipartite entanglement decomposes into fewer-partite entanglement among subsystems of each party. Each class of Sperner states is associated with one antichain hypergraph and each hypergraph encodes the maximal entanglement structure permissible under its constraints. We introduce a Multi-entanglement Measure Space (MEMS) where each Sperner class corresponds to a linear subspace defined by the vanishing of specific linear combinations of bipartite and multipartite measures. The nonvanishing of such combinations signals multipartite entanglement beyond the associated hypergraph, thereby distinguishing entanglement structures. We build a two way connection between each hypergraph entanglement structure and a distinct set of combinations, thereby quantifying the entanglement pattern and providing a unified basis for classifying all multipartite entanglement.

</details>


### [11] [Floquet implementation of a 3d fermionic toric code with full logical code space](https://arxiv.org/abs/2602.12685)
*Yoshito Watanabe,Bianca Bannenberg,Simon Trebst*

Main category: quant-ph

TL;DR: 提出一种三维Floquet量子纠错码，通过动态测量实现三维费米子环面码，同时在整个测量序列中保持三个逻辑量子比特。


<details>
  <summary>Details</summary>
Motivation: 虽然高维稳定子码具有更强的内在鲁棒性，但高维Floquet码的研究还很有限。需要开发能保持逻辑信息的三维Floquet码，避免在测量过程中丢失逻辑量子比特。

Method: 设计三维三配位晶格几何结构，该结构具有特殊性质：删除任意一种边颜色后，剩余的双色子图会分解为短闭合环而非同调非平凡链。通过测量序列提取缺失的纠错综合征而不干扰逻辑子空间。

Result: 成功构建了三维Floquet码，其瞬时稳定子群实现三维费米子环面码，并在整个测量序列中保持所有三个逻辑量子比特。该晶格几何还定义了三维监控Kitaev模型家族。

Conclusion: 三维Floquet码设计的关键在于合适的晶格几何结构，避免逻辑信息崩溃。该工作不仅推进了量子纠错码设计，还为研究测量诱导的纠缠相和量子临界点提供了新平台。

Abstract: Floquet quantum error-correcting codes provide an operationally economical route to fault tolerance by dynamically generating stabilizer structures using only two-body Pauli measurements. But while it is well established that stabilizer codes in higher spatial dimensions gain additional levels of intrinsic robustness, higher-dimensional Floquet codes have hitherto been explored only in limited scope. Here we introduce a 3d generalization of a Floquet code whose instantaneous stabilizer group realizes a 3d fermionic toric code, while crucially preserving all three logical qubits throughout the entire measurement sequence. One central ingredient is the identification of a 3d lattice geometry that generalizes the features of the Kekulé lattice underlying the 2d Hastings-Haah code - specifically, a structure where deleting any one edge color yields a two-color subgraph that decomposes into short, closed loops rather than homologically nontrivial chains. This loop property avoids the collapse of logical information that plagues naive sequential two-color measurement schedules on many 3d lattices. Although, for our lattice geometry, a simple 3-round cycle that sequentially measures the three types of parity checks does not expose the full error syndrome set, we show that one can append a measurement sequence to extract the missing syndromes without disturbing the logical subspace. Beyond code design, 3d tricoordinated lattice geometries define a family of 3d monitored Kitaev models, in which random measurements of the non-commuting parity checks give rise to dynamically created entangled phases with nontrivial topology. In discussing the general structure of their underlying phase diagrams and, in particular, the existence of certain quantum critical points, we again make a connection to the general preservation of logical information in time-ordered Floquet protocols.

</details>


### [12] [Reverse Delegated Training and Private Inference via Perfectly-Secure Quantum Homomorphic Encryption](https://arxiv.org/abs/2602.12712)
*Sergio A. Ortega,Miguel A. Martin-Delgado*

Main category: quant-ph

TL;DR: 首次实现完美安全的量子同态加密应用于量子神经网络，展示了量子卷积神经网络在云环境中的两种应用场景：反向委托训练和隐私推理。


<details>
  <summary>Details</summary>
Motivation: 在云环境中进行量子机器学习时，需要保护敏感数据同时支持远程计算。量子同态加密能够实现数据加密状态下进行计算，解决隐私保护与计算需求之间的矛盾。

Method: 采用高效的Clifford+T分解实现量子卷积神经网络，设计了两种应用场景：1) 反向委托训练 - 多个数据提供者的加密数据通过联邦聚合训练用户的网络；2) 隐私推理 - 用户使用远程量子网络处理加密数据。同时分析了服务器电路隐私，通过Pauli门隐藏实现概率性模型保护。

Result: 首次实现了完美安全的量子同态加密方案在量子神经网络中的实际应用，展示了量子卷积神经网络在两种互补场景下的可行性。服务器电路隐私分析表明可以通过Pauli门隐藏实现概率性模型保护。

Conclusion: 完美安全的量子同态加密为多方量子机器学习提供了一个实用的框架，能够在保护数据隐私的同时实现远程量子计算，为量子云服务的安全应用奠定了基础。

Abstract: Quantum machine learning in cloud environments requires protecting sensitive data while enabling remote computation. Here we demonstrate the first realistic implementations of a perfectly-secure quantum homomorphic encryption (QHE) scheme applied to quantum neural networks (QNN). Using efficient Clifford+$T$ decomposition, we implement quantum convolutional neural networks for two complementary scenarios: (i) reverse delegated training, where encrypted data from multiple providers trains a user's network via federated aggregation; (ii) private inference, where users process encrypted data with remote quantum networks. Moreover, analysis of server circuit privacy reveals probabilistic model protection through Pauli gate concealment. These results establish perfectly-secure QHE as a practical framework for multi-party quantum machine learning.

</details>


### [13] [Preparing Quantum Backflow States by Large Momentum Transfer](https://arxiv.org/abs/2602.12767)
*Yuchong Chen,Yijun Tang*

Main category: quant-ph

TL;DR: 该论文研究了通过大动量转移技术在超冷原子BEC中制备可调谐量子回流态的新方法


<details>
  <summary>Details</summary>
Motivation: 量子回流态具有负概率密度通量但完全正动量谱的特性。先前研究使用单激光脉冲在BEC中制备量子回流态，但尚未研究通过大动量转移技术实现灵活制备的方法。

Method: 结合原子干涉仪理论和非相互作用BEC波函数，求解BEC波包在原子干涉仪序列下的演化。采用大动量转移技术实现量子回流态的可调谐制备。

Result: 模拟结果显示，该方案能实现高度可调谐的回流通量和临界密度，并且可以超越现有数值范围。提供了比单脉冲方法更灵活的量子回流态制备能力。

Conclusion: 通过大动量转移技术成功实现了可调谐量子回流态的制备，为量子回流现象的研究提供了更灵活的实验方案，扩展了量子态制备的能力。

Abstract: A quantum backflow state refers to a quantum state exhibiting negative probability density flux albeit a completely positive momentum spectrum. Extending earlier work that uses single laser pulse to prepare quantum backflow state in an ultracold atomic BEC [1], we theoretical investigated flexible quantum backflow state preparation via large momentum transfer technique, which to our knowledge, has not been studied before. By combining atom interferometry theory and non-interacting BEC wave function, we solve for the evolution of a BEC wavepacket under atom interferometry sequence. Simulation results show a highly tunable backflow flux and critical density under our scheme, and can be manipulated to go beyond existing numbers.

</details>


### [14] [Design and Operation of Wafer-Scale Packages Containing >500 Superconducting Qubits](https://arxiv.org/abs/2602.12773)
*Oscar W. Kennedy,Waqas Ahmad,Robert Armstrong,Amir Awawdeh,Anirban Bose,Kevin G. Crawford,Sergey Danilin,William D. David,Hamid El Maazouz,Darren J. Hayton,George B. Long,Alexey Lyapin,Scott A. Manifold,Kowsar Shahbazi,Ryan Wesley,Evan Wong,Connor D. Shelly*

Main category: quant-ph

TL;DR: 该论文提出了一种支持500+量子比特的晶圆级封装架构，在保持高相干时间的同时实现了大规模集成，为容错量子计算提供了关键硬件基础。


<details>
  <summary>Details</summary>
Motivation: 实现容错量子计算机需要支持大规模高相干量子比特阵列的封装技术，同时需要高通量计量来优化制造工艺。现有封装方案难以同时满足大规模集成和高性能要求。

Method: 设计了晶圆级封装架构，通过仿真优化抑制寄生RF模式、减少材料损耗，并管理热收缩差异以确保在毫开尔文温度下的稳定运行。系统级热负荷计算验证了商用稀释制冷机的适用性。

Result: 封装支持500+量子比特，测量显示中位T1、T2e约100μs（约100个量子比特），读取保真度97.5%（54个量子比特），量子比特温度36 mK。性能验证显示大规模集成不牺牲器件性能。

Conclusion: 该封装架构成功实现了大规模量子比特集成且保持高性能，为容错量子计算提供了可行方案。同时可作为高通量测试平台，识别相干分布中的异常值，指导高质量量子比特和量子处理器的制造。

Abstract: Packages capable of supporting large arrays of high-coherence superconducting qubits are vital for the realisation of fault-tolerant quantum computers and the necessary high-throughput metrology required to optimise fabrication and manufacturing processes. We present a wafer-scale packaging architecture supporting over 500 qubits on a single 3-inch die. The package is engineered to suppress parasitic RF modes, and to mitigate material loss through simulation-informed design while managing differential thermal contraction to ensure robust operation at millikelvin temperatures. System-level heat-load calculations from a large wiring payload show this package may be operated in commercial dilution refrigerators. Measurements of the qubits loaded into the package show median $T_1$, $T_{2e} \sim 100~μ$s ($\sim$100 qubits) alongside readout with median fidelity of 97.5% (54 qubits) and a median qubit temperature of 36 mK (54 qubits). These results validate the performance of these packages and demonstrate that large-scale integration can be achieved without compromising device performance. Finally, we highlight the utility of these packages as a tool for high throughput feedback on qubit figures of merit over large sample sizes, allowing identification of performance outliers in the tails of the coherence distribution, a critical capability for informing fabrication and manufacture of high-quality quantum qubits and quantum processors.

</details>


### [15] [Equilibrium thermometry in the multilevel quantum Rabi model](https://arxiv.org/abs/2602.12787)
*Tabitha Doicin,Luis A. Correa,Jonas Glatthard,Andrew D. Armour,Gerardo Adesso*

Main category: quant-ph

TL;DR: 多能级量子Rabi模型在腔QED系统中作为热敏计的研究，通过暗态和亮态饱和机制实现高灵敏度和宽温度范围响应


<details>
  <summary>Details</summary>
Motivation: 研究多能级量子Rabi模型的热测量性能，探索能级简并结构如何影响热敏计的灵敏度和温度响应范围，为设计高性能量子热敏计提供理论指导

Method: 将标准量子Rabi模型推广到绝热区域，得到热量子Fisher信息的近似闭式表达式，分析暗态饱和和亮态饱和两种极限情况

Result: 暗态饱和产生鲁棒的热灵敏度峰值，亮态饱和产生宽带热响应，且随着能级数增加，对随机光-物质耦合的稳定性增强

Conclusion: 腔QED模型丰富的谱结构使其成为在宽温度范围内多功能且灵敏的平衡热敏计

Abstract: The temperature sensitivity of a probe in equilibrium can be gauged by its thermal quantum Fisher information (QFI). It is known that probes exhibiting degeneracy in their energy-level structure can achieve larger sensitivities, while probes with a more uniform spectrum may remain sensitive over a broader temperature range. Here, we study the thermometric performance of a multilevel quantum Rabi model in which two well-separated atomic manifolds of near-degenerate levels couple to a single cavity mode. We generalise the standard quantum Rabi treatment in the adiabatic regime to find an approximate closed-form expression for the thermal QFI. We then characterise two complementary limits. On the one hand, a large dark-state manifold (dark-manifold saturation) produces a robust peak in thermal sensitivity due to bright--dark population transfer. Such increase in sensitivity is further maximised at an intermediate light--matter coupling strength. Maximising instead the number of bright states (bright-manifold saturation) generates a broadband thermal response that becomes increasingly stable under random light--matter couplings as the number of levels is increased. The rich spectral structure of our cavity-QED model thus makes it a versatile and sensitive equilibrium thermometer over a broad range of temperatures.

</details>


### [16] [Towards Trapped-Ion Thermometry Using Cavity-Based EIT](https://arxiv.org/abs/2602.12823)
*Abhijit Kundu,Vijay Bhatt,Arijit Sharma*

Main category: quant-ph

TL;DR: 提出一种基于腔增强电磁诱导透明技术测量离子温度的方法，适用于强耦合腔QED系统，可高效提取亚多普勒冷却后离子的声子占据数。


<details>
  <summary>Details</summary>
Motivation: 在腔QED系统中，需要一种简化的方法来测量亚多普勒冷却后离子的温度，特别是接近运动基态时的声子占据数。传统方法可能复杂，而腔EIT技术提供了一种更直接的测量途径。

Method: 通过监测腔探针透射率，在控制光束建立腔EIT后扫描探针激光频率。该方法依赖于热态对EIT线宽的影响，将腔EIT透射作为测温工具来推导离子温度和运动态。

Result: 理论上建立了模型，展示了囚禁离子热态对测量的EIT线宽的影响。该方法能够作为测温工具推导离子温度，但仅适用于分辨边带区域，需要选择适当的三能级系统或强约束条件（约10MHz的塞库拉频率）。

Conclusion: 腔EIT技术提供了一种简化且有效的离子温度测量方法，特别适用于强耦合腔QED系统中的亚多普勒冷却离子，但应用范围受限于分辨边带区域的操作条件。

Abstract: We present a technique for measuring ion temperature using cavity-based electromagnetically induced transparency (EIT) applicable for cavity-qed systems in the strong coupling regime. This method enables efficient extraction of the ion's phonon occupation number following sub-Doppler cooling close to the motional ground state. The proposed method relies on monitoring the cavity probe transmission while scanning the probe laser frequency once cavity EIT is established using the control beam, significantly simplifying the measurement procedure. We theoretically establish a model that demonstrates influence of thermal state of the trapped ion vis a vis the EIT linewidth measured. We show how the cavity EIT transmission may be used as a thermometry tool to deduce the ion temperature as well as the motional state for an ion in the sub-Doppler cooling regime. The current method can only be used for operation in the resolved-sideband regime, where individual motional states can be selectively addressed for all relevant transitions either by selecting appropriate energy levels for the three-level system or by employing strong confinement with high secular frequencies ($\sim 10 MHz$).

</details>


### [17] [Optimized Compilation of Logical Clifford Circuits](https://arxiv.org/abs/2602.12831)
*Alexander Popov,Nico Meyer,Daniel D. Scherer,Guido Dietl*

Main category: quant-ph

TL;DR: 提出一种基于量子模拟原语的编译方法，将原语作为整体块进行编译，相比逐门编译能显著减少电路深度和错误率


<details>
  <summary>Details</summary>
Motivation: 容错量子计算需要高效的逻辑编译，但传统的逐门编译会产生深度电路，需要大量开销来确保容错性。需要寻找替代方案来减少编译开销。

Method: 研究[[n,n-2,2]]码族，将量子模拟原语作为单个块进行编译。基于小电路实例的详尽比较，提出一种方法将这些原语提升为尺寸不变、深度高效的编译策略。

Result: 恢复了中等Hadamard门数量电路的已知方法，并为稀疏和密集布局提供了改进实现。模拟显示编译后电路的错误率显著降低。

Conclusion: 该方法可作为窥孔编译器的核心组件，其灵活性和低手工制作负担使其易于扩展到其他电路结构和码族。

Abstract: Fault-tolerant quantum computing hinges on efficient logical compilation, in particular, translating high-level circuits into code-compatible implementations. Gate-by-gate compilation often yields deep circuits, requiring significant overhead to ensure fault-tolerance. As an alternative, we investigate the compilation of primitives from quantum simulation as single blocks. We focus our study on the [[n,n-2,2]] code family, which allows for the exhaustive comparison of potential compilation primitives on small circuit instances. Based upon that, we then introduce a methodology that lifts these primitives into size-invariant, depth-efficient compilation strategies. This recovers known methods for circuits with moderate Hadamard counts and yields improved realizations for sparse and dense placements. Simulations show significant error-rate reductions in the compiled circuits. We envision the approach as a core component of peephole-based compilers. Its flexibility and low hand-crafting burden make it readily extensible to other circuit structures and code families.

</details>


### [18] [Airline Fleet Assignment Problems with Binary and Integer Programming models: Classical vs Quantum Annealing](https://arxiv.org/abs/2602.12840)
*Kuntal Adak,Sakshi Kaushik,Rahul Rana*

Main category: quant-ph

TL;DR: 量子退火在航空业大规模优化问题中的应用潜力研究，展示了其处理特定规模问题的效率，同时承认当前局限性


<details>
  <summary>Details</summary>
Motivation: 探索量子退火技术在航空业大规模优化问题（特别是机队分配）中的应用潜力，比较先进计算技术的性能

Method: 采用量子退火方法处理航空业优化问题，并进行比较分析，评估不同问题规模下的性能表现

Result: 量子退火对某些问题规模表现出高效性，但存在当前技术限制；比较分析为优化技术提供了有价值的见解

Conclusion: 量子退火在航空业优化中具有潜力，为机队分配等问题的进一步优化技术发展铺平道路，但需要克服当前限制

Abstract: This research highlights the potential of quantum annealing in tackling large-scale optimization problems within the airline industry,demonstrating its efficiency for certain problem sizes while also acknowledging its current limitations. The comparative analysis provides valuable insights into the performance of advanced computational techniques, paving the way for further advancements in optimizing fleet assignments in the aviation sector.

</details>


### [19] [Quantum logic control and entanglement in hybrid atom-molecule arrays](https://arxiv.org/abs/2602.12909)
*Chi Zhang,Sara Murciano,Nathanan Tantivasadakarn,Ran Finkelstein*

Main category: quant-ph

TL;DR: 提出混合平台方案，利用原子-分子量子门和原子辅助测量，克服分子平台限制，实现快速大规模分子纠缠态制备


<details>
  <summary>Details</summary>
Motivation: 极性分子具有丰富内部结构，在基础物理、量子技术和受控化学中潜力巨大，但当前受限于缓慢不完美的状态检测和弱偶极相互作用，难以实现快速大规模纠缠生成

Method: 提出极性分子与中性原子的混合平台方案，利用原子-分子受控相位门（基于分子转动跃迁与原子里德堡跃迁的共振偶极-偶极交换），以及高保真度原子辅助测量

Result: 原子-分子受控相位门比任何直接分子-分子纠缠门快三个数量级，能够制备分子GHZ态用于量子增强精密测量、具有拓扑序的奇异分子qudit态，并实现测量改变临界性

Conclusion: 该方案适用于任何极性分子，扩展了量子逻辑控制范式，为大规模分子纠缠态铺平道路，展示了混合量子系统中各量子比特最优利用和测量方法在近期设备中的显著优势

Abstract: Polar molecules, with their rich internal structure, offer immense potential for fundamental physics, quantum technology, and controlled chemistry. However, their utilization is currently limited because of slow and imperfect state detection and weak dipolar interaction, limiting fast and large-scale entanglement generation. We propose and analyze a scheme for quantum logic control and measurement-based state preparation in a hybrid platform of polar molecules and neutral atoms. The method leverages fast, high-fidelity atom-molecule gates and high-fidelity atomic ancilla measurements to overcome the common challenges in molecule-only platforms, while preserving their diverse structural advantages. The proposed atom-molecule controlled-phase gate is based on resonant dipole-dipole exchange between a molecular rotational transition and an atomic Rydberg transition, rendering it three orders of magnitude faster than any direct molecule-molecule entangling gate. We further study several applications of our scheme including the preparation of molecular GHZ states for quantum enhanced precision measurements, the preparation of exotic molecular qudit states with topological order, and measurement-altered criticality. Our scheme is applicable to any polar molecule. It expands the paradigm of quantum logic control and paves the way to large-scale molecular entangled states. More generally, it highlights a concrete hybrid quantum system in which each qubit is utilized in an optimal way and where the measurement-based approach can yield a significant advantage in near-term devices.

</details>


### [20] [Quantum metrology with partially accessible chaotic sensors](https://arxiv.org/abs/2602.12914)
*Harshita Sharma,Sayan Choudhury,Jayendra N. Bandyopadhyay*

Main category: quant-ph

TL;DR: 量子混沌传感器利用混沌动力学，即使初始非纠缠态和部分测量可访问性也能实现海森堡极限的量子费舍尔信息标度，为现实约束下的量子增强传感提供了新途径。


<details>
  <summary>Details</summary>
Motivation: 传统量子计量协议需要高度纠缠的探针态和全局可访问测量，这在现实多体传感器中难以实现。需要寻找在部分测量可访问性约束下仍能实现量子增强传感的方法。

Method: 利用量子混沌传感器，研究混沌动力学如何使初始非纠缠态在部分测量可访问性下实现海森堡标度。在弱混沌区域，将自旋相干态置于混合经典相空间中规则岛边缘；在强混沌区域，研究初始态不敏感性。

Result: 量子混沌传感器中，即使只有约5%的量子比特可访问，也能实现量子费舍尔信息的海森堡时间标度。弱混沌区域中，规则岛边缘的自旋相干态是最优初始态；强混沌区域中，量子费舍尔信息对初始态选择不敏感。

Conclusion: 量子混沌是现实可访问性约束下实现量子增强传感的稳健资源，突破了传统协议对高度纠缠态和全局测量的严格要求。

Abstract: Most quantum metrology protocols harness highly entangled probe states and globally accessible measurements to surpass the standard quantum limit. However, it is challenging to satisfy these requirements in realistic many-body sensors. We demonstrate that both of these constraints can be overcome in quantum chaotic sensors. Crucially, we establish that even in the presence of partial measurement accessibility, chaotic dynamics enables initial unentangled states to exhibit Heisenberg scaling of the quantum Fisher information, $I_α$ with time. In the weakly chaotic regime, we identify spin-coherent states placed at the edge of the regular islands in the mixed classical phase space as optimal initial states for enhanced sensitivity. On the other hand, in the strongly chaotic regime, $I_α$ is insensitive to the choice of the initial state. Notably, quantum-enhanced sensitivity is achieved even when a very low fraction ($\sim 5\%$) of the qubits are accessible. These results establish quantum chaos as a robust resource for quantum-enhanced sensing under realistic accessibility constraints on accessibility.

</details>


### [21] [Effective classical potential for quantum statistical averages](https://arxiv.org/abs/2602.13006)
*Vijay Ganesh Sadhasivam,Stuart C. Althorpe,Venkat Kapil*

Main category: quant-ph

TL;DR: 提出一种有效势能方法，将量子热期望值转换为经典系综平均，基于Feynman-Hibbs方法但采用起点平均场处理而非路径质心处理


<details>
  <summary>Details</summary>
Motivation: 开发一种更稳健的方法来计算位置依赖可观测量在量子热力学中的期望值，避免传统方法中的数值不稳定问题

Method: 基于Feynman-Hibbs方法，但采用路径起点而非质心的平均场处理量子涨落，探索近似函数形式而非完全变分优化，得到闭式有效势能

Result: 方法在经典和谐振极限下精确；在一维四次势、Morse势和双阱势的精确位置分布基准测试中，对具有谐振支撑的势能表现出良好一致性

Conclusion: 提出的有效势能方法为计算量子热期望值提供了一种稳健且准确的闭式近似，特别适用于具有谐振支撑的势能系统

Abstract: We present an effective potential that allows quantum thermal expectation values of a position-dependent observable to be estimated as a classical ensemble average of the corresponding function. We follow the approach of Feynman and Hibbs, but perform the mean-field treatment of quantum fluctuations about the path starting point rather than the path centroid. Furthermore, rather than performing a full variational optimization of the potential, we explore approximate functional forms that yield a numerical robustness. The resulting closed-form potential is exact in the classical and harmonic limits; benchmarks against exact position distributions for one-dimensional quartic, Morse, and double-well potentials, show good agreement for potentials with harmonic support.

</details>


### [22] [Weighted graph states as a resource for quantum metrology](https://arxiv.org/abs/2602.13026)
*B. J. Alexander,Ş. K. Özdemir,M. S. Tame*

Main category: quant-ph

TL;DR: 加权图态作为量子计量学中更易实现的资源，能以较少的纠缠接近海森堡极限


<details>
  <summary>Details</summary>
Motivation: 当前量子计量学大多依赖高度纠缠的资源态，这些态在物理系统中难以生成和控制。需要寻找更易实现的替代资源

Method: 研究加权图态作为量子计量学资源，分析其量子费舍尔信息和优化估计方差，特别关注两个子类在任意N量子比特下的表现

Result: 加权图态在权重变化方面表现出鲁棒性，权重要求比标准图态更宽松，所需纠缠显著减少，但仍能超越经典极限并接近海森堡极限

Conclusion: 加权图态为量子增强计量学提供了使用弱纠缠态的新机会，降低了实际实现的难度

Abstract: Quantum metrology exploits quantum mechanical effects to increase the precision of measurements of physical quantities. A wide variety of applications are currently being developed for scientific and technological purposes, however, most research relies on the use of highly entangled resource states that are challenging to generate and control in a given physical system. Here, we study the use of weighted graph states as more accessible resources for quantum metrology, which yield a favorable precision beyond the classical limit, approaching the Heisenberg limit. We find a notable robustness to variation in weights and less challenging weight requirements compared to standard graph states, which require a maximal weight at all edges. Both of these aspects reduce the practical demands in a physical setup, with the latter implying significantly less entanglement is required to gain a quantum advantage in metrology. We study the quantum Fisher information and optimized estimator variance of two identified sub classes of weighted graph states for an arbitrary number of N qubits, providing analytical forms and investigating their scaling. Our work opens up opportunities for using weakly entangled states in quantum-enhanced metrology.

</details>


### [23] [A Quantum Reservoir Computing Approach to Quantum Stock Price Forecasting in Quantum-Invested Markets](https://arxiv.org/abs/2602.13094)
*Wendy Otieno,Alexandre Zagoskin,Alexander G. Balanov,Juan Totero Gongora,Sergey E. Savel'ev*

Main category: quant-ph

TL;DR: 提出基于最多6个量子比特的小规模量子储层计算框架，用于非线性金融时间序列预测，在量子板块股票交易量预测中达到超过86%的趋势分类准确率。


<details>
  <summary>Details</summary>
Motivation: 探索小规模量子系统在复杂金融时间序列预测中的应用潜力，利用量子储层计算处理非线性金融数据的时序相关性，为近期量子硬件在现实世界预测任务中的应用提供可能。

Method: 采用量子储层计算框架，使用最多6个相互作用的量子比特构建小规模量子系统，应用于20家量子板块上市公司的日交易量和分钟级交易量预测，寻找最优储层参数。

Result: 识别出最优储层参数，在股票趋势（上涨/下跌）分类任务中达到超过86%的准确率，模型具有平台无关性，可在超导电路和囚禁离子等多种物理实现中实现。

Conclusion: 小规模量子储层计算在金融数据复杂时序相关性建模方面展现出强大的表达能力和鲁棒性，为近期量子硬件在现实世界预测任务中的应用提供了有前景的途径。

Abstract: We present a quantum reservoir computing (QRC) framework based on a small-scale quantum system comprising at most six interacting qubits, designed for nonlinear financial time-series forecasting. We apply the model to predict future daily closing trading volumes of 20 quantum-sector publicly traded companies over the period from April 11, 2020, to April 11, 2025, as well as minute-by-minute trading volumes during out-of-market hours on July 7, 2025. Our analysis identifies optimal reservoir parameters that yield stock trend (up/down) classification accuracies exceeding $86 \%$. Importantly, the QRC model is platform-agnostic and can be realized across diverse physical implementations of qubits, including superconducting circuits and trapped ions. These results demonstrate the expressive power and robustness of small-scale quantum reservoirs for modeling complex temporal correlations in financial data, highlighting their potential applicability to real-world forecasting tasks on near-term quantum hardware.

</details>


### [24] [Theory of Steady States for Lindblad Equations beyond Time-Independence: Classification, Uniqueness and Symmetry](https://arxiv.org/abs/2602.13095)
*Hironobu Yoshida,Ryusuke Hamazaki*

Main category: quant-ph

TL;DR: 该论文对含时准周期GKSL方程在厄米跳跃算子假设下的渐近行为进行了严格分类，提出了稳态唯一性判据，并引入了两种强对称性概念来完全分类含时开放量子系统的渐近动力学。


<details>
  <summary>Details</summary>
Motivation: 为含时开放量子系统提供严格的数学框架，以控制耗散量子系统的动力学行为，特别是理解非平凡时间依赖稳态（如相干振荡）的产生机制。

Method: 1. 基于GKSL生成元生成的代数建立稳态唯一性判据；2. 引入薛定谔绘景和相互作用绘景两种强对称性概念；3. 在解析时间依赖条件下进行严格数学分析；4. 通过量子多体自旋链等原型示例验证理论。

Result: 1. 建立了含时GKSL方程稳态唯一性的充要条件；2. 发现相互作用绘景强对称性导致非平凡时间依赖稳态（如相干振荡），而薛定谔绘景强对称性控制时间无关稳态的存在；3. 统一了强动力学对称性和Floquet动力学对称性等已知机制；4. 揭示了新一类开放量子系统中对称性预测的时间依赖渐近动力学。

Conclusion: 该框架为时间依赖方式控制耗散量子系统提供了严格数学基础，不仅统一了现有理论，还揭示了新的对称性机制，对量子控制和量子信息处理具有重要意义。

Abstract: We present a rigorous and comprehensive classification of the asymptotic behavior of time-quasiperiodic Gorini-Kossakowski-Sudarshan-Lindblad (GKSL) equations under the assumption of Hermitian jump operators. Our main contributions are twofold: first, we establish a criterion for the uniqueness of steady states. The criterion is formulated in terms of the algebra generated by the GKSL generators and provides a necessary and sufficient condition when the generators are analytic functions of time. We demonstrate the utility of our criterion through prototypical examples, including quantum many-body spin chains. Second, we extend the concept of strong symmetry for time-dependent GKSL equations by introducing two distinct forms, strong symmetry in the Schrödinger picture and that in the interaction picture, and completely classify the asymptotic dynamics with them. More concretely, we rigorously uncover that the strong symmetry in the interaction picture is responsible for non-trivial time-dependent steady states, such as coherent oscillations, whereas that in the Schrödinger picture controls the existence of time-independent steady states. This classification not only encompasses established mechanisms underlying non-trivial oscillatory steady states, such as strong dynamical symmetry and Floquet dynamical symmetry, but also reveals symmetry-predicted, time-dependent asymptotic dynamics in a novel class of open quantum systems. Our framework thus provides a rigorous foundation for controlling dissipative quantum systems in a time-dependent manner.

</details>


### [25] [Stronger Welch Bounds and Optimal Approximate $k$-Designs](https://arxiv.org/abs/2602.13099)
*Riccardo Castellano,Dmitry Grinko,Sadra Boreiri,Nicolas Brunner,Jef Pauwels*

Main category: quant-ph

TL;DR: 论文提出了针对有限量子态集合在希尔伯特空间中分布均匀性的强化Welch型不等式，在态数量不足构成精确k-design时仍保持尖锐性，证明了SICs和完备MUB集合是最优近似3-design，并提供了维度6中不存在完备MUB集合的数值证据。


<details>
  <summary>Details</summary>
Motivation: 研究有限纯量子态集合在希尔伯特空间中的分布均匀性问题。传统Welch界限在态数量不足构成精确k-design时变得不具信息性，需要发展在低态数量下仍保持尖锐性的不等式。

Method: 利用部分转置的秩约束和部分转置Haar矩算子的谱性质，推导强化Welch型不等式。计算了部分转置对称子空间投影算子的完整谱（包括多重性和特征向量），作为关键技术工具。

Result: 证明了与Welch界限的偏差捕获了平均情况近似误差，表征了固定基数下的最小可实现误差。对于k=3，证明了SICs和完备MUB集合达到界限，成为其基数下的最优近似3-design。提供了维度6中不存在完备MUB集合的数值证据。

Conclusion: 提出的强化Welch不等式在态数量不足构成精确k-design时仍保持尖锐性，为量子态分布均匀性提供了更精细的刻画。SICs和完备MUB集合是最优近似3-design，维度6中可能不存在完备MUB集合。部分转置对称子空间投影算子的谱分析具有超越本工作的应用价值。

Abstract: A fundamental question asks how uniformly finite sets of pure quantum states can be distributed in a Hilbert space. The Welch bounds address this question, and are saturated by $k$-designs, i.e. sets of states reproducing the $k$-th Haar moments. However, these bounds quickly become uninformative when the number of states is below that required for an exact $k$-design. We derive strengthened Welch-type inequalities that remain sharp in this regime by exploiting rank constraints from partial transposition and spectral properties of the partially transposed Haar moment operator. We prove that the deviation from the Welch bound captures the average-case approximation error, hence characterizing a natural notion of minimum achievable error at fixed cardinality. For $k=3$, we prove that SICs and complete MUB sets saturate our bounds, making them optimal approximate 3-designs of their cardinality. This leads a natural variational criterion to rule out the existence of a complete set MUBs, which we use to obtain numerical evidence against such set in dimension $6$. As a key technical ingredient, we compute the complete spectrum of the partially transposed symmetric-subspace projector, including multiplicities and eigenvectors, which may find applications beyond the present work.

</details>


### [26] [Single snapshot non-Markovianity of Pauli channels](https://arxiv.org/abs/2602.13145)
*Alireza Seif,Moein Malekakhlagh,Swarnadeep Majumder Luke C. G. Govia*

Main category: quant-ph

TL;DR: 研究发现多量子比特泡利通道几乎总是非马尔可夫的，其生成元速率可能为负或复数，即使底层物理噪声是马尔可夫的。实验验证了包含负速率的噪声模型比限制为非负速率更准确。


<details>
  <summary>Details</summary>
Motivation: 泡利通道广泛用于描述量子计算机中的错误，通常假设它们具有马尔可夫生成元（非负速率的泡利-林德布拉德模型），但这一假设的有效性尚未得到系统检验。

Method: 使用CP不可分性作为非马尔可夫性标准，从动力学的单次快照研究多量子比特泡利通道。分析随机泡利通道和物理噪声模型（包括单量子比特过旋转和双量子比特振幅阻尼错误），并将概率误差放大和消除推广到非马尔可夫生成元。

Result: 随机泡利通道几乎总是非马尔可夫的，负速率概率随量子比特数量双指数收敛于1。即使底层物理噪声是马尔可夫的，泡利扭转塑造的噪声模型也普遍存在负速率。包含负速率的噪声模型比限制为非负速率更准确。

Conclusion: 泡利通道通常是非马尔可夫的，其生成元速率可能为负或复数。这一发现对量子误差缓解和噪声建模有重要影响，需要重新评估当前基于马尔可夫假设的量子计算错误分析方法。

Abstract: Pauli channels are widely used to describe errors in quantum computers, particularly when noise is shaped via Pauli twirling. A common assumption is that such channels admit a Markovian generator, namely a Pauli-Lindblad model with non-negative rates, but the validity of this assumption has not been systematically examined. Here, using CP-indivisibility as our criterion for non-Markovianity, we study multi-qubit Pauli channels from a single snapshot of the dynamics. We find that while the generator always has the same structure as the standard Pauli-Lindblad model, the rates may be negative or complex. We show that random Pauli channels are almost always non-Markovian, with the probability of encountering a negative rate converging doubly exponentially to unity with the number of qubits. For physically motivated noise models shaped by Pauli twirling, including single-qubit over-rotations and two-qubit amplitude damping errors, we find that negative rates are generic, even when the underlying physical noise is Markovian. We generalize probabilistic error amplification and cancellation to non-Markovian generators, and quantify the sampling overhead introduced by negative and complex rates. Experiments on superconducting qubits confirm that allowing negative rates in the learned noise model yields more accurate predictions than restricting to non-negative rates.

</details>


### [27] [Mean-Force Hamiltonians from Influence Functionals](https://arxiv.org/abs/2602.13146)
*Gerard McCaul*

Main category: quant-ph

TL;DR: 提出了一种基于淬火密度框架的强耦合热力学方法，通过Hubbard-Stratonovich变换将约化平衡态重写为虚时局域传播子的平均，严格分离环境统计定义与系统响应代数结构。


<details>
  <summary>Details</summary>
Motivation: 平均力哈密顿量（HMF）是强耦合热力学的标准起点，但仅在受限情况下已知其显式算符形式。需要开发新方法来处理更一般的强耦合热力学问题。

Method: 提出淬火密度框架，利用Hubbard-Stratonovich变换将约化平衡态表示为虚时局域传播子的平均。该方法严格分离环境统计定义与系统响应代数结构。应用于谐振环境与系统哈密顿量对易耦合的最小情况。

Result: 在该最小情况下，获得了对平均力哈密顿量修正的精确闭式表达式。通过有限浴迹计算和五能级投影耦合模型中的随机虚时采样验证了该结果。

Conclusion: 淬火密度框架为强耦合热力学提供了新方法，能够处理更一般的耦合情况，并在特定场景下获得精确结果，为理解强耦合系统的热力学性质提供了新工具。

Abstract: The Hamiltonian of mean force (HMF) provides the standard starting point for strong-coupling thermodynamics, yet explicit operator forms are known only in restricted settings. We present a quenched density framework that uses the Hubbard-Stratonovich transformation to rewrite the reduced equilibrium state as an average over local propagators in imaginary time. This approach rigorously separates the statistical definition of the environment from the algebraic structure of the system response. We apply this framework to the minimal case of a harmonic environment with a coupling commuting with the system Hamiltonian. In this scenario the correction to the HMF has an exact, closed-form expression. We validate this result against finite-bath trace-out calculations and stochastic imaginary-time sampling in a five-level projector-coupled model.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [28] [A new model for two-layer liquid-gas stratified flows in pipes with general cross sections](https://arxiv.org/abs/2602.12290)
*Sarswati Shah,Gerardo Hernández-Dueñas*

Main category: physics.comp-ph

TL;DR: 提出了一种适用于任意截面管道中不混溶两层气液分层流动的新模型，包含不可压缩液体层和可压缩气体层，通过非保守项耦合，分析了双曲性质并进行了数值验证。


<details>
  <summary>Details</summary>
Motivation: 现有的分层流动模型通常针对特定截面或简化假设，需要建立适用于任意截面管道、能准确描述气液两相分层流动的通用模型，特别是考虑密度差异显著和不显著的情况。

Method: 采用浅水近似处理底部不可压缩液体层（静水压力），理想气体定律处理顶部可压缩气体层（质量、动量和能量守恒），通过非保守乘积项耦合两层间的动量和能量交换，分析模型的双曲性质、熵不等式和特征值近似。

Result: 成功建立了具有良好双曲性质的耦合模型，数值测试验证了模型的平衡性、黎曼问题求解能力、扰动响应和稳态收敛性，适用于水-空气（密度差异大）和液氢-气氢（密度差异小）等不同场景。

Conclusion: 该模型为任意截面管道中的气液分层流动提供了有效的数学框架，通过适当的数值方法能够准确模拟不同密度比下的流动行为，具有实际工程应用价值。

Abstract: In this work, we derive a new model for immiscible two-layer gas-liquid stratified flows in pipes with general cross sections. The bottom layer is occupied by an incompressible fluid in liquid phase with hydrodynamics based on a hydrostatic pressure, following a shallow water approximation. The top layer is occupied by a compressible gas, following an ideal gas law leading to conservation of mass, momentum and energy. The two subsystems are linked through non-conservative products, representing momentum and energy exchanges between layers. The hyperbolic properties of the resulting model are analyzed, including the derivation of entropy inequalities, and the approximations of eigenvalues of the corresponding coefficient matrix. Numerical tests are included to demonstrate the merits of the model and the numerical approximations, including well-balancedness, Riemann problems, and perturbations and convergence toward steady states at rest. Besides simulations of water and air where the density difference between layers is significant, a case where such difference is not so pronounced (like gas and liquid hydrogen) is also shown.

</details>


### [29] [Accelerated Markov Chain Monte Carlo Simulation via Neural Network-Driven Importance Sampling](https://arxiv.org/abs/2602.12294)
*Michael Kim,Wei Cai*

Main category: physics.comp-ph

TL;DR: 提出一种基于神经网络偏置势的重要性采样方法，用于加速MCMC模拟中的稀有事件采样，并通过BRW技术提高效率。


<details>
  <summary>Details</summary>
Motivation: 原子模拟受限于时间尺度，系统常被困在亚稳态中，而亚稳态间的稀有跃迁事件难以观测，但却是控制长期演化的关键。

Method: 使用神经网络表示偏置势的重要性采样方法，增强稀有跃迁事件的采样，同时保持不同跃迁路径的相对概率；采用分支随机游走技术提高效率和降低方差。

Result: 方法在2维和14维系统上验证，证明了其准确性和可扩展性。

Conclusion: 提出的方法能够有效加速MCMC模拟的时间尺度，准确采样稀有跃迁事件，为高维系统的模拟提供了灵活有效的解决方案。

Abstract: Atomistic simulations provide valuable insights into the physical processes governing material behavior. However, their applicability is fundamentally constrained by the limited time scales accessible to brute-force simulations. This bottleneck often stems from complex energy landscapes where the systems stay trapped in metastable states for long periods of time. Yet, the long-term evolution is controlled by the transitions between the metastable states, which are rare events and difficult to observe. We present an importance sampling method designed to accelerate the time scale of Markov chain Monte Carlo (MCMC) simulations. By employing a bias potential, our approach enhances the sampling of rare transition events while preserving the relative probabilities of distinct transition pathways. The bias potential is represented by a neural network which enables the flexibility needed for high-dimensional systems. We propose a rigorous formulation to obtain the original transition rates between metastable states using transition paths obtained from the biased simulation. We further use a branching random walk (BRW) technique to enhance efficiency and to reduce variance. The proposed methodology is validated on 2-dimensional and 14-dimensional systems, demonstrating its accuracy and scalability.

</details>


### [30] [Electrohydrodynamic instability of Cu, W and Ti metal nanomelts under radiofrequency E-fields from multiphysics molecular dynamics simulations with coarse-grained density field analysis](https://arxiv.org/abs/2602.12558)
*Shangyong Wua,Rui Chua,Wenqian Konga,Hongyu Zhanga,Le Shia,Kai Wua,Yonghong Chenga,Guodong Menga,Bing Xiaoa*

Main category: physics.comp-ph

TL;DR: 该研究结合ED-MD模拟和电毛细波不稳定性理论，研究了Cu、Ti、W纳米尖端在射频电场下的结构演化和热失控过程，发现纳米熔体的粘度和密度与块体液态金属显著不同。


<details>
  <summary>Details</summary>
Motivation: 研究纳米尖端在射频电场下的电流体动力学不稳定性和热失控过程，这对于理解纳米尺度电场效应和器件稳定性至关重要。

Method: 采用电动力学耦合分子动力学（ED-MD）模拟和电毛细波动态不稳定性理论，分析1nm和5nm曲率半径的Cu、Ti、W纳米尖端在不同射频电场条件下的行为。

Result: 发现纳米熔体的质量和运动粘度与块体液态金属在熔点时有显著差异；纳米熔体在射频电场下的粘度比块体液态金属高几个数量级；时间延迟与电场频率呈非单调变化；存在触发热失控的临界射频电场振幅。

Conclusion: 纳米熔体的物理性质与块体材料存在根本差异，这导致电毛细波不稳定性理论在粘度主导区域的空间和时间尺度显著增加。对于W纳米尖端，两种方法在临界波长和时间延迟方面表现良好一致。

Abstract: Employing both electrodynamics coupled with molecular dynamics (ED-MD) simulations for atomistic models and the dynamic instability theory of electrocapillary wave, we investigate the structure evolutions and thermal runaway process of Cu, Ti and W nanotips with radii of curvature of 1 nm and 5 nm under various radiofrequency electric field conditions. The associated critical parameters including the critical electric field, spatial and temporal scales of the electrohydrodynamic instability of molten apexes are obtained by proposing the workflows that utilize the atomistic models in ED-MD simulations to calculate kinematic viscosity tensor components and mass density spatial distributions for the nanomelts with electric fields. Our current ED-MD simulations for nanotips show a non-monotonical variation of the time delay versus the electric field frequency for metal nanotips, and the presence of a critical rf electric field amplitude triggering the thermal runaway regardless of the field frequency. The calculated mass densities and kinematic viscosities of nanomelts for metal nanotips are found to be drastically different to those of bulk liquid metals at the melting point. Specifically, the viscosity of nanomelt under the rf electric field is revealed to be several orders of magnitude higher than the bulk liquid metal, resulting in substantial increase of spatial and temporal scales in the instability theory of electrocapillary wave within the viscosity-dominated regime, compared to the results of ED-MD simulations for Cu and Ti metals, while good agreement between the two methods on the critical wavelength and time delay of thermal runway is found for W nanotips.

</details>


### [31] [A T-matrix scattering formalism for electron-beam spectroscopy](https://arxiv.org/abs/2602.12743)
*P. Elli Stamatopoulou,Carsten Rockstuhl*

Main category: physics.comp-ph

TL;DR: 开发了基于T矩阵散射理论的电子束光谱计算工具treams_ebeam，用于快速准确模拟阴极发光和电子能量损失谱


<details>
  <summary>Details</summary>
Motivation: 需要先进的计算工具来描述电子与结构化纳米光子材料的相互作用，以支持理论预测、设计任务和实验结果解释，特别是自由电子驱动的纳米光子光源

Method: 将T矩阵散射框架扩展到快速电子相互作用，开发了treams_ebeam软件，可模拟复杂光子材料的阴极发光和电子能量损失谱

Result: 成功实现了电子束光谱计算工具，能够处理各种形状和材料的单散射体、周期性椭圆纳米盘链以及二维晶格纳米球簇等复杂结构

Conclusion: 通过将快速电子物理与先进散射理论结合，为设计、理解和工程下一代纳米尺度光-物质相互作用开辟了新可能性

Abstract: Advanced computational tools that describe the interaction of electrons with structured nanophotonic materials are crucial for theoretical predictions, specific design tasks, and the interpretation of experimental results. These tools open the door to systematic exploration of free-electron-driven nanophotonic light sources, among others. Here, we report on the implementation of electron-beam spectroscopy in a T-matrix-based scattering formulation. Such a framework is quite versatile in predicting the electromagnetic response of complex photonic materials composed of periodically or aperiodically arranged individual scatterers. By extending this formalism to describe interactions with fast electrons, we provide a fast and accurate numerical tool for simulating cathodoluminescence (CL) and electron energy-loss spectroscopy (EELS) measurements. The desired functionalities are implemented into the existing software suite treams for electromagnetic scattering computations, and the extended code treams_ebeam is available online at https://github.com/tfp-photonics/treams_ebeam. We demonstrate the implementation details on a carefully selected set of problems, including single scatterers of various shapes and materials, a periodic chain of elliptical nanodisks, and a finite cluster of nanospheres arranged in a two-dimensional (2D) lattice. By uniting fast-electron physics with advanced scattering theory, our framework unlocks new possibilities for designing, understanding, and engineering next-generation nanoscale light-matter interactions.

</details>


### [32] [Estimating Full Path Lengths and Kinetics from Partial Path Transition Interface Sampling Simulations](https://arxiv.org/abs/2602.12835)
*Wouter Vervust,Elias Wils,Sina Safaei,Daniel T. Zhang,An Ghysels*

Main category: physics.comp-ph

TL;DR: 该研究开发了一个马尔可夫状态模型框架，用于从REPPTIS算法生成的短部分路径中提取完整路径长度和动力学性质，包括首次通过时间、通量和速率常数。


<details>
  <summary>Details</summary>
Motivation: REPPTIS算法虽然能高效研究稀有慢速事件，但缺乏从短部分路径中提取时间相关性质（如平均首次通过时间、通量和速率）的形式化方法。

Method: 引入马尔可夫状态模型框架，推导出REPPTIS穿越概率、平均首次通过时间、通量和速率常数的闭式公式，并在布朗运动、朗之万粒子的一维势能剖面和KCl溶液解离中验证。

Result: MSM框架能准确再现精确动力学基准，在生物系统测试中（胰蛋白酶-苯甲脒复合物解离）计算了解离速率，但低估了实验值。

Conclusion: MSM框架为REPPTIS模拟提供了坚实的理论和实践基础，能够从计算高效的部分路径中提取动力学信息。

Abstract: Assessing the time scale of biological processes using molecular dynamics (MD) simulations with sufficient statistical accuracy is a challenging task, as processes are often rare and/or slow events, which may extend largely beyond the time scale of what is accessible with modern day high performance computational infrastructure. Recently, the replica exchange partial path transition interface sampling (REPPTIS) algorithm was developed to study rare and slow events involving metastable states along their reactive pathways. REPPTIS is a path sampling method where paths are cut short to reduce the computational cost, while combining this with the efficiency offered by replica exchange between the partial path ensembles. However, REPPTIS still lacks a formalism to extract time-dependent properties, such as mean first passage times, fluxes, and rates, from the short partial paths. In this work, we introduce a Markov state model (MSM) framework to estimate full path lengths and kinetic properties from the overlapping partial paths generated by REPPTIS. The framework results in newly derived closed formulas for the REPPTIS crossing probability, mean first passage times (MFPTs), flux, and rate constant. Our approach is then validated using simulations of Brownian and Langevin particles on a series of one-dimensional potential energy profiles as well as the dissociation of KCl in solution, demonstrating that REPPTIS accurately reproduces the exact kinetics benchmark. The MSM framework is further applied to the trypsin-benzamidine complex to compute the dissociation rate as a test case of a biological system, albeit the computed rate underestimates the experimental value. In conclusion, our MSM framework equips REPPTIS simulations with a robust theoretical and practical foundation for extracting kinetic information from computationally efficient partial paths.

</details>


### [33] [Tensor Network Compression for Fully Spectral Vlasov-Poisson Simulation](https://arxiv.org/abs/2602.13092)
*Erik M. Åsgrim,Luca Pennati,Marco Pasquale,Stefano Markidis*

Main category: physics.comp-ph

TL;DR: 提出一种基于低秩张量网络的自适应压缩方法用于动理学等离子体模拟，在压缩形式下直接进行谱变换和时间推进，无需重构完整相空间网格。


<details>
  <summary>Details</summary>
Motivation: 传统动理学等离子体模拟需要处理高维相空间，计算成本巨大。需要开发能够自适应压缩相空间分布函数的方法，在保持精度的同时显著降低计算复杂度。

Method: 使用低秩张量网络表示相空间分布函数，采用Strang分裂法推进Vlasov-Poisson系统，每个子步在对应变量上进行谱处理。将分布函数和傅里叶变换都表示为张量网络对象，直接在压缩形式下应用谱变换。自洽电场也在张量形式下计算，通过收缩速度自由度并提取零傅里叶模式获得电荷密度作为谱泊松求解器的源项。

Result: 方法在标准基准测试（朗道阻尼和双流不稳定性）上得到验证。系统研究了压缩参数（截断容差和内部秩/键维数）对动量能量守恒、正定性行为、细丝化鲁棒性和计算成本的影响。

Conclusion: 提出的张量网络方法能够有效压缩相空间分布函数，在保持物理精度的同时显著降低计算成本，为高维动理学模拟提供了有前景的数值框架。

Abstract: We propose a numerical method for kinetic plasma simulation in which the phase-space distribution function is represented by a low-rank tensor network with an adaptive level of compression. The Vlasov-Poisson system is advanced using Strang splitting, and each substep is treated spectrally in the corresponding variable. By expressing both the distribution function and the Fourier transform as tensor network objects (state and operator representations), spectral transforms are applied directly in compressed form, enabling time stepping without reconstructing the full phase-space grid. The self-consistent electric field is also computed within the tensor formalism. The charge density is obtained by contracting over velocity degrees of freedom and extracting the zero Fourier mode, which provides the source term for a spectral Poisson solver. We validate the approach on standard benchmarks, including Landau damping and the two-stream instability. Finally, we systematically study how compression parameters, including truncation tolerances and internal ranks (bond dimensions), affect momentum and energy conservation, positivity behavior, robustness to filamentation, and computational cost.

</details>


### [34] [An Always-Accepting Algorithm for Transition Path Sampling](https://arxiv.org/abs/2602.13130)
*Magdalena Häupl,Sebastian Falkner,Peter G. Bolhuis,Christoph Dellago,Alessandro Coretti*

Main category: physics.comp-ph

TL;DR: 提出一种单向射击算法用于过渡路径采样，该算法接受所有提议轨迹但仍能正确采样过渡路径集合，显著提高采样效率。


<details>
  <summary>Details</summary>
Motivation: 传统过渡路径采样方法存在大量轨迹被拒绝的问题，导致计算效率低下，特别是在研究复杂系统（如CO₂笼形水合物形成）时，难以在特定温度和压力条件下进行充分采样。

Method: 基于两个关键要素：1) 提出总是反应性轨迹的程序；2) 重新加权方案来纠正因总是接受提议路径引入的偏差。该方法针对过阻尼随机动力学系统设计。

Result: 算法消除了与生成被拒绝轨迹相关的成本，显著提高了过渡路径采样的效率。在CO₂笼形水合物形成研究中，能够对传统算法难以访问的温度和压力条件下的晶体水合物形成进行适当采样。

Conclusion: 该单向射击算法通过接受所有提议轨迹并采用重新加权校正，实现了高效的过渡路径采样，为研究复杂反应机制（如笼形水合物形成）提供了有效工具。

Abstract: We present a one-way shooting algorithm for transition path sampling that accepts every proposed trajectory yet samples the correct transition path ensemble for systems with overdamped stochastic dynamics. The method is based on two key elements: a procedure to propose trajectories that are always reactive, and a reweighting scheme that corrects for the bias introduced by always accepting the proposed paths. This approach significantly improves the efficiency of transition path sampling by eliminating the cost associated with generating trajectories that are then rejected. We demonstrate the algorithm by investigating the formation of CO$_2$ clathrate hydrates along different reaction mechanisms, showing that the increased efficiency allows proper sampling of the formation of crystalline hydrates at temperatures and pressures that are difficult to access with conventional algorithms.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [35] [Non-vacuum black holes in new general relativity](https://arxiv.org/abs/2602.12496)
*D. F. López,A. A. Coley,B. Yildirim*

Main category: gr-qc

TL;DR: 新广义相对论(NGR)无法描述物理上有意义的非平凡黑洞，因为黑洞解的存在要求参数取到已知的病态模型值


<details>
  <summary>Details</summary>
Motivation: 研究新广义相对论(NGR)中黑洞解的存在性和物理意义，检验该理论是否能描述现实的引力现象

Method: 在静态球对称配置下求解NGR场方程，假设存在局部黑洞视界，分析真空和非真空情况

Result: 黑洞解的存在对参数(c_a,c_v,c_t)施加了代数约束，这些约束强制参数取到已知病态模型的值

Conclusion: NGR无法描述物理上有意义的非平凡黑洞，这对该理论作为可行引力理论的地位提出了严重质疑

Abstract: New general relativity (NGR) possesses a region in the \((c_{a},c_{v},c_{t})\)-parameter space corresponding to physically acceptable models. However, when solving the field equations for vacuum and non--vacuum static and spherically symmetric configurations under the assumption of the existence of a local black hole horizon, we find that the mere existence of such solutions imposes algebraic constraints that fix the parameters to values associated with known pathological models. As a consequence, we conclude that NGR is unable to describe physically meaningful non-trivial black holes.

</details>


### [36] [Impact of Spin Priors on the Population Inference of Merging Binary Black Holes](https://arxiv.org/abs/2602.12509)
*Kazuya Kobayashi,Masaki Iwaya,Soichiro Morisaki,Kenta Hotokezaka,Tomoya Kinugawa*

Main category: gr-qc

TL;DR: 提出新的自旋先验分布，在有效自旋参数Xeff和Xp上均匀分布，能更准确地恢复黑洞并合系统的真实自旋分布，特别是对于自旋对齐构型。


<details>
  <summary>Details</summary>
Motivation: 引力波观测中黑洞自旋参数通常约束较弱，传统均匀各向同性先验会排除自旋轨道对齐构型，可能产生偏差。先验选择还会影响群体层次贝叶斯推断中蒙特卡洛积分的收敛性。

Method: 提出新的自旋先验分布，在有效自旋参数Xeff和Xp上均匀分布，并以质量比为条件。使用模拟的黑洞并合群体验证该先验的效果。

Result: 推断的自旋群体分布确实依赖于先验选择，提出的新先验能更准确地恢复底层自旋分布，特别是当真实分布偏向自旋对齐构型时。由于质量和自旋测量相关，该先验也能更准确地恢复质量分布。

Conclusion: 在有效自旋参数上均匀分布的新先验优于传统均匀各向同性先验，能减少推断偏差，提高黑洞并合系统形成通道研究的准确性。

Abstract: The spins of merging binary black holes (BBHs) inferred from gravitational-wave (GW) observations provide key insights into their formation channels. However, spin parameters are typically weakly constrained from data, and their inferred values are often strongly influenced by the assumed prior in Bayesian analyses. A commonly used prior, uniform in spin magnitudes and isotropic in spin directions, assigns vanishing probability density to spin-orbit-aligned configurations, potentially biasing inferences for BBH parameters. The prior choice can also affect population-level analyses by degrading the convergence of Monte Carlo integrations used to evaluate the likelihood in hierarchical Bayesian inference. In this work, we propose a novel spin prior that is uniform in the effective spin parameters Xeff and Xp, two spin combinations that can be relatively well measured from GW data, conditioned on the mass ratio. Using simulated BBH populations, we show that the inferred spin population can depend on the choice of prior, and that the proposed prior more accurately recovers the underlying spin population, particularly when the true distribution favors aligned-spin configurations. Because mass and spin measurements are correlated, our prior also enables a more accurate recovery of the underlying mass distribution.

</details>


### [37] [Cosmological perturbations and gravitational waves in the general Einstein-vector theory](https://arxiv.org/abs/2602.12536)
*Xiao-Bin Lai,Yu-Zhi Fan,Yu-Qi Dong,Yu-Xiao Liu*

Main category: gr-qc

TL;DR: 研究四维广义爱因斯坦-矢量理论在宇宙学背景下的稳定性和引力波特性，分析参数空间中不同模式的传播度、稳定性条件及引力波观测约束。


<details>
  <summary>Details</summary>
Motivation: 研究广义爱因斯坦-矢量理论在宇宙学背景下的稳定性，探索该理论中引力波的特性，为通过引力波观测检验该理论提供理论基础。

Method: 在线性扰动水平上系统分析鬼场、拉普拉斯和快子不稳定性；研究小波数极限下的标量不稳定性；在稳定参数空间内分析引力波模式的数量、传播速度和观测约束。

Result: 理论最多包含两个张量模式、两个矢量模式和一个标量模式；张量扰动稳定性条件易满足，矢量扰动对参数空间有非平凡约束；背景矢量场非零时标量部门在小波数下不稳定；矢量引力波超光速传播，但如果张量引力波精确以光速传播，则矢量引力波被禁止。

Conclusion: 广义爱因斯坦-矢量理论具有独特的引力波特征，特别是矢量引力波的超光速传播特性及其与张量引力波传播速度的关系，为该理论提供了关键的观测检验特征。

Abstract: We investigate the stability and gravitational waves (GWs) in the four-dimensional general Einstein-vector theory in a cosmological background. The theory accommodates up to six propagating degrees of freedom, comprising two tensor, two vector, and two scalar modes, in addition to matter perturbations. In certain regions of the parameter space, the number of scalar degrees of freedom is reduced to one or even zero. To investigate the stability, we systematically analyze ghost, Laplacian, and tachyonic instabilities at the linear perturbative level. The stability conditions are easily satisfied for tensor perturbations, but impose nontrivial constraints on the parameter space for vector perturbations. Furthermore, in the presence of a nonvanishing background vector field, the scalar sector becomes unstable at small wavenumbers $|\vec{k}|$. In the small-scale limit ($|\vec{k}|\rightarrow\infty$), we further investigate the GW properties of the general Einstein-vector theory within the stable parameter space, including the number of independent modes, their propagation speeds, and observational constraints from GW experiments. We find that there are at most two tensor modes, two vector modes, and one scalar mode. Notably, vector GWs propagate superluminally, yet they are forbidden if tensor GWs travel exactly at light speed. This distinctive feature provides a key observational signature for testing the theory.

</details>


### [38] [Dynamical system and statefinder analysis of cosmological models in f(T, B) gravity](https://arxiv.org/abs/2602.12728)
*Jianwen Liu,Fabao Gao,Aqeela Razzaq*

Main category: gr-qc

TL;DR: 该研究系统分析了f(T,B)引力理论中两种函数形式在平坦FLRW宇宙中的宇宙学动力学，使用动力系统方法识别了德西特吸引子，并通过状态诊断器区分这些模型与ΛCDM范式。


<details>
  <summary>Details</summary>
Motivation: 研究f(T,B)引力理论作为解释宇宙加速膨胀的几何框架，提供传统暗能量模型的替代方案，探索其理论一致性和观测可区分性。

Method: 使用动力系统技术构建自治系统，分析乘法幂律模型f(T,B)=c₁TᵅBᵝ和加法混合幂律模型f(T,B)=c₂Tᵅ+c₃Bᵝ，推导固定点的解析稳定性条件，并进行数值模拟。

Result: 识别了德西特吸引子能自然解释晚期宇宙加速，数值模拟揭示了螺旋轨迹和阻尼振荡等特征演化模式，状态诊断器定量区分了这些模型与ΛCDM范式。

Conclusion: f(T,B)引力理论为解释宇宙加速提供了一个理论上一致且观测上可区分的几何框架，是传统暗能量模型的有力替代方案。

Abstract: This study systematically investigates the cosmological dynamics of two well-motivated functional forms in $f(T,B)$ gravity within a flat Friedmann-Lemaître-Robertson-Walker (FLRW) universe. Here $T$ denotes the torsion scalar and $B$ the boundary term, with the special choice $f(T,B) = - T + B$ recovering General Relativity. We focus on a multiplicative power-law model $f(T,B) = c_1 T^αB^β$ and an additive mixed power-law model $f(T,B) = c_2 T^α+ c_3 B^β$. Using dynamical system techniques, we construct autonomous systems and identify de Sitter attractors that naturally explain late-time cosmic acceleration. Analytical stability conditions for these fixed points are derived, and numerical simulations reveal characteristic evolutionary patterns, such as spiral trajectories and damped oscillations in the additive mixed power-law model. Furthermore, statefinder diagnostics are applied to quantitatively distinguish these models from the standard $Λ$CDM paradigm and other dark energy scenarios. The results indicate that $f(T,B)$ gravity offers a theoretically consistent and observationally distinguishable geometric framework for explaining cosmic acceleration, presenting a compelling alternative to conventional dark energy models.

</details>


### [39] [Statistics of time and frequency-averaged spectra in gravitational-wave background searches](https://arxiv.org/abs/2602.12781)
*Quentin Baghi,Nikolaos Karnesis,Jean-Baptiste Bayle*

Main category: gr-qc

TL;DR: 该论文探讨了引力波探测器时间序列分析中假设时间块或频率箱不相关的有效性，特别针对随机引力波背景搜索，分析了平均处理的影响并提出了基于Fisher信息的量化工具。


<details>
  <summary>Details</summary>
Motivation: 引力波探测器的时间序列分析通常假设时间块或频率箱之间不相关，但这种近似在随机引力波背景搜索中的有效性需要验证。此外，为减少计算成本而采用的时间/频率平均技术的影响也需要评估。

Method: 1) 检验时间块和频率箱不相关假设的有效性；2) 分析时间和频率平均技术的影响；3) 引入基于Fisher信息的分析工具来量化忽略这些效应导致的参数推断误差；4) 处理局部平稳过程和最优时间分块问题。

Result: 论文提出了一个量化框架，能够评估忽略时间/频率相关性和平均处理对参数推断精度的影响。该分析工具可以帮助研究人员确定在特定搜索场景下这些近似是否可接受。

Conclusion: 在随机引力波背景搜索中，传统的时间块/频率箱不相关假设需要谨慎验证。基于Fisher信息的分析工具为量化这些近似误差提供了有效方法，有助于优化数据处理策略和参数推断精度。

Abstract: Time series analysis from gravitational-wave detectors often relies on the assumption that time chunks, or frequency bins, are uncorrelated. We discuss the validity of this approximation in the context of searches for stochastic gravitational-wave backgrounds. We examine the impact of averaging over time and frequency, a reduction technique commonly employed to minimize the computational expense of likelihood evaluations. We introduce an analytical tool based on Fisher information to quantify the error in parameter inference arising from ignoring these effects. Finally, we address the issue of locally stationary processes and optimal time chunking.

</details>


### [40] [New Horizons in Effective Field Theory?](https://arxiv.org/abs/2602.12802)
*Stefan Hollands,Dustin Urbiks*

Main category: gr-qc

TL;DR: 该论文证明在四维最一般的宇称对称有效标量张量理论中，尽管存在三个不同的传播锥，但稳态黑洞的视界相对于度规仍然是零性的，且所有传播锥在视界处接触。


<details>
  <summary>Details</summary>
Motivation: 在包含四阶导数项的最一般宇称对称有效标量张量理论中，存在三个不同的传播锥，它们通常与度规定义的光锥不一致。这引发了一个问题：黑洞视界应该如何定义？是相对于最宽的传播锥还是度规？这种多重传播锥结构可能对黑洞热力学产生悖论。

Method: 作者提供了两个定理来证明：1）稳态黑洞的视界相对于度规是零性的；2）所有三个传播锥在视界处接触。这些定理允许旋转黑洞的情况，通过数学证明而非数值模拟来建立这些结论。

Result: 证明了在包含四阶导数项的最一般宇称对称有效标量张量理论中，尽管存在三个不同的传播锥，但稳态黑洞的视界相对于度规仍然是零性的，且所有传播锥在视界处接触。这意味着Killing视界的概念在黑洞热力学讨论中仍然保持其基本地位。

Conclusion: 该研究表明，即使在存在多个传播锥的有效标量张量理论中，Killing视界的概念仍然是根本性的，并且与多重传播锥相关的某些热力学悖论可以被避免。这为黑洞热力学在更一般的引力理论中的一致性提供了理论支持。

Abstract: We consider the most general parity symmetric effective scalar tensor theory in four dimensions containing terms up to fourth derivative order in the Lagrangian. It has been shown [H.S. Reall, Phys. Rev. D 103 (2021), 084027] that this theory has three polarizations generically goverened by different (nested) propagation cones, neither of which in general coincides with the lightcone as defined by the metric. Consequently, the notion of black hole horizon must be defined relative to the widest propagation cone, and not with respect to the metric. We provide two theorems stating that, nevertheless, the horizon of a \emph{stationary} black hole is null with respect to the metric, and that, in fact, all three propagation cones touch on the horizon. The conditions in these theorems allow for rotating black holes. Our theorems thereby suggest that the notion of Killing horizon, central in most discussions of black hole thermodynamics, retains its fundamental status, and that certain thermodynamic paradoxes associated with multiple propagation cones are evaded.

</details>


### [41] [Are black hole spins truly near-zero?](https://arxiv.org/abs/2602.12859)
*Vaishak Prasad,B. S. Sathyaprakash*

Main category: gr-qc

TL;DR: GWTC-4.0报告了153个双黑洞合并事件，发现自旋测量不确定性大，大多数黑洞自旋接近零，但这一结论可能受先验分布影响而非数据本身要求。


<details>
  <summary>Details</summary>
Motivation: 研究GWTC-4.0引力波目录中黑洞自旋测量的可靠性，揭示当前自旋接近零的结论可能受到先验分布选择的影响，而非数据本身的必然结果。

Method: 分析GWTC-4.0的153个双黑洞合并事件，比较不同先验分布（均匀自旋幅度先验 vs 自旋矢量构型空间均匀先验）对自旋参数推断的影响。

Result: 使用均匀自旋幅度先验时，90%的黑洞自旋幅度χ<0.57，自旋幅度后验峰值在0.01-0.23范围；但采用自旋矢量构型空间均匀先验时，虽然χ_eff约束相似，但自旋幅度推断结果显著不同。

Conclusion: 数据本身并不要求自旋为零，而是先验分布导致这一结论。考虑先验影响对于广义相对论检验、黑洞形成机制和层次增长研究至关重要。

Abstract: The fourth gravitational-wave transient catalog, GWTC-4.0, reports 153 binary black hole mergers with false-alarm rates $<1,\mathrm{yr}^{-1}$. Chirp masses are typically measured well, with the smallest fractional uncertainty being $2%$ at the $90%$ credible level. Spins, on the other hand, are poorly constrained: the median of the best-measured spin component of the population, the effective spin, is $χ_{\rm eff}=0.04$, with a typical $90%$ credible uncertainty of $Δχ_{\rm eff}=0.44$. The large majority -- $90%$ of the observed black holes -- are consistent with spin magnitudes $χ<0.57$ and are weakly aligned with the orbits. At $90%$ credibility, the peaks of the inferred posteriors for spin magnitude are found to lie in the range $0.01$--$0.23$.
  We show that this ``near-zero spins'' conclusion may be prior-driven, and that uniform-in-magnitude spin priors lead to under-exploration of the moderate-to-high spin region of parameter space. Adopting a physically agnostic prior that is uniform in spin-vector configuration space (i.e., spin states uniform within a unit sphere) yields similar constraints on $χ_{\rm eff}$, but substantially different spin-magnitude inferences than GWTC-4.0. The resulting shift in spins directly impacts tests of general relativity, constraints on near-extremal Kerr remnants, and astrophysical conclusions, including diagnostics of formation channels and hierarchical growth. In short, the data do not require vanishing spins -- the prior does, and accounting for this is essential for robust GR tests and population inferences.

</details>


### [42] [Scalar field coupled to boundary in non-metricity: a new avenue towards dark energy](https://arxiv.org/abs/2602.12981)
*Ghulam Murtaza,Avik De,Tee-How Loo,Andronikos Paliathanasis*

Main category: gr-qc

TL;DR: 论文提出了一种新的非度量性标量张量引力模型，其中标量场与非度量性标量Q和边界项C非最小耦合，该模型能在爱因斯坦框架下恢复STEGR，并提供了处理不同仿射连接选择的统一自治系统框架。


<details>
  <summary>Details</summary>
Motivation: 标准非度量性理论缺乏在共形变换下恢复广义相对论的能力，而度量性标量张量理论则具备这一特性。通过引入边界项C，可以构建能够恢复对称远平行广义相对论等效理论(STEGR)的非度量性标量张量理论。

Method: 提出了一种新的引力模型，其中标量场与非度量性标量Q和边界项C非最小耦合。在宇宙学场景中，建立了协变表述和统一的自治系统框架，该框架能够平等处理包括重合规范和非重合规范在内的各种仿射连接选择。

Result: 通过对三个连接分支的动力学分析，发现模型能够产生标准的热历史并存在稳定的德西特吸引子。这些结果表明边界项耦合为解决晚期宇宙加速膨胀问题提供了一条定义良好且几何上灵活的途径。

Conclusion: 边界项耦合为构建几何上灵活的非度量性标量张量引力理论提供了有效途径，这些理论能够在爱因斯坦框架下恢复STEGR，并自然地解释晚期宇宙加速膨胀现象，同时保持标准的热历史演化。

Abstract: While conformal transformations in metric scalar-tensor theories recover General Relativity, this feature is notably absent in standard non-metricity-based theories. We demonstrate that by introducing the boundary term C, a non-metricity scalar-tensor theory can recover Symmetric Teleparallel Equivalent of General Relativity (STEGR) in the Einstein frame. Motivated by this, we propose a novel gravity model where a scalar field couples nonminimally to both the non-metricity scalar Q and the boundary term C. We focus in the cosmological scenario where we present the covariant formulation and a unified autonomous system framework that treats generic affine-connection choices, including coincident and non-coincident gauges, on an equal footing. Our dynamical analysis across three connection branches reveals standard thermal histories and stable de Sitter attractors. These results show that boundary-term couplings provide a well-posed, geometrically flexible route to addressing late-time cosmic acceleration.

</details>


### [43] [Evolution of Linear Perturbations under Time-Dependent Hubble Friction I: SR-USR-SR Inflation](https://arxiv.org/abs/2602.13074)
*Wen Li,Chao Chen*

Main category: gr-qc

TL;DR: 重新研究SR-USR-SR暴胀模型中线性扰动动力学，推导出模式函数演化和功率谱的精确渐近表达式，揭示功率谱凹陷源于两个增长模式的抵消而非常数项与增长项的抵消。


<details>
  <summary>Details</summary>
Motivation: 重新审视SR-USR-SR暴胀模型中线性扰动（包括共动曲率扰动和场扰动）的动力学，特别是在瞬时转变情况下的行为。先前研究对功率谱特征的物理机制理解有限，需要更精确的解析描述。

Method: 使用连接方法和汉克尔函数的渐近展开，基于三个系统规则识别转变过程中的主导项，推导出模式函数时间演化和功率谱的精确渐近表达式。

Result: 发现最终功率谱的有限凹陷源于线性扰动理论中两个增长模式的抵消，而非先前认为的常数项与增长项的抵消。提供了功率谱振幅增强和振荡特征的解析描述，与数值计算结果一致。

Conclusion: 推导出的简单可处理公式不仅便于理论计算，还为未来CMB观测提供了可检验的预测，深化了对SR-USR-SR暴胀模型中扰动动力学的理解。

Abstract: In this paper, we revisit the linear perturbation (including the comoving curvature perturbation and field perturbation) dynamics in the SR-USR-SR inflation with instantaneous transitions. Using the junction method and asymptotic expansions of Hankel functions, we derive accurate asymptotic expressions for the time evolution of mode functions and the resulting power spectrum, based on three systematic rules for identifying the dominant terms across transitions. Our results reveal that a finite dip of the final power spectrum arises from the cancellation between two growing modes within the linear perturbation theory, rather than between constant and growing terms as previously suggested. We also provide analytical descriptions of the amplitude enhancement and oscillatory features in the linear power spectrum, in agreement with numerical computations. These simple, tractable formulas not only facilitate theoretical calculations but also yield testable predictions for future CMB observations.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [44] [OptiML: An End-to-End Framework for Program Synthesis and CUDA Kernel Optimization](https://arxiv.org/abs/2602.12305)
*Arijit Bhattacharjee,Heng Ping,Son Vu Le,Paul Bogdan,Nesreen K. Ahmed,Ali Jannesari*

Main category: cs.LG

TL;DR: OptiML是一个端到端框架，通过将CUDA内核优化形式化为验证下的搜索问题，将自然语言意图或输入CUDA代码映射到性能优化的CUDA内核。它包含两个解耦阶段：自然语言输入时使用混合思维生成器生成初始程序，然后通过蒙特卡洛树搜索在LLM驱动的编辑上进行搜索优化。


<details>
  <summary>Details</summary>
Motivation: 生成高性能CUDA内核具有挑战性，因为需要在噪声且昂贵的硬件反馈下探索组合空间中的低级变换。虽然大语言模型可以合成功能正确的CUDA代码，但要获得有竞争力的性能需要系统性地探索和验证优化选择。

Method: OptiML包含两个阶段：1) OptiML-G作为提案策略，当输入为自然语言时生成初始可执行程序；2) OptiML-X使用蒙特卡洛树搜索在LLM驱动的编辑上进行搜索优化，通过Nsight Compute编译、验证和分析每个候选变换，使用结合运行时、硬件瓶颈代理和回归防护的复合目标进行评估。

Result: 在多样化的CUDA内核套件上评估OptiML，结果显示OptiML始终能发现比强LLM基线更好的已验证性能改进，并生成基于分析器证据的可解释优化轨迹。

Conclusion: OptiML通过将内核优化形式化为验证下的搜索问题，能够系统地探索优化空间，在合成与优化以及纯优化场景中都能产生性能改进，并提供了基于硬件反馈的可解释优化路径。

Abstract: Generating high-performance CUDA kernels remains challenging due to the need to navigate a combinatorial space of low-level transformations under noisy and expensive hardware feedback. Although large language models can synthesize functionally correct CUDA code, achieving competitive performance requires systematic exploration and verification of optimization choices. We present OptiML, an end-to-end framework that maps either natural-language intent or input CUDA code to performance-optimized CUDA kernels by formulating kernel optimization as search under verification. OptiML consists of two decoupled stages. When the input is natural language, a Mixture-of-Thoughts generator (OptiML-G) acts as a proposal policy over kernel implementation strategies, producing an initial executable program. A search-based optimizer (OptiML-X) then refines either synthesized or user-provided kernels using Monte Carlo Tree Search over LLM-driven edits, guided by a hardware-aware reward derived from profiler feedback. Each candidate transformation is compiled, verified, and profiled with Nsight Compute, and evaluated by a composite objective that combines runtime with hardware bottleneck proxies and guardrails against regressions. We evaluate OptiML in both synthesis-and-optimize and optimization-only settings on a diverse suite of CUDA kernels. Results show that OptiML consistently discovers verified performance improvements over strong LLM baselines and produces interpretable optimization trajectories grounded in profiler evidence.

</details>


### [45] [Abstractive Red-Teaming of Language Model Character](https://arxiv.org/abs/2602.12318)
*Nate Rahn,Allison Qi,Avery Griffin,Jonathan Michala,Henry Sleight,Erik Jones*

Main category: cs.LG

TL;DR: 提出抽象红队测试方法，通过搜索自然语言查询类别来识别可能导致语言模型违反角色规范的问题，使用强化学习和LLM迭代合成两种算法，在12个原则规范和7个目标模型上表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 语言模型助手需要遵循角色规范，但在大规模部署中偶尔会违反这些规范。传统测试方法需要大量计算资源，需要一种更高效的方法在部署前识别可能导致违规的查询类型。

Method: 提出抽象红队测试方法，搜索自然语言查询类别（如"查询是中文的"、"查询询问家庭角色"）来识别违规模式。开发两种算法：1）基于强化学习的类别生成器LLM；2）利用强LLM从高分查询迭代合成类别。使用角色特质特定的奖励模型进行评估。

Result: 在12个原则的角色规范和7个目标模型上，算法始终优于基线方法。发现了有趣的违规类别：让Llama-3.1-8B-Instruct预测未来会导致AI统治人类的回答；让GPT-4.1-Mini推荐监狱生存必需品会热情推荐非法武器。

Conclusion: 抽象红队测试方法为语言模型角色规范的实际部署前审计迈出了重要一步，能够以远低于部署级计算资源识别可能导致违规的查询类型。

Abstract: We want language model assistants to conform to a character specification, which asserts how the model should act across diverse user interactions. While models typically follow these character specifications, they can occasionally violate them in large-scale deployments. In this work, we aim to identify types of queries that are likely to produce such character violations at deployment, using much less than deployment-level compute. To do this, we introduce abstractive red-teaming, where we search for natural-language query categories, e.g. "The query is in Chinese. The query asks about family roles," that routinely elicit violations. These categories abstract over the many possible variants of a query which could appear in the wild. We introduce two algorithms for efficient category search against a character-trait-specific reward model: one based on reinforcement learning on a category generator LLM, and another which leverages a strong LLM to iteratively synthesize categories from high-scoring queries. Across a 12-principle character specification and 7 target models, we find that our algorithms consistently outperform baselines, and generate qualitatively interesting categories; for example, queries which ask Llama-3.1-8B-Instruct to predict the future lead to responses saying that AI will dominate humanity, and queries that ask GPT-4.1-Mini for essential prison survival items lead to enthusiastic recommendation of illegal weapons. Overall, we believe our results represent an important step towards realistic pre-deployment auditing of language model character.

</details>


### [46] [The Appeal and Reality of Recycling LoRAs with Adaptive Merging](https://arxiv.org/abs/2602.12323)
*Haokun Liu,Gyung Hyun Je,Marco Ciccone,Zhenlin Xu,Prasanth YSS,Colin Raffel*

Main category: cs.LG

TL;DR: 本研究探讨了从公开模型库中回收利用LoRA模块进行自适应合并的效果，发现自适应合并方法相比基础模型有提升，但与在新数据上训练新LoRA相比优势有限，且LoRA的选择对性能影响不大，随机初始化的LoRA也能达到类似效果，表明自适应合并可能主要通过正则化效应而非跨任务迁移发挥作用。


<details>
  <summary>Details</summary>
Motivation: 随着针对开放预训练模型的微调LoRA模块广泛可用，研究者对能够自适应合并LoRA以提升性能的方法产生了兴趣。然而，过去的研究尚未尝试从模型库（如Hugging Face Hub）中回收利用"野生"LoRA模块。本研究旨在填补这一空白，探索从近1000个用户贡献的LoRA模块中进行回收利用的效果。

Method: 研究使用了基于Llama 3.1 8B-Instruct语言模型训练的约1000个用户贡献的LoRA模块池。除了评估现有的自适应和非自适应合并方法外，还通过广泛搜索方法设计空间开发了一种新方法。实验比较了不同合并策略的效果，并特别测试了使用随机初始化LoRA参数的情况。

Result: 自适应合并方法相比基础模型能提升性能，但与在相同数据上训练新LoRA相比优势有限。研究发现：1) 具体选择哪些LoRA进行合并对性能影响不大；2) 使用随机初始化参数值的LoRA也能达到类似性能。这表明自适应合并可能主要通过正则化效应而非跨任务迁移发挥作用。只有在池中存在高度相关的LoRA时，才能观察到正向迁移效应。

Conclusion: 从回收的LoRA模块进行自适应合并虽然能提升基础模型性能，但其主要作用机制可能是正则化而非跨任务知识迁移。这一发现对理解LoRA合并方法的有效性机制具有重要意义，并提示在实际应用中需要重新评估LoRA选择策略的重要性。

Abstract: The widespread availability of fine-tuned LoRA modules for open pre-trained models has led to an interest in methods that can adaptively merge LoRAs to improve performance. These methods typically include some way of selecting LoRAs from a pool and tune merging coefficients based on a task-specific dataset. While adaptive merging methods have demonstrated improvements in some settings, no past work has attempted to recycle LoRAs found "in the wild" on model repositories like the Hugging Face Hub. To address this gap, we consider recycling from a pool of nearly 1,000 user-contributed LoRAs trained from the Llama 3.1 8B-Instruct language model. Our empirical study includes a range of adaptive and non-adaptive merging methods in addition to a new method designed via a wide search over the methodological design space. We demonstrate that adaptive merging methods can improve performance over the base model but provide limited benefit over training a new LoRA on the same data used to set merging coefficients. We additionally find not only that the specific choice of LoRAs to merge has little importance, but that using LoRAs with randomly initialized parameter values yields similar performance. This raises the possibility that adaptive merging from recycled LoRAs primarily works via some kind of regularization effect, rather than by enabling positive cross-task transfer. To better understand why past work has proven successful, we confirm that positive transfer is indeed possible when there are highly relevant LoRAs in the pool. We release the model checkpoints and code online.

</details>


### [47] [Wireless TokenCom: RL-Based Tokenizer Agreement for Multi-User Wireless Token Communications](https://arxiv.org/abs/2602.12338)
*Farshad Zeinali,Mahdi Boloursaz Mashhadi,Dusit Niyato,Rahim Tafazolli*

Main category: cs.LG

TL;DR: 提出混合强化学习框架解决多用户无线TokenCom中的tokenizer协议问题，联合优化tokenizer选择、子信道分配和波束成形，显著提升语义质量和资源效率。


<details>
  <summary>Details</summary>
Motivation: TokenCom作为新兴的多模态通信范式，需要收发双方就tokenizer模型和码本达成一致。在多用户下行无线场景中，如何高效进行tokenizer协议、子信道分配和波束成形是一个复杂的混合整数非凸优化问题。

Method: 提出混合强化学习框架：使用深度Q网络（DQN）联合优化tokenizer协议和子信道分配，使用深度确定性策略梯度（DDPG）优化波束成形，解决混合整数非凸优化问题。

Result: 仿真结果表明，所提框架在语义质量和资源效率方面优于基线方法，相比传统H.265方案将视频传输中的卡顿事件减少了68%。

Conclusion: 混合强化学习框架能有效解决多用户无线TokenCom中的联合优化问题，显著提升通信性能，为未来无线网络中的语义和目标导向通信提供了有效解决方案。

Abstract: Token Communications (TokenCom) has recently emerged as an effective new paradigm, where tokens are the unified units of multimodal communications and computations, enabling efficient digital semantic- and goal-oriented communications in future wireless networks. To establish a shared semantic latent space, the transmitters/receivers in TokenCom need to agree on an identical tokenizer model and codebook. To this end, an initial Tokenizer Agreement (TA) process is carried out in each communication episode, where the transmitter/receiver cooperate to choose from a set of pre-trained tokenizer models/ codebooks available to them both for efficient TokenCom. In this correspondence, we investigate TA in a multi-user downlink wireless TokenCom scenario, where the base station equipped with multiple antennas transmits video token streams to multiple users. We formulate the corresponding mixed-integer non-convex problem, and propose a hybrid reinforcement learning (RL) framework that integrates a deep Q-network (DQN) for joint tokenizer agreement and sub-channel assignment, with a deep deterministic policy gradient (DDPG) for beamforming. Simulation results show that the proposed framework outperforms baseline methods in terms of semantic quality and resource efficiency, while reducing the freezing events in video transmission by 68% compared to the conventional H.265-based scheme.

</details>


### [48] [Intrinsic Credit Assignment for Long Horizon Interaction](https://arxiv.org/abs/2602.12342)
*Ilze Amanda Auzina,Joschka Strüber,Sergio Hernández-Gutiérrez,Shashwat Goel,Ameya Prabhu,Matthias Bethge*

Main category: cs.LG

TL;DR: ΔBelief-RL利用语言模型自身的内在信念来奖励中间进展，通过目标解概率变化进行信用分配，在长时程不确定性导航中优于纯结果奖励


<details>
  <summary>Details</summary>
Motivation: 如何训练智能体在长时程不确定性中进行导航？需要解决中间进展的信用分配问题，使智能体能够有效探索和收集信息

Method: 提出ΔBelief-RL方法，利用语言模型对目标解的内在信念概率变化作为奖励信号，通过合成交互数据进行训练

Result: 方法在信息寻求能力上持续优于纯结果奖励，泛化到客服、个性化等分布外应用，测试时交互效率随交互规模增加而提升

Conclusion: ΔBelief-RL为长时程不确定性导航提供了可扩展的训练策略，通过内在信念奖励实现中间动作的信用分配

Abstract: How can we train agents to navigate uncertainty over long horizons? In this work, we propose ΔBelief-RL, which leverages a language model's own intrinsic beliefs to reward intermediate progress. Our method utilizes the change in the probability an agent assigns to the target solution for credit assignment. By training on synthetic interaction data, ΔBelief-RL teaches information-seeking capabilities that consistently outperform purely outcome-based rewards for Reinforcement Learning, with improvements generalizing to out-of-distribution applications ranging from customer service to personalization. Notably, the performance continues to improve as we scale test-time interactions beyond the training horizon, with interaction-efficiency increasing even on Pass@k metrics. Overall, our work introduces a scalable training strategy for navigating uncertainty over a long-horizon, by enabling credit assignment to intermediate actions via intrinsic ΔBelief rewards.

</details>


### [49] [A Machine Learning Approach to the Nirenberg Problem](https://arxiv.org/abs/2602.12368)
*Gianfranco Cortés,Maria Esteban-Casadevall,Yueqing Feng,Jonas Henkel,Edward Hirst,Tancredi Schettini Gherardini,Alexander G. Stapleton*

Main category: cs.LG

TL;DR: 提出Nirenberg神经网络，一种基于物理信息神经网络(PINN)的数值方法，用于解决S²球面上规定高斯曲率的Nirenberg问题，通过全局参数化共形因子和几何感知损失训练，能够区分可实现和不可实现的曲率函数。


<details>
  <summary>Details</summary>
Motivation: 解决Nirenberg问题——在S²球面上规定高斯曲率，这是一个经典的几何分析问题。传统方法存在计算困难，需要探索新的数值方法为长期存在性问题提供计算视角。

Method: 采用无网格的物理信息神经网络(PINN)方法，全局参数化共形因子，使用几何感知损失函数强制曲率方程。通过Gauss-Bonnet定理进行一致性检查，并使用球谐展开拟合学习模型以提高可解释性。

Result: 对于已知可实现的曲率函数，神经网络达到极低损失(10⁻⁷-10⁻¹⁰)；对于不可实现的曲率函数，损失显著更高。这种区分能力使得能够评估未知情况，分离可能可实现的函数与不可实现的函数。

Conclusion: Nirenberg神经网络展示了神经求解器可以作为几何分析中的探索工具，为长期存在性问题提供定量的计算视角，在曲率可实现性评估方面表现出色。

Abstract: This work introduces the Nirenberg Neural Network: a numerical approach to the Nirenberg problem of prescribing Gaussian curvature on $S^2$ for metrics that are pointwise conformal to the round metric. Our mesh-free physics-informed neural network (PINN) approach directly parametrises the conformal factor globally and is trained with a geometry-aware loss enforcing the curvature equation. Additional consistency checks were performed via the Gauss-Bonnet theorem, and spherical-harmonic expansions were fit to the learnt models to provide interpretability.
  For prescribed curvatures with known realisability, the neural network achieves very low losses ($10^{-7} - 10^{-10}$), while unrealisable curvatures yield significantly higher losses. This distinction enables the assessment of unknown cases, separating likely realisable functions from non-realisable ones. The current capabilities of the Nirenberg Neural Network demonstrate that neural solvers can serve as exploratory tools in geometric analysis, offering a quantitative computational perspective on longstanding existence questions.

</details>


### [50] [Policy4OOD: A Knowledge-Guided World Model for Policy Intervention Simulation against the Opioid Overdose Crisis](https://arxiv.org/abs/2602.12373)
*Yijun Ma,Zehong Wang,Weixiang Sun,Zheyuan Zhang,Kaiwen Shi,Nitesh Chawla,Yanfang Ye*

Main category: cs.LG

TL;DR: 提出Policy4OOD世界模型，通过知识引导的时空建模统一预测、反事实推理和政策优化，用于应对阿片类药物危机。


<details>
  <summary>Details</summary>
Motivation: 阿片类药物危机是美国严重的公共卫生危机，但政策干预评估困难：多种政策在动态系统中相互作用，针对一个风险路径可能无意中放大另一个风险。需要能够预测当前政策下的未来结果、进行替代过去决策的反事实推理、以及对候选干预措施进行优化的能力。

Method: 提出Policy4OOD知识引导的时空世界模型，联合编码政策知识图谱、州级空间依赖关系和社会经济时间序列到政策条件化的Transformer中，预测未来阿片类药物结果。训练后的世界模型作为模拟器：预测只需前向传播，反事实分析在历史序列中替换替代政策编码，政策优化在学习的模拟器上使用蒙特卡洛树搜索。

Result: 实验表明，空间依赖性和结构化政策知识显著提高了预测准确性，验证了每个架构组件以及世界建模在数据驱动的公共卫生决策支持中的潜力。

Conclusion: 通过世界建模统一预测、反事实推理和政策优化，为应对阿片类药物危机提供了有效的政策评估框架，展示了数据驱动的公共卫生决策支持的新途径。

Abstract: The opioid epidemic remains one of the most severe public health crises in the United States, yet evaluating policy interventions before implementation is difficult: multiple policies interact within a dynamic system where targeting one risk pathway may inadvertently amplify another. We argue that effective opioid policy evaluation requires three capabilities -- forecasting future outcomes under current policies, counterfactual reasoning about alternative past decisions, and optimization over candidate interventions -- and propose to unify them through world modeling. We introduce Policy4OOD, a knowledge-guided spatio-temporal world model that addresses three core challenges: what policies prescribe, where effects manifest, and when effects unfold.Policy4OOD jointly encodes policy knowledge graphs, state-level spatial dependencies, and socioeconomic time series into a policy-conditioned Transformer that forecasts future opioid outcomes.Once trained, the world model serves as a simulator: forecasting requires only a forward pass, counterfactual analysis substitutes alternative policy encodings in the historical sequence, and policy optimization employs Monte Carlo Tree Search over the learned simulator. To support this framework, we construct a state-level monthly dataset (2019--2024) integrating opioid mortality, socioeconomic indicators, and structured policy encodings. Experiments demonstrate that spatial dependencies and structured policy knowledge significantly improve forecasting accuracy, validating each architectural component and the potential of world modeling for data-driven public health decision support.

</details>


### [51] [Value Bonuses using Ensemble Errors for Exploration in Reinforcement Learning](https://arxiv.org/abs/2602.12375)
*Abdul Wahab,Raksha Kumaraswamy,Martha White*

Main category: cs.LG

TL;DR: VBE算法通过随机动作价值函数集合的估计误差设计价值奖励，实现首次访问乐观探索，在经典环境和Atari上优于Bootstrap DQN和奖励奖励方法


<details>
  <summary>Details</summary>
Motivation: 现有基于乐观价值估计的探索方法存在局限性：价值奖励只能在学习到更高奖励奖励后回溯性增加，无法鼓励智能体首次访问某个状态-动作对，这限制了探索效率

Method: 提出VBE算法，维护随机动作价值函数集合，利用这些函数的估计误差设计价值奖励。关键创新是设计这些函数的奖励，使得价值奖励可以降为零，从而实现首次访问乐观和深度探索

Result: VBE在多个经典探索测试环境中优于Bootstrap DQN和两种奖励奖励方法（RND和ACB），并能轻松扩展到Atari等复杂环境

Conclusion: VBE通过随机动作价值函数集合的估计误差设计价值奖励，有效解决了现有探索方法无法鼓励首次访问的问题，实现了更高效的探索策略

Abstract: Optimistic value estimates provide one mechanism for directed exploration in reinforcement learning (RL). The agent acts greedily with respect to an estimate of the value plus what can be seen as a value bonus. The value bonus can be learned by estimating a value function on reward bonuses, propagating local uncertainties around rewards. However, this approach only increases the value bonus for an action retroactively, after seeing a higher reward bonus from that state and action. Such an approach does not encourage the agent to visit a state and action for the first time. In this work, we introduce an algorithm for exploration called Value Bonuses with Ensemble errors (VBE), that maintains an ensemble of random action-value functions (RQFs). VBE uses the errors in the estimation of these RQFs to design value bonuses that provide first-visit optimism and deep exploration. The key idea is to design the rewards for these RQFs in such a way that the value bonus can decrease to zero. We show that VBE outperforms Bootstrap DQN and two reward bonus approaches (RND and ACB) on several classic environments used to test exploration and provide demonstrative experiments that it can scale easily to more complex environments like Atari.

</details>


### [52] [Deep Doubly Debiased Longitudinal Effect Estimation with ICE G-Computation](https://arxiv.org/abs/2602.12379)
*Wenxin Chen,Weishen Pan,Kyra Gan,Fei Wang*

Main category: cs.LG

TL;DR: D3-Net是一个用于估计纵向治疗效果的新框架，通过双重稳健伪结果训练ICE序列来减少误差传播，并使用多任务Transformer和最终LTMLE校正来提高估计的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 纵向治疗效果估计对于序列决策至关重要，但由于治疗-混杂因素反馈而具有挑战性。传统的ICE G-computation方法存在误差传播问题，会污染学习的结果回归模型。

Method: 1. 使用序列双重稳健(SDR)伪结果训练ICE序列，为每个回归提供偏差校正目标，中断误差传播
2. 采用多任务Transformer，包含协变量模拟头进行辅助监督，正则化表示以抵抗噪声伪结果的污染
3. 使用目标网络稳定训练动态
4. 最终估计阶段丢弃SDR校正，使用未校正的干扰模型对原始结果执行纵向目标最小损失估计(LTMLE)

Result: 综合实验表明，D3-Net在不同时间范围、反事实和时变混杂情况下，相比现有最先进的ICE基估计器，能够稳健地减少偏差和方差。

Conclusion: D3-Net通过结合SDR伪结果训练、多任务Transformer正则化和最终LTMLE校正，有效解决了ICE G-computation中的误差传播问题，提供了更稳健的纵向治疗效果估计。

Abstract: Estimating longitudinal treatment effects is essential for sequential decision-making but is challenging due to treatment-confounder feedback. While Iterative Conditional Expectation (ICE) G-computation offers a principled approach, its recursive structure suffers from error propagation, corrupting the learned outcome regression models. We propose D3-Net, a framework that mitigates error propagation in ICE training and then applies a robust final correction. First, to interrupt error propagation during learning, we train the ICE sequence using Sequential Doubly Robust (SDR) pseudo-outcomes, which provide bias-corrected targets for each regression. Second, we employ a multi-task Transformer with a covariate simulator head for auxiliary supervision, regularizing representations against corruption by noisy pseudo-outcomes, and a target network to stabilize training dynamics. For the final estimate, we discard the SDR correction and instead use the uncorrected nuisance models to perform Longitudinal Targeted Minimum Loss-Based Estimation (LTMLE) on the original outcomes. This second-stage, targeted debiasing ensures robustness and optimal finite-sample properties. Comprehensive experiments demonstrate that our model, D3-Net, robustly reduces bias and variance across different horizons, counterfactuals, and time-varying confoundings, compared to existing state-of-the-art ICE-based estimators.

</details>


### [53] [TFT-ACB-XML: Decision-Level Integration of Customized Temporal Fusion Transformer and Attention-BiLSTM with XGBoost Meta-Learner for BTC Price Forecasting](https://arxiv.org/abs/2602.12380)
*Raiz Ud Din,Saddam Hussain Khan*

Main category: cs.LG

TL;DR: 提出TFT-ACB-XML混合堆叠泛化框架，结合定制TFT和ACB模型，通过XGBoost元学习器预测比特币收盘价，在测试期取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 比特币预测面临非线性、高波动性和时间不规则性挑战，现有深度学习模型在可解释性和跨市场条件泛化能力方面存在不足。

Method: 使用混合堆叠泛化框架：1) 定制TFT处理长程依赖和全局时间动态；2) ACB模块（注意力定制BiLSTM）捕捉短期序列依赖；3) 基于验证性能的误差倒数加权策略；4) XGBoost元学习器捕捉非线性残差并生成最终预测。

Result: 在2014年10月1日至2026年1月5日BTC数据上验证，相比现有深度学习和Transformer基线模型表现更优。测试期一步前向预测的MAPE为0.65%，MAE为198.15，RMSE为258.30，涵盖2024年减半和ETF时期。

Conclusion: TFT-ACB-XML框架有效解决了比特币预测的挑战，在波动市场条件下表现出色，为加密货币价格预测提供了可解释且泛化能力强的解决方案。

Abstract: Accurate forecasting of Bitcoin (BTC) has always been a challenge because decentralized markets are non-linear, highly volatile, and have temporal irregularities. Existing deep learning models often struggle with interpretability and generalization across diverse market conditions. This research presents a hybrid stacked-generalization framework, TFT-ACB-XML, for BTC closing price prediction. The framework integrates two parallel base learners: a customized Temporal Fusion Transformer (TFT) and an Attention-Customized Bidirectional Long Short-Term Memory network (ACB), followed by an XGBoost regressor as the meta-learner. The customized TFT model handles long-range dependencies and global temporal dynamics via variable selection networks and interpretable single-head attention. The ACB module uses a new attention mechanism alongside the customized BiLSTM to capture short-term sequential dependencies. Predictions from both customized TFT and ACB are weighted through an error-reciprocal weighting strategy. These weights are derived from validation performance, where a model showing lower prediction error receives a higher weight. Finally, the framework concatenates these weighted outputs into a feature vector and feeds the vector to an XGBoost regressor, which captures non-linear residuals and produces the final BTC closing price prediction. Empirical validation using BTC data from October 1, 2014, to January 5, 2026, shows improved performance of the proposed framework compared to recent Deep Learning and Transformer baseline models. The results show a MAPE of 0.65%, an MAE of 198.15, and an RMSE of 258.30 for one-step-ahead out-of-sample under a walk-forward evaluation on the test block. The evaluation period spans the 2024 BTC halving and the spot ETFs (exchange-traded funds) period, which coincide with major liquidity and volatility shifts.

</details>


### [54] [Why Deep Jacobian Spectra Separate: Depth-Induced Scaling and Singular-Vector Alignment](https://arxiv.org/abs/2602.12384)
*Nathanaël Haas,Francçois Gatine,Augustin M Cosse,Zied Bouraoui*

Main category: cs.LG

TL;DR: 论文提出了一种分析深度网络隐式偏置的新方法，基于深度雅可比矩阵的两个特征：深度诱导的有序奇异值指数缩放和强谱分离，在固定门控视角下证明了李雅普诺夫指数的存在，并展示了奇异值动态的有效解耦。


<details>
  <summary>Details</summary>
Motivation: 理解深度网络中基于梯度的训练为何表现出强隐式偏置具有挑战性，因为通常只有在平衡的深度线性模型中才能获得可处理的奇异值动态。需要一种替代方法来分析深度雅可比矩阵的特征。

Method: 采用分段线性网络的固定门控视角，将雅可比矩阵简化为单个激活区域内掩码线性映射的乘积。证明了初始化时控制顶部奇异值的李雅普诺夫指数的存在性，给出了可处理掩码模型中的闭式表达式，并量化了有限深度修正。

Result: 证明了足够强的谱分离会迫使矩阵乘积中的奇异向量对齐，产生中间雅可比矩阵的近似共享奇异基。这些结果共同激发了一个近似机制，其中奇异值动态变得有效解耦，无需平衡即可镜像经典的平衡深度线性分析。

Conclusion: 实验验证了预测的缩放、对齐和动态特性，支持将涌现的低秩雅可比结构作为驱动隐式偏置的机制性解释。该方法为理解深度网络训练动态提供了新视角。

Abstract: Understanding why gradient-based training in deep networks exhibits strong implicit bias remains challenging, in part because tractable singular-value dynamics are typically available only for balanced deep linear models. We propose an alternative route based on two theoretically grounded and empirically testable signatures of deep Jacobians: depth-induced exponential scaling of ordered singular values and strong spectral separation. Adopting a fixed-gates view of piecewise-linear networks, where Jacobians reduce to products of masked linear maps within a single activation region, we prove the existence of Lyapunov exponents governing the top singular values at initialization, give closed-form expressions in a tractable masked model, and quantify finite-depth corrections. We further show that sufficiently strong separation forces singular-vector alignment in matrix products, yielding an approximately shared singular basis for intermediate Jacobians. Together, these results motivate an approximation regime in which singular-value dynamics become effectively decoupled, mirroring classical balanced deep-linear analyses without requiring balancing. Experiments in fixed-gates settings validate the predicted scaling, alignment, and resulting dynamics, supporting a mechanistic account of emergent low-rank Jacobian structure as a driver of implicit bias.

</details>


### [55] [Rational Neural Networks have Expressivity Advantages](https://arxiv.org/abs/2602.12390)
*Maosen Tang,Alex Townsend*

Main category: cs.LG

TL;DR: 可训练低阶有理激活函数比传统固定激活函数更高效，在逼近误差ε下，有理网络只需poly(loglog(1/ε))规模即可逼近传统网络，而传统网络需要Ω(log(1/ε))参数，存在指数级差距。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络使用固定激活函数（如ReLU、Sigmoid等）在表达能力和参数效率上存在限制，需要探索更高效的激活函数来提升网络性能。

Method: 提出使用可训练的低阶有理函数作为激活函数，研究其在理论逼近能力和实际应用中的表现，并与传统固定激活函数进行对比分析。

Result: 理论证明：有理激活网络只需poly(loglog(1/ε))规模即可逼近传统网络，而传统网络需要Ω(log(1/ε))参数，存在指数级差距。实际应用中，有理激活函数能无缝集成到标准架构中，在相同条件下匹配或超越传统激活函数。

Conclusion: 可训练有理激活函数在理论表达能力和实际性能上都优于传统固定激活函数，为神经网络设计提供了更高效的选择。

Abstract: We study neural networks with trainable low-degree rational activation functions and show that they are more expressive and parameter-efficient than modern piecewise-linear and smooth activations such as ELU, LeakyReLU, LogSigmoid, PReLU, ReLU, SELU, CELU, Sigmoid, SiLU, Mish, Softplus, Tanh, Softmin, Softmax, and LogSoftmax. For an error target of $\varepsilon>0$, we establish approximation-theoretic separations: Any network built from standard fixed activations can be uniformly approximated on compact domains by a rational-activation network with only $\mathrm{poly}(\log\log(1/\varepsilon))$ overhead in size, while the converse provably requires $Ω(\log(1/\varepsilon))$ parameters in the worst case. This exponential gap persists at the level of full networks and extends to gated activations and transformer-style nonlinearities. In practice, rational activations integrate seamlessly into standard architectures and training pipelines, allowing rationals to match or outperform fixed activations under identical architectures and optimizers.

</details>


### [56] [High-dimensional Level Set Estimation with Trust Regions and Double Acquisition Functions](https://arxiv.org/abs/2602.12391)
*Giang Ngo,Dat Phan Trong,Dang Nguyen,Sunil Gupta*

Main category: cs.LG

TL;DR: TRLSE是一种用于高维水平集估计的主动学习算法，通过全局和局部双采集函数识别并细化阈值边界区域，在样本效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 高维空间中的水平集估计面临挑战，因为搜索空间随维度增加呈指数级增长，而主动学习设置中初始数据有限，需要高效获取信息点来构建准确分类器。

Method: 提出TRLSE算法，采用全局和局部双采集函数来识别和细化阈值边界区域，在全局层面探索潜在边界区域，在局部层面精确细化边界。

Result: 理论分析证明了TRLSE的准确性，在多个合成和真实世界水平集估计问题上的广泛评估显示，其样本效率优于现有方法。

Conclusion: TRLSE算法有效解决了高维水平集估计的挑战，通过双采集策略在有限数据下实现了高效的边界识别和分类器构建。

Abstract: Level set estimation (LSE) classifies whether an unknown function's value exceeds a specified threshold for given inputs, a fundamental problem in many real-world applications. In active learning settings with limited initial data, we aim to iteratively acquire informative points to construct an accurate classifier for this task. In high-dimensional spaces, this becomes challenging where the search volume grows exponentially with increasing dimensionality. We propose TRLSE, an algorithm for high-dimensional LSE, which identifies and refines regions near the threshold boundary with dual acquisition functions operating at both global and local levels. We provide a theoretical analysis of TRLSE's accuracy and show its superior sample efficiency against existing methods through extensive evaluations on multiple synthetic and real-world LSE problems.

</details>


### [57] [Synthetic Interaction Data for Scalable Personalization in Large Language Models](https://arxiv.org/abs/2602.12394)
*Yuchen Ma,Yue Huang,Wenjie Wang,Xiaonan Luo,Xiangliang Zhang,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: 提出PersonaGym框架生成高质量个性化交互数据，并开发PPOpt框架优化用户提示而不修改LLM模型


<details>
  <summary>Details</summary>
Motivation: 现有提示优化方法主要关注任务级优化，忽略了用户特定偏好和潜在约束，主要由于缺乏高质量隐私敏感数据和个人偏好奖励信号

Method: 1) PersonaGym：基于代理LLM系统模拟动态偏好行为生成合成数据；2) PersonaAtlas：大规模高质量个性化交互数据集；3) PPOpt：基于推理-优化范式，从交互历史推断用户画像并优化提示

Result: 实验表明在任务性能、个性化质量和鲁棒性方面均优于现有基线方法，对噪声和稀疏偏好信号具有良好鲁棒性

Conclusion: PersonaGym解决了数据稀缺问题，PPOpt提供可扩展的模型无关个性化提示优化框架，为LLM个性化部署提供了有效解决方案

Abstract: Personalized prompting offers large opportunities for deploying large language models (LLMs) to diverse users, yet existing prompt optimization methods primarily focus on task-level optimization while largely overlooking user-specific preferences and latent constraints of individual users. This gap is primarily due to (i) the absence of high-quality, privacy-sensitive data that capture personalized user-LLM interactions at scale, and (ii) the lack of robust reward signals for individual preferences. To overcome existing data limitations, we introduce a high-fidelity synthetic data generation framework called PersonaGym. Unlike prior work that treats personalization as static persona-preference pairs, PersonaGym models a dynamic preference process via an agentic LLM system to simulate realistic preference behaviors and semantic-aware noise in order to generate personalized multi-turn interaction trajectories. Using PersonaGym, we release PersonaAtlas, a large-scale, high-quality, and diverse synthetic dataset of high-fidelity multi-turn personalized interaction trajectories that closely mirror real-world preference expression and noise patterns. We further propose Personalized Prompt Optimization (PPOpt), a scalable and model-agnostic framework that optimizes user prompts based on interaction histories without modifying the deployed LLM. PPOpt adopts a reason-then-optimize paradigm that infers an explicit user profile and conditions prompt rewriting on the user profile to avoid reward hacking. Our training procedure for PPOpt integrates a cold-start supervised prior with outcome-driven multi-objective reinforcement learning. We present extensive experiments to demonstrate consistent improvements over state-of-the-art baselines in terms of task performance, personalization quality, and robustness to noisy as well as to sparse preference signals.

</details>


### [58] [AstRL: Analog and Mixed-Signal Circuit Synthesis with Deep Reinforcement Learning](https://arxiv.org/abs/2602.12402)
*Felicia B. Guo,Ken T. Ho,Andrei Vladimirescu,Borivoje Nikolic*

Main category: cs.LG

TL;DR: AstRL：一种基于深度强化学习的模拟混合信号电路自动综合方法，将电路设计建模为图生成问题，通过策略梯度方法直接优化用户指定目标，实现晶体管级细粒度拓扑生成。


<details>
  <summary>Details</summary>
Motivation: 模拟混合信号集成电路在现代计算和通信系统中至关重要，但设计复杂度持续上升而自动化进展有限。现有方法难以开发适用于不同、受限且不可微设计空间的通用优化方法。

Method: 将电路设计建模为图生成问题，提出基于深度强化学习的AstRL方法。采用策略梯度方法，在仿真器嵌入环境中直接生成针对用户指定目标优化的电路。通过行为克隆和基于判别器的相似性奖励实现专家对齐的通用电路生成。

Result: 在三个实际设计任务中，相比最先进基线方法，在传统设计指标上取得显著改进。100%的生成设计结构正确，超过90%的设计展示所需功能。

Conclusion: AstRL首次展示了专家对齐的通用电路生成范式，能够在晶体管级别实现高表达性、细粒度的拓扑生成，为模拟混合信号电路自动化设计提供了有前景的解决方案。

Abstract: Analog and mixed-signal (AMS) integrated circuits (ICs) lie at the core of modern computing and communications systems. However, despite the continued rise in design complexity, advances in AMS automation remain limited. This reflects the central challenge in developing a generalized optimization method applicable across diverse circuit design spaces, many of which are distinct, constrained, and non-differentiable. To address this, our work casts circuit design as a graph generation problem and introduces a novel method of AMS synthesis driven by deep reinforcement learning (AstRL). Based on a policy-gradient approach, AstRL generates circuits directly optimized for user-specified targets within a simulator-embedded environment that provides ground-truth feedback during training. Through behavioral-cloning and discriminator-based similarity rewards, our method demonstrates, for the first time, an expert-aligned paradigm for generalized circuit generation validated in simulation. Importantly, the proposed approach operates at the level of individual transistors, enabling highly expressive, fine-grained topology generation. Strong inductive biases encoded in the action space and environment further drive structurally consistent and valid generation. Experimental results for three realistic design tasks illustrate substantial improvements in conventional design metrics over state-of-the-art baselines, with 100% of generated designs being structurally correct and over 90% demonstrating required functionality.

</details>


### [59] [Soft Contamination Means Benchmarks Test Shallow Generalization](https://arxiv.org/abs/2602.12413)
*Ari Spiesberger,Juan J. Vazquez,Nicky Pochinkov,Tomáš Gavenčiak,Peli Grietzer,Gavin Leech,Nandi Schoots*

Main category: cs.LG

TL;DR: 论文研究发现LLM训练数据中存在大量语义重复的基准测试数据污染，这导致基准性能评估存在偏差，无法准确反映模型在真实分布外场景的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试数据去污染方法主要基于n-gram匹配，无法检测语义重复内容。当训练数据中包含语义重复的基准测试数据时，基准性能评估会产生偏差，无法准确衡量模型在真实分布外场景的泛化能力。

Method: 通过嵌入Olmo3训练语料库进行研究，分析语义重复污染情况。实验包括：1)检测训练数据中基准测试数据的语义重复；2)研究包含语义重复数据对基准性能的影响；3)分析在基准数据重复上进行微调对真正保留数据性能的影响。

Result: 研究发现：1)污染现象普遍存在，如78%的CodeForces问题和50%的ZebraLogic问题存在语义重复；2)包含基准数据的语义重复确实会提高基准性能；3)在基准数据重复上微调也能提高同一基准中真正保留数据的性能。

Conclusion: 近期基准性能提升存在混淆因素：既反映了真实能力提升，也反映了训练语料中积累的测试数据及其有效重复。语义重复污染普遍存在，导致基准评估无法准确反映模型在分布外场景的真实泛化能力。

Abstract: If LLM training data is polluted with benchmark test data, then benchmark performance gives biased estimates of out-of-distribution (OOD) generalization. Typical decontamination filters use n-gram matching which fail to detect semantic duplicates: sentences with equivalent (or near-equivalent) content that are not close in string space. We study this soft contamination of training data by semantic duplicates. Among other experiments, we embed the Olmo3 training corpus and find that: 1) contamination remains widespread, e.g. we find semantic duplicates for 78% of CodeForces and exact duplicates for 50% of ZebraLogic problems; 2) including semantic duplicates of benchmark data in training does improve benchmark performance; and 3) when finetuning on duplicates of benchmark datapoints, performance also improves on truly-held-out datapoints from the same benchmark. We argue that recent benchmark gains are thus confounded: the prevalence of soft contamination means gains reflect both genuine capability improvements and the accumulation of test data and effective test data in growing training corpora.

</details>


### [60] [Stabilizing Native Low-Rank LLM Pretraining](https://arxiv.org/abs/2602.12429)
*Paul Janson,Edouard Oyallon,Eugene Belilovsky*

Main category: cs.LG

TL;DR: 该论文提出Spectron方法，通过谱重归一化和正交化技术，实现了从头开始训练低秩分解的LLMs，无需全秩指导，解决了低秩训练中的不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 基础模型参数数量增长带来显著的计算和内存挑战，低秩分解是降低训练和推理成本的有前途途径，但现有方法缺乏从头训练纯低秩权重模型并匹配密集模型性能的稳定方案。

Method: 提出Spectron方法：谱重归一化与正交化，动态限制权重更新的谱范数（最大奇异值），解决低秩训练中的不稳定性和损失尖峰问题，实现端到端的因子化训练。

Result: 成功从头训练纯低秩分解的LLMs，无需辅助全秩指导，训练稳定且开销可忽略，建立了计算最优的低秩transformer缩放定律，展示了可预测的幂律行为和相对于密集模型改进的推理效率。

Conclusion: Spectron方法为从头训练低秩分解模型提供了稳定方案，解决了谱范数失控增长问题，实现了与密集模型相当的性能，同时显著降低了计算和内存成本。

Abstract: Foundation models have achieved remarkable success, yet their growing parameter counts pose significant computational and memory challenges. Low-rank factorization offers a promising route to reduce training and inference costs, but the community lacks a stable recipe for training models from scratch using exclusively low-rank weights while matching the performance of the dense model. We demonstrate that Large Language Models (LLMs) can be trained from scratch using exclusively low-rank factorized weights for all non-embedding matrices without auxiliary "full-rank" guidance required by prior methods. While native low-rank training often suffers from instability and loss spikes, we identify uncontrolled growth in the spectral norm (largest singular value) of the weight matrix update as the dominant factor. To address this, we introduce Spectron: Spectral renormalization with orthogonalization, which dynamically bounds the resultant weight updates based on the current spectral norms of the factors. Our method enables stable, end-to-end factorized training with negligible overhead. Finally, we establish compute-optimal scaling laws for natively low-rank transformers, demonstrating predictable power-law behavior and improved inference efficiency relative to dense models.

</details>


### [61] [Safe Reinforcement Learning via Recovery-based Shielding with Gaussian Process Dynamics Models](https://arxiv.org/abs/2602.12444)
*Alexander W. Goodall,Francesco Belardinelli*

Main category: cs.LG

TL;DR: 提出一种基于恢复的屏蔽框架，通过高斯过程不确定性量化预测安全约束违反，仅在必要时动态恢复至安全轨迹，实现安全强化学习。


<details>
  <summary>Details</summary>
Motivation: 强化学习在安全关键应用中缺乏可证明的安全保证，特别是对于未知和非线性连续动态系统。

Method: 集成备份策略（屏蔽）与RL代理，利用高斯过程不确定性量化预测安全约束违反，通过基于内部模型的采样进行策略优化。

Result: 在连续控制环境中表现出强大的性能和严格的安全合规性，实现无限制探索和样本高效学习。

Conclusion: 提出的恢复屏蔽框架为未知非线性连续系统提供了可证明的安全下界，实现了安全强化学习。

Abstract: Reinforcement learning (RL) is a powerful framework for optimal decision-making and control but often lacks provable guarantees for safety-critical applications. In this paper, we introduce a novel recovery-based shielding framework that enables safe RL with a provable safety lower bound for unknown and non-linear continuous dynamical systems. The proposed approach integrates a backup policy (shield) with the RL agent, leveraging Gaussian process (GP) based uncertainty quantification to predict potential violations of safety constraints, dynamically recovering to safe trajectories only when necessary. Experience gathered by the 'shielded' agent is used to construct the GP models, with policy optimization via internal model-based sampling - enabling unrestricted exploration and sample efficient learning, without compromising safety. Empirically our approach demonstrates strong performance and strict safety-compliance on a suite of continuous control environments.

</details>


### [62] [Computationally sufficient statistics for Ising models](https://arxiv.org/abs/2602.12449)
*Abhijith Jayakumar,Shreya Shukla,Marc Vuffray,Andrey Y. Lokhov,Sidhant Misra*

Main category: cs.LG

TL;DR: 该论文研究了在仅能观测到充分统计量而非完整样本配置的情况下，如何高效学习吉布斯分布（以伊辛模型为例），并探讨了计算能力与观测能力之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 在物理系统中，获取完整样本配置通常不切实际，而传统高效学习算法需要完整样本。因此需要开发仅基于有限统计量就能高效学习吉布斯分布的方法。

Method: 以伊辛模型为范例，通过观测到O(γ)阶的统计量（其中γ是模型的ℓ₁宽度）来重构模型参数。还讨论了在已知模型结构先验信息的情况下，可以用更有限的观测能力解决问题。

Result: 证明了通过观测O(γ)阶的统计量，可以重构具有ℓ₁宽度γ的模型参数，既能推断模型结构，也能学习其耦合参数和磁场参数。

Conclusion: 在仅能访问有限统计量的情况下，通过适当的计算-观测权衡，可以高效学习吉布斯分布，特别是在有结构先验信息时，所需的观测能力可以进一步降低。

Abstract: Learning Gibbs distributions using only sufficient statistics has long been recognized as a computationally hard problem. On the other hand, computationally efficient algorithms for learning Gibbs distributions rely on access to full sample configurations generated from the model. For many systems of interest that arise in physical contexts, expecting a full sample to be observed is not practical, and hence it is important to look for computationally efficient methods that solve the learning problem with access to only a limited set of statistics. We examine the trade-offs between the power of computation and observation within this scenario, employing the Ising model as a paradigmatic example. We demonstrate that it is feasible to reconstruct the model parameters for a model with $\ell_1$ width $γ$ by observing statistics up to an order of $O(γ)$. This approach allows us to infer the model's structure and also learn its couplings and magnetic fields. We also discuss a setting where prior information about structure of the model is available and show that the learning problem can be solved efficiently with even more limited observational power.

</details>


### [63] [Continuous Diffusion Models Can Obey Formal Syntax](https://arxiv.org/abs/2602.12468)
*Jinwoo Kim,Taylor Berg-Kirkpatrick,Loris D'Antoni*

Main category: cs.LG

TL;DR: 提出Diffinity方法，通过训练自由引导技术使连续扩散语言模型满足正则表达式约束，在JSON和自然语言任务上实现高约束满足率


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型具有全局、非因果生成过程的优势，但其连续潜在动态特性使得难以施加离散约束（如输出需匹配特定JSON模式）。需要一种方法能在不训练额外分类器的情况下引导扩散模型满足形式化语法约束。

Method: 提出训练自由的引导方法，构建分析性评分函数估计潜在状态解码后能被给定正则表达式接受的概率，利用其梯度引导采样过程。在Diffinity中实现，基于PLAID扩散模型，通过去噪过程针对语法有效性条件化的基础模型。

Result: 在180个正则表达式约束的JSON和自然语言基准测试中，Diffinity达到68-96%的约束满足率，相对于无约束采样仅产生较小的困惑度代价，在约束满足率和输出质量方面均优于自回归约束解码方法。

Conclusion: 该方法成功实现了对连续扩散语言模型施加形式化语法约束的训练自由引导，为扩散模型在需要精确结构输出的应用场景中提供了有效解决方案。

Abstract: Diffusion language models offer a promising alternative to autoregressive models due to their global, non-causal generation process, but their continuous latent dynamics make discrete constraints -- e.g., the output should be a JSON file that matches a given schema -- difficult to impose. We introduce a training-free guidance method for steering continuous diffusion language models to satisfy formal syntactic constraints expressed using regular expressions. Our approach constructs an analytic score estimating the probability that a latent state decodes to a valid string accepted by a given regular expression, and uses its gradient to guide sampling, without training auxiliary classifiers. The denoising process targets the base model conditioned on syntactic validity.
  We implement our method in Diffinity on top of the PLAID diffusion model and evaluate it on 180 regular-expression constraints over JSON and natural-language benchmarks. Diffinity achieves 68-96\% constraint satisfaction while incurring only a small perplexity cost relative to unconstrained sampling, outperforming autoregressive constrained decoding in both constraint satisfaction and output quality.

</details>


### [64] [Regularized Meta-Learning for Improved Generalization](https://arxiv.org/abs/2602.12469)
*Noor Islam S. Mohammad,Md Muntaqim Meherab*

Main category: cs.LG

TL;DR: 提出正则化元学习框架，通过四阶段流水线解决深度集成方法冗余、不稳定加权和过拟合问题，在基准测试中优于简单平均和传统Ridge堆叠


<details>
  <summary>Details</summary>
Motivation: 深度集成方法存在三个实际限制：基础模型冗余导致计算成本增加和条件恶化，多重共线性下的不稳定加权，以及元学习管道中的过拟合问题

Method: 四阶段流水线：冗余感知投影、统计元特征增强、交叉验证正则化元模型（Ridge、Lasso、ElasticNet）、逆RMSE混合。使用多指标去重策略（τ_corr=0.95）移除近共线预测器，设计集成统计量和交互项恢复高阶结构

Result: 在Playground Series S6E1基准测试中，OOF RMSE为8.582，优于简单平均（8.894）和传统Ridge堆叠（8.627），匹配贪婪爬山法（8.603）但运行时间快4倍。条件数减少53.7%

Conclusion: 正则化元学习为高维集成系统提供了稳定且部署高效的堆叠策略，去重、统计元特征和元集成混合均有显著贡献

Abstract: Deep ensemble methods often improve predictive performance, yet they suffer from three practical limitations: redundancy among base models that inflates computational cost and degrades conditioning, unstable weighting under multicollinearity, and overfitting in meta-learning pipelines. We propose a regularized meta-learning framework that addresses these challenges through a four-stage pipeline combining redundancy-aware projection, statistical meta-feature augmentation, and cross-validated regularized meta-models (Ridge, Lasso, and ElasticNet). Our multi-metric de-duplication strategy removes near-collinear predictors using correlation and MSE thresholds ($τ_{\text{corr}}=0.95$), reducing the effective condition number of the meta-design matrix while preserving predictive diversity. Engineered ensemble statistics and interaction terms recover higher-order structure unavailable to raw prediction columns. A final inverse-RMSE blending stage mitigates regularizer-selection variance. On the Playground Series S6E1 benchmark (100K samples, 72 base models), the proposed framework achieves an out-of-fold RMSE of 8.582, improving over simple averaging (8.894) and conventional Ridge stacking (8.627), while matching greedy hill climbing (8.603) with substantially lower runtime (4 times faster). Conditioning analysis shows a 53.7\% reduction in effective matrix condition number after redundancy projection. Comprehensive ablations demonstrate consistent contributions from de-duplication, statistical meta-features, and meta-ensemble blending. These results position regularized meta-learning as a stable and deployment-efficient stacking strategy for high-dimensional ensemble systems.

</details>


### [65] [Designing RNAs with Language Models](https://arxiv.org/abs/2602.12470)
*Milan Gautam,Ning Dai,Tianshuo Zhou,Bowen Xie,David Mathews,Liang Huang*

Main category: cs.LG

TL;DR: 将RNA设计重构为条件序列生成问题，使用自回归语言模型从目标结构直接生成序列，结合监督学习和强化学习优化，在多个数据集上超越现有方法


<details>
  <summary>Details</summary>
Motivation: RNA设计（寻找折叠成目标二级结构的序列）具有重要的生物学和生物医学意义，但由于序列空间指数级庞大且存在大量竞争性折叠，计算上极具挑战性。传统方法将其视为优化问题，依赖实例特定的启发式方法或基于约束的搜索。

Method: 将RNA设计重构为条件序列生成问题，引入可重用的神经近似器（自回归语言模型），直接从目标结构映射到序列。首先在随机诱导的结构-序列对上进行监督训练，然后使用强化学习优化端到端指标。还提出了选择强化学习子集的方法以提高效率和效果。

Result: 在四个数据集上，该方法在关键指标（如玻尔兹曼概率）上优于最先进的系统，同时速度快1.7倍，确立了条件语言模型生成作为RNA设计中可扩展、任务无关的替代方案。

Conclusion: 条件语言模型生成为RNA设计提供了一种可扩展、任务无关的替代方案，超越了传统的基于实例优化的方法，在效率和效果上都有显著提升。

Abstract: RNA design, the task of finding a sequence that folds into a target secondary structure, has broad biological and biomedical impact but remains computationally challenging due to the exponentially large sequence space and exponentially many competing folds. Traditional approaches treat it as an optimization problem, relying on per-instance heuristics or constraint-based search. We instead reframe RNA design as conditional sequence generation and introduce a reusable neural approximator, instantiated as an autoregressive language model (LM), that maps target structures directly to sequences. We first train our model in a supervised setting on random-induced structure-sequence pairs, and then use reinforcement learning (RL) to optimize end-to-end metrics. We also propose methods to select a small subset for RL that greatly improves RL efficiency and quality. Across four datasets, our approach outperforms state-of-the-art systems on key metrics such as Boltzmann probability while being 1.7x faster, establishing conditional LM generation as a scalable, task-agnostic alternative to per-instance optimization for RNA design. Our code and data are available at https://github.com/KuNyaa/RNA-Design-LM.

</details>


### [66] [Tight Bounds for Logistic Regression with Large Stepsize Gradient Descent in Low Dimension](https://arxiv.org/abs/2602.12471)
*Michael Crawshaw,Mingrui Liu*

Main category: cs.LG

TL;DR: 本文针对二维可分数据上的逻辑回归问题，分析了梯度下降在大学习率下的收敛行为，证明了在特定条件下可以获得比传统分析更快的收敛速率。


<details>
  <summary>Details</summary>
Motivation: 最近研究表明，对于可分数据的逻辑回归，通过选择大学习率可以获得加速的1/T²收敛速率，尽管这会导致损失函数非单调。然而，现有分析不够紧致，特别是在理解梯度下降从不稳定（非单调损失）到稳定（单调损失）的过渡时间方面。

Method: 针对二维数据集，对梯度下降进行更精细的分析。特别关注梯度下降在正交于最大间隔分类器的子空间中的振荡动力学，通过严格分析GD从不稳定到稳定的过渡时间τ，获得更紧致的收敛界。

Result: 证明了当T ≥ Ω(n/γ + 1/γ²)时，GD在大学习率η下能找到损失小于O(1/(ηT))的点。提供了过渡时间τ的上界和下界，两者在log因子内匹配，表明分析是紧致的。

Conclusion: 本文为二维可分数据的逻辑回归梯度下降提供了更精细的分析，揭示了在大学习率下从不稳定到稳定行为的过渡机制，并获得了比现有结果更紧致的收敛速率。

Abstract: We consider the optimization problem of minimizing the logistic loss with gradient descent to train a linear model for binary classification with separable data. With a budget of $T$ iterations, it was recently shown that an accelerated $1/T^2$ rate is possible by choosing a large step size $η= Θ(γ^2 T)$ (where $γ$ is the dataset's margin) despite the resulting non-monotonicity of the loss. In this paper, we provide a tighter analysis of gradient descent for this problem when the data is two-dimensional: we show that GD with a sufficiently large learning rate $η$ finds a point with loss smaller than $\mathcal{O}(1/(ηT))$, as long as $T \geq Ω(n/γ+ 1/γ^2)$, where $n$ is the dataset size. Our improved rate comes from a tighter bound on the time $τ$ that it takes for GD to transition from unstable (non-monotonic loss) to stable (monotonic loss), via a fine-grained analysis of the oscillatory dynamics of GD in the subspace orthogonal to the max-margin classifier. We also provide a lower bound of $τ$ matching our upper bound up to logarithmic factors, showing that our analysis is tight.

</details>


### [67] [Geometric separation and constructive universal approximation with two hidden layers](https://arxiv.org/abs/2602.12482)
*Chanyoung Sung*

Main category: cs.LG

TL;DR: 提出一种几何构造方法，使两层隐藏层的神经网络能够分离ℝⁿ中的不相交紧致子集，并得到构造性的通用逼近定理


<details>
  <summary>Details</summary>
Motivation: 为神经网络提供几何构造方法，证明具有两层隐藏层的神经网络（使用sigmoid或ReLU激活函数）能够分离ℝⁿ中的不相交紧致子集，并建立构造性的通用逼近理论

Method: 采用几何构造方法，构建具有两层隐藏层的神经网络，支持sigmoid（严格单调有界连续）或ReLU激活函数，能够分离ℝⁿ中的不相交紧致子集

Result: 证明了具有两层隐藏层的神经网络可以在任意紧致集K⊂ℝⁿ上以任意精度一致逼近任何实值连续函数；对于有限集K，构造简化并得到深度2（单隐藏层）的尖锐逼近结果

Conclusion: 通过几何构造方法，建立了神经网络分离紧致子集的能力，并提供了构造性的通用逼近定理，为两层隐藏层神经网络的逼近能力提供了理论保证

Abstract: We give a geometric construction of neural networks that separate disjoint compact subsets of $\Bbb R^n$, and use it to obtain a constructive universal approximation theorem. Specifically, we show that networks with two hidden layers and either a sigmoidal activation (i.e., strictly monotone bounded continuous) or the ReLU activation can approximate any real-valued continuous function on an arbitrary compact set $K\subset\Bbb R^n$ to any prescribed accuracy in the uniform norm. For finite $K$, the construction simplifies and yields a sharp depth-2 (single hidden layer) approximation result.

</details>


### [68] [A Theoretical Analysis of Mamba's Training Dynamics: Filtering Relevant Features for Generalization in State Space Models](https://arxiv.org/abs/2602.12499)
*Mugunthan Shandirasegaran,Hongkang Li,Songyang Zhang,Meng Wang,Shuai Zhang*

Main category: cs.LG

TL;DR: 该论文首次对Mamba风格的选择性状态空间模型（SSM）的理论基础进行了分析，证明了在结构化数据模型下，简化的Mamba块能够实现有保证的泛化，其门控机制能够选择类别相关特征而忽略无关特征。


<details>
  <summary>Details</summary>
Motivation: Mamba和其他选择性状态空间模型在序列建模中取得了经验成功，但其理论基础尚未得到充分探索。研究者希望理解这些非注意力架构何时以及为何能够高效学习，为Transformer中心化的解释提供理论对照。

Method: 研究采用简化的Mamba块：单层单头选择性SSM，具有输入依赖的门控机制，后接两层MLP，通过梯度下降训练。分析基于结构化数据模型，包含类别相关和类别无关模式以及令牌级噪声，考察多数投票和局部结构化数据两种典型机制。

Result: 证明了模型能够实现有保证的泛化，建立了非渐近样本复杂度和收敛率界限，这些界限随着有效信号增加和噪声减少而改善。门控向量能够与类别相关特征对齐而忽略无关特征，实现了类似注意力但通过选择性递归实现的特征选择作用。

Conclusion: 该研究为Mamba风格选择性SSM的高效学习提供了原则性见解，阐明了其门控机制如何实现特征选择，为理解非注意力序列建模架构提供了理论基础。

Abstract: The recent empirical success of Mamba and other selective state space models (SSMs) has renewed interest in non-attention architectures for sequence modeling, yet their theoretical foundations remain underexplored. We present a first-step analysis of generalization and learning dynamics for a simplified but representative Mamba block: a single-layer, single-head selective SSM with input-dependent gating, followed by a two-layer MLP trained via gradient descent (GD). Our study adopts a structured data model with tokens that include both class-relevant and class-irrelevant patterns under token-level noise and examines two canonical regimes: majority-voting and locality-structured data sequences. We prove that the model achieves guaranteed generalization by establishing non-asymptotic sample complexity and convergence rate bounds, which improve as the effective signal increases and the noise decreases. Furthermore, we show that the gating vector aligns with class-relevant features while ignoring irrelevant ones, thereby formalizing a feature-selection role similar to attention but realized through selective recurrence. Numerical experiments on synthetic data justify our theoretical results. Overall, our results provide principled insight into when and why Mamba-style selective SSMs learn efficiently, offering a theoretical counterpoint to Transformer-centric explanations.

</details>


### [69] [On Robustness and Chain-of-Thought Consistency of RL-Finetuned VLMs](https://arxiv.org/abs/2602.12506)
*Rosie Zhao,Anshul Shah,Xiaoyu Zhu,Xinke Deng,Zhongyu Jiang,Yang Yang,Joerg Liebelt,Arnab Mondal*

Main category: cs.LG

TL;DR: 研究发现RL微调的视觉语言模型在视觉推理任务中虽然提升了基准准确率，但存在视觉基础薄弱、幻觉和过度依赖文本线索等脆弱性，揭示了准确性与忠实性之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 随着RL微调在增强大语言模型推理能力方面取得成功，研究者希望将其扩展到视觉语言模型。然而，现有的RL微调VLMs虽然在视觉推理基准上有所改进，但仍存在视觉基础薄弱、幻觉和过度依赖文本线索等脆弱性问题，需要深入分析这些模型的鲁棒性和可靠性。

Method: 通过简单的受控文本扰动（误导性标题或错误的思维链痕迹）来测试模型的鲁棒性；使用基于熵的指标分析模型不确定性和正确选项的概率分布；分析RL微调动态，研究准确性与忠实性之间的权衡；探索对抗性增强和忠实性感知奖励对模型性能的影响。

Result: 文本扰动导致模型鲁棒性和置信度显著下降，特别是在考虑思维链一致性时；RL微调虽然提高了基准准确率，但会削弱思维链的可靠性和对上下文变化的鲁棒性；对抗性增强能提高鲁棒性但无法防止忠实性漂移；忠实性感知奖励可以恢复答案与推理的对齐，但与增强结合时可能导致训练塌陷到捷径策略。

Conclusion: 仅基于准确率的评估存在局限性，需要开发同时强调正确性、鲁棒性和视觉基础推理忠实性的训练和评估协议，以确保VLMs在实际应用中的可靠性和可信度。

Abstract: Reinforcement learning (RL) fine-tuning has become a key technique for enhancing large language models (LLMs) on reasoning-intensive tasks, motivating its extension to vision language models (VLMs). While RL-tuned VLMs improve on visual reasoning benchmarks, they remain vulnerable to weak visual grounding, hallucinations, and over-reliance on textual cues. We show that simple, controlled textual perturbations--misleading captions or incorrect chain-of-thought (CoT) traces--cause substantial drops in robustness and confidence, and that these effects are more pronounced when CoT consistency is taken into account across open-source multimodal reasoning models. Entropy-based metrics further show that these perturbations reshape model uncertainty and probability mass on the correct option, exposing model-specific trends in miscalibration. To better understand these vulnerabilities, we further analyze RL fine-tuning dynamics and uncover an accuracy-faithfulness trade-off: fine-tuning raises benchmark accuracy, but can simultaneously erode the reliability of the accompanying CoT and its robustness to contextual shifts. Although adversarial augmentation improves robustness, it does not by itself prevent faithfulness drift. Incorporating a faithfulness-aware reward can restore alignment between answers and reasoning, but when paired with augmentation, training risks collapsing onto shortcut strategies and robustness remains elusive. Together, these findings highlight the limitations of accuracy-only evaluations and motivate training and assessment protocols that jointly emphasize correctness, robustness, and the faithfulness of visually grounded reasoning.

</details>


### [70] [Bench-MFG: A Benchmark Suite for Learning in Stationary Mean Field Games](https://arxiv.org/abs/2602.12517)
*Lorenzo Magnino,Jiacheng Shen,Matthieu Geist,Olivier Pietquin,Mathieu Laurière*

Main category: cs.LG

TL;DR: 该论文提出了Bench-MFG，一个用于Mean Field Games（MFGs）和强化学习（RL）算法的标准化基准套件，包含问题分类、原型环境和随机实例生成方法，并对多种学习算法进行了基准测试。


<details>
  <summary>Details</summary>
Motivation: 当前MFG与RL交叉领域缺乏标准化的评估协议，研究者依赖自定义、孤立且简单的环境，导致难以评估新方法的鲁棒性、泛化能力和失败模式。

Method: 提出了Bench-MFG基准套件，专注于离散时间、离散空间、平稳设置；引入了问题分类学（从无交互到动态耦合游戏）；提出了MF-Garnets方法生成随机MFG实例；并测试了多种学习算法，包括新颖的黑盒方法MF-PSO。

Result: 通过广泛的实证结果，展示了不同算法在各种MFG环境中的表现，并基于这些结果提出了标准化未来实验比较的指导方针。

Conclusion: Bench-MFG为MFG-RL领域提供了首个全面的基准套件，有助于标准化评估、促进算法比较，并推动该领域的发展。

Abstract: The intersection of Mean Field Games (MFGs) and Reinforcement Learning (RL) has fostered a growing family of algorithms designed to solve large-scale multi-agent systems. However, the field currently lacks a standardized evaluation protocol, forcing researchers to rely on bespoke, isolated, and often simplistic environments. This fragmentation makes it difficult to assess the robustness, generalization, and failure modes of emerging methods. To address this gap, we propose a comprehensive benchmark suite for MFGs (Bench-MFG), focusing on the discrete-time, discrete-space, stationary setting for the sake of clarity. We introduce a taxonomy of problem classes, ranging from no-interaction and monotone games to potential and dynamics-coupled games, and provide prototypical environments for each. Furthermore, we propose MF-Garnets, a method for generating random MFG instances to facilitate rigorous statistical testing. We benchmark a variety of learning algorithms across these environments, including a novel black-box approach (MF-PSO) for exploitability minimization. Based on our extensive empirical results, we propose guidelines to standardize future experimental comparisons. Code available at \href{https://github.com/lorenzomagnino/Bench-MFG}{https://github.com/lorenzomagnino/Bench-MFG}.

</details>


### [71] [QTabGAN: A Hybrid Quantum-Classical GAN for Tabular Data Synthesis](https://arxiv.org/abs/2602.12704)
*Subhangi Kumari,Rakesh Achutha,Vignesh Sivaraman*

Main category: cs.LG

TL;DR: QTabGAN是一种混合量子-经典生成对抗框架，用于合成表格数据，在数据稀缺或隐私受限场景下表现优异，相比现有方法在分类数据集上提升达54.07%


<details>
  <summary>Details</summary>
Motivation: 表格数据合成面临特征类型异构和高维度的挑战，特别是在真实数据稀缺或受隐私限制的场景下，需要更有效的生成模型

Method: 提出QTabGAN混合量子-经典生成对抗框架：利用量子电路学习复杂数据分布，然后通过经典神经网络映射到表格特征

Result: 在多个分类和回归数据集上评估，相比领先的生成模型，QTabGAN在分类数据集上实现了高达54.07%的性能提升

Conclusion: QTabGAN为表格数据合成提供了可扩展的量子方法，展示了量子辅助生成建模的潜力，特别适用于数据稀缺或隐私受限场景

Abstract: Synthesizing realistic tabular data is challenging due to heterogeneous feature types and high dimensionality. We introduce QTabGAN, a hybrid quantum-classical generative adversarial framework for tabular data synthesis. QTabGAN is especially designed for settings where real data are scarce or restricted by privacy constraints. The model exploits the expressive power of quantum circuits to learn complex data distributions, which are then mapped to tabular features using classical neural networks. We evaluate QTabGAN on multiple classification and regression datasets and benchmark it against leading state-of-the-art generative models. Experiments show that QTabGAN achieves up to 54.07% improvement across various classification datasets and evaluation metrics, thus establishing a scalable quantum approach to tabular data synthesis and highlighting its potential for quantum-assisted generative modelling.

</details>


### [72] [Multi-Agent Model-Based Reinforcement Learning with Joint State-Action Learned Embeddings](https://arxiv.org/abs/2602.12520)
*Zhizun Wang,David Meger*

Main category: cs.LG

TL;DR: 提出一种基于模型的多智能体强化学习框架，结合联合状态-动作表示学习与想象轨迹，通过SALE嵌入提升多智能体在部分可观测动态环境中的协调能力


<details>
  <summary>Details</summary>
Motivation: 在多智能体部分可观测且高度动态的环境中，需要信息丰富的表示和数据高效的训练方法。现有方法在联合状态-动作表示学习和数据效率方面存在挑战。

Method: 提出基于模型的多智能体强化学习框架，使用变分自编码器训练世界模型，引入状态-动作学习嵌入(SALE)。SALE嵌入注入到想象模块（预测未来轨迹）和联合智能体网络（通过混合网络组合个体动作值估计联合动作值函数）。

Result: 在StarCraft II微操、Multi-Agent MuJoCo和Level-Based Foraging等基准测试中，该方法相比基线算法取得一致性能提升，验证了联合状态-动作学习嵌入在多智能体基于模型范式中的有效性。

Conclusion: 通过将想象轨迹与基于SALE的动作值相结合，智能体能更好地理解其选择对集体结果的影响，从而在有限真实环境交互下实现改进的长期规划和优化。

Abstract: Learning to coordinate many agents in partially observable and highly dynamic environments requires both informative representations and data-efficient training. To address this challenge, we present a novel model-based multi-agent reinforcement learning framework that unifies joint state-action representation learning with imaginative roll-outs. We design a world model trained with variational auto-encoders and augment the model using the state-action learned embedding (SALE). SALE is injected into both the imagination module that forecasts plausible future roll-outs and the joint agent network whose individual action values are combined through a mixing network to estimate the joint action-value function. By coupling imagined trajectories with SALE-based action values, the agents acquire a richer understanding of how their choices influence collective outcomes, leading to improved long-term planning and optimization under limited real-environment interactions. Empirical studies on well-established multi-agent benchmarks, including StarCraft II Micro-Management, Multi-Agent MuJoCo, and Level-Based Foraging challenges, demonstrate consistent gains of our method over baseline algorithms and highlight the effectiveness of joint state-action learned embeddings within a multi-agent model-based paradigm.

</details>


### [73] [Constraint-Rectified Training for Efficient Chain-of-Thought](https://arxiv.org/abs/2602.12526)
*Qinhang Wu,Sen Lin,Ming Zhang,Yingbin Liang,Ness B. Shroff*

Main category: cs.LG

TL;DR: CRT框架通过约束优化方法，在保持答案质量的同时显著减少推理长度和令牌使用，实现高效推理


<details>
  <summary>Details</summary>
Motivation: 现有CoT推理方法存在推理长度过长、推理成本高、冗余步骤多（过度思考）的问题，而基于启发式的方法存在准确性下降严重、对超参数敏感等缺陷

Method: 提出CRT（约束修正训练）框架，基于参考保护的约束优化，交替最小化推理长度和在性能低于参考时修正准确性；采用两阶段训练方案：先发现最短可靠推理模式，然后在学习到的长度预算下优化准确性

Result: CRT框架能持续减少令牌使用，同时保持答案质量在稳健可靠水平；不仅缩短响应长度，还减少内部语言冗余；产生一系列中间检查点，可在不重新训练的情况下精细控制推理详细程度

Conclusion: CRT提供了一个稳定、可解释的高效推理训练框架，解决了现有方法的局限性，实现了推理长度与准确性的更好平衡

Abstract: Chain-of-Thought (CoT) has significantly enhanced the reasoning capabilities of Large Language Models (LLMs), especially when combined with reinforcement learning (RL) based post-training methods. While longer reasoning traces can improve answer quality and unlock abilities such as self-correction, they also incur high inference costs and often introduce redundant steps, known as overthinking. Recent research seeks to develop efficient reasoning strategies that balance reasoning length and accuracy, either through length-aware reward design or prompt-based calibration. However, these heuristic-based approaches may suffer from severe accuracy drop and be very sensitive to hyperparameters. To address these problems, we introduce CRT (Constraint-Rectified Training), a principled post-training framework based on reference-guarded constrained optimization, yielding a more stable and interpretable formulation for efficient reasoning. CRT alternates between minimizing reasoning length and rectifying accuracy only when performance falls below the reference, enabling stable and effective pruning of redundant reasoning. We further extend CRT with a two-stage training scheme that first discovers the shortest reliable reasoning patterns and then refines accuracy under a learnt length budget, preventing the re-emergence of verbose CoT. Our comprehensive evaluation shows that this framework consistently reduces token usage while maintaining answer quality at a robust and reliable level. Further analysis reveals that CRT improves reasoning efficiency not only by shortening responses but also by reducing internal language redundancy, leading to a new evaluation metric. Moreover, CRT-based training naturally yields a sequence of intermediate checkpoints that span a spectrum of explanation lengths while preserving correctness, enabling fine-grained control over reasoning verbosity without retraining.

</details>


### [74] [Analytical Results for Two Exponential Family Distributions in Hierarchical Dirichlet Processes](https://arxiv.org/abs/2602.12527)
*Naiqi Li*

Main category: cs.LG

TL;DR: 本文研究了分层狄利克雷过程（HDP）框架下指数族分布的解析结果，特别是推导了Gamma-Poisson和Normal-Gamma-Normal共轭对的闭式表达式，扩展了HDP在非狄利克雷-多项式共轭结构中的应用。


<details>
  <summary>Details</summary>
Motivation: 现有HDP应用主要集中于狄利克雷-多项式共轭结构，但HDP框架本身更为通用，理论上可容纳更广泛的共轭先验-似然对。指数族分布提供了一个统一且解析可处理的建模范式，包含许多常用分布。本文旨在探索HDP框架下指数族分布的解析结果。

Method: 研究了指数族中两个重要成员在HDP框架下的解析性质：泊松分布和正态分布。推导了Gamma-Poisson和Normal-Gamma-Normal共轭对在分层狄利克雷过程构造下的显式闭式表达式。提供了详细的推导和证明，阐明底层数学结构，展示如何在分层非参数模型中系统利用共轭性。

Result: 成功推导出Gamma-Poisson和Normal-Gamma-Normal共轭对在HDP框架下的闭式解析表达式。这些结果为研究人员提供了实用的解析工具，扩展了HDP在分层贝叶斯非参数建模中的应用范围。

Conclusion: 本文工作将HDP的适用性扩展到狄利克雷-多项式设置之外，为使用分层贝叶斯非参数方法的研究人员提供了实用的解析结果，展示了HDP框架在更广泛共轭结构中的通用性和灵活性。

Abstract: The Hierarchical Dirichlet Process (HDP) provides a flexible Bayesian nonparametric framework for modeling grouped data with a shared yet unbounded collection of mixture components. While existing applications of the HDP predominantly focus on the Dirichlet-multinomial conjugate structure, the framework itself is considerably more general and, in principle, accommodates a broad class of conjugate prior-likelihood pairs. In particular, exponential family distributions offer a unified and analytically tractable modeling paradigm that encompasses many commonly used distributions. In this paper, we investigate analytic results for two important members of the exponential family within the HDP framework: the Poisson distribution and the normal distribution. We derive explicit closed-form expressions for the corresponding Gamma-Poisson and Normal-Gamma-Normal conjugate pairs under the hierarchical Dirichlet process construction. Detailed derivations and proofs are provided to clarify the underlying mathematical structure and to demonstrate how conjugacy can be systematically exploited in hierarchical nonparametric models. Our work extends the applicability of the HDP beyond the Dirichlet-multinomial setting and furnishes practical analytic results for researchers employing hierarchical Bayesian nonparametrics.

</details>


### [75] [Flow-Factory: A Unified Framework for Reinforcement Learning in Flow-Matching Models](https://arxiv.org/abs/2602.12529)
*Bowen Ping,Chengyou Jia,Minnan Luo,Hangwei Qian,Ivor Tsang*

Main category: cs.LG

TL;DR: Flow-Factory是一个统一的强化学习框架，用于对齐扩散和流匹配模型与人类偏好，解决了现有代码库碎片化、模型特定实现和工程复杂性问题。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习在扩散和流匹配模型对齐方面面临代码库碎片化、模型特定实现和工程复杂性等挑战，阻碍了研究进展。

Method: 采用模块化、基于注册表的架构设计，将算法、模型和奖励解耦，支持GRPO、DiffusionNFT和AWM等算法在Flux、Qwen-Image和WAN视频模型上的集成。

Result: Flow-Factory提供了生产级内存优化、灵活的多奖励训练和无缝分布式训练支持，显著降低了实现开销，使研究人员能够快速原型化和扩展创新。

Conclusion: Flow-Factory通过统一的模块化框架解决了强化学习在扩散模型对齐中的工程挑战，为未来创新提供了高效的研究平台。

Abstract: Reinforcement learning has emerged as a promising paradigm for aligning diffusion and flow-matching models with human preferences, yet practitioners face fragmented codebases, model-specific implementations, and engineering complexity. We introduce Flow-Factory, a unified framework that decouples algorithms, models, and rewards through through a modular, registry-based architecture. This design enables seamless integration of new algorithms and architectures, as demonstrated by our support for GRPO, DiffusionNFT, and AWM across Flux, Qwen-Image, and WAN video models. By minimizing implementation overhead, Flow-Factory empowers researchers to rapidly prototype and scale future innovations with ease. Flow-Factory provides production-ready memory optimization, flexible multi-reward training, and seamless distributed training support. The codebase is available at https://github.com/X-GenGroup/Flow-Factory.

</details>


### [76] [AMPS: Adaptive Modality Preference Steering via Functional Entropy](https://arxiv.org/abs/2602.12533)
*Zihan Huang,Xintong Li,Rohan Surana,Tong Yu,Rui Wang,Julian McAuley,Jingbo Shang,Junda Wu*

Main category: cs.LG

TL;DR: 本文提出了一种实例感知的模态偏好调控方法，通过量化每个模态的信息贡献并识别样本特异性敏感性，实现更精细化的多模态大语言模型调控。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型存在显著的模态偏好问题，即倾向于过度依赖某一模态（如语言先验或视觉证据）。现有方法采用统一的调控强度，但强调控会损害标准推理并增加错误率，而弱调控往往无效。由于不同多模态实例对调控的敏感性差异很大，单一的全局强度难以校准。

Method: 1. 提出实例感知的诊断指标，量化每个模态的信息贡献并揭示样本特异性敏感性；2. 基于这些洞察提出缩放策略，减少对敏感样本的调控；3. 设计可学习模块推断缩放模式，实现实例感知的模态偏好控制。

Result: 实验结果表明，实例感知的调控方法在调节模态偏好方面优于传统调控方法，能够实现有效调整的同时保持较低的生成错误率。

Conclusion: 通过实例感知的模态偏好调控方法，可以更精细地控制多模态大语言模型的模态偏好，在有效调整偏好的同时最小化对推理过程的干扰，解决了传统统一调控方法的局限性。

Abstract: Multimodal Large Language Models (MLLMs) often exhibit significant modality preference, which is a tendency to favor one modality over another. Depending on the input, they may over-rely on linguistic priors relative to visual evidence, or conversely over-attend to visually salient but facts in textual contexts. Prior work has applied a uniform steering intensity to adjust the modality preference of MLLMs. However, strong steering can impair standard inference and increase error rates, whereas weak steering is often ineffective. In addition, because steering sensitivity varies substantially across multimodal instances, a single global strength is difficult to calibrate. To address this limitation with minimal disruption to inference, we introduce an instance-aware diagnostic metric that quantifies each modality's information contribution and reveals sample-specific susceptibility to steering. Building on these insights, we propose a scaling strategy that reduces steering for sensitive samples and a learnable module that infers scaling patterns, enabling instance-aware control of modality preference. Experimental results show that our instance-aware steering outperforms conventional steering in modulating modality preference, achieving effective adjustment while keeping generation error rates low.

</details>


### [77] [Exploring Accurate and Transparent Domain Adaptation in Predictive Healthcare via Concept-Grounded Orthogonal Inference](https://arxiv.org/abs/2602.12542)
*Pengfei Hu,Chang Lu,Feifan Liu,Yue Ning*

Main category: cs.LG

TL;DR: ExtraCare提出了一种可解释的领域自适应方法，通过将患者表征分解为不变和协变分量来提高临床事件预测的准确性和透明度。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHR）的深度学习模型在不同数据分布下部署时性能会下降。虽然领域自适应方法可以缓解这种偏移，但其"黑盒"性质阻碍了在临床实践中的广泛应用，因为透明度对于信任和安全至关重要。

Method: ExtraCare将患者表征分解为不变和协变分量，通过监督这两个分量并在训练中强制它们正交，保留标签信息同时暴露领域特定变化。通过将稀疏潜在维度映射到医学概念并通过针对性消融量化其贡献，提供人类可理解的解释。

Result: 在两个真实世界EHR数据集上的多个领域划分设置中评估，ExtraCare表现出优越性能，并通过广泛的案例研究证明了其准确预测和解释能力，实现了增强的透明度。

Conclusion: ExtraCare不仅提高了临床事件预测的准确性，还提供了可解释的领域自适应方法，解决了临床实践中对透明度的需求，为医疗AI的可信部署提供了重要工具。

Abstract: Deep learning models for clinical event prediction on electronic health records (EHR) often suffer performance degradation when deployed under different data distributions. While domain adaptation (DA) methods can mitigate such shifts, its "black-box" nature prevents widespread adoption in clinical practice where transparency is essential for trust and safety. We propose ExtraCare to decompose patient representations into invariant and covariant components. By supervising these two components and enforcing their orthogonality during training, our model preserves label information while exposing domain-specific variation at the same time for more accurate predictions than most feature alignment models. More importantly, it offers human-understandable explanations by mapping sparse latent dimensions to medical concepts and quantifying their contributions via targeted ablations. ExtraCare is evaluated on two real-world EHR datasets across multiple domain partition settings, demonstrating superior performance along with enhanced transparency, as evidenced by its accurate predictions and explanations from extensive case studies.

</details>


### [78] [SD-MoE: Spectral Decomposition for Effective Expert Specialization](https://arxiv.org/abs/2602.12556)
*Ruijun Huang,Fang Dong,Xin Zhang,Hengjie Cao,Zhendong Huang,Anrui Chen,Jixian Zhou,Mengyi Chen,Yifeng Yang,Mingzhi Dong,Yujiang Wang,Jinlong Hou,Qin Lv,Robert P. Dick,Yuan Cheng,Fan Yang,Tun Lu,Chun Zhang,Li Shang*

Main category: cs.LG

TL;DR: 论文提出SD-MoE方法，通过谱空间解耦参数和梯度来解决MoE架构中专家专业化不足的问题，提升模型性能


<details>
  <summary>Details</summary>
Motivation: MoE架构通过条件计算实现专家专业化来扩展大语言模型，但在实践中专家专业化常常失败：一些专家功能相似，另一些则成为事实上的共享专家，限制了有效容量和模型性能

Method: 从谱角度分析参数和梯度空间，发现专家参数共享高度重叠的谱成分，梯度子空间强对齐，门控机制偏好沿主导方向路由。提出Spectral-Decoupled MoE (SD-MoE)，在谱空间解耦参数和梯度

Result: SD-MoE在下游任务中提升性能，实现有效的专家专业化，引入最小额外计算，并能无缝集成到现有MoE架构中（包括Qwen和DeepSeek）

Conclusion: 通过谱空间解耦参数和梯度能有效解决MoE架构中专家专业化不足的问题，提升模型性能并保持计算效率

Abstract: Mixture-of-Experts (MoE) architectures scale Large Language Models via expert specialization induced by conditional computation. In practice, however, expert specialization often fails: some experts become functionally similar, while others functioning as de facto shared experts, limiting the effective capacity and model performance. In this work, we analysis from a spectral perspective on parameter and gradient spaces, uncover that (1) experts share highly overlapping dominant spectral components in their parameters, (2) dominant gradient subspaces are strongly aligned across experts, driven by ubiquitous low-rank structure in human corpus, and (3) gating mechanisms preferentially route inputs along these dominant directions, further limiting specialization. To address this, we propose Spectral-Decoupled MoE (SD-MoE), which decomposes both parameter and gradient in the spectral space. SD-MoE improves performance across downstream tasks, enables effective expert specialization, incurring minimal additional computation, and can be seamlessly integrated into a wide range of existing MoE architectures, including Qwen and DeepSeek.

</details>


### [79] [Fractional Order Federated Learning for Battery Electric Vehicle Energy Consumption Modeling](https://arxiv.org/abs/2602.12567)
*Mohammad Partohaghighi,Roummel Marcia,Bruce J. West,YangQuan Chen*

Main category: cs.LG

TL;DR: 本文提出FO-RI-FedAvg方法，通过自适应粗糙度感知的正则化和非整数阶优化来提升电动汽车联邦学习的稳定性


<details>
  <summary>Details</summary>
Motivation: 电动汽车联邦学习面临间歇性连接、客户端参与度时变以及运行条件差异导致的客户端间显著变化等不稳定问题，传统FedAvg方法在这些现实约束下容易产生过度漂移和收敛退化

Method: 提出FO-RI-FedAvg方法，包含两个客户端机制：1) 自适应粗糙度感知近端正则化，基于局部损失函数粗糙度动态调整向全局模型的拉力；2) 非整数阶局部优化，引入短期记忆平滑冲突的更新方向。该方法保持标准FedAvg服务器聚合，仅增加元素级操作

Result: 在VED和eVED两个真实世界电动汽车能量预测数据集上的实验表明，FO-RI-FedAvg相比强联邦基线实现了更高的准确性和更稳定的收敛，特别是在客户端参与度降低的情况下

Conclusion: FO-RI-FedAvg是一种轻量级模块化扩展，通过客户端侧的两个互补机制有效提升了电动汽车联邦学习的稳定性，特别适用于现实世界中的连接不稳定和客户端参与度变化场景

Abstract: Federated learning on connected electric vehicles (BEVs) faces severe instability due to intermittent connectivity, time-varying client participation, and pronounced client-to-client variation induced by diverse operating conditions. Conventional FedAvg and many advanced methods can suffer from excessive drift and degraded convergence under these realistic constraints. This work introduces Fractional-Order Roughness-Informed Federated Averaging (FO-RI-FedAvg), a lightweight and modular extension of FedAvg that improves stability through two complementary client-side mechanisms: (i) adaptive roughness-informed proximal regularization, which dynamically tunes the pull toward the global model based on local loss-landscape roughness, and (ii) non-integer-order local optimization, which incorporates short-term memory to smooth conflicting update directions. The approach preserves standard FedAvg server aggregation, adds only element-wise operations with amortizable overhead, and allows independent toggling of each component. Experiments on two real-world BEV energy prediction datasets, VED and its extended version eVED, show that FO-RI-FedAvg achieves improved accuracy and more stable convergence compared to strong federated baselines, particularly under reduced client participation.

</details>


### [80] [VI-CuRL: Stabilizing Verifier-Independent RL Reasoning via Confidence-Guided Variance Reduction](https://arxiv.org/abs/2602.12579)
*Xin-Qiang Cai,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 提出VI-CuRL框架，通过基于模型内在置信度构建课程学习，解决无验证器强化学习中梯度方差导致的训练崩溃问题


<details>
  <summary>Details</summary>
Motivation: 现有基于验证器的强化学习(RLVR)依赖外部验证器，可扩展性受限。无验证器设置中，标准方法面临破坏性梯度方差导致训练崩溃的问题

Method: 提出VI-CuRL框架，利用模型内在置信度构建独立于外部验证器的课程学习，优先处理高置信度样本，管理偏差-方差权衡，特别针对减少动作和问题方差

Result: 理论分析证明估计器保证渐近无偏性。在六个具有挑战性的基准测试中，VI-CuRL促进稳定性并持续优于无验证器基线方法

Conclusion: VI-CuRL通过内在置信度驱动的课程学习，有效解决了无验证器强化学习中的梯度方差问题，为可扩展的LLM推理训练提供了新方法

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a dominant paradigm for enhancing Large Language Models (LLMs) reasoning, yet its reliance on external verifiers limits its scalability. Recent findings suggest that RLVR primarily functions by eliciting latent capabilities, motivating the development of verifier-free algorithms. However, in such settings, standard methods like Group Relative Policy Optimization face a critical challenge: destructive gradient variance that often leads to training collapse. To address this issue, we introduceVerifier-Independent Curriculum Reinforcement Learning (VI-CuRL), a framework that leverages the model's intrinsic confidence to construct a curriculum independent from external verifiers. By prioritizing high-confidence samples, VI-CuRL effectively manages the bias-variance trade-off, specifically targeting the reduction of action and problem variance. We provide a rigorous theoretical analysis, proving that our estimator guarantees asymptotic unbiasedness. Empirically, VI-CuRL promotes stability and consistently outperforms verifier-independent baselines across six challenging benchmarks with/without verifiers.

</details>


### [81] [Multi-Head Attention as a Source of Catastrophic Forgetting in MoE Transformers](https://arxiv.org/abs/2602.12587)
*Anrui Chen,Ruijun Huang,Xin Zhang,Fang Dong,Hengjie Cao,Zhendong Huang,Yifeng Yang,Mengyi Chen,Jixian Zhou,Mingzhi Dong,Yujiang Wang,Jinlong Hou,Qin Lv,Robert P. Dick,Yuan Cheng,Tun Lu,Fan Yang,Li Shang*

Main category: cs.LG

TL;DR: MH-MoE通过头级路由减少混合专家模型中的特征组合碰撞，有效缓解持续学习中的遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 尽管MoE架构理论上适合持续学习（稀疏路由应能局部化更新并减少干扰），但实际中MoE Transformer仍然存在显著遗忘。研究发现这是由于预路由瓶颈导致的：多头注意力将头特定信号拼接成单个后注意力路由器输入，迫使路由基于共现的特征组合而非可分离的头通道。

Method: 提出MH-MoE（多头混合专家），在子表示上执行头级路由以增加路由粒度并减少组合碰撞。通过量化路由有效组合数N_eff来评估碰撞效应，并设计更细粒度的路由机制。

Result: 在TRACE基准测试中使用Qwen3-0.6B/8B模型，MH-MoE有效缓解了遗忘，将Qwen3-0.6B的BWT（后向转移）从LoRAMoE的11.2%降低到4.5%。

Conclusion: 通过头级路由增加路由粒度可以显著减少MoE架构中的特征组合碰撞，从而有效缓解持续学习中的遗忘问题，为MoE在持续学习中的应用提供了改进方向。

Abstract: Mixture-of-Experts (MoE) architectures are often considered a natural fit for continual learning because sparse routing should localize updates and reduce interference, yet MoE Transformers still forget substantially even with sparse, well-balanced expert utilization. We attribute this gap to a pre-routing bottleneck: multi-head attention concatenates head-specific signals into a single post-attention router input, forcing routing to act on co-occurring feature compositions rather than separable head channels. We show that this router input simultaneously encodes multiple separately decodable semantic and structural factors with uneven head support, and that different feature compositions induce weakly aligned parameter-gradient directions; as a result, routing maps many distinct compositions to the same route. We quantify this collision effect via a route-wise effective composition number $N_{eff}$ and find that higher $N_{eff}$ is associated with larger old-task loss increases after continual training. Motivated by these findings, we propose MH-MoE, which performs head-wise routing over sub-representations to increase routing granularity and reduce composition collisions. On TRACE with Qwen3-0.6B/8B, MH-MoE effectively mitigates forgetting, reducing BWT on Qwen3-0.6B from 11.2% (LoRAMoE) to 4.5%.

</details>


### [82] [Vehicle behaviour estimation for abnormal event detection using distributed fiber optic sensing](https://arxiv.org/abs/2602.12591)
*Hemant Prasad,Daisuke Ikefuji,Shin Tominaga,Hitoshi Sakurai,Manabu Otani*

Main category: cs.LG

TL;DR: 提出一种基于分布式光纤传感的交通监测方法，通过追踪车辆路径和检测车道变换来识别单车道异常


<details>
  <summary>Details</summary>
Motivation: 分布式光纤传感系统虽然能有效检测交通拥堵，但难以识别导致拥堵的单车道异常。这些异常可以通过监测车辆为避免拥堵而进行的车道变换行为来检测

Method: 通过聚类技术估计车辆在所有时间点的位置并拟合路径，通过追踪参考车辆并监测其振动频谱质心的变化来检测车道变换

Result: 使用真实交通数据评估，车道变换检测准确率达到80%，能够有效识别异常情况

Conclusion: 该方法能够有效检测单车道异常，为分布式光纤传感系统提供了更精细的交通监测能力

Abstract: The distributed fiber-optic sensing (DFOS) system is a cost-effective wide-area traffic monitoring technology that utilizes existing fiber infrastructure to effectively detect traffic congestions. However, detecting single-lane abnormalities, that lead to congestions, is still a challenge. These single-lane abnormalities can be detected by monitoring lane change behaviour of vehicles, performed to avoid congestion along the monitoring section of a road. This paper presents a method to detect single-lane abnormalities by tracking individual vehicle paths and detecting vehicle lane changes along a section of a road. We propose a method to estimate the vehicle position at all time instances and fit a path using clustering techniques. We detect vehicle lane change by monitoring any change in spectral centroid of vehicle vibrations by tracking a reference vehicle along a highway. The evaluation of our proposed method with real traffic data showed 80% accuracy for lane change detection events that represent presence of abnormalities.

</details>


### [83] [Power Interpretable Causal ODE Networks: A Unified Model for Explainable Anomaly Detection and Root Cause Analysis in Power Systems](https://arxiv.org/abs/2602.12592)
*Yue Sun,Likai Wang,Rick S. Blum,Parv Venkitasubramaniam*

Main category: cs.LG

TL;DR: 提出PICODE网络，一种因果感知的统一架构，用于电力系统异常检测与解释，同时进行根因定位、异常类型分类和异常形状表征。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列异常检测模型多为黑盒，仅提供二元输出而缺乏解释（如异常类型和来源），难以满足电力系统等关键基础设施的安全需求。

Method: 提出PICODE网络，这是一种因果感知的常微分方程架构，联合执行异常检测并提供解释，包括根因定位、异常类型分类和异常形状表征。

Result: 在电力系统实验中，PICODE实现了有竞争力的检测性能，同时提高了可解释性，减少了对标记数据或外部因果图的依赖。

Conclusion: PICODE为关键基础设施提供了一种可解释的异常检测解决方案，理论结果证明了异常函数形状与提取因果图权重变化的一致性。

Abstract: Anomaly detection and root cause analysis (RCA) are critical for ensuring the safety and resilience of cyber-physical systems such as power grids. However, existing machine learning models for time series anomaly detection often operate as black boxes, offering only binary outputs without any explanation, such as identifying anomaly type and origin. To address this challenge, we propose Power Interpretable Causality Ordinary Differential Equation (PICODE) Networks, a unified, causality-informed architecture that jointly performs anomaly detection along with the explanation why it is detected as an anomaly, including root cause localization, anomaly type classification, and anomaly shape characterization. Experimental results in power systems demonstrate that PICODE achieves competitive detection performance while offering improved interpretability and reduced reliance on labeled data or external causal graphs. We provide theoretical results demonstrating the alignment between the shape of anomaly functions and the changes in the weights of the extracted causal graphs.

</details>


### [84] [HyperMLP: An Integrated Perspective for Sequence Modeling](https://arxiv.org/abs/2602.12601)
*Jiecheng Lu,Shihao Yang*

Main category: cs.LG

TL;DR: 论文提出将自注意力重新解释为动态两层MLP，并基于此设计了HyperMLP/HyperGLU，在相同参数量下优于softmax注意力基线


<details>
  <summary>Details</summary>
Motivation: 传统将自注意力视为概率查询-键查找的视角限制了设计空间，作者主张更简单统一的视角：将自回归注意力头视为动态两层MLP

Method: 提出HyperMLP和HyperGLU，使用反向偏移（滞后）布局在特征空间和序列空间学习动态混合，将注意力分数视为不断增长的隐藏表示

Result: 在匹配参数预算下，HyperMLP/HyperGLU始终优于强大的softmax注意力基线模型

Conclusion: 将注意力重新解释为动态MLP提供了更统一的理论框架，并启发了更有效的架构设计，超越了传统的概率查询-键查找视角

Abstract: Self-attention is often viewed as probabilistic query-key lookup, motivating designs that preserve normalized attention scores and fixed positional semantics. We advocate a simpler and more unified perspective: an autoregressive attention head can be viewed as a dynamic two-layer MLP whose weights are instantiated from the context history. From this view, attention scores form an ever-growing hidden representation, and standard MLP activations such as ReLU or GLU naturally implement input-conditioned selection over a context-dependent memory pool rather than a probability distribution. Based on this formulation, we introduce HyperMLP and HyperGLU, which learn dynamic mixing in both feature space and sequence space, using a reverse-offset (lag) layout to align temporal mixing with autoregressive semantics. We provide theoretical characterizations of the expressivity and implications of this structure, and empirically show that HyperMLP/HyperGLU consistently outperform strong softmax-attention baselines under matched parameter budgets.

</details>


### [85] [Block-Sample MAC-Bayes Generalization Bounds](https://arxiv.org/abs/2602.12605)
*Matthias Frey,Jingge Zhu,Michael C. Gastpar*

Main category: cs.LG

TL;DR: 提出了一族新颖的块样本MAC-Bayes边界，这些边界约束期望泛化误差而非传统PAC-Bayes的高概率边界，通过数据分块减少散度项，在某些情况下能提供更紧的边界。


<details>
  <summary>Details</summary>
Motivation: 传统PAC-Bayes边界虽然能提供高概率保证，但可能过于保守甚至无意义。MAC-Bayes边界约束期望泛化误差，但现有方法仍有改进空间。作者希望开发更紧的边界，特别是通过数据分块策略来减少散度项的影响。

Method: 提出了一族新的MAC-Bayes边界，这些边界可以看作是已知PAC-Bayes边界期望版本的推广。关键创新在于引入数据分块（block）概念，使得边界中的散度项仅依赖于训练数据的子集，而非整个数据集。通过控制块大小来平衡边界的紧致性。

Result: 1. 新提出的MAC-Bayes边界在简单数值例子中表现优于传统方法：当原始PAC-Bayes边界无论先验如何选择都无意义时，新边界在适当块大小下能给出有限值。
2. 证明了高概率版本（即类似形式的PAC-Bayes边界）通常不可行：当MAC-Bayes边界以O(n^{-1/2})速率消失时，无法建立以O(1/log n)更快速率消失且对误差概率呈对数依赖的PAC-Bayes边界。

Conclusion: 提出的块样本MAC-Bayes边界家族在紧致性方面优于传统PAC-Bayes和MAC-Bayes边界，特别是在某些情况下能提供有意义的边界。然而，这种改进的代价是无法直接转化为高概率保证，揭示了期望边界和高概率边界之间的基本权衡。

Abstract: We present a family of novel block-sample MAC-Bayes bounds (mean approximately correct). While PAC-Bayes bounds (probably approximately correct) typically give bounds for the generalization error that hold with high probability, MAC-Bayes bounds have a similar form but bound the expected generalization error instead. The family of bounds we propose can be understood as a generalization of an expectation version of known PAC-Bayes bounds. Compared to standard PAC-Bayes bounds, the new bounds contain divergence terms that only depend on subsets (or \emph{blocks}) of the training data. The proposed MAC-Bayes bounds hold the promise of significantly improving upon the tightness of traditional PAC-Bayes and MAC-Bayes bounds. This is illustrated with a simple numerical example in which the original PAC-Bayes bound is vacuous regardless of the choice of prior, while the proposed family of bounds are finite for appropriate choices of the block size. We also explore the question whether high-probability versions of our MAC-Bayes bounds (i.e., PAC-Bayes bounds of a similar form) are possible. We answer this question in the negative with an example that shows that in general, it is not possible to establish a PAC-Bayes bound which (a) vanishes with a rate faster than $\mathcal{O}(1/\log n)$ whenever the proposed MAC-Bayes bound vanishes with rate $\mathcal{O}(n^{-1/2})$ and (b) exhibits a logarithmic dependence on the permitted error probability.

</details>


### [86] [RelBench v2: A Large-Scale Benchmark and Repository for Relational Data](https://arxiv.org/abs/2602.12606)
*Justin Gu,Rishabh Ranjan,Charilaos Kanatsoulis,Haiming Tang,Martin Jurkovic,Valter Hudovernik,Mark Znidar,Pranshu Chaturvedi,Parth Shroff,Fengyu Li,Jure Leskovec*

Main category: cs.LG

TL;DR: RelBench v2是一个扩展的关系深度学习基准测试，新增4个大规模数据集、自动补全任务，并整合外部基准，共包含11个数据集、2200万行数据，证明关系模型优于单表基线。


<details>
  <summary>Details</summary>
Motivation: 随着关系深度学习向更大模型和关系基础模型发展，需要可扩展且真实的基准测试来进行系统评估和推动进展。现有的基准测试需要扩展以支持更广泛的应用场景和任务类型。

Method: 1) 新增4个大规模关系数据集（学术出版物、企业资源规划、消费者平台、临床记录）；2) 引入自动补全任务，要求模型在关系表中推断缺失属性值；3) 整合外部基准：将Temporal Graph Benchmark转换为关系模式，与ReDeLEx接口连接70+真实数据库，纳入4DBInfer数据集。

Result: RelBench v2扩展到11个数据集，包含超过2200万行数据和29个表。实验结果显示，关系深度学习模型在自动补全、预测和推荐任务上持续优于单表基线，证明了显式建模关系结构的重要性。

Conclusion: RelBench v2为关系深度学习提供了一个全面、可扩展的基准测试套件，支持多种任务类型和数据集，有助于系统评估和推动关系深度学习领域的发展。

Abstract: Relational deep learning (RDL) has emerged as a powerful paradigm for learning directly on relational databases by modeling entities and their relationships across multiple interconnected tables. As this paradigm evolves toward larger models and relational foundation models, scalable and realistic benchmarks are essential for enabling systematic evaluation and progress. In this paper, we introduce RelBench v2, a major expansion of the RelBench benchmark for RDL. RelBench v2 adds four large-scale relational datasets spanning scholarly publications, enterprise resource planning, consumer platforms, and clinical records, increasing the benchmark to 11 datasets comprising over 22 million rows across 29 tables. We further introduce autocomplete tasks, a new class of predictive objectives that require models to infer missing attribute values directly within relational tables while respecting temporal constraints, expanding beyond traditional forecasting tasks constructed via SQL queries. In addition, RelBench v2 expands beyond its native datasets by integrating external benchmarks and evaluation frameworks: we translate event streams from the Temporal Graph Benchmark into relational schemas for unified relational-temporal evaluation, interface with ReDeLEx to provide uniform access to 70+ real-world databases suitable for pretraining, and incorporate 4DBInfer datasets and tasks to broaden multi-table prediction coverage. Experimental results demonstrate that RDL models consistently outperform single-table baselines across autocomplete, forecasting, and recommendation tasks, highlighting the importance of modeling relational structure explicitly.

</details>


### [87] [Coden: Efficient Temporal Graph Neural Networks for Continuous Prediction](https://arxiv.org/abs/2602.12613)
*Zulun Zhu,Siqiang Luo*

Main category: cs.LG

TL;DR: Coden是一个为动态图连续预测设计的TGNN模型，通过创新方法克服现有TGNN在连续预测场景下的计算瓶颈，同时保持预测准确性，在效率和效果上都超越现有基准。


<details>
  <summary>Details</summary>
Motivation: 现有TGNN主要针对特定时间跨度的一次性预测，而许多实际应用需要频繁的连续预测。直接将现有TGNN适配到连续预测场景会带来显著计算开销或预测质量问题，特别是对于大型图。

Method: 提出Coden模型，创新性地克服现有TGNN的关键复杂度瓶颈，同时保持可比的预测准确性。模型还提供理论分析，验证其有效性和效率，并阐明其与RNN基和注意力基模型的对偶关系。

Result: 在五个动态数据集上的评估表明，Coden在效率和效果上都超越了现有性能基准，成为演化图环境中连续预测的优越解决方案。

Conclusion: Coden为TGNN中的连续预测挑战提供了高效有效的解决方案，通过创新设计克服了现有方法的计算瓶颈，在动态图学习场景中表现出色。

Abstract: Temporal Graph Neural Networks (TGNNs) are pivotal in processing dynamic graphs. However, existing TGNNs primarily target one-time predictions for a given temporal span, whereas many practical applications require continuous predictions, that predictions are issued frequently over time. Directly adapting existing TGNNs to continuous-prediction scenarios introduces either significant computational overhead or prediction quality issues especially for large graphs. This paper revisits the challenge of { continuous predictions} in TGNNs, and introduces {\sc Coden}, a TGNN model designed for efficient and effective learning on dynamic graphs. {\sc Coden} innovatively overcomes the key complexity bottleneck in existing TGNNs while preserving comparable predictive accuracy. Moreover, we further provide theoretical analyses that substantiate the effectiveness and efficiency of {\sc Coden}, and clarify its duality relationship with both RNN-based and attention-based models. Our evaluations across five dynamic datasets show that {\sc Coden} surpasses existing performance benchmarks in both efficiency and effectiveness, establishing it as a superior solution for continuous prediction in evolving graph environments.

</details>


### [88] [Efficient Personalized Federated PCA with Manifold Optimization for IoT Anomaly Detection](https://arxiv.org/abs/2602.12622)
*Xianchao Xiu,Chenyi Huang,Wei Zhang,Wanquan Liu*

Main category: cs.LG

TL;DR: 提出FedEP方法，一种高效的个性化联邦PCA方法，用于物联网网络中的异常检测，通过局部表示和鲁棒性约束提升性能。


<details>
  <summary>Details</summary>
Motivation: 物联网网络由于其分布式特性和资源限制面临日益增长的安全威胁。现有的联邦PCA方法缺乏个性化和鲁棒性的集成，这对于有效的异常检测至关重要。

Method: 提出FedEP方法：通过ℓ₁范数引入局部表示实现个性化（元素级稀疏性），通过ℓ₂,₁范数强制局部模型实现鲁棒性（行级稀疏性）。使用基于ADMM的流形优化算法解决非凸问题，并提供严格的理论收敛保证。

Result: 实验结果表明，FedEP在多个物联网安全场景中优于最先进的FedPG方法，取得了优秀的F1分数和准确率。

Conclusion: FedEP方法成功解决了联邦PCA中个性化和鲁棒性的集成问题，为物联网网络异常检测提供了有效的解决方案。

Abstract: Internet of things (IoT) networks face increasing security threats due to their distributed nature and resource constraints. Although federated learning (FL) has gained prominence as a privacy-preserving framework for distributed IoT environments, current federated principal component analysis (PCA) methods lack the integration of personalization and robustness, which are critical for effective anomaly detection. To address these limitations, we propose an efficient personalized federated PCA (FedEP) method for anomaly detection in IoT networks. The proposed model achieves personalization through introducing local representations with the $\ell_1$-norm for element-wise sparsity, while maintaining robustness via enforcing local models with the $\ell_{2,1}$-norm for row-wise sparsity. To solve this non-convex problem, we develop a manifold optimization algorithm based on the alternating direction method of multipliers (ADMM) with rigorous theoretical convergence guarantees. Experimental results confirm that the proposed FedEP outperforms the state-of-the-art FedPG, achieving excellent F1-scores and accuracy in various IoT security scenarios. Our code will be available at \href{https://github.com/xianchaoxiu/FedEP}{https://github.com/xianchaoxiu/FedEP}.

</details>


### [89] [Formalizing the Sampling Design Space of Diffusion-Based Generative Models via Adaptive Solvers and Wasserstein-Bounded Timesteps](https://arxiv.org/abs/2602.12624)
*Sangwoo Jo,Sungjoon Choi*

Main category: cs.LG

TL;DR: SDM提出了一种基于几何视角的扩散模型采样框架，通过分析ODE动态特性，在早期高噪声阶段使用低阶求解器，后期非线性增强时逐步采用高阶求解器，并通过Wasserstein有界优化框架自适应调整时间步长，在减少函数评估次数的同时达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 扩散生成模型在实际部署中受到高采样成本的限制。现有方法主要关注训练目标或单个求解器，而采样过程的整体设计（特别是求解器选择和时间调度）仍由静态启发式方法主导。本文从几何视角重新审视这一挑战，提出更原则性的框架。

Method: SDM框架通过分析ODE动态特性，发现早期高噪声阶段使用低阶求解器足够高效，后期非线性增强时逐步部署高阶求解器。引入Wasserstein有界优化框架，系统推导自适应时间步长，显式约束局部离散化误差，确保采样过程忠实于底层连续动态。

Result: 无需额外训练或架构修改，SDM在标准基准测试中达到最先进性能：CIFAR-10上FID为1.93，FFHQ上为2.41，AFHQv2上为1.98，同时相比现有采样器减少了函数评估次数。

Conclusion: SDM提供了一个原则性的扩散模型采样框架，通过几何视角优化求解器选择和时间调度，在保持采样质量的同时显著降低计算成本，为扩散模型的实际部署提供了有效解决方案。

Abstract: Diffusion-based generative models have achieved remarkable performance across various domains, yet their practical deployment is often limited by high sampling costs. While prior work focuses on training objectives or individual solvers, the holistic design of sampling, specifically solver selection and scheduling, remains dominated by static heuristics. In this work, we revisit this challenge through a geometric lens, proposing SDM, a principled framework that aligns the numerical solver with the intrinsic properties of the diffusion trajectory. By analyzing the ODE dynamics, we show that efficient low-order solvers suffice in early high-noise stages while higher-order solvers can be progressively deployed to handle the increasing non-linearity of later stages. Furthermore, we formalize the scheduling by introducing a Wasserstein-bounded optimization framework. This method systematically derives adaptive timesteps that explicitly bound the local discretization error, ensuring the sampling process remains faithful to the underlying continuous dynamics. Without requiring additional training or architectural modifications, SDM achieves state-of-the-art performance across standard benchmarks, including an FID of 1.93 on CIFAR-10, 2.41 on FFHQ, and 1.98 on AFHQv2, with a reduced number of function evaluations compared to existing samplers. Our code is available at https://github.com/aiimaginglab/sdm.

</details>


### [90] [Dual-Granularity Contrastive Reward via Generated Episodic Guidance for Efficient Embodied RL](https://arxiv.org/abs/2602.12636)
*Xin Liu,Yixuan Li,Yuhui Chen,Yuxing Qin,Haoran Li,Dongbin Zhao*

Main category: cs.LG

TL;DR: DEG：利用视频生成模型生成任务指导，通过双粒度对比奖励实现无人工标注的高效强化学习


<details>
  <summary>Details</summary>
Motivation: 强化学习中的奖励设计挑战：轨迹成功奖励稀疏影响样本效率，现有密集奖励方法依赖高质量人工标注或专家监督

Method: 提出DEG框架：利用大型视频生成模型先验知识，仅需少量专家视频进行领域适应，为每个RL episode生成专用任务指导；设计双粒度对比奖励（粗粒度探索+细粒度匹配），在对比自监督潜在空间中引导智能体顺序逼近生成的指导视频

Result: 在模拟和真实世界的18个多样化任务上实验表明，DEG既能作为高效探索刺激帮助智能体快速发现稀疏成功奖励，也能独立引导有效RL和稳定策略收敛

Conclusion: DEG框架无需人工标注或大量监督，通过视频生成模型和双粒度对比奖励实现了样本高效的密集奖励设计，显著提升了强化学习在具身操作任务中的性能

Abstract: Designing suitable rewards poses a significant challenge in reinforcement learning (RL), especially for embodied manipulation. Trajectory success rewards are suitable for human judges or model fitting, but the sparsity severely limits RL sample efficiency. While recent methods have effectively improved RL via dense rewards, they rely heavily on high-quality human-annotated data or abundant expert supervision. To tackle these issues, this paper proposes Dual-granularity contrastive reward via generated Episodic Guidance (DEG), a novel framework to seek sample-efficient dense rewards without requiring human annotations or extensive supervision. Leveraging the prior knowledge of large video generation models, DEG only needs a small number of expert videos for domain adaptation to generate dedicated task guidance for each RL episode. Then, the proposed dual-granularity reward that balances coarse-grained exploration and fine-grained matching, will guide the agent to efficiently approximate the generated guidance video sequentially in the contrastive self-supervised latent space, and finally complete the target task. Extensive experiments on 18 diverse tasks across both simulation and real-world settings show that DEG can not only serve as an efficient exploration stimulus to help the agent quickly discover sparse success rewards, but also guide effective RL and stable policy convergence independently.

</details>


### [91] [Unifying Model-Free Efficiency and Model-Based Representations via Latent Dynamics](https://arxiv.org/abs/2602.12643)
*Jashaswimalya Acharjee,Balaraman Ravindran*

Main category: cs.LG

TL;DR: ULD是一种新型强化学习算法，通过将状态-动作对嵌入到潜在空间，使真实价值函数近似线性，从而统一了无模型方法的效率和基于模型方法的表示优势，无需规划开销。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习中，无模型方法效率高但表示能力有限，基于模型方法表示能力强但计算开销大。需要一种方法既能保持无模型方法的效率，又能获得基于模型方法的表示优势，同时避免规划开销。

Method: 1. 将状态-动作对嵌入到潜在空间，使真实价值函数在该空间中近似线性；2. 使用基于嵌入的时间差分更新；3. 同步更新编码器、价值和策略网络；4. 引入短时域预测动力学的辅助损失；5. 使用奖励尺度归一化确保稀疏奖励下的稳定学习。

Result: 在80个环境（包括Gym运动控制、DeepMind Control和Atari游戏）的评估中，ULD匹配或超过了专门的无模型和通用基于模型基线的性能，实现了跨领域能力，仅需最小调参和少量参数。

Conclusion: 与价值对齐的潜在表示本身就能提供传统上归因于完整基于模型规划的适应性和样本效率，表明无需完整模型规划也能获得类似优势。

Abstract: We present Unified Latent Dynamics (ULD), a novel reinforcement learning algorithm that unifies the efficiency of model-free methods with the representational strengths of model-based approaches, without incurring planning overhead. By embedding state-action pairs into a latent space in which the true value function is approximately linear, our method supports a single set of hyperparameters across diverse domains -- from continuous control with low-dimensional and pixel inputs to high-dimensional Atari games. We prove that, under mild conditions, the fixed point of our embedding-based temporal-difference updates coincides with that of a corresponding linear model-based value expansion, and we derive explicit error bounds relating embedding fidelity to value approximation quality. In practice, ULD employs synchronized updates of encoder, value, and policy networks, auxiliary losses for short-horizon predictive dynamics, and reward-scale normalization to ensure stable learning under sparse rewards. Evaluated on 80 environments spanning Gym locomotion, DeepMind Control (proprioceptive and visual), and Atari, our approach matches or exceeds the performance of specialized model-free and general model-based baselines -- achieving cross-domain competence with minimal tuning and a fraction of the parameter footprint. These results indicate that value-aligned latent representations alone can deliver the adaptability and sample efficiency traditionally attributed to full model-based planning.

</details>


### [92] [Uncovering spatial tissue domains and cell types in spatial omics through cross-scale profiling of cellular and genomic interactions](https://arxiv.org/abs/2602.12651)
*Rui Yan,Xiaohan Xing,Xun Wang,Zixia Zhou,Md Tauhidul Islam,Lei Xing*

Main category: cs.LG

TL;DR: CellScape是一个深度学习框架，通过联合建模组织空间中的细胞相互作用和细胞间的基因组关系，解决空间转录组数据分析的噪声大、结构复杂问题，实现高性能的空间模式发现。


<details>
  <summary>Details</summary>
Motivation: 空间转录组学(ST)数据具有前所未有的潜力，可以研究细胞身份和功能与其内在基因组组成和外在空间环境的关系。然而，ST数据本质上噪声大、数据量大且结构复杂，现有计算方法难以有效捕捉空间相互作用与内在基因组关系之间的相互作用，限制了发现关键生物学模式的能力。

Method: CellScape是一个深度学习框架，联合建模组织空间中的细胞相互作用和细胞间的基因组关系，生成综合表示，将空间信号与潜在的基因调控机制无缝整合。

Result: 该方法能够发现具有生物学信息性的模式，改善空间域分割，并支持跨不同转录组数据集的全面空间细胞分析，为ST数据的深度分析和解释提供了一个准确且通用的框架。

Conclusion: CellScape通过整合空间和基因组信息，克服了现有空间转录组数据分析方法的局限性，为理解细胞在组织微环境中的空间和功能组织提供了强大的计算工具。

Abstract: Cellular identity and function are linked to both their intrinsic genomic makeup and extrinsic spatial context within the tissue microenvironment. Spatial transcriptomics (ST) offers an unprecedented opportunity to study this, providing in situ gene expression profiles at single-cell resolution and illuminating the spatial and functional organization of cells within tissues. However, a significant hurdle remains: ST data is inherently noisy, large, and structurally complex. This complexity makes it intractable for existing computational methods to effectively capture the interplay between spatial interactions and intrinsic genomic relationships, thus limiting our ability to discern critical biological patterns. Here, we present CellScape, a deep learning framework designed to overcome these limitations for high-performance ST data analysis and pattern discovery. CellScape jointly models cellular interactions in tissue space and genomic relationships among cells, producing comprehensive representations that seamlessly integrate spatial signals with underlying gene regulatory mechanisms. This technique uncovers biologically informative patterns that improve spatial domain segmentation and supports comprehensive spatial cellular analyses across diverse transcriptomics datasets, offering an accurate and versatile framework for deep analysis and interpretation of ST data.w

</details>


### [93] [SLA2: Sparse-Linear Attention with Learnable Routing and QAT](https://arxiv.org/abs/2602.12675)
*Jintao Zhang,Haoxu Wang,Kai Jiang,Kaiwen Zheng,Youhe Jiang,Ion Stoica,Jianfei Chen,Jun Zhu,Joseph E. Gonzalez*

Main category: cs.LG

TL;DR: SLA2改进了稀疏线性注意力机制，通过可学习路由器和更直接的稀疏-线性注意力分解，在视频扩散模型中实现97%注意力稀疏度和18.6倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏线性注意力(SLA)存在两个问题：(1)基于注意力权重大小的启发式分割可能不是最优的；(2)形式化分析发现SLA与稀疏-线性注意力直接分解存在不匹配。

Method: 提出SLA2，包含三个创新：(1)可学习路由器动态选择稀疏或线性注意力；(2)更忠实直接的稀疏-线性注意力公式，使用可学习比例结合两个分支；(3)稀疏+低比特注意力设计，通过量化感知微调减少量化误差。

Result: 在视频扩散模型中，SLA2能够实现97%的注意力稀疏度，提供18.6倍的注意力加速，同时保持生成质量。

Conclusion: SLA2通过改进的稀疏-线性注意力机制，在保持生成质量的同时显著加速视频扩散模型的注意力计算。

Abstract: Sparse-Linear Attention (SLA) combines sparse and linear attention to accelerate diffusion models and has shown strong performance in video generation. However, (i) SLA relies on a heuristic split that assigns computations to the sparse or linear branch based on attention-weight magnitude, which can be suboptimal. Additionally, (ii) after formally analyzing the attention error in SLA, we identify a mismatch between SLA and a direct decomposition into sparse and linear attention. We propose SLA2, which introduces (I) a learnable router that dynamically selects whether each attention computation should use sparse or linear attention, (II) a more faithful and direct sparse-linear attention formulation that uses a learnable ratio to combine the sparse and linear attention branches, and (III) a sparse + low-bit attention design, where low-bit attention is introduced via quantization-aware fine-tuning to reduce quantization error. Experiments show that on video diffusion models, SLA2 can achieve 97% attention sparsity and deliver an 18.6x attention speedup while preserving generation quality.

</details>


### [94] [Flow Matching from Viewpoint of Proximal Operators](https://arxiv.org/abs/2602.12683)
*Kenji Fukumizu,Wei Huang,Han Bao,Shuntuo Xu,Nisha Chandramoothy*

Main category: cs.LG

TL;DR: 本文重新表述了最优传输条件流匹配（OT-CFM），证明了其具有精确的近端形式，无需假设目标分布有密度，并分析了其收敛性和流形支撑目标的终端正态双曲性。


<details>
  <summary>Details</summary>
Motivation: 现有OT-CFM方法通常假设目标分布具有密度，这限制了其应用范围。本文旨在建立OT-CFM的精确数学框架，无需密度假设，并深入分析其动力学特性。

Method: 通过扩展的Brenier势重新表述OT-CFM，证明其具有精确的近端算子形式。使用凸势的二阶上导数分析流形支撑目标的动力学行为，证明其终端正态双曲性。

Result: 1. OT-CFM可精确表示为近端算子形式，无需密度假设；2. 小批量OT-CFM随批量增大收敛于总体形式；3. 对于流形支撑目标，OT-CFM在时间重标度后呈现终端正态双曲性：法向指数收缩，切向保持中性。

Conclusion: 本文为OT-CFM建立了严格的数学基础，证明了其近端表示和收敛性，并揭示了其在流形支撑目标上的特殊动力学特性，为理解生成模型的几何行为提供了新视角。

Abstract: We reformulate Optimal Transport Conditional Flow Matching (OT-CFM), a class of dynamical generative models, showing that it admits an exact proximal formulation via an extended Brenier potential, without assuming that the target distribution has a density. In particular, the mapping to recover the target point is exactly given by a proximal operator, which yields an explicit proximal expression of the vector field. We also discuss the convergence of minibatch OT-CFM to the population formulation as the batch size increases. Finally, using second epi-derivatives of convex potentials, we prove that, for manifold-supported targets, OT-CFM is terminally normally hyperbolic: after time rescaling, the dynamics contracts exponentially in directions normal to the data manifold while remaining neutral along tangential directions.

</details>


### [95] [Trust the uncertain teacher: distilling dark knowledge via calibrated uncertainty](https://arxiv.org/abs/2602.12687)
*Jeonghyun Kim,SooKyung Kim,Richeng Xuan,Hyunsoo Cho*

Main category: cs.LG

TL;DR: 提出CUD框架，通过校准教师模型的不确定性分布，让学生模型更好地学习教师的"暗知识"，提高学生模型的准确性、校准性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏中，使用交叉熵训练的教师模型会产生过度自信的预测分布，导致"暗知识"（类间关系和不确定性分布）丢失。这种过度自信在高基数任务中尤其成问题，且会降低学生模型在分布偏移下的鲁棒性。

Method: 提出校准不确定性蒸馏（CUD）框架：不直接采用教师模型的过度自信预测，而是引导教师模型在信息丰富的地方揭示不确定性，指导学生从校准而非过度自信的目标中学习。通过直接塑造教师模型的预测分布再进行知识转移。

Result: 在多个基准测试中，CUD训练的学生模型不仅更准确，而且在分布偏移下校准性更好，在模糊、长尾输入上更可靠。

Conclusion: CUD框架通过校准教师模型的不确定性分布，让学生模型能够更好地学习教师的"暗知识"，在保持准确性的同时提高校准性和鲁棒性，特别适用于高基数任务和分布偏移场景。

Abstract: The core of knowledge distillation lies in transferring the teacher's rich 'dark knowledge'-subtle probabilistic patterns that reveal how classes are related and the distribution of uncertainties. While this idea is well established, teachers trained with conventional cross-entropy often fail to preserve such signals. Their distributions collapse into sharp, overconfident peaks that appear decisive but are in fact brittle, offering little beyond the hard label or subtly hindering representation-level transfer. This overconfidence is especially problematic in high-cardinality tasks, where the nuances among many plausible classes matter most for guiding a compact student. Moreover, such brittle targets reduce robustness under distribution shift, leaving students vulnerable to miscalibration in real-world conditions. To address this limitation, we revisit distillation from a distributional perspective and propose Calibrated Uncertainty Distillation (CUD), a framework designed to make dark knowledge more faithfully accessible. Instead of uncritically adopting the teacher's overconfidence, CUD encourages teachers to reveal uncertainty where it is informative and guides students to learn from targets that are calibrated rather than sharpened certainty. By directly shaping the teacher's predictive distribution before transfer, our approach balances accuracy and calibration, allowing students to benefit from both confident signals on easy cases and structured uncertainty on hard ones. Across diverse benchmarks, CUD yields students that are not only more accurate, but also more calibrated under shift and more reliable on ambiguous, long-tail inputs.

</details>


### [96] [Leverage-Weighted Conformal Prediction](https://arxiv.org/abs/2602.12693)
*Shreyas Fadnavis*

Main category: cs.LG

TL;DR: 提出LWCP方法，通过统计杠杆加权非符合性分数，无需训练辅助模型即可实现自适应预测区间，在保持边缘覆盖的同时改善条件覆盖。


<details>
  <summary>Details</summary>
Motivation: 传统分割符合预测产生恒定宽度的预测区间，在低方差区域过度覆盖，在高方差区域覆盖不足。现有自适应方法需要训练辅助模型，增加了复杂性。

Method: 提出杠杆加权符合预测(LWCP)，使用统计杠杆（帽子矩阵的对角线）对非符合性分数进行加权，从设计矩阵的几何结构中获取适应性，无需辅助模型拟合。

Result: LWCP保持有限样本边缘有效性；在异方差通过杠杆因子化时实现渐近最优条件覆盖；在保持分布自由保证的同时恢复高斯假设下的经典预测区间形式；随机杠杆近似保持精确覆盖；相比传统方法显著减少条件覆盖差异。

Conclusion: LWCP提供了一种简单有效的方法，通过统计杠杆加权改善预测区间的条件覆盖，无需额外模型训练，计算开销小，在理论和实验中均表现出优越性能。

Abstract: Split conformal prediction provides distribution-free prediction intervals with finite-sample marginal coverage, but produces constant-width intervals that overcover in low-variance regions and undercover in high-variance regions. Existing adaptive methods require training auxiliary models. We propose Leverage-Weighted Conformal Prediction (LWCP), which weights nonconformity scores by a function of the statistical leverage -- the diagonal of the hat matrix -- deriving adaptivity from the geometry of the design matrix rather than from auxiliary model fitting. We prove that LWCP preserves finite-sample marginal validity for any weight function; achieves asymptotically optimal conditional coverage at essentially no width cost when heteroscedasticity factors through leverage; and recovers the form and width of classical prediction intervals under Gaussian assumptions while retaining distribution-free guarantees. We further establish that randomized leverage approximations preserve coverage exactly with controlled width perturbation, and that vanilla CP suffers a persistent, sample-size-independent conditional coverage gap that LWCP eliminates. The method requires no hyperparameters beyond the choice of weight function and adds negligible computational overhead to vanilla CP. Experiments on synthetic and real data confirm the theoretical predictions, demonstrating substantial reductions in conditional coverage disparity across settings.

</details>


### [97] [SWING: Unlocking Implicit Graph Representations for Graph Random Features](https://arxiv.org/abs/2602.12703)
*Alessandro Manenti,Avinava Dubey,Arijit Sehanobish,Cesare Alippi,Krzysztof Choromanski*

Main category: cs.LG

TL;DR: SWING是一种用于隐式图表示（i-graphs）计算的新算法，通过连续空间游走而非节点游走来近似图随机特征计算，无需显式构建图结构。


<details>
  <summary>Details</summary>
Motivation: 传统图计算方法需要显式构建图结构，对于隐式表示的图（如ε-邻域图）效率低下。这些图的边权重由节点特征向量的双变量函数定义，在机器学习中广泛应用。

Method: 提出SWING算法：在连续嵌入空间中进行游走而非节点游走；使用定制化的Gumbel-softmax采样机制；结合线性化核（通过随机特征获得）和重要性采样技术；利用隐式图与傅里叶分析之间的深层联系。

Result: SWING算法能够准确高效地近似原始组合计算；对加速器友好；无需显式构建输入图；通过在不同类型i-graphs上的实验验证了其有效性。

Conclusion: SWING为隐式图表示的计算提供了一种新颖有效的算法框架，通过连续空间游走和傅里叶分析连接，解决了传统方法需要显式构建图结构的问题。

Abstract: We propose SWING: Space Walks for Implicit Network Graphs, a new class of algorithms for computations involving Graph Random Features on graphs given by implicit representations (i-graphs), where edge-weights are defined as bi-variate functions of feature vectors in the corresponding nodes. Those classes of graphs include several prominent examples, such as: $ε$-neighborhood graphs, used on regular basis in machine learning. Rather than conducting walks on graphs' nodes, those methods rely on walks in continuous spaces, in which those graphs are embedded. To accurately and efficiently approximate original combinatorial calculations, SWING applies customized Gumbel-softmax sampling mechanism with linearized kernels, obtained via random features coupled with importance sampling techniques. This algorithm is of its own interest. SWING relies on the deep connection between implicitly defined graphs and Fourier analysis, presented in this paper. SWING is accelerator-friendly and does not require input graph materialization. We provide detailed analysis of SWING and complement it with thorough experiments on different classes of i-graphs.

</details>


### [98] [Physics-Informed Laplace Neural Operator for Solving Partial Differential Equations](https://arxiv.org/abs/2602.12706)
*Heechang Kim,Qianying Cao,Hyomin Shin,Seungchul Lee,George Em Karniadakis,Minseok Choi*

Main category: cs.LG

TL;DR: 提出PILNO方法，将物理约束嵌入拉普拉斯神经算子中，通过虚拟输入和时序因果加权改进小数据场景下的精度和泛化能力


<details>
  <summary>Details</summary>
Motivation: 传统数据驱动的神经算子需要大量训练数据，在小数据场景和分布外输入下泛化能力差，需要将物理约束嵌入模型以提升数据效率和鲁棒性

Method: 1) 提出增强版ALNO骨干网络，保留极点-残差瞬态表示，用FNO风格傅里叶乘子替换稳态分支；2) 引入虚拟输入提供无标签物理监督；3) 使用时序因果加权优先处理早期动态

Result: 在四个基准测试（Burgers方程、Darcy流、反应扩散系统、强迫KdV方程）中，PILNO在小数据场景（N_train ≤ 27）下显著提升精度，减少随机种子间的运行变异性，并实现更强的分布外泛化能力

Conclusion: PILNO通过物理约束嵌入有效解决了数据驱动神经算子的局限性，在小数据场景下实现了更准确、更稳定的预测，并提升了分布外泛化能力

Abstract: Neural operators have emerged as fast surrogate solvers for parametric partial differential equations (PDEs). However, purely data-driven models often require extensive training data and can generalize poorly, especially in small-data regimes and under unseen (out-of-distribution) input functions that are not represented in the training data. To address these limitations, we propose the Physics-Informed Laplace Neural Operator (PILNO), which enhances the Laplace Neural Operator (LNO) by embedding governing physics into training through PDE, boundary condition, and initial condition residuals. To improve expressivity, we first introduce an Advanced LNO (ALNO) backbone that retains a pole-residue transient representation while replacing the steady-state branch with an FNO-style Fourier multiplier. To make physics-informed training both data-efficient and robust, PILNO further leverages (i) virtual inputs: an unlabeled ensemble of input functions spanning a broad spectral range that provides abundant physics-only supervision and explicitly targets out-of-distribution (OOD) regimes; and (ii) temporal-causality weighting: a time-decaying reweighting of the physics residual that prioritizes early-time dynamics and stabilizes optimization for time-dependent PDEs. Across four representative benchmarks -- Burgers' equation, Darcy flow, a reaction-diffusion system, and a forced KdV equation -- PILNO consistently improves accuracy in small-data settings (e.g., N_train <= 27), reduces run-to-run variability across random seeds, and achieves stronger OOD generalization than purely data-driven baselines.

</details>


### [99] [Mixture of Predefined Experts: Maximizing Data Usage on Vertical Federated Learning](https://arxiv.org/abs/2602.12708)
*Jon Irureta,Gorka Azkune,Jon Imaz,Aizea Lojo,Javier Fernandez-Marques*

Main category: cs.LG

TL;DR: Split-MoPE：一种结合分割学习与预定义专家混合架构的垂直联邦学习框架，无需完全样本对齐，单轮通信即可达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有垂直联邦学习框架大多假设参与者间有完全样本对齐，这在现实场景中很少成立。需要解决样本不对齐问题，同时减少通信开销并提高鲁棒性

Method: 提出Split-MoPE框架，结合分割学习和预定义专家混合架构。MoPE使用预定义专家处理特定数据对齐模式，而非动态学习路由。利用预训练编码器，单轮通信即可完成训练

Result: 在视觉数据集（CIFAR-10/100）和表格数据集（Breast Cancer Wisconsin）上的评估显示，Split-MoPE在数据缺失率高的挑战性场景中，性能优于LASER和Vertical SplitNN等SOTA系统

Conclusion: Split-MoPE解决了垂直联邦学习中样本不对齐的现实问题，通过预定义专家架构实现了高效的单轮通信训练，同时提供了对抗恶意参与者的鲁棒性和每个预测的可解释性

Abstract: Vertical Federated Learning (VFL) has emerged as a critical paradigm for collaborative model training in privacy-sensitive domains such as finance and healthcare. However, most existing VFL frameworks rely on the idealized assumption of full sample alignment across participants, a premise that rarely holds in real-world scenarios. To bridge this gap, this work introduces Split-MoPE, a novel framework that integrates Split Learning with a specialized Mixture of Predefined Experts (MoPE) architecture. Unlike standard Mixture of Experts (MoE), where routing is learned dynamically, MoPE uses predefined experts to process specific data alignments, effectively maximizing data usage during both training and inference without requiring full sample overlap. By leveraging pretrained encoders for target data domains, Split-MoPE achieves state-of-the-art performance in a single communication round, significantly reducing the communication footprint compared to multi-round end-to-end training. Furthermore, unlike existing proposals that address sample misalignment, this novel architecture provides inherent robustness against malicious or noisy participants and offers per-sample interpretability by quantifying each collaborator's contribution to each prediction. Extensive evaluations on vision (CIFAR-10/100) and tabular (Breast Cancer Wisconsin) datasets demonstrate that Split-MoPE consistently outperforms state-of-the-art systems such as LASER and Vertical SplitNN, particularly in challenging scenarios with high data missingness.

</details>


### [100] [ADEPT: RL-Aligned Agentic Decoding of Emotion via Evidence Probing Tools -- From Consensus Learning to Ambiguity-Driven Emotion Reasoning](https://arxiv.org/abs/2602.12714)
*Esther Sun,Bo-Hao Su,Abinay Reddy Naini,Shinji Watanabe,Carlos Busso*

Main category: cs.LG

TL;DR: ADEPT框架将SLLM转化为代理，通过多轮询问和专用工具调用进行基于证据的情感识别，从共识学习转向模糊驱动推理，显著提升主要情感准确性和次要情感表征。


<details>
  <summary>Details</summary>
Motivation: 现有SLLM在情感推理中产生无根据、文本偏见的判断，缺乏可验证的声学证据；而自监督语音编码器（如WavLM）虽然提供强声学表征，但缺乏可解释性。需要弥合这一差距，实现基于证据的情感识别。

Method: ADEPT框架将SLLM转化为代理，通过结构化管道（候选生成、证据收集、裁决）进行多轮询问，调用专用语义和声学探测工具，集成GRPO和证据信任门，将工具使用行为与预测质量明确耦合。

Result: 实验表明ADEPT在大多数设置中提高了主要情感准确性，同时显著改善了次要情感表征，产生基于可审计声学和语义证据的解释。

Conclusion: ADEPT实现了从共识学习到模糊驱动情感推理的范式转变，将少数标注视为信息性感知信号而非噪声，通过基于证据的推理提高了情感识别的准确性和可解释性。

Abstract: Speech Large Language Models (SLLMs) enable high-level emotion reasoning but often produce ungrounded, text-biased judgments without verifiable acoustic evidence. In contrast, self-supervised speech encoders such as WavLM provide strong acoustic representations yet remain opaque discriminative models with limited interpretability. To bridge this gap, we introduce ADEPT (Agentic Decoding of Emotion via Evidence Probing Tools), a framework that reframes emotion recognition as a multi-turn inquiry process rather than a single-pass prediction. ADEPT transforms an SLLM into an agent that maintains an evolving candidate emotion set and adaptively invokes dedicated semantic and acoustic probing tools within a structured pipeline of candidate generation, evidence collection, and adjudication. Crucially, ADEPT enables a paradigm shift from consensus learning to ambiguity-driven emotion reasoning. Since human affect exhibits inherent complexity and frequent co-occurrence of emotions, we treat minority annotations as informative perceptual signals rather than discarding them as noise. Finally, we integrate Group Relative Policy Optimization (GRPO) with an Evidence Trust Gate to explicitly couple tool-usage behaviors with prediction quality and enforce evidence-grounded reasoning. Experiments show that ADEPT improves primary emotion accuracy in most settings while substantially improving minor emotion characterization, producing explanations grounded in auditable acoustic and semantic evidence.

</details>


### [101] [Adaptive Structured Pruning of Convolutional Neural Networks for Time Series Classification](https://arxiv.org/abs/2602.12744)
*Javidan Abdullayev,Maxime Devanne,Cyril Meyer,Ali Ismail-Fawaz,Jonathan Weber,Germain Forestier*

Main category: cs.LG

TL;DR: DSP是一种完全自动化的结构化剪枝框架，用于卷积时间序列分类模型，无需预定义剪枝比例，能显著压缩模型大小（LITETime压缩58%，InceptionTime压缩75%）同时保持分类精度。


<details>
  <summary>Details</summary>
Motivation: 时间序列分类的深度学习模型虽然预测性能强，但计算和内存需求高，难以部署在资源受限设备上。现有结构化剪枝方法依赖手动调整超参数（如剪枝比例），限制了跨数据集的扩展性和泛化能力。

Method: 提出动态结构化剪枝（DSP）框架：1）训练时引入实例级稀疏损失以诱导通道级稀疏；2）通过全局激活分析识别并剪除冗余滤波器，无需预定义剪枝比例。

Result: 在128个UCR数据集上验证，使用LITETime和InceptionTime架构：LITETime平均压缩58%，InceptionTime平均压缩75%，同时保持分类精度。冗余分析证实DSP产生紧凑且信息丰富的表示。

Conclusion: DSP为卷积时间序列分类模型提供了完全自动化的结构化剪枝方案，解决了深度模型在资源受限设备上的部署瓶颈，为可扩展和高效的时间序列分类部署提供了实用路径。

Abstract: Deep learning models for Time Series Classification (TSC) have achieved strong predictive performance but their high computational and memory requirements often limit deployment on resource-constrained devices. While structured pruning can address these issues by removing redundant filters, existing methods typically rely on manually tuned hyperparameters such as pruning ratios which limit scalability and generalization across datasets. In this work, we propose Dynamic Structured Pruning (DSP), a fully automatic, structured pruning framework for convolution-based TSC models. DSP introduces an instance-wise sparsity loss during training to induce channel-level sparsity, followed by a global activation analysis to identify and prune redundant filters without needing any predefined pruning ratio. This work tackles computational bottlenecks of deep TSC models for deployment on resource-constrained devices. We validate DSP on 128 UCR datasets using two different deep state-of-the-art architectures: LITETime and InceptionTime. Our approach achieves an average compression of 58% for LITETime and 75% for InceptionTime architectures while maintaining classification accuracy. Redundancy analyses confirm that DSP produces compact and informative representations, offering a practical path for scalable and efficient deep TSC deployment.

</details>


### [102] [Hierarchical Successor Representation for Robust Transfer](https://arxiv.org/abs/2602.12753)
*Changmin Yu,Máté Lengyel*

Main category: cs.LG

TL;DR: 提出分层后继表示（HSR）来解决经典后继表示的政策依赖性和谱扩散问题，通过时间抽象和NMF分解获得稀疏、低秩的状态表示，实现高效任务迁移和探索。


<details>
  <summary>Details</summary>
Motivation: 经典后继表示（SR）存在两个主要问题：1）政策依赖性——政策变化会使预测表示过时；2）谱扩散——在复杂拓扑环境中产生密集重叠的特征，扩展性差。需要一种更稳健的表示方法。

Method: 提出分层后继表示（HSR），将时间抽象融入预测表示构建中，然后应用非负矩阵分解（NMF）获得稀疏、低秩的状态表示，形成政策无关的分层地图。

Result: HSR-NMF在多隔间环境中实现了高度样本效率的任务迁移，发现了可解释的拓扑结构，有效桥接了无模型最优性和基于模型的灵活性，并能驱动高效探索扩展到大型程序生成环境。

Conclusion: HSR克服了经典SR的局限性，通过时间抽象和稀疏分解提供了稳健、可解释的预测表示，既能支持高效任务迁移，又能驱动探索，为强化学习提供了新的表示框架。

Abstract: The successor representation (SR) provides a powerful framework for decoupling predictive dynamics from rewards, enabling rapid generalisation across reward configurations. However, the classical SR is limited by its inherent policy dependence: policies change due to ongoing learning, environmental non-stationarities, and changes in task demands, making established predictive representations obsolete. Furthermore, in topologically complex environments, SRs suffer from spectral diffusion, leading to dense and overlapping features that scale poorly. Here we propose the Hierarchical Successor Representation (HSR) for overcoming these limitations. By incorporating temporal abstractions into the construction of predictive representations, HSR learns stable state features which are robust to task-induced policy changes. Applying non-negative matrix factorisation (NMF) to the HSR yields a sparse, low-rank state representation that facilitates highly sample-efficient transfer to novel tasks in multi-compartmental environments. Further analysis reveals that HSR-NMF discovers interpretable topological structures, providing a policy-agnostic hierarchical map that effectively bridges model-free optimality and model-based flexibility. Beyond providing a useful basis for task-transfer, we show that HSR's temporally extended predictive structure can also be leveraged to drive efficient exploration, effectively scaling to large, procedurally generated environments.

</details>


### [103] [Closing the Loop: A Control-Theoretic Framework for Provably Stable Time Series Forecasting with LLMs](https://arxiv.org/abs/2602.12756)
*Xingyu Zhang,Hanyun Du,Zeen Song,Jianqi Zhang,Changwen Zheng,Wenwen Qiang*

Main category: cs.LG

TL;DR: 提出F-LLM框架，通过控制理论中的闭环反馈机制解决LLM在时间序列预测中的误差累积问题，相比传统开环自回归方法显著提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM时间序列预测方法采用朴素的自回归生成策略，在推理时以开环方式运行，使用自身生成的输出进行递归预测，导致早期微小误差会随时间累积并放大，造成显著的轨迹漂移问题（暴露偏差）。

Method: 从控制理论角度重新构建自回归预测，提出F-LLM（反馈驱动LLM）闭环框架。该框架包含可学习的残差估计器（观测器）和反馈控制器，能够主动稳定预测轨迹，而不是被动传播误差。理论证明在基础模型满足局部Lipschitz约束条件下，闭环机制能确保误差有界。

Result: 大量实验表明，F-LLM能显著减轻误差传播，在多个时间序列基准测试中取得了良好性能。

Conclusion: 通过引入控制理论的闭环反馈机制，F-LLM有效解决了LLM在时间序列预测中的误差累积问题，为自回归预测提供了更稳定的框架，并提供了理论保证。

Abstract: Large Language Models (LLMs) have recently shown exceptional potential in time series forecasting, leveraging their inherent sequential reasoning capabilities to model complex temporal dynamics. However, existing approaches typically employ a naive autoregressive generation strategy. We identify a critical theoretical flaw in this paradigm: during inference, the model operates in an open-loop manner, consuming its own generated outputs recursively. This leads to inevitable error accumulation (exposure bias), where minor early deviations cascade into significant trajectory drift over long horizons. In this paper, we reformulate autoregressive forecasting through the lens of control theory, proposing \textbf{F-LLM} (Feedback-driven LLM), a novel closed-loop framework. Unlike standard methods that passively propagate errors, F-LLM actively stabilizes the trajectory via a learnable residual estimator (Observer) and a feedback controller. Furthermore, we provide a theoretical guarantee that our closed-loop mechanism ensures uniformly bounded error, provided the base model satisfies a local Lipschitz constraint. Extensive experiments demonstrate that F-LLM significantly mitigates error propagation, achieving good performance on time series benchmarks.

</details>


### [104] [Can Neural Networks Provide Latent Embeddings for Telemetry-Aware Greedy Routing?](https://arxiv.org/abs/2602.12798)
*Andreas Boltres,Niklas Freymuth,Gerhard Neumann*

Main category: cs.LG

TL;DR: Placer：一种基于消息传递网络的算法，通过将网络状态转换为潜在节点嵌入，实现快速贪婪下一跳路由，同时保持路由决策的可解释性


<details>
  <summary>Details</summary>
Motivation: 现有基于机器学习的遥测感知路由方法虽然能处理网络状态与路由之间的复杂依赖关系，但由于神经路由模块的黑盒特性，牺牲了路由决策的可解释性。需要一种既能有效应对流量激增，又能解释路由决策的方法。

Method: 提出Placer算法，使用消息传递网络将网络状态转换为潜在节点嵌入。这些嵌入支持快速贪婪下一跳路由，无需直接解决全对最短路径问题，并能可视化网络事件如何影响路由决策。

Result: Placer能够提高网络效能和对流量激增的响应能力，同时保持路由决策的可解释性，允许可视化网络事件对路由的影响。

Conclusion: Placer提供了一种新颖的遥测感知路由方法，结合了机器学习的效能优势和可解释性，解决了现有黑盒神经路由模块的问题。

Abstract: Telemetry-Aware routing promises to increase efficacy and responsiveness to traffic surges in computer networks. Recent research leverages Machine Learning to deal with the complex dependency between network state and routing, but sacrifices explainability of routing decisions due to the black-box nature of the proposed neural routing modules. We propose \emph{Placer}, a novel algorithm using Message Passing Networks to transform network states into latent node embeddings. These embeddings facilitate quick greedy next-hop routing without directly solving the all-pairs shortest paths problem, and let us visualize how certain network events shape routing decisions.

</details>


### [105] [GRAIL: Geometry-Aware Retrieval-Augmented Inference with LLMs over Hyperbolic Representations of Patient Trajectories](https://arxiv.org/abs/2602.12828)
*Zhan Qu,Michael Färber*

Main category: cs.LG

TL;DR: GRAIL框架通过结构化几何表示和结构感知检索预测未来临床事件，结合确定性编码系统层次结构和数据驱动的时间关联，在双曲空间中嵌入临床图，并使用LLM作为约束推理时间重排器。


<details>
  <summary>Details</summary>
Motivation: 从纵向电子健康记录预测未来临床事件面临三大挑战：稀疏的多类型临床事件、层次化医学术语体系，以及大型语言模型在处理长结构化历史时容易产生幻觉。需要解决这些挑战来准确预测患者下一次就诊的临床事件。

Method: 提出GRAIL框架：1) 构建统一的临床图，结合编码系统层次结构和事件类型间的数据驱动时间关联；2) 在双曲空间中嵌入该图；3) 将每次就诊总结为概率性中心事件以去噪稀疏观测；4) 推理时检索结构化的临床合理未来事件集；5) 可选使用LLM作为约束推理时间重排器优化排名。

Result: 在MIMIC-IV数据集上的实验表明，GRAIL持续改进多类型下次就诊预测，并产生更符合层次结构一致的预测结果。

Conclusion: GRAIL通过结构化几何表示和结构感知检索有效解决了纵向EHR预测的挑战，结合确定性层次结构和数据驱动关联，在双曲空间中建模临床关系，并使用LLM进行约束重排，显著提升了预测准确性和临床一致性。

Abstract: Predicting future clinical events from longitudinal electronic health records (EHRs) is challenging due to sparse multi-type clinical events, hierarchical medical vocabularies, and the tendency of large language models (LLMs) to hallucinate when reasoning over long structured histories. We study next-visit event prediction, which aims to forecast a patient's upcoming clinical events based on prior visits. We propose GRAIL, a framework that models longitudinal EHRs using structured geometric representations and structure-aware retrieval. GRAIL constructs a unified clinical graph by combining deterministic coding-system hierarchies with data-driven temporal associations across event types, embeds this graph in hyperbolic space, and summarizes each visit as a probabilistic Central Event that denoises sparse observations. At inference time, GRAIL retrieves a structured set of clinically plausible future events aligned with hierarchical and temporal progression, and optionally refines their ranking using an LLM as a constrained inference-time reranker. Experiments on MIMIC-IV show that GRAIL consistently improves multi-type next-visit prediction and yields more hierarchy-consistent forecasts.

</details>


### [106] [FLAC: Maximum Entropy RL via Kinetic Energy Regularized Bridge Matching](https://arxiv.org/abs/2602.12829)
*Lei Lv,Yunfei Li,Yu Luo,Fuchun Sun,Xiao Ma*

Main category: cs.LG

TL;DR: FLAC是一个免似然框架，通过惩罚速度场的动能来调节策略随机性，将策略优化表述为广义薛定谔桥问题，无需显式动作密度估计。


<details>
  <summary>Details</summary>
Motivation: 迭代生成策略（如扩散模型和流匹配）在连续控制中具有优越表达能力，但由于动作对数密度不可直接访问，使最大熵强化学习复杂化。需要解决这一限制。

Method: 将策略优化表述为相对于高熵参考过程（如均匀分布）的广义薛定谔桥问题。通过惩罚速度场的动能来调节策略随机性，推导出能量正则化的策略迭代方案和实用的离策略算法，通过拉格朗日对偶机制自动调整动能。

Result: 在高维基准测试中，FLAC相对于强基线实现了优越或可比的性能，同时避免了显式密度估计。

Conclusion: FLAC提供了一个物理基础框架，通过动能正则化实现最大熵强化学习，无需显式动作密度，为迭代生成策略在连续控制中的应用提供了有效解决方案。

Abstract: Iterative generative policies, such as diffusion models and flow matching, offer superior expressivity for continuous control but complicate Maximum Entropy Reinforcement Learning because their action log-densities are not directly accessible. To address this, we propose Field Least-Energy Actor-Critic (FLAC), a likelihood-free framework that regulates policy stochasticity by penalizing the kinetic energy of the velocity field. Our key insight is to formulate policy optimization as a Generalized Schrödinger Bridge (GSB) problem relative to a high-entropy reference process (e.g., uniform). Under this view, the maximum-entropy principle emerges naturally as staying close to a high-entropy reference while optimizing return, without requiring explicit action densities. In this framework, kinetic energy serves as a physically grounded proxy for divergence from the reference: minimizing path-space energy bounds the deviation of the induced terminal action distribution. Building on this view, we derive an energy-regularized policy iteration scheme and a practical off-policy algorithm that automatically tunes the kinetic energy via a Lagrangian dual mechanism. Empirically, FLAC achieves superior or comparable performance on high-dimensional benchmarks relative to strong baselines, while avoiding explicit density estimation.

</details>


### [107] [TRACE: Temporal Reasoning via Agentic Context Evolution for Streaming Electronic Health Records (EHRs)](https://arxiv.org/abs/2602.12833)
*Zhan Qu,Michael Färber*

Main category: cs.LG

TL;DR: TRACE框架通过双记忆架构和智能体组件，让冻结的LLM能够进行时序临床推理，无需扩展上下文窗口或更新参数，显著提升长时序患者轨迹的预测准确性和临床安全性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然编码了丰富的医学知识，但在处理纵向患者轨迹时表现不佳，因为临床状态演变、不规则时间间隔和异质事件会随时间推移降低模型性能。现有方法依赖微调或检索增强，存在计算开销、隐私限制或长上下文不稳定的问题。

Method: 提出TRACE框架，采用双记忆架构：静态全局协议编码机构临床规则，动态个体协议跟踪患者特定状态。通过四个智能体组件（路由器、推理器、审计员、管理员）协调结构化记忆，支持时序推理和状态演化。通过结构化状态压缩控制推理成本，选择性审计安全关键决策。

Result: 在MIMIC-IV纵向临床事件流上的评估显示，TRACE显著提高了下一事件预测准确性、协议遵循度和临床安全性，优于长上下文和检索增强基线方法，同时产生可解释和可审计的推理轨迹。

Conclusion: TRACE框架通过结构化上下文维护而非扩展上下文窗口或更新参数，使冻结的LLM能够有效进行时序临床推理，在保持有界推理成本的同时提高预测性能和临床安全性，为医疗AI系统提供了可解释和可审计的解决方案。

Abstract: Large Language Models (LLMs) encode extensive medical knowledge but struggle to apply it reliably to longitudinal patient trajectories, where evolving clinical states, irregular timing, and heterogeneous events degrade performance over time. Existing adaptation strategies rely on fine-tuning or retrieval-based augmentation, which introduce computational overhead, privacy constraints, or instability under long contexts. We introduce TRACE (Temporal Reasoning via Agentic Context Evolution), a framework that enables temporal clinical reasoning with frozen LLMs by explicitly structuring and maintaining context rather than extending context windows or updating parameters. TRACE operates over a dual-memory architecture consisting of a static Global Protocol encoding institutional clinical rules and a dynamic Individual Protocol tracking patient-specific state. Four agentic components, Router, Reasoner, Auditor, and Steward, coordinate over this structured memory to support temporal inference and state evolution. The framework maintains bounded inference cost via structured state compression and selectively audits safety-critical clinical decisions. Evaluated on longitudinal clinical event streams from MIMIC-IV, TRACE significantly improves next-event prediction accuracy, protocol adherence, and clinical safety over long-context and retrieval-augmented baselines, while producing interpretable and auditable reasoning traces.

</details>


### [108] [Amortized Reasoning Tree Search: Decoupling Proposal and Decision in Large Language Models](https://arxiv.org/abs/2602.12846)
*Zesheng Hong,Jiadong Yu,Hui Pan*

Main category: cs.LG

TL;DR: RLVR方法在强化LLM推理能力时存在"归一化挤压"问题，会抑制罕见但正确的推理路径。作者提出ARTS方法，通过解耦生成与验证，使用流匹配目标来导航稀疏高熵搜索空间，在保持生成主干不变的情况下恢复长尾推理性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于可验证奖励的强化学习（RLVR）虽然能增强LLM的推理能力，但存在一个关键缺陷：它会系统性地抑制有效但罕见（在基础模型分布中概率较低）的推理路径。作者将这种现象理论化为"归一化挤压"，即模式寻求策略梯度和有限采样的相互作用会像高通似然滤波器一样，将罕见正确推理路径的概率推向统计灭绝。

Method: 提出摊销推理树搜索（ARTS）方法，与传统方法强制通过参数更新进行内部化不同，ARTS通过解耦生成与验证来优先考虑深思熟虑。引入流匹配目标，重新利用验证器来估计概率流的守恒，从而在传统判别目标失败的稀疏、高熵搜索空间中实现稳健导航。

Result: 在MATH-500基准测试中，ARTS实现了74.6%的性能（BoN@16），有效匹配了完全微调策略（74.7%）的性能，且无需修改生成主干。关键的是，在耦合RL优化崩溃到0% pass@k的长尾子集上，ARTS独特地恢复了显著性能，表明解耦验证与生成为解决复杂推理任务提供了更稳健的途径。

Conclusion: 解耦验证与生成比耦合的RL优化提供了更稳健的复杂推理任务解决途径。ARTS方法能够在保持生成主干不变的情况下，有效恢复被传统RLVR方法抑制的罕见正确推理路径，为解决长尾推理问题提供了新思路。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has established itself as the dominant paradigm for instilling rigorous reasoning capabilities in Large Language Models. While effective at amplifying dominant behaviors, we identify a critical pathology in this alignment process: the systematic suppression of valid but rare (low-likelihood under the base model distribution) reasoning paths. We theoretically characterize this phenomenon as a "Normalization Squeeze," where the interplay between mode-seeking policy gradients and finite sampling acts as a high-pass likelihood filter, driving the probability of rare correct traces to statistical extinction. To counteract this collapse without discarding the base model's latent diversity, we propose Amortized Reasoning Tree Search (ARTS). Unlike standard approaches that force internalization via parameter updates, ARTS prioritizes deliberation by decoupling generation from verification. We introduce a Flow Matching objective that repurposes the verifier to estimate the conservation of probability flow, enabling robust navigation through sparse, high-entropy search spaces where traditional discriminative objectives fail. Extensive experiments on the MATH-500 benchmark demonstrate that ARTS achieves a performance of 74.6% (BoN@16), effectively matching fully fine-tuned policies (74.7%) without modifying the generative backbone. Crucially, on the long-tail subset where coupled RL optimization collapses to 0% pass@k, ARTS uniquely recovers significant performance, suggesting that disentangling verification from generation offers a more robust pathway for solving complex reasoning tasks.

</details>


### [109] [X-VORTEX: Spatio-Temporal Contrastive Learning for Wake Vortex Trajectory Forecasting](https://arxiv.org/abs/2602.12869)
*Zhan Qu,Michael Färber*

Main category: cs.LG

TL;DR: X-VORTEX：基于增强重叠理论的时空对比学习框架，从无标签LiDAR点云序列中学习物理感知表示，仅需1%标注数据即可实现优于监督基线的涡旋中心定位


<details>
  <summary>Details</summary>
Motivation: 飞机尾涡是强烈的空气湍流，对空中交通管理构成重大安全和容量挑战。现有方法将每次扫描视为独立的完全监督分割问题，忽略了时间结构，且无法扩展到实践中收集的大量无标签数据。需要一种能够处理传感器稀疏性和时变涡旋动力学的解决方案。

Method: 提出X-VORTEX时空对比学习框架，基于增强重叠理论。通过结合弱扰动序列和强增强对应序列（通过时间子采样和空间掩码生成）构建配对输入，鼓励模型在缺失帧和部分观测之间对齐表示。架构包括时间分布几何编码器和序列聚合器，用于提取每帧特征并建模可变长度序列中的涡旋状态演化。

Result: 在超过一百万次LiDAR扫描的真实世界数据集上评估，X-VORTEX实现了优越的涡旋中心定位性能，仅需监督基线所需标注数据的1%。学习到的表示还支持准确的轨迹预测。

Conclusion: X-VORTEX通过时空对比学习有效解决了尾涡跟踪中的传感器稀疏性和时变动力学挑战，显著减少了对昂贵标注数据的依赖，为大规模无标签LiDAR数据的物理感知表示学习提供了可行方案。

Abstract: Wake vortices are strong, coherent air turbulences created by aircraft, and they pose a major safety and capacity challenge for air traffic management. Tracking how vortices move, weaken, and dissipate over time from LiDAR measurements is still difficult because scans are sparse, vortex signatures fade as the flow breaks down under atmospheric turbulence and instabilities, and point-wise annotation is prohibitively expensive. Existing approaches largely treat each scan as an independent, fully supervised segmentation problem, which overlooks temporal structure and does not scale to the vast unlabeled archives collected in practice. We present X-VORTEX, a spatio-temporal contrastive learning framework grounded in Augmentation Overlap Theory that learns physics-aware representations from unlabeled LiDAR point cloud sequences. X-VORTEX addresses two core challenges: sensor sparsity and time-varying vortex dynamics. It constructs paired inputs from the same underlying flight event by combining a weakly perturbed sequence with a strongly augmented counterpart produced via temporal subsampling and spatial masking, encouraging the model to align representations across missing frames and partial observations. Architecturally, a time-distributed geometric encoder extracts per-scan features and a sequential aggregator models the evolving vortex state across variable-length sequences. We evaluate on a real-world dataset of over one million LiDAR scans. X-VORTEX achieves superior vortex center localization while using only 1% of the labeled data required by supervised baselines, and the learned representations support accurate trajectory forecasting.

</details>


### [110] [Transporting Task Vectors across Different Architectures without Training](https://arxiv.org/abs/2602.12952)
*Filippo Rinaldi,Aniello Panariello,Giacomo Salici,Angelo Porrello,Simone Calderara*

Main category: cs.LG

TL;DR: Theseus是一种无需训练的方法，可将任务特定更新在不同宽度的异构模型间传输，通过功能匹配而非参数匹配实现。


<details>
  <summary>Details</summary>
Motivation: 大型预训练模型适应下游任务时会产生昂贵的任务特定参数更新，现有方法只能在相同架构模型间传输更新，而无法在不同宽度的异构模型间有效传输。

Method: 通过正交Procrustes分析对齐表示空间，将任务向量传输形式化为观测激活的功能匹配问题，获得稳定的闭式解，保持更新的几何结构。

Result: 在视觉和语言模型的不同宽度上评估Theseus，相比强基线获得一致改进，无需额外训练或反向传播。

Conclusion: 当任务身份通过功能而非参数定义时，任务更新可以在不同架构间有意义地传输。

Abstract: Adapting large pre-trained models to downstream tasks often produces task-specific parameter updates that are expensive to relearn for every model variant. While recent work has shown that such updates can be transferred between models with identical architectures, transferring them across models of different widths remains largely unexplored. In this work, we introduce Theseus, a training-free method for transporting task-specific updates across heterogeneous models. Rather than matching parameters directly, we characterize a task update by the functional effect it induces on intermediate representations. We formalize task-vector transport as a functional matching problem on observed activations and show that, after aligning representation spaces via orthogonal Procrustes analysis, it admits a stable closed-form solution that preserves the geometry of the update. We evaluate Theseus on vision and language models across different widths, showing consistent improvements over strong baselines without additional training or backpropagation. Our results show that task updates can be meaningfully transferred across architectures when task identity is defined functionally rather than parametrically.

</details>


### [111] [Ca-MCF: Category-level Multi-label Causal Feature selection](https://arxiv.org/abs/2602.12961)
*Wanfu Gao,Yanan Wang,Yonghao Li*

Main category: cs.LG

TL;DR: Ca-MCF是一种新的多标签因果特征选择方法，通过标签类别扁平化分解标签变量，利用类别感知恢复机制和结构对称性检查，显著提升了特征选择性能。


<details>
  <summary>Details</summary>
Motivation: 现有多标签因果特征选择方法主要在标签层面操作，将每个标签变量视为整体，忽略了各个类别特有的细粒度因果机制，需要更精细的方法来建模标签空间中的因果结构。

Method: 提出Ca-MCF方法：1) 使用标签类别扁平化将标签变量分解为特定类别节点；2) 引入基于解释性竞争的类别感知恢复机制，利用SCSMI和DCSMI恢复被标签相关性掩盖的因果特征；3) 结合结构对称性检查和跨维度冗余消除，确保马尔可夫毯的鲁棒性和紧凑性。

Result: 在7个真实世界数据集上的广泛实验表明，Ca-MCF显著优于现有最先进基准方法，在减少特征维度的同时实现了更优的预测准确性。

Conclusion: Ca-MCF通过类别级建模和创新的恢复机制，有效解决了多标签因果特征选择中的细粒度因果建模问题，为多标签学习提供了更精确的特征选择方法。

Abstract: Multi-label causal feature selection has attracted extensive attention in recent years. However, current methods primarily operate at the label level, treating each label variable as a monolithic entity and overlooking the fine-grained causal mechanisms unique to individual categories. To address this, we propose a Category-level Multi-label Causal Feature selection method named Ca-MCF. Ca-MCF utilizes label category flattening to decompose label variables into specific category nodes, enabling precise modeling of causal structures within the label space. Furthermore, we introduce an explanatory competition-based category-aware recovery mechanism that leverages the proposed Specific Category-Specific Mutual Information (SCSMI) and Distinct Category-Specific Mutual Information (DCSMI) to salvage causal features obscured by label correlations. The method also incorporates structural symmetry checks and cross-dimensional redundancy removal to ensure the robustness and compactness of the identified Markov Blankets. Extensive experiments across seven real-world datasets demonstrate that Ca-MCF significantly outperforms state-of-the-art benchmarks, achieving superior predictive accuracy with reduced feature dimensionality.

</details>


### [112] [Extending confidence calibration to generalised measures of variation](https://arxiv.org/abs/2602.12975)
*Andrew Thompson,Vivek Desai*

Main category: cs.LG

TL;DR: 提出Variation Calibration Error (VCE)作为评估机器学习分类器校准的新指标，扩展了传统的Expected Calibration Error (ECE)，能够评估概率分布的整体变化而不仅仅是最大概率


<details>
  <summary>Details</summary>
Motivation: 现有校准指标如ECE只评估最大概率（置信度）的校准，忽略了完整概率分布的信息。需要一种能够评估概率分布整体变化的校准指标，如考虑香农熵等分布变化度量

Method: 将ECE方法从仅评估置信度校准扩展到评估任何变化度量的校准，提出VCE指标。通过合成预测数据的数值实验验证方法有效性

Result: 在完美校准的合成预测数据上，VCE随着数据样本增加趋近于零，而文献中提出的另一种基于熵的校准指标(UCE)不具备这一理想性质

Conclusion: VCE是评估分类器校准的有效指标，能够全面评估概率分布的变化校准，相比现有方法具有更好的理论性质

Abstract: We propose the Variation Calibration Error (VCE) metric for assessing the calibration of machine learning classifiers. The metric can be viewed as an extension of the well-known Expected Calibration Error (ECE) which assesses the calibration of the maximum probability or confidence. Other ways of measuring the variation of a probability distribution exist which have the advantage of taking into account the full probability distribution, for example the Shannon entropy. We show how the ECE approach can be extended from assessing confidence calibration to assessing the calibration of any metric of variation. We present numerical examples upon synthetic predictions which are perfectly calibrated by design, demonstrating that, in this scenario, the VCE has the desired property of approaching zero as the number of data samples increases, in contrast to another entropy-based calibration metric (the UCE) which has been proposed in the literature.

</details>


### [113] [Drift-Aware Variational Autoencoder-based Anomaly Detection with Two-level Ensembling](https://arxiv.org/abs/2602.12976)
*Jin Li,Kleanthis Malialis,Christos G. Panayiotou,Marios M. Polycarpou*

Main category: cs.LG

TL;DR: VAE++ESDD：一种结合增量学习和两级集成（VAE集成用于异常预测，概念漂移检测器集成）的新方法，用于处理非平稳环境中的流数据异常检测


<details>
  <summary>Details</summary>
Motivation: 在数字世界中，大量流数据通常是无标签的，难以识别异常事件。在非平稳环境中，概念漂移会导致模型性能随时间下降，这使异常检测任务更加困难。

Method: 提出VAE++ESDD方法，采用增量学习和两级集成：1）变分自编码器(VAE)集成用于异常预测；2）概念漂移检测器集成，每个检测器使用基于统计的概念漂移机制。

Result: 在具有极低异常率和各种漂移特性的真实世界和合成数据集上进行综合实验，结果显示该方法显著优于强基准方法和最先进方法。

Conclusion: VAE++ESDD方法通过增量学习和两级集成有效解决了非平稳环境中流数据异常检测的挑战，在低异常率和概念漂移情况下表现出优越性能。

Abstract: In today's digital world, the generation of vast amounts of streaming data in various domains has become ubiquitous. However, many of these data are unlabeled, making it challenging to identify events, particularly anomalies. This task becomes even more formidable in nonstationary environments where model performance can deteriorate over time due to concept drift. To address these challenges, this paper presents a novel method, VAE++ESDD, which employs incremental learning and two-level ensembling: an ensemble of Variational AutoEncoder(VAEs) for anomaly prediction, along with an ensemble of concept drift detectors. Each drift detector utilizes a statistical-based concept drift mechanism. To evaluate the effectiveness of VAE++ESDD, we conduct a comprehensive experimental study using real-world and synthetic datasets characterized by severely or extremely low anomalous rates and various drift characteristics. Our study reveals that the proposed method significantly outperforms both strong baselines and state-of-the-art methods.

</details>


### [114] [MAUNet-Light: A Concise MAUNet Architecture for Bias Correction and Downscaling of Precipitation Estimates](https://arxiv.org/abs/2602.12980)
*Sumanta Chandra Mishra Sharma,Adway Mitra,Auroop Ratan Ganguly*

Main category: cs.LG

TL;DR: 提出轻量级神经网络MAUNet-Light，用于降水数据的偏差校正和空间降尺度，通过师生学习范式从MAUNet迁移知识，在保持精度的同时降低计算需求。


<details>
  <summary>Details</summary>
Motivation: 卫星数据和气候模型模拟存在系统性偏差，传统深度学习方法计算和内存需求高，需要开发轻量级神经网络同时进行偏差校正和降尺度。

Method: 采用师生学习范式，从训练好的MAUNet迁移知识，设计紧凑轻量的MAUNet-Light架构，同时处理偏差校正和空间降尺度任务。

Result: MAUNet-Light在保持与最先进方法相当精度的同时，显著降低了计算需求，验证了MAUNet在偏差校正任务上的适应性。

Conclusion: 师生学习范式可有效开发轻量级神经网络，MAUNet-Light为降水数据的偏差校正和降尺度提供了高效解决方案，平衡了精度和计算效率。

Abstract: Satellite-derived data products and climate model simulations of geophysical variables like precipitation, often exhibit systematic biases compared to in-situ measurements. Bias correction and spatial downscaling are fundamental components to develop operational weather forecast systems, as they seek to improve the consistency between coarse-resolution climate model simulations or satellite-based estimates and ground-based observations. In recent years, deep learning-based models have been increasingly replaced traditional statistical methods to generate high-resolution, bias free projections of climate variables. For example, Max-Average U-Net (MAUNet) architecture has been demonstrated for its ability to downscale precipitation estimates. The versatility and adaptability of these neural models make them highly effective across a range of applications, though this often come at the cost of high computational and memory requirements. The aim of this research is to develop light-weight neural network architectures for both bias correction and downscaling of precipitation, for which the teacher-student based learning paradigm is explored. This research demonstrates the adaptability of MAUNet to the task of bias correction, and further introduces a compact, lightweight neural network architecture termed MAUNet-Light.The proposed MAUNet-Light model is developed by transferring knowledge from the trained MAUNet, and it is designed to perform both downscaling and bias correction with reduced computational requirements without any significant loss in accuracy compared to state-of-the-art.

</details>


### [115] [Multi-Dimensional Visual Data Recovery: Scale-Aware Tensor Modeling and Accelerated Randomized Computation](https://arxiv.org/abs/2602.12982)
*Wenjin Qin,Hailin Wang,Jiangjun Peng,Jianjun Wang,Tingwen Huang*

Main category: cs.LG

TL;DR: 本文提出了一种基于全连接张量网络分解的广义非凸正则化方法，用于多维数据恢复，通过ADMM框架和随机压缩算法提高计算效率，并在量化观测数据上验证了方法的优越性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于FCTN分解的多维数据恢复方法在计算效率和建模能力方面仍有改进空间，特别是在处理大规模数据时存在计算瓶颈。

Method: 1) 从梯度映射角度提出FCTN广义非凸正则化范式；2) 构建从非量化观测到粗粒度量化观测的可靠可扩展恢复模型；3) 基于ADMM框架设计收敛保证的优化算法；4) 利用数值线性代数中的草图技术设计随机压缩算法加速计算。

Result: 理论推导了近似误差上界和收敛性分析，大量数值实验表明该方法在定量指标、视觉质量和运行时间方面优于现有最先进方法。

Conclusion: 提出的方法有效解决了FCTN分解在多维数据恢复中的计算效率和建模能力问题，通过非凸正则化和随机压缩技术实现了高效且性能优越的数据恢复。

Abstract: The recently proposed fully-connected tensor network (FCTN) decomposition has demonstrated significant advantages in correlation characterization and transpositional invariance, and has achieved notable achievements in multi-dimensional data processing and analysis. However, existing multi-dimensional data recovery methods leveraging FCTN decomposition still have room for further enhancement, particularly in computational efficiency and modeling capability. To address these issues, we first propose a FCTN-based generalized nonconvex regularization paradigm from the perspective of gradient mapping. Then, reliable and scalable multi-dimensional data recovery models are investigated, where the model formulation is shifted from unquantized observations to coarse-grained quantized observations. Based on the alternating direction method of multipliers (ADMM) framework, we derive efficient optimization algorithms with convergence guarantees to solve the formulated models. To alleviate the computational bottleneck encountered when processing large-scale multi-dimensional data, fast and efficient randomized compression algorithms are devised in virtue of sketching techniques in numerical linear algebra. These dimensionality-reduction techniques serve as the computational acceleration core of our proposed algorithm framework. Theoretical results on approximation error upper bounds and convergence analysis for the proposed method are derived. Extensive numerical experiments illustrate the effectiveness and superiority of the proposed algorithm over other state-of-the-art methods in terms of quantitative metrics, visual quality, and running time.

</details>


### [116] [Uncertainty in Federated Granger Causality: From Origins to Systemic Consequences](https://arxiv.org/abs/2602.13004)
*Ayush Mohanty,Nazal Mohamed,Nagi Gebraeel*

Main category: cs.LG

TL;DR: 提出首个联邦格兰杰因果分析中的不确定性量化方法，系统分类不确定性来源并推导闭式递归公式，显著提升联邦因果推断的可靠性和可解释性


<details>
  <summary>Details</summary>
Motivation: 现有联邦格兰杰因果分析仅提供确定性点估计，忽略了不确定性量化，这在分布式高维数据应用中限制了模型的可靠性和可解释性

Method: 系统分类不确定性来源（数据噪声vs模型变异性），推导闭式递归公式建模客户端-服务器交互中的不确定性传播，识别四种新的交叉协方差分量，定义严格的收敛条件

Result: 收敛分析表明稳态方差仅取决于客户端数据统计量，消除了对初始先验的依赖；在合成基准和真实工业数据集上的实验显示，显式表征不确定性显著提升了联邦因果推断的可靠性和可解释性

Conclusion: 建立了首个联邦格兰杰因果分析中不确定性量化的方法论框架，为分布式因果推断提供了更可靠和可解释的工具

Abstract: Granger Causality (GC) provides a rigorous framework for learning causal structures from time-series data. Recent federated variants of GC have targeted distributed infrastructure applications (e.g., smart grids) with distributed clients that generate high-dimensional data bound by data-sovereignty constraints. However, Federated GC algorithms only yield deterministic point estimates of causality and neglect uncertainty. This paper establishes the first methodology for rigorously quantifying uncertainty and its propagation within federated GC frameworks. We systematically classify sources of uncertainty, explicitly differentiating aleatoric (data noise) from epistemic (model variability) effects. We derive closed-form recursions that model the evolution of uncertainty through client-server interactions and identify four novel cross-covariance components that couple data uncertainties with model parameter uncertainties across the federated architecture. We also define rigorous convergence conditions for these uncertainty recursions and obtain explicit steady-state variances for both server and client model parameters. Our convergence analysis demonstrates that steady-state variances depend exclusively on client data statistics, thus eliminating dependence on initial epistemic priors and enhancing robustness. Empirical evaluations on synthetic benchmarks and real-world industrial datasets demonstrate that explicitly characterizing uncertainty significantly improves the reliability and interpretability of federated causal inference.

</details>


### [117] [Machine Learning-Based Classification of Jhana Advanced Concentrative Absorption Meditation (ACAM-J) using 7T fMRI](https://arxiv.org/abs/2602.13008)
*Puneet Kumar,Winson F. Z. Yang,Alakhsimar Singh,Xiaobai Li,Matthew D. Sacchet*

Main category: cs.LG

TL;DR: 使用fMRI区域同质性特征和机器学习方法，可以区分高级禅定冥想状态与非冥想状态，准确率达66.82%，前额叶和前扣带回是关键区分区域。


<details>
  <summary>Details</summary>
Motivation: 研究高级禅定冥想（ACAM-J）的神经相关性对于理解意识和幸福感至关重要，需要探索能否使用fMRI区域同质性特征通过机器学习方法有效分类冥想状态。

Method: 收集20名高级冥想者的fMRI数据训练分类器，使用单个案例的密集数据进行泛化评估。计算ReHo图，从预定义脑区提取特征，采用分层交叉验证训练多个机器学习分类器，使用集成模型区分ACAM-J与非冥想状态。

Result: 集成模型在区分ACAM-J与控制条件时达到66.82%准确率（p < 0.05）。特征重要性分析显示前额叶和前扣带回区域对模型决策贡献最大，Cohen's kappa显示中等一致性，支持机器学习区分冥想状态的可行性。

Conclusion: 机器学习方法可用于分类高级冥想状态，前额叶和前扣带回在区分中起关键作用，为未来神经调控研究和高级冥想的机制模型提供了基础。

Abstract: Jhana advanced concentration absorption meditation (ACAM-J) is related to profound changes in consciousness and cognitive processing, making the study of their neural correlates vital for insights into consciousness and well-being. This study evaluates whether functional MRI-derived regional homogeneity (ReHo) can be used to classify ACAM-J using machine-learning approaches. We collected group-level fMRI data from 20 advanced meditators to train the classifiers, and intensive single-case data from an advanced practitioner performing ACAM-J and control tasks to evaluate generalization. ReHo maps were computed, and features were extracted from predefined brain regions of interest. We trained multiple machine learning classifiers using stratified cross-validation to evaluate whether ReHo patterns distinguish ACAM-J from non-meditative states. Ensemble models achieved 66.82% (p < 0.05) accuracy in distinguishing ACAM-J from control conditions. Feature-importance analysis indicated that prefrontal and anterior cingulate areas contributed most to model decisions, aligning with established involvement of these regions in attentional regulation and metacognitive processes. Moreover, moderate agreement reflected in Cohen's kappa supports the feasibility of using machine learning to distinguish ACAM-J from non-meditative states. These findings advocate machine-learning's feasibility in classifying advanced meditation states, future research on neuromodulation and mechanistic models of advanced meditation.

</details>


### [118] [Probabilistic Wind Power Forecasting with Tree-Based Machine Learning and Weather Ensembles](https://arxiv.org/abs/2602.13010)
*Max Bruninx,Diederik van Binsbergen,Timothy Verstraeten,Ann Nowé,Jan Helsen*

Main category: cs.LG

TL;DR: 本文比较了三种概率预测方法（保形分位数回归、自然梯度提升和条件扩散模型）结合梯度提升树进行风电功率日前预测，相比传统工程方法显著提升精度，其中条件扩散模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源并网需求增加，准确的风电功率预测对电网稳定运行至关重要。传统工程方法存在精度限制，需要探索更先进的机器学习方法来提升预测性能。

Method: 采用三种概率预测方法：保形分位数回归、自然梯度提升和条件扩散模型，结合梯度提升树和天气预报集合。使用比利时海上风电场四年数据进行验证，并与基于功率曲线和校准尾流模型的确定性工程方法进行基准比较。

Result: 机器学习方法相比功率曲线方法和校准尾流模型，分别将平均绝对误差降低了53%和33%。条件扩散模型在概率预测和点估计方面表现最佳。使用天气预报集合可将点预测精度提升达23%。

Conclusion: 机器学习方法显著优于传统工程方法，条件扩散模型是风电功率概率预测的最佳选择，天气预报集合能有效提升预测精度，为可再生能源并网提供更可靠的预测工具。

Abstract: Accurate production forecasts are essential to continue facilitating the integration of renewable energy sources into the power grid. This paper illustrates how to obtain probabilistic day-ahead forecasts of wind power generation via gradient boosting trees using an ensemble of weather forecasts. To this end, we perform a comparative analysis across three state-of-the-art probabilistic prediction methods-conformalised quantile regression, natural gradient boosting and conditional diffusion models-all of which can be combined with tree-based machine learning. The methods are validated using four years of data for all wind farms present within the Belgian offshore zone. Additionally, the point forecasts are benchmarked against deterministic engineering methods, using either the power curve or an advanced approach incorporating a calibrated analytical wake model. The experimental results show that the machine learning methods improve the mean absolute error by up to 53% and 33% compared to the power curve and the calibrated wake model. Considering the three probabilistic prediction methods, the conditional diffusion model is found to yield the best overall probabilistic and point estimate of wind power generation. Moreover, the findings suggest that the use of an ensemble of weather forecasts can improve point forecast accuracy by up to 23%.

</details>


### [119] [Prior-Guided Symbolic Regression: Towards Scientific Consistency in Equation Discovery](https://arxiv.org/abs/2602.13021)
*Jing Xiao,Xinhai Chen,Jiaming Peng,Qinglin Wang,Menghan Jia,Zhiquan Lai,Guangping Yu,Dongsheng Li,Tiejun Li,Jie Liu*

Main category: cs.LG

TL;DR: PG-SR是一个先验引导的符号回归框架，通过三阶段流程和先验约束检查器，将领域先验编码为可执行约束程序，避免伪方程陷阱，确保科学一致性。


<details>
  <summary>Details</summary>
Motivation: 现有符号回归方法常陷入伪方程陷阱：产生的方程能很好拟合观测数据，但与基本科学原理不一致。主要原因是这些方法过度依赖经验风险最小化，缺乏确保科学一致性的显式约束。

Method: 提出PG-SR框架，采用三阶段流程：预热、进化和精炼。引入先验约束检查器将领域先验编码为可执行约束程序，在进化阶段采用先验退火约束评估机制，逐步引导发现过程朝向科学一致区域。

Result: 理论上证明PG-SR降低了假设空间的Rademacher复杂度，得到更紧的泛化边界，建立了对抗伪方程的保证。实验上PG-SR在多个领域超越最先进基线，对先验质量、噪声数据和数据稀缺保持鲁棒性。

Conclusion: PG-SR通过显式编码领域先验作为约束，有效解决了符号回归中的伪方程问题，确保发现的方程既拟合数据又符合科学原理，为科学发现提供了更可靠的工具。

Abstract: Symbolic Regression (SR) aims to discover interpretable equations from observational data, with the potential to reveal underlying principles behind natural phenomena. However, existing approaches often fall into the Pseudo-Equation Trap: producing equations that fit observations well but remain inconsistent with fundamental scientific principles. A key reason is that these approaches are dominated by empirical risk minimization, lacking explicit constraints to ensure scientific consistency. To bridge this gap, we propose PG-SR, a prior-guided SR framework built upon a three-stage pipeline consisting of warm-up, evolution, and refinement. Throughout the pipeline, PG-SR introduces a prior constraint checker that explicitly encodes domain priors as executable constraint programs, and employs a Prior Annealing Constrained Evaluation (PACE) mechanism during the evolution stage to progressively steer discovery toward scientifically consistent regions. Theoretically, we prove that PG-SR reduces the Rademacher complexity of the hypothesis space, yielding tighter generalization bounds and establishing a guarantee against pseudo-equations. Experimentally, PG-SR outperforms state-of-the-art baselines across diverse domains, maintaining robustness to varying prior quality, noisy data, and data scarcity.

</details>


### [120] [Resource-Efficient Gesture Recognition through Convexified Attention](https://arxiv.org/abs/2602.13030)
*Daniel Schwartz,Dario Salvucci,Yusuf Osmanlioglu,Richard Vallett,Genevieve Dion,Ali Shokoufandeh*

Main category: cs.LG

TL;DR: 提出一种凸化注意力机制，用于可穿戴电子纺织品手势识别，仅需120-360个参数，在电容传感器上实现100%准确率，推理时间低于300微秒。


<details>
  <summary>Details</summary>
Motivation: 可穿戴电子纺织品需要手势识别功能，但面临功耗、计算能力和尺寸的严格限制，传统深度学习方法不实用。现有轻量级架构仍需数千参数，难以在纺织品集成平台上部署。

Method: 引入凸化注意力机制，通过非扩张单纯形投影和凸损失函数动态加权特征。使用欧几里得投影到概率单纯形结合多类铰链损失，替代传统非凸softmax操作，确保全局收敛保证。

Result: 在四连接点纺织品电容传感器上，点击和滑动手势均达到100%准确率，参数仅需120-360个（减少97%），推理时间290-296微秒，存储需求小于7KB。

Conclusion: 凸优化方法能够为纺织品接口实现高效的设备端机器学习，在实验室条件下验证了基本手势交互的可行性，实际部署需要多用户、多环境和更复杂手势的验证。

Abstract: Wearable e-textile interfaces require gesture recognition capabilities but face severe constraints in power consumption, computational capacity, and form factor that make traditional deep learning impractical. While lightweight architectures like MobileNet improve efficiency, they still demand thousands of parameters, limiting deployment on textile-integrated platforms. We introduce a convexified attention mechanism for wearable applications that dynamically weights features while preserving convexity through nonexpansive simplex projection and convex loss functions. Unlike conventional attention mechanisms using non-convex softmax operations, our approach employs Euclidean projection onto the probability simplex combined with multi-class hinge loss, ensuring global convergence guarantees. Implemented on a textile-based capacitive sensor with four connection points, our approach achieves 100.00\% accuracy on tap gestures and 100.00\% on swipe gestures -- consistent across 10-fold cross-validation and held-out test evaluation -- while requiring only 120--360 parameters, a 97\% reduction compared to conventional approaches. With sub-millisecond inference times (290--296$μ$s) and minimal storage requirements ($<$7KB), our method enables gesture interfaces directly within e-textiles without external processing. Our evaluation, conducted in controlled laboratory conditions with a single-user dataset, demonstrates feasibility for basic gesture interactions. Real-world deployment would require validation across multiple users, environmental conditions, and more complex gesture vocabularies. These results demonstrate how convex optimization can enable efficient on-device machine learning for textile interfaces.

</details>


### [121] [Look Inward to Explore Outward: Learning Temperature Policy from LLM Internal States via Hierarchical RL](https://arxiv.org/abs/2602.13035)
*Yixiao Zhou,Yang Li,Dongzhou Cheng,Hehe Fan,Yu Cheng*

Main category: cs.LG

TL;DR: 提出Introspective LLM框架，通过分层强化学习让LLM在生成过程中学习控制采样温度，实现基于任务奖励的动态探索-利用权衡


<details>
  <summary>Details</summary>
Motivation: 现有方法使用静态温度或启发式调整，与任务级奖励解耦，无法在推理过程中根据上下文动态调整探索策略

Method: 分层强化学习框架：在每个解码步骤，模型根据隐藏状态选择温度，从相应分布采样下一个token；通过坐标上升方案联合优化温度和token策略

Result: 在数学推理基准测试中，学习到的温度策略优于固定和启发式基线，并展现出与推理不确定性对齐的可解释探索行为

Conclusion: 将温度控制作为可学习组件集成到RLVR框架中，使LLM能够根据上下文动态调整探索策略，提升推理性能

Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) trains large language models (LLMs) from sampled trajectories, making decoding strategy a core component of learning rather than a purely inference-time choice. Sampling temperature directly controls the exploration--exploitation trade-off by modulating policy entropy, yet existing methods rely on static values or heuristic adaptations that are decoupled from task-level rewards. We propose Introspective LLM, a hierarchical reinforcement learning framework that learns to control sampling temperature during generation. At each decoding step, the model selects a temperature based on its hidden state and samples the next token from the resulting distribution. Temperature and token policies are jointly optimized from downstream rewards using a coordinate ascent scheme. Experiments on mathematical reasoning benchmarks show that learned temperature policies outperform fixed and heuristic baselines, while exhibiting interpretable exploration behaviors aligned with reasoning uncertainty.

</details>


### [122] [TCRL: Temporal-Coupled Adversarial Training for Robust Constrained Reinforcement Learning in Worst-Case Scenarios](https://arxiv.org/abs/2602.13040)
*Wentao Xu,Zhongming Yao,Weihao Li,Zhenghang Song,Yumeng Song,Tianyi Li,Yushuai Li*

Main category: cs.LG

TL;DR: 提出TCRL框架，通过时间耦合对抗训练增强约束强化学习的鲁棒性，针对连续时间扰动而非单步独立攻击


<details>
  <summary>Details</summary>
Motivation: 现有鲁棒约束强化学习方法主要关注单步扰动和时间独立的对抗模型，缺乏对时间耦合扰动的显式建模，而现实世界中的攻击往往是连续相关的

Method: 1) 引入最坏情况感知成本约束函数，估计时间耦合扰动下的安全成本，无需显式建模对抗攻击者；2) 建立奖励的双重约束防御机制，对抗时间耦合对手同时保持奖励不可预测性

Result: 实验结果表明，TCRL在各种CRL任务中，在对抗时间耦合扰动攻击的鲁棒性方面持续优于现有方法

Conclusion: TCRL框架有效解决了约束强化学习中对时间耦合扰动的鲁棒性问题，为安全关键领域提供了更可靠的决策方案

Abstract: Constrained Reinforcement Learning (CRL) aims to optimize decision-making policies under constraint conditions, making it highly applicable to safety-critical domains such as autonomous driving, robotics, and power grid management. However, existing robust CRL approaches predominantly focus on single-step perturbations and temporally independent adversarial models, lacking explicit modeling of robustness against temporally coupled perturbations. To tackle these challenges, we propose TCRL, a novel temporal-coupled adversarial training framework for robust constrained reinforcement learning (TCRL) in worst-case scenarios. First, TCRL introduces a worst-case-perceived cost constraint function that estimates safety costs under temporally coupled perturbations without the need to explicitly model adversarial attackers. Second, TCRL establishes a dual-constraint defense mechanism on the reward to counter temporally coupled adversaries while maintaining reward unpredictability. Experimental results demonstrate that TCRL consistently outperforms existing methods in terms of robustness against temporally coupled perturbation attacks across a variety of CRL tasks.

</details>


### [123] [GPTZero: Robust Detection of LLM-Generated Texts](https://arxiv.org/abs/2602.13042)
*George Alexandru Adam,Alexander Cui,Edwin Thomas,Emily Napier,Nazar Shmatko,Jacob Schnell,Jacob Junqi Tian,Alekhya Dronavalli,Edward Tian,Dongwon Lee*

Main category: cs.LG

TL;DR: GPTZero是一种先进的AI文本检测工具，能准确区分人类写作和AI生成文本，具有分层多任务架构、高精度检测、对抗攻击鲁棒性等特点。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的普及，AI生成文本带来了新的挑战：如何区分人类写作和AI生成内容。这涉及到技能评估的可靠性、低质量内容的大规模生产以及错误信息的传播等问题。

Method: 引入GPTZero，采用分层多任务架构，支持灵活的人类与AI文本分类体系。通过多层自动红队测试实现对抗攻击和改写的鲁棒性，提供可解释的检测结果。

Result: GPTZero在多个领域实现了最先进的检测精度，提供细粒度预测，对对抗攻击和改写具有优越的鲁棒性，能够准确可靠地检测AI生成文本。

Conclusion: GPTZero为AI文本检测提供了工业级解决方案，不仅准确检测AI生成内容，还教育用户负责任地使用，确保文本评估的公平性和透明度。

Abstract: While historical considerations surrounding text authenticity revolved primarily around plagiarism, the advent of large language models (LLMs) has introduced a new challenge: distinguishing human-authored from AI-generated text. This shift raises significant concerns, including the undermining of skill evaluations, the mass-production of low-quality content, and the proliferation of misinformation. Addressing these issues, we introduce GPTZero a state-of-the-art industrial AI detection solution, offering reliable discernment between human and LLM-generated text. Our key contributions include: introducing a hierarchical, multi-task architecture enabling a flexible taxonomy of human and AI texts, demonstrating state-of-the-art accuracy on a variety of domains with granular predictions, and achieving superior robustness to adversarial attacks and paraphrasing via multi-tiered automated red teaming. GPTZero offers accurate and explainable detection, and educates users on its responsible use, ensuring fair and transparent assessment of text.

</details>


### [124] [Geometric Manifold Rectification for Imbalanced Learning](https://arxiv.org/abs/2602.13045)
*Xubin Wang,Qing Li,Weijia Jia*

Main category: cs.LG

TL;DR: 提出GMR框架，通过几何置信度估计和非对称清洗处理不平衡分类问题，特别针对噪声和类别重叠的表格数据。


<details>
  <summary>Details</summary>
Motivation: 不平衡分类在机器学习中是一个严峻挑战，尤其是在表格数据存在噪声和类别边界重叠的情况下。从几何角度看，核心困难在于多数类拓扑侵入少数类流形，模糊了真实决策边界。传统欠采样方法（如ENN）使用对称清洗规则和统一投票，无法捕捉局部流形结构，且常常无意中移除信息丰富的少数类样本。

Method: 提出GMR（几何流形矫正）框架：1）几何置信度估计：使用逆距离加权kNN投票和自适应距离度量来捕捉局部可靠性；2）非对称清洗：对多数类样本严格清洗，同时通过设置少数类移除上限保守地保护少数类样本。

Result: 在多个基准数据集上的广泛实验表明，GMR与强大的采样基线方法具有竞争力。

Conclusion: GMR框架通过利用局部几何先验，能够鲁棒地处理不平衡结构化数据，有效解决传统方法在捕捉局部流形结构和保护少数类样本方面的不足。

Abstract: Imbalanced classification presents a formidable challenge in machine learning, particularly when tabular datasets are plagued by noise and overlapping class boundaries. From a geometric perspective, the core difficulty lies in the topological intrusion of the majority class into the minority manifold, which obscures the true decision boundary. Traditional undersampling techniques, such as Edited Nearest Neighbours (ENN), typically employ symmetric cleaning rules and uniform voting, failing to capture the local manifold structure and often inadvertently removing informative minority samples. In this paper, we propose GMR (Geometric Manifold Rectification), a novel framework designed to robustly handle imbalanced structured data by exploiting local geometric priors. GMR makes two contributions: (1) Geometric confidence estimation that uses inverse-distance weighted kNN voting with an adaptive distance metric to capture local reliability; and (2) asymmetric cleaning that is strict on majority samples while conservatively protecting minority samples via a safe-guarding cap on minority removal. Extensive experiments on multiple benchmark datasets show that GMR is competitive with strong sampling baselines.

</details>


### [125] [Quantization-Aware Collaborative Inference for Large Embodied AI Models](https://arxiv.org/abs/2602.13052)
*Zhonghao Lyu,Ming Xiao,Mikael Skoglund,Merouane Debbah,H. Vincent Poor*

Main category: cs.LG

TL;DR: 本文研究面向具身AI系统的量化感知协同推理，提出量化诱导推理失真的可处理近似方法，推导量化率-失真函数上下界，并设计联合量化比特宽和计算频率优化方案。


<details>
  <summary>Details</summary>
Motivation: 大型人工智能模型（LAIMs）作为具身AI应用的核心智能引擎，其庞大的参数量和计算需求对资源受限的具身代理构成重大挑战，需要研究高效的推理方法。

Method: 首先开发量化诱导推理失真的可处理近似方法，基于此推导量化率-失真函数的上下界；然后提出在延迟和能量约束下的联合量化比特宽和计算频率设计问题，以最小化失真上界。

Result: 广泛评估验证了所提出的失真近似方法、推导的率-失真界以及联合设计的有效性。仿真和真实测试实验表明，该设计能在边缘具身AI系统中有效平衡推理质量、延迟和能耗。

Conclusion: 本文提出的量化感知协同推理框架为资源受限的具身AI系统提供了有效的解决方案，通过联合优化量化比特宽和计算频率，在满足延迟和能量约束的同时最小化推理失真。

Abstract: Large artificial intelligence models (LAIMs) are increasingly regarded as a core intelligence engine for embodied AI applications. However, the massive parameter scale and computational demands of LAIMs pose significant challenges for resource-limited embodied agents. To address this issue, we investigate quantization-aware collaborative inference (co-inference) for embodied AI systems. First, we develop a tractable approximation for quantization-induced inference distortion. Based on this approximation, we derive lower and upper bounds on the quantization rate-inference distortion function, characterizing its dependence on LAIM statistics, including the quantization bit-width. Next, we formulate a joint quantization bit-width and computation frequency design problem under delay and energy constraints, aiming to minimize the distortion upper bound while ensuring tightness through the corresponding lower bound. Extensive evaluations validate the proposed distortion approximation, the derived rate-distortion bounds, and the effectiveness of the proposed joint design. Particularly, simulations and real-world testbed experiments demonstrate the effectiveness of the proposed joint design in balancing inference quality, latency, and energy consumption in edge embodied AI systems.

</details>


### [126] [Diverging Flows: Detecting Extrapolations in Conditional Generation](https://arxiv.org/abs/2602.13061)
*Constantinos Tsakonas,Serena Ivaldi,Jean-Baptiste Mouret*

Main category: cs.LG

TL;DR: 本文提出Diverging Flows方法，通过结构性地强制离流输入的低效传输，使单个模型能同时进行条件生成和原生外推检测，解决流匹配模型在安全关键部署中的外推风险问题。


<details>
  <summary>Details</summary>
Motivation: 流匹配（FM）在复杂条件分布建模方面表现出色，但在安全关键部署中存在严重的外推风险：由于平滑性偏差，流模型即使对于离流输入也会产生看似合理的输出，导致难以区分的静默故障。

Method: 提出Diverging Flows方法，通过结构性地强制离流输入的低效传输，使单个模型能同时执行条件生成和原生外推检测。该方法确保模型在遇到离流输入时产生可识别的异常行为。

Result: 在合成流形、跨域风格迁移和天气温度预测等任务上验证，Diverging Flows能有效检测外推，同时不损害预测保真度或推理延迟，实现了可靠的外推检测能力。

Conclusion: Diverging Flows为可信赖的流模型提供了稳健解决方案，为在医学、机器人和气候科学等领域的可靠部署铺平了道路，解决了安全关键应用中的外推风险问题。

Abstract: The ability of Flow Matching (FM) to model complex conditional distributions has established it as the state-of-the-art for prediction tasks (e.g., robotics, weather forecasting). However, deployment in safety-critical settings is hindered by a critical extrapolation hazard: driven by smoothness biases, flow models yield plausible outputs even for off-manifold conditions, resulting in silent failures indistinguishable from valid predictions. In this work, we introduce Diverging Flows, a novel approach that enables a single model to simultaneously perform conditional generation and native extrapolation detection by structurally enforcing inefficient transport for off-manifold inputs. We evaluate our method on synthetic manifolds, cross-domain style transfer, and weather temperature forecasting, demonstrating that it achieves effective detection of extrapolations without compromising predictive fidelity or inference latency. These results establish Diverging Flows as a robust solution for trustworthy flow models, paving the way for reliable deployment in domains such as medicine, robotics, and climate science.

</details>


### [127] [Backdoor Attacks on Contrastive Continual Learning for IoT Systems](https://arxiv.org/abs/2602.13062)
*Alfous Tim,Kuniyilh Simi D*

Main category: cs.LG

TL;DR: 本文分析了物联网系统中对比持续学习面临的后门攻击安全漏洞，揭示了嵌入对齐和回放强化机制如何被利用植入持久恶意行为，并提出了针对物联网约束的防御策略。


<details>
  <summary>Details</summary>
Motivation: 物联网系统越来越依赖持续学习来适应非平稳环境，但对比持续学习结合对比表示学习和增量适应时，其几何特性和回放机制会引入新的安全漏洞，特别是后门攻击可能利用嵌入对齐和回放强化植入持久恶意行为。

Method: 1) 形式化嵌入级攻击目标；2) 分析物联网部署中独特的持久性机制；3) 开发针对物联网的分层分类法；4) 比较不同学习范式的漏洞；5) 在物联网约束下评估防御策略（有限内存、边缘计算、联邦聚合）。

Result: 研究发现对比持续学习虽然能增强物联网的自适应智能，但如果安全措施不足，可能会提升长期存在的表示级威胁。后门攻击能够利用嵌入对齐和回放强化机制，在更新和部署周期中保持持久性。

Conclusion: 对比持续学习在提升物联网自适应智能方面有效，但需要充分的安全保障措施来防范持久性表示级威胁。物联网系统的有限资源约束使防御更具挑战性，需要专门的安全策略。

Abstract: The Internet of Things (IoT) systems increasingly depend on continual learning to adapt to non-stationary environments. These environments can include factors such as sensor drift, changing user behavior, device aging, and adversarial dynamics. Contrastive continual learning (CCL) combines contrastive representation learning with incremental adaptation, enabling robust feature reuse across tasks and domains. However, the geometric nature of contrastive objectives, when paired with replay-based rehearsal and stability-preserving regularization, introduces new security vulnerabilities. Notably, backdoor attacks can exploit embedding alignment and replay reinforcement, enabling the implantation of persistent malicious behaviors that endure through updates and deployment cycles. This paper provides a comprehensive analysis of backdoor attacks on CCL within IoT systems. We formalize the objectives of embedding-level attacks, examine persistence mechanisms unique to IoT deployments, and develop a layered taxonomy tailored to IoT. Additionally, we compare vulnerabilities across various learning paradigms and evaluate defense strategies under IoT constraints, including limited memory, edge computing, and federated aggregation. Our findings indicate that while CCL is effective for enhancing adaptive IoT intelligence, it may also elevate long-lived representation-level threats if not adequately secured.

</details>


### [128] [Memory-Efficient Structured Backpropagation for On-Device LLM Fine-Tuning](https://arxiv.org/abs/2602.13069)
*Juneyoung Park,Yuri Hong,Seongwan Kim,Jaeho Lee*

Main category: cs.LG

TL;DR: MeSP是一种内存高效的结构化反向传播方法，通过利用LoRA的低秩结构手动推导反向传播，在移动设备上实现隐私保护的LLM个性化微调，相比传统方法减少49%内存使用。


<details>
  <summary>Details</summary>
Motivation: 移动设备内存有限（通常6-12GB），现有方法需要在精确梯度（高内存）和噪声估计（低内存）之间权衡，限制了在设备上隐私保护微调的可行性。

Method: 提出Memory-efficient Structured Backpropagation (MeSP)，手动推导利用LoRA低秩结构的反向传播。关键洞察：中间投影h=xA可以在反向传播时以最小成本重新计算（因为秩r远小于输入维度d_in），无需存储。

Result: MeSP在Qwen2.5模型（0.5B-3B）上相比MeBP平均减少49%内存，同时计算数学上完全相同的梯度。对于Qwen2.5-0.5B，峰值内存从361MB降至136MB。分析还显示MeZO的梯度估计与真实梯度相关性极低（余弦相似度≈0.001）。

Conclusion: MeSP通过结构化反向传播方法，在移动设备内存约束下实现了精确梯度计算，填补了现有方法在内存效率和梯度精度之间的空白，使之前不可行的微调场景成为可能。

Abstract: On-device fine-tuning enables privacy-preserving personalization of large language models, but mobile devices impose severe memory constraints, typically 6--12GB shared across all workloads. Existing approaches force a trade-off between exact gradients with high memory (MeBP) and low memory with noisy estimates (MeZO). We propose Memory-efficient Structured Backpropagation (MeSP), which bridges this gap by manually deriving backward passes that exploit LoRA's low-rank structure. Our key insight is that the intermediate projection $h = xA$ can be recomputed during backward at minimal cost since rank $r \ll d_{in}$, eliminating the need to store it. MeSP achieves 49\% average memory reduction compared to MeBP on Qwen2.5 models (0.5B--3B) while computing mathematically identical gradients. Our analysis also reveals that MeZO's gradient estimates show near-zero correlation with true gradients (cosine similarity $\approx$0.001), explaining its slow convergence. MeSP reduces peak memory from 361MB to 136MB for Qwen2.5-0.5B, enabling fine-tuning scenarios previously infeasible on memory-constrained devices.

</details>


### [129] [Bus-Conditioned Zero-Shot Trajectory Generation via Task Arithmetic](https://arxiv.org/abs/2602.13071)
*Shuai Liu,Ning Cao,Yile Chen,Yue Jiang,Gao Cong*

Main category: cs.LG

TL;DR: MobTA：首个将任务算术引入轨迹生成的方法，利用源城市移动数据和两地公交时刻表，在零目标城市移动数据情况下生成目标城市轨迹


<details>
  <summary>Details</summary>
Motivation: 移动轨迹数据对智慧城市应用至关重要但难以获取，现有方法通常需要目标城市的部分真实数据，限制了在数据不可访问场景下的应用

Method: 提出公交条件零样本轨迹生成问题设置，引入任务算术：在源城市建模从公交时刻表轨迹生成到移动轨迹生成的参数偏移，通过任务向量算术操作将该偏移应用到目标城市

Result: MobTA显著优于现有方法，性能接近使用目标城市移动轨迹微调的模型，并在理论和实验上验证了其在基础模型和指令调优LLM上的稳定性

Conclusion: MobTA首次将任务算术应用于轨迹生成，实现了无需目标城市真实移动数据的轨迹生成，为数据不可访问场景提供了有效解决方案

Abstract: Mobility trajectory data provide essential support for smart city applications. However, such data are often difficult to obtain. Meanwhile, most existing trajectory generation methods implicitly assume that at least a subset of real mobility data from target city is available, which limits their applicability in data-inaccessible scenarios. In this work, we propose a new problem setting, called bus-conditioned zero-shot trajectory generation, where no mobility trajectories from a target city are accessible. The generation process relies solely on source city mobility data and publicly available bus timetables from both cities. Under this setting, we propose MobTA, the first approach to introduce task arithmetic into trajectory generation. MobTA models the parameter shift from bus-timetable-based trajectory generation to mobility trajectory generation in source city, and applies this shift to target city through arithmetic operations on task vectors. This enables trajectory generation that reflects target-city mobility patterns without requiring any real mobility data from it. Furthermore, we theoretically analyze MobTA's stability across base and instruction-tuned LLMs. Extensive experiments show that MobTA significantly outperforms existing methods, and achieves performance close to models finetuned using target city mobility trajectories.

</details>


### [130] [LCSB: Layer-Cyclic Selective Backpropagation for Memory-Efficient On-Device LLM Fine-Tuning](https://arxiv.org/abs/2602.13073)
*Juneyoung Park,Eunbeen Yoon,Seongwan Kim. Jaeho Lee*

Main category: cs.LG

TL;DR: 提出Layer-Cyclic Selective Backpropagation (LCSB)，通过每步只计算部分层的梯度来加速内存高效反向传播，在移动设备上实现1.4倍加速且质量损失小于2%。


<details>
  <summary>Details</summary>
Motivation: 现有内存高效反向传播(MeBP)虽然能在移动设备上微调大语言模型，但需要每步计算所有transformer层的梯度，其中权重解压缩就占用了32-42%的反向传播时间，效率仍有提升空间。

Method: 提出层循环选择性反向传播(LCSB)，每步只计算部分层的梯度。利用残差连接保证梯度通过恒等路径流动，同时AdamW动量机制为未选择的层提供隐式更新。该方法可解释为LoRA参数空间上的块坐标下降。

Result: 在5个模型和3个任务上实现最高1.4倍加速，质量损失小于2%。在4位量化设置下，LCSB表现出更好的稳定性：一个3B模型在完全反向传播下会发散，但使用LCSB能平滑收敛，显示出选择性梯度计算的隐式正则化效果。

Conclusion: LCSB通过选择性梯度计算显著提升了移动设备上大语言模型微调的效率，同时保持了模型质量，并在量化设置下展现出更好的训练稳定性，为资源受限环境下的模型优化提供了有效解决方案。

Abstract: Memory-efficient backpropagation (MeBP) has enabled first-order fine-tuning of large language models (LLMs) on mobile devices with less than 1GB memory. However, MeBP requires backward computation through all transformer layers at every step, where weight decompression alone accounts for 32--42% of backward time. We propose Layer-Cyclic Selective Backpropagation (LCSB), which computes gradients for only a subset of layers per step. Our key insight is that residual connections guarantee gradient flow through identity paths, while AdamW momentum provides implicit updates for non-selected layers. We interpret LCSB as Block Coordinate Descent on the LoRA parameter space, providing theoretical justification for convergence. LCSB achieves up to 1.40$\times$ speedup with less than 2\% quality degradation across five models and three tasks. Surprisingly, in 4-bit quantized settings, LCSB exhibits superior stability: a 3B model that completely diverges under full backpropagation converges smoothly with LCSB, suggesting an implicit regularization effect from selective gradient computation.

</details>


### [131] [Unified Multi-Domain Graph Pre-training for Homogeneous and Heterogeneous Graphs via Domain-Specific Expert Encoding](https://arxiv.org/abs/2602.13075)
*Chundong Liang,Yongqi Huang,Dongxiao He,Peiyuan Li,Yawen Li,Di Jin,Weixiong Zhang*

Main category: cs.LG

TL;DR: GPH²：一种统一的多领域图预训练方法，同时处理同质和异质图，通过统一编码器和专家融合策略实现跨图类型的稳定迁移。


<details>
  <summary>Details</summary>
Motivation: 现有图预训练方法主要针对单一图类型（同质或异质图），而现实应用中混合图普遍存在，且上游预训练与下游部署间存在分布偏移。需要统一的跨图类型预训练方法。

Method: 提出GPH²方法：1）统一多视图图构建，同时编码同质和异质图；2）领域特定专家编码，每个专家在单一图上独立预训练以捕获领域知识；3）面向任务的专家融合策略，自适应整合多个专家。

Result: 在混合图上的大量实验表明，GPH²能够实现跨图类型和领域的稳定迁移，显著优于现有图预训练方法。

Conclusion: GPH²通过统一框架处理混合图预训练问题，解决了现有方法局限于单一图类型的局限性，为现实世界中的混合图应用提供了有效的预训练解决方案。

Abstract: Graph pre-training has achieved remarkable success in recent years, delivering transferable representations for downstream adaptation. However, most existing methods are designed for either homogeneous or heterogeneous graphs, thereby hindering unified graph modeling across diverse graph types. This separation contradicts real-world applications, where mixed homogeneous and heterogeneous graphs are ubiquitous, and distribution shifts between upstream pre-training and downstream deployment are common. In this paper, we empirically demonstrate that a balanced mixture of homogeneous and heterogeneous graph pre-training benefits downstream tasks and propose a unified multi-domain \textbf{G}raph \textbf{P}re-training method across \textbf{H}omogeneous and \textbf{H}eterogeneous graphs ($\mathbf{GPH^{2}}$). To address the lack of a unified encoder for homogeneous and heterogeneous graphs, we propose a Unified Multi-View Graph Construction that simultaneously encodes both without explicit graph-type-specific designs. To cope with the increased cross-domain distribution discrepancies arising from mixed graphs, we introduce domain-specific expert encoding. Each expert is independently pre-trained on a single graph to capture domain-specific knowledge, thereby shielding the pre-training encoder from the adverse effects of cross-domain discrepancies. For downstream tasks, we further design a Task-oriented Expert Fusion Strategy that adaptively integrates multiple experts based on their discriminative strengths. Extensive experiments on mixed graphs demonstrate that $\text{GPH}^{2}$ enables stable transfer across graph types and domains, significantly outperforming existing graph pre-training methods.

</details>


### [132] [EXCODER: EXplainable Classification Of DiscretE time series Representations](https://arxiv.org/abs/2602.13087)
*Yannik Hahn,Antonin Königsfeld,Hasan Tercan,Tobias Meisen*

Main category: cs.LG

TL;DR: 该论文研究如何通过离散潜在表示（如VQ-VAE和DVAE）增强时间序列分类的可解释性，并提出新的评估指标SSA来量化可解释性方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 深度学习在时间序列分类方面取得了显著进展，但模型缺乏可解释性是一个主要挑战。现有的可解释AI（XAI）技术在处理高维度和噪声的原始时间序列数据时效果受限，需要找到更好的方法来提高时间序列分类模型的可解释性。

Method: 1. 使用向量量化变分自编码器（VQ-VAE）和离散变分自编码器（DVAE）将时间序列转换为离散潜在表示；2. 在这些压缩表示上应用XAI方法生成解释；3. 提出相似子序列准确率（SSA）这一新指标，定量评估XAI识别的显著子序列与训练数据中标签分布的一致性。

Result: 研究表明，离散潜在表示不仅保留了分类所需的基本特征，还能产生更简洁、结构化、忠实且不牺牲分类性能的解释。SSA指标能够系统地验证XAI方法识别的特征是否真正代表了学习到的分类模式。

Conclusion: 离散潜在表示为时间序列分析提供了更紧凑、可解释且计算高效的解释途径，通过减少冗余和聚焦于最信息丰富的模式，显著增强了时间序列分类模型的可解释性。

Abstract: Deep learning has significantly improved time series classification, yet the lack of explainability in these models remains a major challenge. While Explainable AI (XAI) techniques aim to make model decisions more transparent, their effectiveness is often hindered by the high dimensionality and noise present in raw time series data. In this work, we investigate whether transforming time series into discrete latent representations-using methods such as Vector Quantized Variational Autoencoders (VQ-VAE) and Discrete Variational Autoencoders (DVAE)-not only preserves but enhances explainability by reducing redundancy and focusing on the most informative patterns. We show that applying XAI methods to these compressed representations leads to concise and structured explanations that maintain faithfulness without sacrificing classification performance. Additionally, we propose Similar Subsequence Accuracy (SSA), a novel metric that quantitatively assesses the alignment between XAI-identified salient subsequences and the label distribution in the training data. SSA provides a systematic way to validate whether the features highlighted by XAI methods are truly representative of the learned classification patterns. Our findings demonstrate that discrete latent representations not only retain the essential characteristics needed for classification but also offer a pathway to more compact, interpretable, and computationally efficient explanations in time series analysis.

</details>


### [133] [R-Diverse: Mitigating Diversity Illusion in Self-Play LLM Training](https://arxiv.org/abs/2602.13103)
*Gengsheng Li,Jinghan He,Shijie Wang,Dan Zhang,Ruiqi Liu,Renrui Zhang,Zijun Yao,Junfeng Fang,Haiyun Guo,Jinqiao Wang*

Main category: cs.LG

TL;DR: R-Diverse通过引入记忆增强惩罚和技能感知测量来解决自博弈中的多样性幻觉问题，在10个数学和通用推理基准上持续改进并超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有自博弈框架（如R-Zero）存在非持续改进问题，早期收益随着自博弈进行而退化。研究发现关键失败模式是"多样性幻觉"：训练信号看似多样，实则崩溃为重复模式，包括局部多样性幻觉（仅批内多样性导致跨迭代模式循环）和表面多样性幻觉（问题表面变化但所需推理技能几乎相同）。

Method: 提出R-Diverse框架，包含两个创新：1) 记忆增强惩罚(MAP)：使用持久记忆库来防止跨迭代重复；2) 技能感知测量(SAM)：通过评估问题所需的推理技能而非表面变化来衡量多样性。

Result: 在10个数学和通用推理基准测试中，R-Diverse能够维持更多迭代的改进，并一致优于先前的自博弈方法。

Conclusion: 通过解决多样性幻觉问题，R-Diverse实现了持续的自博弈改进，为LLM推理训练提供了更有效的框架。

Abstract: Self-play bootstraps LLM reasoning through an iterative Challenger-Solver loop: the Challenger is trained to generate questions that target the Solver's capabilities, and the Solver is optimized on the generated data to expand its reasoning skills. However, existing frameworks like R-Zero often exhibit non-sustained improvement, where early gains degrade as self-play continues. We identify a key failure mode, Diversity Illusion, where the Solver's training signals appear diverse yet collapse into recurring underlying patterns. It manifests as (1) Local Diversity Illusion, where diversity is enforced only within-batch, inducing cross-iteration mode cycling; and (2) Surface Diversity Illusion, where questions vary superficially but require near-identical reasoning skills. To mitigate them, we propose R-Diverse with two aligned innovations: Memory-Augmented Penalty (MAP), which uses a persistent memory bank to discourage recycling across iterations, and Skill-Aware Measurement (SAM), which evaluates diversity by the reasoning skills exercised rather than surface variation of questions. Across 10 math and general reasoning benchmarks, R-Diverse sustains gains over more iterations and consistently outperforms prior self-play methods. Code is available at https://github.com/Gengsheng-Li/R-Diverse.

</details>


### [134] [Which Algorithms Can Graph Neural Networks Learn?](https://arxiv.org/abs/2602.13106)
*Solveig Wittig,Antonis Vasileiou,Robert R. Nerem,Timo Stoll,Floris Geerts,Yusu Wang,Christopher Morris*

Main category: cs.LG

TL;DR: 本文提出了一个理论框架，用于分析图神经网络（MPNNs）如何从有限训练集中学习算法，并能泛化到任意大小的输入，同时建立了某些算法任务对标准MPNNs的不可学习性。


<details>
  <summary>Details</summary>
Motivation: 当前神经算法推理研究缺乏形式化保证，主要关注表达能力而忽略泛化问题。需要理论框架来分析MPNNs何时以及如何从有限训练集学习算法并泛化到任意规模输入。

Method: 提出通用理论框架，为MPNNs学习算法提供充分条件，证明其能从有限训练集泛化到任意大小输入。框架适用于单源最短路径、最小生成树、动态规划等算法。同时建立标准MPNNs的不可学习性结果，并提出更具表达力的MPNN变体。

Result: 理论框架证明了MPNNs学习算法的充分条件，建立了标准MPNNs对某些算法任务的不可学习性，并提出了克服这些限制的增强架构。对Bellman-Ford算法的细化分析显著减少了所需训练集规模，并支持可微正则化损失。

Conclusion: 本文为神经算法推理提供了坚实的理论基础，不仅建立了MPNNs学习算法的充分条件和泛化保证，还揭示了标准架构的局限性并提出了改进方案，为构建可靠的算法推理系统提供了理论指导。

Abstract: In recent years, there has been growing interest in understanding neural architectures' ability to learn to execute discrete algorithms, a line of work often referred to as neural algorithmic reasoning. The goal is to integrate algorithmic reasoning capabilities into larger neural pipelines. Many such architectures are based on (message-passing) graph neural networks (MPNNs), owing to their permutation equivariance and ability to deal with sparsity and variable-sized inputs. However, existing work is either largely empirical and lacks formal guarantees or it focuses solely on expressivity, leaving open the question of when and how such architectures generalize beyond a finite training set. In this work, we propose a general theoretical framework that characterizes the sufficient conditions under which MPNNs can learn an algorithm from a training set of small instances and provably approximate its behavior on inputs of arbitrary size. Our framework applies to a broad class of algorithms, including single-source shortest paths, minimum spanning trees, and general dynamic programming problems, such as the $0$-$1$ knapsack problem. In addition, we establish impossibility results for a wide range of algorithmic tasks, showing that standard MPNNs cannot learn them, and we derive more expressive MPNN-like architectures that overcome these limitations. Finally, we refine our analysis for the Bellman-Ford algorithm, yielding a substantially smaller required training set and significantly extending the recent work of Nerem et al. [2025] by allowing for a differentiable regularization loss. Empirical results largely support our theoretical findings.

</details>


### [135] [Eventizing Traditionally Opaque Binary Neural Networks as 1-safe Petri net Models](https://arxiv.org/abs/2602.13128)
*Mohamed Tarraf,Alex Chan,Alex Yakovlev,Rishad Shafik*

Main category: cs.LG

TL;DR: 提出基于Petri网的框架，将二值神经网络的操作建模为事件驱动过程，实现因果透明度和形式化验证。


<details>
  <summary>Details</summary>
Motivation: 二值神经网络虽然低复杂度、高能效，但其离散非线性行为难以解释、验证和形式化验证，限制了在安全关键领域的应用。

Method: 构建基于Petri网的框架，将BNN内部操作建模为事件驱动过程，创建核心组件（激活、梯度计算、权重更新）的模块化Petri网蓝图，并组合成系统级模型。

Result: 验证了组合Petri网与参考软件BNN的一致性，通过可达性和结构检查验证了1-安全性、无死锁、互斥和正确因果序列，使用Workcraft工具评估了可扩展性和复杂性。

Conclusion: 该框架实现了二值神经网络的因果内省，使其具有透明性和事件驱动特性，适合形式化推理和验证。

Abstract: Binary Neural Networks (BNNs) offer a low-complexity and energy-efficient alternative to traditional full-precision neural networks by constraining their weights and activations to binary values. However, their discrete, highly non-linear behavior makes them difficult to explain, validate and formally verify. As a result, BNNs remain largely opaque, limiting their suitability in safety-critical domains, where causal transparency and behavioral guarantees are essential. In this work, we introduce a Petri net (PN)-based framework that captures the BNN's internal operations as event-driven processes. By "eventizing" their operations, we expose their causal relationships and dependencies for a fine-grained analysis of concurrency, ordering, and state evolution. Here, we construct modular PN blueprints for core BNN components including activation, gradient computation and weight updates, and compose them into a complete system-level model. We then validate the composed PN against a reference software-based BNN, verify it against reachability and structural checks to establish 1-safeness, deadlock-freeness, mutual exclusion and correct-by-construction causal sequencing, before we assess its scalability and complexity at segment, component, and system levels using the automated measurement tools in Workcraft. Overall, this framework enables causal introspection of transparent and event-driven BNNs that are amenable to formal reasoning and verification.

</details>


### [136] [Order Matters in Retrosynthesis: Structure-aware Generation via Reaction-Center-Guided Discrete Flow Matching](https://arxiv.org/abs/2602.13136)
*Chenguang Wang,Zihan Zhou,Lei Bai,Tianshu Yu*

Main category: cs.LG

TL;DR: 提出RetroDiT框架，通过原子排序的位置归纳偏置改进逆合成分析，在USPTO数据集上达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有模板自由方法将逆合成视为黑盒序列生成，学习效率低；半模板方法依赖刚性反应库，泛化受限。作者发现神经表示中的原子排序对化学推理至关重要。

Method: 提出结构感知的模板自由框架，将化学反应的两阶段特性编码为位置归纳偏置：将反应中心原子置于序列头部。使用RetroDiT（带旋转位置嵌入的图Transformer）作为主干，结合离散流匹配实现20-50步生成（相比传统扩散方法的500步）。

Result: 在USPTO-50k上达到61.2% top-1准确率，USPTO-Full上达到51.3% top-1（使用预测反应中心）。使用oracle中心时分别达到71.1%和63.4%，超越使用100亿反应训练的基座模型。280K参数模型通过结构先验匹配65M参数模型的性能。

Conclusion: 原子排序的位置归纳偏置比暴力扩展模型规模更有效，结构先验能显著提升逆合成分析的性能和效率，为化学AI提供了新的设计原则。

Abstract: Template-free retrosynthesis methods treat the task as black-box sequence generation, limiting learning efficiency, while semi-template approaches rely on rigid reaction libraries that constrain generalization. We address this gap with a key insight: atom ordering in neural representations matters. Building on this insight, we propose a structure-aware template-free framework that encodes the two-stage nature of chemical reactions as a positional inductive bias. By placing reaction center atoms at the sequence head, our method transforms implicit chemical knowledge into explicit positional patterns that the model can readily capture. The proposed RetroDiT backbone, a graph transformer with rotary position embeddings, exploits this ordering to prioritize chemically critical regions. Combined with discrete flow matching, our approach decouples training from sampling and enables generation in 20--50 steps versus 500 for prior diffusion methods. Our method achieves state-of-the-art performance on both USPTO-50k (61.2% top-1) and the large-scale USPTO-Full (51.3% top-1) with predicted reaction centers. With oracle centers, performance reaches 71.1% and 63.4% respectively, surpassing foundation models trained on 10 billion reactions while using orders of magnitude less data. Ablation studies further reveal that structural priors outperform brute-force scaling: a 280K-parameter model with proper ordering matches a 65M-parameter model without it.

</details>


### [137] [FlashSchNet: Fast and Accurate Coarse-Grained Neural Network Molecular Dynamics](https://arxiv.org/abs/2602.13140)
*Pingzhi Li,Hongxuan Li,Zirui Liu,Xingcheng Lin,Tianlong Chen*

Main category: cs.LG

TL;DR: FlashSchNet：一种高效的IO感知图神经网络分子动力学框架，通过融合计算内核、优化内存访问和量化技术，在保持SchNet精度和可迁移性的同时，实现比经典力场更快的模拟速度。


<details>
  <summary>Details</summary>
Motivation: 现有的GNN势能模型（如SchNet）虽然提高了分子动力学模拟的精度和可迁移性，但由于碎片化的计算内核和内存受限的流水线，导致GPU利用率不足，速度仍慢于经典力场。需要一种IO感知的设计原则来优化GPU内存访问。

Method: 提出了FlashSchNet框架，包含四项关键技术：1）闪存径向基函数：融合距离计算、高斯基展开和余弦包络；2）闪存消息传递：融合截断、邻居收集、滤波器乘法和归约；3）闪存聚合：通过CSR分段归约重新实现散射加法；4）通道级16位量化：利用MLP权重的低动态范围。

Result: 在单个NVIDIA RTX PRO 6000上，FlashSchNet在包含269个珠子的粗粒度蛋白质上实现了1000 ns/天的聚合模拟吞吐量（比CGSchNet基线快6.5倍，峰值内存减少80%），超越了经典力场（如MARTINI），同时保持了SchNet级别的精度和可迁移性。

Conclusion: 通过IO感知的设计原则优化GNN-MD的内存访问模式，FlashSchNet在保持高精度的同时实现了显著的性能提升，为大规模分子动力学模拟提供了高效准确的解决方案。

Abstract: Graph neural network (GNN) potentials such as SchNet improve the accuracy and transferability of molecular dynamics (MD) simulation by learning many-body interactions, but remain slower than classical force fields due to fragmented kernels and memory-bound pipelines that underutilize GPUs. We show that a missing principle is making GNN-MD IO-aware, carefully accounting for reads and writes between GPU high-bandwidth memory (HBM) and on-chip SRAM. We present FlashSchNet, an efficient and accurate IO-aware SchNet-style GNN-MD framework built on four techniques: (1) flash radial basis, which fuses pairwise distance computation, Gaussian basis expansion, and cosine envelope into a single tiled pass, computing each distance once and reusing it across all basis functions; (2) flash message passing, which fuses cutoff, neighbor gather, filter multiplication, and reduction to avoid materializing edge tensors in HBM; (3) flash aggregation, which reformulates scatter-add via CSR segment reduce, reducing atomic writes by a factor of feature dimension and enabling contention-free accumulation in both forward and backward passes; (4) channel-wise 16-bit quantization that exploits the low per-channel dynamic range in SchNet MLP weights to further improve throughput with negligible accuracy loss. On a single NVIDIA RTX PRO 6000, FlashSchNet achieves 1000 ns/day aggregate simulation throughput over 64 parallel replicas on coarse-grained (CG) protein containing 269 beads (6.5x faster than CGSchNet baseline with 80% reduction of peak memory), surpassing classical force fields (e.g. MARTINI) while retaining SchNet-level accuracy and transferability.

</details>


### [138] [Quantization-Robust LLM Unlearning via Low-Rank Adaptation](https://arxiv.org/abs/2602.13151)
*João Vitor Boer Abitante,Joana Meneguzzo Pasquali,Luan Fonseca Garcia,Ewerton de Oliveira,Thomas da Silva Paula,Rodrigo C. Barros,Lucas S. Kupssinskü*

Main category: cs.LG

TL;DR: LoRA-based unlearning方法能有效保持4-bit量化后的遗忘效果，解决标准全参数微调在低比特量化中失效的问题


<details>
  <summary>Details</summary>
Motivation: LLM遗忘技术需要在实际部署中进行后训练量化以实现高效推理，但激进的低比特量化会掩盖或擦除遗忘更新，导致量化模型恢复到遗忘前的行为

Method: 提出基于低秩适配器（LoRA）的量化鲁棒遗忘方法：冻结基础模型，将遗忘集中在可训练的适配器中，确保量化后有效更新得以保留

Result: 在Llama-2-7B和MUSE数据集上，LoRA将4-bit效用提升高达7.93分，显著减少4-bit量化下的隐私泄露，同时保持强大的遗忘效果

Conclusion: 在需要量化部署的场景中，使用LoRA进行机器遗忘是有益的，能有效解决量化导致的遗忘失效问题

Abstract: Large Language Model (LLM) unlearning aims to remove targeted knowledge from a trained model, but practical deployments often require post-training quantization (PTQ) for efficient inference. However, aggressive low-bit PTQ can mask or erase unlearning updates, causing quantized models to revert to pre-unlearning behavior. We show that standard full-parameter fine-tuning often induce parameter changes that are too small to survive 4-bit quantization. We propose quantization-robust unlearning via low-rank adaptation (LoRA): we freeze the base model and concentrate unlearning into trainable adapters so that the effective update is preserved after quantization. On Llama-2-7B evaluated with MUSE dataset (BOOKS and NEWS), LoRA improves 4-bit utility by up to 7.93 points (NPO+GDR on BOOKS: 50.17 to 58.10) and yields higher 4-bit utility on NEWS for GA+GDR (40.06 to 44.82, increase of 4.76). LoRA also substantially reduces privacy leakage under 4-bit PTQ, e.g., for GA+KLR on BOOKS, PrivLeak moves from -25.68 to -5.86 (closer to ideal 0), while maintaining strong forgetting (VerMem and KnowMem near 0). Thus, using LoRA for Machine Unlearning is beneficial for scenarios where quantization is necessary for model deployment.

</details>


### [139] [Learning to Approximate Uniform Facility Location via Graph Neural Networks](https://arxiv.org/abs/2602.13155)
*Chendi Qian,Christopher Morris,Stefanie Jegelka,Christian Sohler*

Main category: cs.LG

TL;DR: 提出一种结合消息传递神经网络与近似算法原理的完全可微分方法，用于解决均匀设施选址问题，无需监督训练或离散松弛，具有可证明的性能保证和尺寸泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的方法（监督学习、强化学习等）存在计算开销大、训练不稳定、缺乏可证明性能保证等问题，而经典近似算法虽然提供性能保证但不可微分且无法自适应利用输入分布的结构规律。需要弥合学习方法和近似算法之间的鸿沟。

Method: 开发完全可微分的消息传递神经网络模型，嵌入近似算法原理，避免求解器监督或离散松弛。模型在训练时无需监督数据，能够自适应利用输入分布的结构规律。

Result: 方法在均匀设施选址问题上表现出色，优于标准非学习近似算法，接近计算密集的整数线性规划方法。具有可证明的近似保证和尺寸泛化能力，能够泛化到比训练时大得多的实例。

Conclusion: 这项工作为离散优化中基于学习的方法和近似算法之间的桥梁提供了重要一步，展示了结合神经网络与算法原理的潜力，为硬组合优化问题提供了新的解决方案。

Abstract: There has been a growing interest in using neural networks, especially message-passing neural networks (MPNNs), to solve hard combinatorial optimization problems heuristically. However, existing learning-based approaches for hard combinatorial optimization tasks often rely on supervised training data, reinforcement learning, or gradient estimators, leading to significant computational overhead, unstable training, or a lack of provable performance guarantees. In contrast, classical approximation algorithms offer such performance guarantees under worst-case inputs but are non-differentiable and unable to adaptively exploit structural regularities in natural input distributions. We address this dichotomy with the fundamental example of Uniform Facility Location (UniFL), a variant of the combinatorial facility location problem with applications in clustering, data summarization, logistics, and supply chain design. We develop a fully differentiable MPNN model that embeds approximation-algorithmic principles while avoiding the need for solver supervision or discrete relaxations. Our approach admits provable approximation and size generalization guarantees to much larger instances than seen during training. Empirically, we show that our approach outperforms standard non-learned approximation algorithms in terms of solution quality, closing the gap with computationally intensive integer linear programming approaches. Overall, this work provides a step toward bridging learning-based methods and approximation algorithms for discrete optimization.

</details>


### [140] [Learning functional components of PDEs from data using neural networks](https://arxiv.org/abs/2602.13174)
*Torkel E. Loman,Yurij Salmaniw,Antonio Leon Villares,Jose A. Carrillo,Ruth E. Baker*

Main category: cs.LG

TL;DR: 使用神经网络嵌入PDE来从数据中恢复未知函数，以非局部聚集-扩散方程为例，从稳态数据中恢复相互作用核和外部势能


<details>
  <summary>Details</summary>
Motivation: 偏微分方程中常包含难以直接测量的未知函数，这限制了从模型推导预测的能力。虽然从数据恢复标量PDE参数的工作流程已有研究，但恢复函数的方法尚不完善

Method: 将神经网络嵌入到PDE中，通过在数据上训练来近似未知函数。以非局部聚集-扩散方程为案例研究，从稳态数据中恢复相互作用核和外部势能

Result: 研究了多种因素对函数恢复能力的影响，包括可用解的数量、解的性质、采样密度和测量噪声。神经网络能够以任意精度近似未知函数

Conclusion: 该方法优势在于可以利用标准的参数拟合工作流程，且训练后的PDE可以像普通PDE一样用于生成系统预测等目的

Abstract: Partial differential equations often contain unknown functions that are difficult or impossible to measure directly, hampering our ability to derive predictions from the model. Workflows for recovering scalar PDE parameters from data are well studied: here we show how similar workflows can be used to recover functions from data. Specifically, we embed neural networks into the PDE and show how, as they are trained on data, they can approximate unknown functions with arbitrary accuracy. Using nonlocal aggregation-diffusion equations as a case study, we recover interaction kernels and external potentials from steady state data. Specifically, we investigate how a wide range of factors, such as the number of available solutions, their properties, sampling density, and measurement noise, affect our ability to successfully recover functions. Our approach is advantageous because it can utilise standard parameter-fitting workflows, and in that the trained PDE can be treated as a normal PDE for purposes such as generating system predictions.

</details>
