<div id=toc></div>

# Table of Contents

- [physics.comp-ph](#physics.comp-ph) [Total: 3]
- [quant-ph](#quant-ph) [Total: 40]
- [cs.LG](#cs.LG) [Total: 134]
- [gr-qc](#gr-qc) [Total: 8]


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [1] [Numerical study of loss of hyperbolicity using a cold plasma model](https://arxiv.org/abs/2602.03859)
*Evgeniy V. Chizhonkov,Olga S. Rozanova*

Main category: physics.comp-ph

TL;DR: 提出一种新的隐式求解方法，用于处理考虑电子-离子碰撞的一维冷等离子体方程，解决了因碰撞系数依赖于电子密度导致系统失去双曲性带来的计算困难。


<details>
  <summary>Details</summary>
Motivation: 当碰撞系数ν依赖于电子密度N时，等离子体系统会失去双曲性，导致传统计算方法失效。特别是在ν(N)线性依赖的临界情况下，解可能失去光滑性，需要开发新的数值方法来克服这些计算困难。

Method: 提出一种新的隐式求解方法，在欧拉变量下处理一维冷等离子体方程。该方法适用于相对论和非相对论情况，并在ν(N)=ν₁+ν₀N的线性依赖临界情况下进行了测试，验证了其有效性。

Result: 计算实验与现有理论结果完全一致。新方法成功克服了因碰撞系数依赖于电子密度导致系统失去双曲性带来的计算困难，能够处理解可能失去光滑性的临界情况。

Conclusion: 提出的隐式求解方法有效解决了考虑电子密度依赖碰撞系数的等离子体方程计算问题，为处理这类失去双曲性的系统提供了可行的数值解决方案。

Abstract: We study a one-dimensional system of cold plasma equations taking into account electron-ion collisions in both relativistic and nonrelativistic cases. It is known that for a constant collision coefficient $ν$, the solution to the Cauchy problem for such a system can lose smoothness. However, if the dependence of $ν$ on the electron density $N$ is more than linear, then the solution remains globally smooth for any initial data. However, the appearance of the dependence $ν(N)$ leads to a change in the type of the system, it loses hyperbolicity, which leads to computational problems. In this paper, we propose a new implicit solution method in Euler variables that overcomes these difficulties. It can be used in both nonrelativistic and relativistic cases and is tested for the threshold case of a linear dependence $ν(N)=ν_1+ν_0 N$, when smoothness can still be lost. The computational experiments carried out are in full agreement with the available theoretical results.

</details>


### [2] [Topology- and Geometry-Exact Coupling for Incompressible Fluids and Thin Deformables](https://arxiv.org/abs/2602.03988)
*Jonathan Panuelos,Eitan Grinspun,David Levin*

Main category: physics.comp-ph

TL;DR: 提出一种保持拓扑结构的离散化方法，用于耦合不可压缩流体与薄可变形结构，通过保持流体域连通性实现防泄漏保证。


<details>
  <summary>Details</summary>
Motivation: 传统流体-结构耦合方法在处理薄结构时容易出现流体泄漏问题，需要一种能够精确保持流体域拓扑连通性、确保防泄漏的数值方法。

Method: 基于拉格朗日流体粒子生成裁剪Voronoi图，应用缝合算法保持障碍物周围的路径连通性，构建几何离散化网格，在流体-固体界面精确施加边界条件。

Result: 方法能够防止流体通过固体泄漏，同时允许流体在流体域内连续路径存在的地方流动，实现了无人工密封或泄漏伪影的鲁棒双向耦合。

Conclusion: 该方法为薄可变形结构与不可压缩流体的耦合提供了一种拓扑保持、防泄漏的数值框架，适用于薄膜流动、复杂几何窄通道和浸入液体中的可变形结构等多种场景。

Abstract: We introduce a topology-preserving discretization for coupling incompressible fluids with thin deformable structures, achieving guaranteed leakproofness through preservation of fluid domain connectivity. Our approach leverages a stitching algorithm applied to a clipped Voronoi diagram generated from Lagrangian fluid particles, in order to maintain path connectivity around obstacles. This geometric discretization naturally conforms to arbitrarily thin structures, enabling boundary conditions to be enforced exactly at fluid-solid interfaces. By discretizing the pressure projection equations on this conforming mesh, we can enforce velocity boundary conditions at the interface for the fluid while applying pressure forces directly on the solid boundary, enabling sharp two-way coupling between phases. The resulting method prevents fluid leakage through solids while permitting flow wherever a continuous path exists through the fluid domain. We demonstrate the effectiveness of our approach on diverse scenarios including flows around thin membranes, complex geometries with narrow passages, and deformable structures immersed in liquid, showcasing robust two-way coupling without artificial sealing or leakage artifacts.

</details>


### [3] [At the Top of the Mountain, the World can Look Boltzmann-Like: Sampling Dynamics of Noisy Double-Well Systems](https://arxiv.org/abs/2602.04014)
*Abir Hasan,Nikhil Shukla*

Main category: physics.comp-ph

TL;DR: 该论文发现双阱能系统的随机动力学在势垒顶部附近具有普适性，所有光滑偶双阱势在鞍点附近都简化为标准四次型，产生与势能具体形状无关的tanh响应，为各种物理平台作为p-bit提供了统一基础。


<details>
  <summary>Details</summary>
Motivation: 晶体管作为数字计算基石的成功，激励人们寻找类似概率计算范式的硬件原语——概率比特（p-bit）。需要为各种物理平台作为p-bit提供统一的理论基础。

Method: 基于莫尔斯理论和奇点理论的拓扑框架，证明所有光滑偶双阱势在鞍点附近都简化为标准四次型。通过分析推导和多个代表性系统的数值模拟，研究噪声、突触偏置和势能曲率的相互作用。

Result: 在势垒顶部附近，系统表现出拓扑鲁棒的短期演化，其特征是tanh响应，能够实现玻尔兹曼采样，且基本独立于势能的具体形状（除了有效温度缩放）。

Conclusion: 该工作为评估和设计包括振荡器、双稳态锁存器和磁性器件在内的广泛物理平台作为p-bit提供了统一基础，使它们能够在同步框架下进行随机采样和概率计算。

Abstract: The success of the transistor as the cornerstone of digital computation motivates analogous efforts to identify an equivalent hardware primitive, the probabilistic bit or p-bit, for the emerging paradigm of probabilistic computing. Here, we uncover a fundamental ubiquity in the stochastic dynamics of double well energy systems when initialized near the barrier top. Using a topological framework grounded in Morse theory and singularity theory, we make use of the result that all smooth, even double well potentials reduce near the saddle point to a canonical quartic normal form. Within this regime, the interplay of noise, synaptic bias, and potential curvature produces a topologically robust short time evolution characterized by a tanh like response. This enables Boltzmann like sampling that is largely independent of the detailed shape of the potential, apart from its effective temperature scaling. Analytical derivations and numerical simulations across multiple representative systems corroborate this behavior. Our work provides a unifying foundation for assessing and engineering a broad class of physical platforms, including oscillators, bistable latches, and magnetic devices, as p-bits operating within a synchronous framework for stochastic sampling and probabilistic computation.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [4] [Detailed, interpretable characterization of mid-circuit measurement on a transmon qubit](https://arxiv.org/abs/2602.03938)
*Piper C. Wysocki,Luke D. Burkhart,Madeline H. Morocco,Corey I. Ostrove,Riley J. Murray,Tristan Brown,Jeffrey M. Gertler,David K. Kim,Nathan E. Miller,Bethany M. Niedzielski,Katrina M. Sliwa,Robin Blume-Kohout,Gabriel O. Samach,Mollie E. Schwartz,Kenneth M. Rudinger*

Main category: quant-ph

TL;DR: 该论文将误差生成器形式主义扩展到中电路测量，用于分解和解释量子测量中的误差机制


<details>
  <summary>Details</summary>
Motivation: 中电路测量是量子纠错协议的关键组件，但实验估计的量子仪器难以解释或与设备物理关联。现有误差生成器形式主义主要用于量子门，需要扩展到测量过程。

Method: 将误差生成器形式主义（先前用于分解量子门误差过程）适配到中电路测量，通过门集层析术自洽地表征量子仪器，并使用简化模型对误差机制进行参数化建模。

Result: 在transmon量子比特设备上成功部署新分析方法，量化了振幅阻尼、读出误差和不完美坍缩等误差机制，展示了这些误差如何随读出脉冲幅度变化，并恢复了理论预测的色散读出关键特征。

Conclusion: 误差生成器形式主义可以有效地扩展到中电路测量，提供物理可解释的误差分解，帮助理解测量过程的基本物理机制，并通过简化模型实现参数化建模。

Abstract: Mid-circuit measurements (MCMs) are critical components of the quantum error correction protocols expected to enable utility-scale quantum computing. MCMs can be modeled by quantum instruments (a type of quantum operation or process), which can be characterized self-consistently using gate set tomography. However, experimentally estimated quantum instruments are often hard to interpret or relate to device physics. We address this challenge by adapting the error generator formalism -- previously used to interpret noisy quantum gates by decomposing their error processes into physically meaningful sums of "elementary errors" -- to MCMs. We deploy our new analysis on a transmon qubit device to tease out and quantify error mechanisms including amplitude damping, readout error, and imperfect collapse. We examine in detail how the magnitudes of these errors vary with the readout pulse amplitude, recover the key features of dispersive readout predicted by theory, and show that these features can be modeled parsimoniously using a reduced model with just a few parameters.

</details>


### [5] [Approximate simulation of complex quantum circuits using sparse tensors](https://arxiv.org/abs/2602.04011)
*Benjamin N. Miller,Peter K. Elgee,Jason R. Pruitt,Kevin C. Cox*

Main category: quant-ph

TL;DR: 提出一种使用稀疏张量近似模拟量子电路的方法，包括稀疏张量数据结构设计和高效收缩算法


<details>
  <summary>Details</summary>
Motivation: 量子电路经典模拟是定义可验证量子优势边界、解决量子多体问题、指导量子软硬件开发的关键研究领域，张量网络已成为前沿数学工具

Method: 引入使用稀疏填充张量近似模拟量子电路的方法，描述能表示无对称性量子态的稀疏张量数据结构，并概述高效收缩和截断这些张量的算法

Result: 数据结构和收缩算法是高效的，相对于量子比特数和电路深度具有预期的运行时标度，为量子模拟中稀疏张量网络的优化研究提供动力

Conclusion: 稀疏张量方法为量子电路模拟提供了高效工具，激励未来在量子模拟中优化稀疏张量网络的研究

Abstract: The study of quantum circuit simulation using classical computers is a key research topic that helps define the boundary of verifiable quantum advantage, solve quantum many-body problems, and inform development of quantum hardware and software. Tensor networks have become forefront mathematical tools for these tasks. Here we introduce a method to approximately simulate quantum circuits using sparsely-populated tensors. We describe a sparse tensor data structure that can represent quantum states with no underlying symmetry, and outline algorithms to efficiently contract and truncate these tensors. We show that the data structure and contraction algorithm are efficient, leading to expected runtime scalings versus qubit number and circuit depth. Our results motivate future research in optimization of sparse tensor networks for quantum simulation.

</details>


### [6] [Correlation-Enabled Beatings in Two-Dimensional Electronic Spectroscopy](https://arxiv.org/abs/2602.04061)
*Sirui Chen,Dragomir Davidović*

Main category: quant-ph

TL;DR: 长寿命拍频源于相关驱动机制，需要慢浴记忆和超快脉冲序列，脉冲序列对浴记忆贡献进行幺正修饰，激活非久期布居-相干转移，维持相干特征远超因子化或弱记忆描述


<details>
  <summary>Details</summary>
Motivation: 二维电子光谱中的长寿命拍频难以用标准激子开放系统模型解释，这些模型通常假设因子化初始化和预测快速相干衰减

Method: 提出相关驱动机制，需要慢浴记忆和超快脉冲序列，脉冲序列对浴记忆贡献进行幺正修饰，激活非久期布居-相干转移

Result: 长寿命拍频可作为协议级动力学效应：超快控制下的相关介导检索，而非激子与振子或量子与经典语义的讨论

Conclusion: 将长寿命拍频重新定义为协议级动力学效应：超快控制下的相关介导检索，这超越了传统因子化或弱记忆描述的限制

Abstract: Long-lived beatings in two-dimensional electronic spectroscopy (2DES) remain difficult to interpret within standard excitonic open-system models, which typically assume factorized initialization and predict rapid coherence decay. We show that persistent beatings can arise from a correlation-driven mechanism that requires both slow bath memory and ultrafast pulse sequences that propagate system-bath correlations across optical interactions. In this regime, the pulse sequence unitarily dresses the bath-memory contribution and activates nonsecular population-coherence transfer during field-free evolution, sustaining coherence signatures far beyond factorized or weak-memory descriptions. Rather than addressing what is oscillating (excitonic versus vibronic) or quantum-versus-classical semantics, this work reframes long-lived beatings as a protocol-level dynamical effect: correlation-mediated retrieval under ultrafast control.

</details>


### [7] [Data Verification is the Future of Quantum Computing Copilots](https://arxiv.org/abs/2602.04072)
*Junhao Song,Ziqian Bi,Xinliang Chia,William Knottenbelt,Yudong Cao*

Main category: quant-ph

TL;DR: 量子程序生成需要高精度，但大语言模型的统计推理可能导致不可避免的幻觉，无法通过扩展解决。作者主张需要优先考虑验证的架构，将验证从事后考虑提升为架构基础。


<details>
  <summary>Details</summary>
Motivation: 量子程序生成需要数学精确性，而大语言模型的统计推理会产生不可避免的幻觉，这无法通过模型扩展来解决。在受约束的领域（如量子计算）中，需要能够确保正确性的AI系统。

Method: 提出基于验证的架构方法：1) 使用经过验证的训练数据，让模型内化精确约束而非统计近似；2) 验证必须约束生成过程而非过滤输出，因为有效设计占据指数级缩小的子空间；3) 在物理定律强加正确性标准的领域，验证应作为架构原语嵌入。

Result: 早期实验显示，没有数据验证的LLM在电路优化中最多只能达到79%的准确率。这表明纯统计方法在需要数学精确性的量子计算领域存在根本性限制。

Conclusion: 量子计算和AI4Research社区应将验证从"事后考虑"提升为"架构基础"。需要优先考虑验证的架构来构建量子编程助手和受约束领域的AI自动化系统，确保物理定律和数学约束得到严格遵守。

Abstract: Quantum program generation demands a level of precision that may not be compatible with the statistical reasoning carried out in the inference of large language models (LLMs). Hallucinations are mathematically inevitable and not addressable by scaling, which leads to infeasible solutions. We argue that architectures prioritizing verification are necessary for quantum copilots and AI automation in domains governed by constraints. Our position rests on three key points: verified training data enables models to internalize precise constraints as learned structures rather than statistical approximations; verification must constrain generation rather than filter outputs, as valid designs occupy exponentially shrinking subspaces; and domains where physical laws impose correctness criteria require verification embedded as architectural primitives. Early experiments showed LLMs without data verification could only achieve a maximum accuracy of 79% in circuit optimization. Our positions are formulated as quantum computing and AI4Research community imperatives, calling for elevating verification from afterthought to architectural foundation in AI4Research.

</details>


### [8] [Symmetric joint measurement as a complement to the elegant joint measurement](https://arxiv.org/abs/2602.04173)
*Ying-Qiu He,Yu-Yan Zhang,Dong Ding,Ting Gao,Feng-Li Yan*

Main category: quant-ph

TL;DR: 该论文提出了一种新的两比特对称联合测量，其纠缠度从0到1/2，补充了现有研究，并探讨了其在三角网络中的应用和向多比特系统的推广。


<details>
  <summary>Details</summary>
Motivation: 传统贝尔态测量和乘积基测量在量子计算发展中至关重要，而优雅联合测量展现了新的四面体对称性。现有研究提出了纠缠度在1/2到1之间的参数化两比特等纠缠基，但缺乏纠缠度从0到1/2的对称联合测量来补充这一范围。

Method: 提出了一种两比特对称联合测量，其纠缠度范围从0到1/2，包含了乘积基测量和原始优雅联合测量。研究了该结构的对称性及其在三角网络中的应用，并将该方法推广到偶数比特的多比特系统。

Result: 研究发现当前基态的约化向量展现旋转对称性而非镜像对称性；三角网络中三方的输出概率分布明确展示了预期的置换对称性；成功将两比特对称联合测量推广到偶数比特的多比特系统。

Conclusion: 该研究填补了纠缠度从0到1/2的对称联合测量的空白，为量子测量理论提供了新的工具，并在网络量子信息处理中展示了应用潜力。

Abstract: Traditional Bell state measurement (BSM) and product basis measurements (PBM) have been integral to nearly the entire development of quantum computing. Unlike the BSM and the PBM, a recently proposed two-qubit joint measurement called the elegant joint measurement (EJM) exhibits novel tetrahedral symmetry in its single-qubit reduced states. In [Phys.Rev.Lett.126:220401], a parameterized two-qubit iso-entangled basis was proposed, with concurrence between 1/2 and 1, perfectly spanning the original EJM and conventional BSM. We present a two-qubit symmetric joint measurement having concurrence from 0 to 1/2, which is complementary to [Phys.Rev.Lett.126:220401] and contains the PBM and the original EJM. We investigate the symmetry of the current structure and its application in triangular networks. The results indicate that the reduction vectors of the current basis states exhibit rotational symmetry, rather than the aforementioned mirror symmetry; moreover, the output probability distributions of three parties in the network explicitly demonstrate the expected permutation symmetry. Furthermore, we generalize the two-qubit symmetric joint measurement to the multiqubit systems with an even number of qubits.

</details>


### [9] [Influence of Noninertial Dynamics on Static Quantum Resource Theories](https://arxiv.org/abs/2602.04199)
*Saveetha Harikrishnan,Tim Byrnes,Chandrashekar Radhakrishnan*

Main category: quant-ph

TL;DR: 研究非惯性动力学对静态量子资源理论的影响，将非惯性效应等价于CPTP映射，其中Unruh效应等价于玻色放大器通道，并分析该映射对资源理论三个核心要素的影响。


<details>
  <summary>Details</summary>
Motivation: 探索非惯性运动如何影响量子资源理论，理解相对论效应与量子信息处理之间的相互作用，为在非惯性参考系中应用量子资源理论提供理论基础。

Method: 首先证明非惯性效应与完全正定保迹(CPTP)映射的等价性，将Unruh效应建模为玻色放大器通道。然后系统研究该CPTP映射对资源理论三个核心要素的影响：自由态、自由操作和资源量化器。

Result: 建立了非惯性效应与CPTP映射的等价关系，发现Unruh效应对应玻色放大器通道。在非惯性运动存在的情况下，可以对资源理论的三个核心组成部分做出若干一般性陈述。

Conclusion: 非惯性动力学对静态量子资源理论有系统性影响，通过CPTP映射框架可以统一分析这种影响，为理解相对论效应下的量子资源处理提供了理论工具。

Abstract: The effect of noninertial dynamics on static quantum resource theories is investigated. To this end, we first show the equivalence between noninertial effects and a completely positive, trace-preserving (CPTP) map. In this formulation, the Unruh effect is equivalent to a bosonic amplifier channel. The effect of this map on a generic quantum resource is investigated by studying the role of the CPTP map on the three core ingredients of a resource theory, namely, the free states, the free operations and the resource quantifiers. We show several general statements can be made about these three components of a resource theory in the presence of noninertial motion.

</details>


### [10] [Restoring Landauer's Principle for Unitarily Transformed Thermal Reservoirs](https://arxiv.org/abs/2602.04552)
*Hao Xu*

Main category: quant-ph

TL;DR: 该论文解决了Landauer原理在压缩热态(STS)中看似被违反的问题，通过引入有效哈密顿量建立了广义Landauer不等式，并将其应用于移动Unruh-DeWitt探测器与量子场耦合的系统。


<details>
  <summary>Details</summary>
Motivation: 当热储层被压缩热态(STS)替代时，Landauer原理（量子信息和热力学的基石）似乎被违反。需要建立适用于这类幺正变换热态的广义Landauer原理框架。

Method: 通过定义有效哈密顿量，严格建立了广义Landauer不等式，该框架自然包含了普通热储层作为特例。使用微扰理论研究了任意移动的Unruh-DeWitt探测器与初始处于STS的量子场耦合的情况。

Result: 明确计算了熵产生并确认其正定性。由于幺正变换引起的对称性破缺，熵产生既依赖于固有时间隔也依赖于绝对时空位置。解决了STS中Landauer原理的明显违反问题。

Conclusion: 该工作解决了Landauer原理在STS中的明显违反问题，为分析非平衡和相对论环境下的量子热力学提供了强大工具，对量子热机和信息协议具有潜在影响。

Abstract: Landauer's principle, a cornerstone of quantum information and thermodynamics, appears to be violated when the thermal reservoir is replaced by a squeezed thermal state (STS). We introduce a formal extension of the principle to such unitarily transformed thermal states. By defining an effective Hamiltonian, we rigorously establish a generalized Landauer inequality, which naturally reduces to the standard case for an ordinary thermal reservoir as a special instance. The framework further yields a consistent definition of entropy production and a proof of its non-negativity. We illustrate its utility by studying an arbitrarily moving Unruh-DeWitt detector coupled to a quantum field initially prepared in the STS. Using perturbation theory, we compute the entropy production explicitly, confirming its positivity. As a result of the symmetry breaking induced by the unitary transformation, it depends on both the proper time interval and the absolute spacetime position. Our work resolves the apparent violation of Landauer's principle with STSs. It also provides a robust tool for analyzing quantum thermodynamics in non-equilibrium and relativistic settings, with potential implications for quantum thermal machines and information protocols.

</details>


### [11] [Benchmarking Quantum and Classical Algorithms for the 1D Burgers Equation: QTN, HSE, and PINN](https://arxiv.org/abs/2602.04239)
*Vanshaj Kerni,Abdelrahman E. Ahmed,Syed Ali Asghar*

Main category: quant-ph

TL;DR: 量子张量网络在求解1D Burgers方程时精度最高且计算时间接近常数，但量子方法目前相比经典求解器没有计算优势


<details>
  <summary>Details</summary>
Motivation: 比较量子张量网络、流体动力学薛定谔方程和物理信息神经网络这三种新兴范式在求解1D Burgers方程时的性能，评估它们相对于经典GMRES和谱方法的优劣

Method: 在网格分辨率从N=4到N=128的范围内，比较QTN、HSE（有限差分和谱方法）、PINN以及经典GMRES和谱基线的求解精度、运行时间缩放和资源开销

Result: QTN求解器达到最高精度（L2误差约10^-7）且运行时间接近常数缩放，能有效利用纠缠压缩捕捉激波前沿；有限差分HSE稳健但谱HSE在高分辨率下数值不稳定；PINN作为无网格求解器灵活但精度较低（L2误差约10^-1）

Conclusion: 量子方法为低分辨率流体动力学提供了新颖的表征优势，但在没有容错能力或处理非线性反馈的显著算法突破的情况下，目前相比经典求解器没有计算优势

Abstract: We present a comparative benchmark of Quantum Tensor Networks (QTN), the Hydrodynamic Schrödinger Equation (HSE), and Physics-Informed Neural Networks (PINN) for simulating the 1D Burgers' equation. Evaluating these emerging paradigms against classical GMRES and Spectral baselines, we analyse solution accuracy, runtime scaling, and resource overhead across grid resolutions ranging from $N=4$ to $N=128$. Our results reveal a distinct performance hierarchy. The QTN solver achieves superior precision ($L_2 \sim 10^{-7}$) with remarkable near-constant runtime scaling, effectively leveraging entanglement compression to capture shock fronts. In contrast, while the Finite-Difference HSE implementation remains robust, the Spectral HSE method suffers catastrophic numerical instability at high resolutions, diverging significantly at $N=128$. PINNs demonstrate flexibility as mesh-free solvers but stall at lower accuracy tiers ($L_2 \sim 10^{-1}$), limited by spectral bias compared to grid-based methods. Ultimately, while quantum methods offer novel representational advantages for low-resolution fluid dynamics, this study confirms they currently yield no computational advantage over classical solvers without fault tolerance or significant algorithmic breakthroughs in handling non-linear feedback.

</details>


### [12] [Constructing Compact ADAPT Unitary Coupled-Cluster Ansatz with Parameter-Based Criterion](https://arxiv.org/abs/2602.04253)
*Runhong He,Xin Hong,Qiaozhen Chai,Ji Guan,Junyuan Zhou,Arapat Ablimit,Guolong Cui,Shenggang Ying*

Main category: quant-ph

TL;DR: Param-ADAPT-VQE：一种改进的ADAPT-VQE算法，通过参数化准则选择激发算符，避免冗余算子，结合子哈密顿量技术和热启动优化策略，显著降低测量成本。


<details>
  <summary>Details</summary>
Motivation: ADAPT-VQE算法在实际应用中面临冗余激发算符和过高测量成本的问题，限制了其可扩展性。

Method: 提出Param-ADAPT-VQE算法：1）使用参数化准则而非传统梯度准则选择激发算符；2）开发子哈密顿量技术；3）集成热启动VQE优化策略。

Result: 在典型分子系统上的数值实验表明，Param-ADAPT-VQE在计算精度、ansatz规模和测量成本方面均优于原始ADAPT-VQE。

Conclusion: Param-ADAPT-VQE是ADAPT-VQE的高效可扩展增强方案，解决了阻碍其实际应用的核心障碍，且与ADAPT-VQE的各种改进版本完全兼容。

Abstract: The adaptive derivative-assembled pseudo-trotter variational quantum eigensolver (ADAPT-VQE) is a promising hybrid quantum-classical algorithm for molecular ground state energy calculation, yet its practical scalability is hampered by redundant excitation operators and excessive measurement costs. To address these challenges, we propose Param-ADAPT-VQE, a novel improved algorithm that selects excitation operators based on a parameter-based criterion instead of the traditional gradient-based metric. This strategy effectively eludes redundant operators. We further develop a sub-Hamiltonian technique and integrate a hot-start VQE optimization strategy, achieving a significant reduction in measurement costs. Numerical experiments on typical molecular systems demonstrate that Param-ADAPT-VQE outperforms the original ADAPT-VQE in computational accuracy, ansatz size, and measurement costs. Furthermore, our scheme retains the fundamental framework of ADAPT-VQE and is thus fully compatible with its various modified versions, enabling further performance improvements in specific aspects. This work presents an efficient and scalable enhancement to ADAPT-VQE, mitigating the core obstacles that impede its practical implementation in the field of molecular quantum chemistry.

</details>


### [13] [Canonical Quantization of Cylindrical Waveguides: A Gauge-Based Approach](https://arxiv.org/abs/2602.04295)
*Alexandre Delattre,Eddy Collin*

Main category: quant-ph

TL;DR: 本文提出了一种圆柱形波导中电磁模式的规范量子化方法，扩展了先前针对笛卡尔几何的规范形式主义，为TEM、TM和TE模式建立了统一的量子理论框架。


<details>
  <summary>Details</summary>
Motivation: 将量子化方法从笛卡尔几何扩展到圆柱形波导，为不同类型的波导模式（特别是TM和TE模式）建立统一的量子理论框架，这对于量子技术应用（尤其是片上共面几何结构）具有重要意义。

Method: 通过引入TEM、TM和TE模式的场正交分量X,Y，识别每个模式的特征一维标量场（广义通量φ），该场满足Klein-Gordon型方程。从麦克斯韦方程推导哈密顿量，构建玻色子阶梯算符，并通过适当的规范选择从电磁势A,V直接推导广义通量。

Result: 成功推导了圆柱形波导中电磁模式的量子化理论，建立了统一的处理框架，能够从场分布推导模式特定的电容和电感，并用正则场变量表达电压和电流，为实验测量提供了适当的量子算符定义。

Conclusion: 该形式主义为圆柱形和笛卡尔波导模式提供了统一的理论框架，确保了理论洞察力和实验相关性，未来可扩展到其他类型波导（特别是量子技术相关的片上共面几何结构）。

Abstract: We present a canonical quantization of electromagnetic modes in cylindrical waveguides, extending a gauge-based formalism previously developed for Cartesian geometries [1]. By introducing the two field quadratures $X,Y$ of TEM (transverse electric-magnetic), but also of TM (transverse magnetic) and TE (transverse electric) traveling modes, we identify for each a characteristic one-dimensional scalar field (a generalized flux $\varphi$) governed by a Klein-Gordon type equation. The associated Hamiltonian is derived explicitly from Maxwell's equations, allowing the construction of bosonic ladder operators. The generalized flux is directly deduced from the electromagnetic potentials $A,V$ by a proper gauge choice, generalizing Devoret's approach [2]. Our analysis unifies the treatment of cylindrical and Cartesian guided modes under a consistent and generic framework, ensuring both theoretical insight and experimental relevance. We derive mode-specific capacitance and inductance from the field profiles and express voltage and current in terms of the canonical field variables. Measurable quantities are therefore properly defined from the mode quantum operators, especially for the non-trivial TM and TE ones. The formalism shall extend in future works to any other type of waveguides, especially on-chip coplanar geometries particularly relevant to quantum technologies.

</details>


### [14] [Does the entropy of systems with larger internal entanglement grow stronger?](https://arxiv.org/abs/2602.04345)
*Daria Gaidukevich*

Main category: quant-ph

TL;DR: 研究系统内部纠缠与熵增长的关系，发现一般情况下内部纠缠越大的系统熵增长越强，但特定条件下可能相反


<details>
  <summary>Details</summary>
Motivation: 探索系统内部纠缠与熵增长之间的关系，澄清当系统与环境相互作用时，内部纠缠较大的系统是否会有更强的熵增长

Method: 使用最简单的模型：量子比特系统与量子谐振子环境相互作用，通过模拟随机状态集合来研究平均情况

Result: 平均情况下答案是肯定的（内部纠缠越大的系统熵增长越强），但特定条件下选择满足某些条件的状态可能使依赖关系反转；纠缠深度对熵增长也有小贡献

Conclusion: 系统内部纠缠与熵增长的关系是复杂的，平均情况下呈正相关，但存在反例；纠缠深度也影响熵增长

Abstract: It is known that when a system interacts with its environment, the entanglement contained in the system is redistributed since parts of the system entangle with the environment. On the other hand, the entanglement of a system with its environment is closely related to the entropy of the system. However, does this imply that the entropy of systems with larger internal entanglement will grow stronger? We study the issue using the simplest model as an example: a system of qubits interacts with the environment described by the quantum harmonic oscillator. The answer to the posed question is ambiguous. However, the study of the situation on average (using the simulation of a set of random states) reveals certain patterns and we can say that the answer is affirmative. At the same time, the choice of states satisfying certain conditions in some cases can change the dependence to the opposite. Additionally, we show that the entanglement depth also makes a small contribution to entropy growth.

</details>


### [15] [Quantum-Assisted Design of Space-Terrestrial Integrated Networks](https://arxiv.org/abs/2602.04350)
*Chiara Vercellino,Giacomo Vitali,Paolo Viviani,Alberto Scionti,Olivier Terzo,Bartolomeo Montrucchio,Pascal Jahan Elahi,Ugo Varetto*

Main category: quant-ph

TL;DR: 该论文提出了一种混合量子-经典方法用于天地一体化网络设计优化，将卫星选择问题映射到量子绝热算法求解，在165个实际区域测试中量子优化性能与经典精确求解器相当，有时甚至超越。


<details>
  <summary>Details</summary>
Motivation: 实现全球普遍连接需要整合卫星和地面网络，特别是为偏远和服务不足地区提供服务。当前网络部署和运营面临复杂的组合优化问题，需要探索量子计算在解决这些问题上的潜力。

Method: 采用混合量子-经典方法：1）将卫星选择问题（SSP）形式化为最大权重独立集问题，映射到中性原子量子处理器上，使用量子绝热算法在Aquila平台上求解；2）后处理确保可行解，指导下游的网关选择问题（GSP）和频谱分配问题（SAP）优化。

Result: 在165个实际偏远区域的测试中：1）量子绝热算法解与经典精确求解器结果接近，优于贪心启发式算法；2）后续GSP和SAP结果对初始卫星选择的差异保持鲁棒性；3）量子优化在端到端STIN设计中性能与经典方法相当，在某些情况下甚至超越最先进求解器。

Conclusion: 量子优化方法在天地一体化网络设计中已能达到与经典方法相当的性能，虽然尚未表现出持续优势，但对于更大或更复杂的组合子问题可能具有竞争优势，展示了量子计算在网络优化领域的应用潜力。

Abstract: Achieving ubiquitous global connectivity requires integrating satellite and terrestrial networks, particularly to serve remote and underserved regions. In this work, we investigate the design and optimization of Space-Terrestrial Integrated Networks (STINs) using a hybrid quantum-classical approach. We formalize three key combinatorial optimization problems: the Satellite Selection Problem (SSP), the Gateway Selection Problem (GSP), and the Spectrum Assignment Problem (SAP), each capturing critical aspects of network deployment and operation. Leveraging neutral-atom quantum processors, we map the SSP onto a Maximum Weight Independent Set problem, embedding it onto the Aquila platform and solving it via the Quantum Adiabatic Algorithm (QAA). Postprocessing ensures feasible solutions that guide downstream GSP and SAP optimization. Benchmarking across 165 realistic remote regions shows that QAA solutions closely match classical exact solvers and outperform greedy heuristics, while subsequent GSP and SAP outcomes remain largely robust to differences in initial satellite selection. These results demonstrate that quantum optimization achieves performance broadly comparable to classical approaches for end-to-end STIN design, with rare instances where it can even surpass state-of-the-art solvers. This suggests that, while not yet consistently superior, quantum methods may offer competitive advantages for larger or more complex instances of the underlying combinatorial subproblems.

</details>


### [16] [Vistas of Algebraic Probability: Quantum Computation and Information](https://arxiv.org/abs/2602.04351)
*Antonio Falcó,Hermann G. Matthies*

Main category: quant-ph

TL;DR: 该论文提出采用代数框架作为概率论的基础，统一处理经典和量子概率，强调非交换性作为量子行为的核心特征，并聚焦有限维代数简化分析。


<details>
  <summary>Details</summary>
Motivation: 经典Kolmogorov概率框架在处理涉及量子效应的随机现象时存在不足，需要更广泛的数学框架来统一处理经典和量子概率问题。

Method: 采用代数方法：从随机变量的代数出发，配备一个特殊的线性泛函（状态），将其解释为期望值。通过限制在有限维代数上，避免分析复杂性，同时保留核心思想。

Result: 代数框架能够同时容纳经典和量子行为，其中可交换性区分经典与量子行为。该框架为量子计算等应用提供了概率基础。

Conclusion: 代数概率框架为经典和量子概率提供了统一的数学基础，特别适用于量子计算等新兴领域，即使在有限维情况下也能保留核心概念和应用价值。

Abstract: Kolmogorov's foundation of probability takes measure spaces, $σ$-algebras, and probability measures as basic objects. It is, however, widely recognized that this classical framework is inadequate for random phenomena involving quantum effects, and more generally for \emph{quantum-like} situations. A broader formulation is provided by an algebraic viewpoint: one starts from an algebra of random variables equipped with a distinguished linear functional -- the \emph{state} -- interpreted as expectation. In this sense, the approach can also be viewed as a modern reading of ideas already implicit in early probability (e.g., the Bernoullis), while its contemporary form has been developed and used extensively in quantum physics.
  The algebraic framework accommodates both classical and quantum-like behaviours, yet it remains underused in classical probability and uncertainty quantification, where it can nevertheless open new perspectives and clarify structural features. Although the language carries a physics flavor, the subject is purely probabilistic. The key distinction between classical and quantum-like behaviour is \emph{commutativity}: its failure produces the characteristic effects of quantum-like situations. The rise of quantum computing is a prominent setting in which such behaviour may become relevant even for practitioners in computational science. Here we focus on the purely algebraic core of the approach. By restricting attention to finite-dimensional algebras, we avoid many analytical subtleties while retaining the main ideas, their classical limit, and their applicability to quantum-like models and quantum computation.

</details>


### [17] [Low resource entanglement classification from neural network interpretability](https://arxiv.org/abs/2602.04366)
*A. García-Velo,R. Puebla,Y. Ban,E. Torrontegui,M. Paraschiv*

Main category: quant-ph

TL;DR: 该论文提出一个统一且可解释的框架，用于SLOCC纠缠分类二比特和三比特量子态（包括纯态和混合态），通过机器学习模型从泡利测量结果中学习，并使用Shapley值解释模型决策。


<details>
  <summary>Details</summary>
Motivation: 纠缠是量子信息和量子技术的核心资源，但其表征面临理论复杂性和测量要求的挑战。机器学习虽能从不完整测量数据中表征纠缠，但模型可解释性仍是问题。

Method: 训练密集和卷积神经网络处理泡利测量结果，提供每种架构的设计指南，并系统比较它们在各类态上的性能。使用Shapley值量化每个测量的贡献，分析不同系统中的测量重要性模式，并基于这些洞察指导测量简化方案。

Result: 通过准确度-测量曲线与解析纠缠判据的比较，展示了可靠分类所需的最小资源，并突出了Shapley值可解释性在机器学习模型用于纠缠检测和分类时的能力与局限性。

Conclusion: 该工作为纠缠分类提供了一个统一且可解释的机器学习框架，通过系统分析测量重要性模式，不仅实现了高效分类，还揭示了模型决策机制，为量子态表征的资源优化提供了指导。

Abstract: Entanglement is a central resource in quantum information and quantum technologies, yet its characterization remains challenging due to both theoretical complexity and measurement requirements. Machine learning has emerged as a promising alternative, enabling entanglement characterization from incomplete measurement data, however model interpretability remains a challenge. In this work, we introduce a unified and interpretable framework for SLOCC entanglement classification of two- and three-qubit states, encompassing both pure and mixed states. We train dense and convolutional neural networks on Pauli-measurement outcomes, provide design guidelines for each architecture, and systematically compare their performance across types of states. To interpret the models, we compute Shapley values to quantify the contribution of each measurement, analyze measurement-importance patterns across different systems, and use these insights to guide a measurement-reduction scheme. Accuracy-versus-measurement curves and comparisons with analytical entanglement criteria demonstrate the minimal resources required for reliable classification and highlight both the capabilities and limitations of Shapley-based interpretability when using machine learning models for entanglement detection and classification.

</details>


### [18] [Limitations of an approximative phase-space description in strong-field quantum optics](https://arxiv.org/abs/2602.04370)
*Rasmus Vesterager Gothelf,Lars Bojer Madsen,Christian Saugbjerg Lange*

Main category: quant-ph

TL;DR: 近似相空间描述方法在模拟强场物理过程时，虽然能准确预测谐波谱，但会误判驱动激光的量子光学特性，将其描述为经典态的混合态，从而无法捕捉亚泊松光子统计和低于真空涨落的压缩效应。


<details>
  <summary>Details</summary>
Motivation: 近年来，由非经典光驱动的强场过程（如高次谐波产生和阈上电离）研究日益流行。理论建模常采用基于相干态的近似相空间展开方法，该方法已被证明能准确预测谐波谱，但其在计算量子光学可观测量（如压缩度和光子统计）方面的准确性尚未得到充分验证。

Method: 1. 引入并讨论近似相空间描述方法的准确性；2. 使用一能带模型的量子高次谐波产生作为基准，该模型具有精确解析解；3. 将近似相空间表示应用于该特定模型，分析其误差。

Result: 研究发现：1. 近似相空间描述会误判驱动激光的量子光学特性，将其描述为经典态的混合态；2. 这种误差会映射到高次谐波产生的发射光上，无法捕捉亚泊松光子统计和低于真空涨落的压缩效应；3. 在一能带模型中，近似相空间表示在发射光正交方差上产生小的定量误差，该误差随脉冲持续时间和发射体密度而缩放。

Conclusion: 使用近似相空间描述方法可能会误判量子光学可观测量。因此，在赋予此类结果物理意义时，应伴随对误差的定量分析。

Abstract: In recent years, strong-field processes such as high-order harmonic generation (HHG) and above-threshold ionization driven by nonclassical states of light have become an increasingly popular field of study. The theoretical modeling of these processes often applies an approximate phase-space expansion of the nonclassical driving field in terms of coherent states, which has been shown to accurately predict the harmonic spectrum. However, its accuracy for the computation of quantum optical observables like the degree of squeezing and photon statistics has not been thoroughly considered. In this work, we introduce this approximative phase-space description and discuss its accuracy, and we find that it mischaracterizes the quantum optical properties of the driving laser by making it an incoherent mixture of classical states. We further show that this error in the driving field description maps onto the light emitted from HHG, as neither sub-Poissonian photon statistics nor quadrature squeezing below vacuum fluctuations can be captured by the approximative phase-space description. Lastly, to benchmark the approximative phase-space description, we consider the quantum HHG from a one-band model, which yields an exact analytical solution. Using the approximative phase-space representation with this specific model, we find a small quantitative error in the quadrature variance of the emitted field that scales with pulse duration and emitter density. Our results show that using this approximative phase-space description can mischaracterize quantum optical observables. Attributing physical meaning to such results should therefore be accompanied by a quantitative analysis of the error.

</details>


### [19] [Squeezing Enhanced Sagnac Sensing based on SU(1,1) Quantum Interference](https://arxiv.org/abs/2602.04394)
*Michal Natan,Saar Levin,Avi Pe'er*

Main category: quant-ph

TL;DR: 提出一种基于SU(1,1)干涉的压缩增强型Sagnac干涉仪设计，通过在Sagnac环内放置光学参量放大器，实现超越经典灵敏度极限的旋转传感。


<details>
  <summary>Details</summary>
Motivation: 传统Sagnac干涉仪的灵敏度受限于散粒噪声极限，需要突破经典极限以实现更高精度的旋转传感。

Method: 在Sagnac环内放置光学参量放大器，使光在环路正反向自动压缩；采用两种探测方案：直接强度探测（需高效率探测器）和参量零差探测（使用额外OPA，对探测器效率无要求但更复杂）。

Result: 在大多数实际损耗和探测器效率条件下，该设计都能实现超经典灵敏度，同时保持与标准Sagnac配置的兼容性。

Conclusion: 该压缩增强型Sagnac干涉仪设计结合了压缩资源和SU(1,1)干涉原理，为超越经典灵敏度极限的旋转传感提供了实用方案。

Abstract: We present a simple and robust design for a squeezing-enhanced Sagnac interferometer that employs the concept of SU(1,1) interference to significantly surpass the classical sensitivity limit (shot-noise limit - SNL) in rotational sensing. By strategically placing an optical parametric amplifier (OPA) inside the Sagnac loop, light is automatically squeezed in both forward and backward directions of the loop, which enhances the detectability of a small phase. For measuring the squeezed quadrature, we explore two approaches: Direct detection of the output intensity, which is simple, but requires a high-efficiency photo-detector; and parametric homodyne with an additional OPA, which accepts practical detectors with no efficiency limitation, but is technically more complex. Our analysis demonstrates super-classical sensitivity under most realistic conditions of loss and detector inefficiency, thereby leveraging the resources of squeezing and the principles of SU(1,1) interference, while maintaining compatibility with standard Sagnac configurations.

</details>


### [20] [Qudit Twisted-Torus Codes in the Bivariate Bicycle Framework](https://arxiv.org/abs/2602.04443)
*Mourad Halla*

Main category: quant-ph

TL;DR: 研究二维环面扭曲边界条件下的有限长度qudit量子LDPC码，通过扭曲广义环面模式显著提升有限尺寸性能指标k d²/n，在有限尺寸下通常比未扭曲版本获得更大距离


<details>
  <summary>Details</summary>
Motivation: 基于近期qubit工作中发现的扭曲广义环面模式能显著提升有限尺寸性能的见解，将这一方法扩展到有限域上的qudit码，寻求更好的码率-距离权衡

Method: 采用二维环面上具有扭曲边界条件的平移不变CSS构造，在双变量自行车视角下，使用代数方法计算逻辑qudit数量，并识别具有有利码率-距离权衡的紧凑码

Result: 对于所探索的有限尺寸，扭曲环面qudit构造通常比未扭曲对应物获得更大距离，并且优于先前报道的扭曲qubit实例，最佳新码已制表列出

Conclusion: 扭曲边界条件能有效提升qudit量子LDPC码的有限尺寸性能，为设计高性能量子纠错码提供了有前景的方向

Abstract: We study finite-length qudit quantum low-density parity-check (LDPC) codes from translation-invariant CSS constructions on two-dimensional tori with twisted boundary conditions. Recent qubit work [PRX Quantum 6, 020357 (2025)] showed that, within the bivariate-bicycle viewpoint, twisting generalized toric patterns can significantly improve finite-size performance as measured by $k d^{2}/n$. Here $n$ denotes the number of physical qudits, $k$ the number of logical qudits, and $d$ the code distance. Building on this insight, we extend the search to qudit codes over finite fields. Using algebraic methods, we compute the number of logical qudits and identify compact codes with favorable rate--distance tradeoffs. Overall, for the finite sizes explored, twisted-torus qudit constructions typically achieve larger distances than their untwisted counterparts and outperform previously reported twisted qubit instances. The best new codes are tabulated.

</details>


### [21] [Influence of environment on quantum correlations in two-spin systems with dipole-dipole interactions](https://arxiv.org/abs/2602.04444)
*G. A. Bochkin,E. B. Fel'dman,E. I. Kuznetsova,E. I. Shipulya*

Main category: quant-ph

TL;DR: 研究环境对量子关联（纠缠和量子失协）的影响，在具有偶极-偶极相互作用的二自旋-1/2系统中基于Lindblad方程分析，考虑环境仅导致系统自旋退相的最简单情况


<details>
  <summary>Details</summary>
Motivation: 研究环境如何影响量子系统中的纠缠和量子失协这两种不同类型的量子关联，特别关注偶极-偶极相互作用系统中环境退相对量子关联的影响

Method: 使用Lindblad方程研究具有偶极-偶极相互作用的二自旋-1/2系统，考虑环境仅导致系统自旋退相的最简单情况，分析纠缠和量子失协随弛豫率的变化

Result: 获得了纠缠和量子失协对弛豫率的依赖关系，比较了环境对纠缠和量子失协的不同影响

Conclusion: 环境对纠缠和量子失协的影响存在差异，量子失协可能比纠缠更鲁棒，在退相环境下表现出不同的衰减特性

Abstract: An influence of environment on quantum correlations (entanglement and quantum discord) is studied in a two-spin-1/2 system with dipole-dipole interactions on the basis of Lindblad equation. We consider the simplest case when the environment causes only dephasing of system spins. The dependencies of entanglement and the quantum discord on the relaxation rate are obtained. We compare the influence of the environment on entanglement and quantum discord.

</details>


### [22] [Optimal Control Design Guided by Adam Algorithm and LSTM-Predicted Open Quantum System Dynamics](https://arxiv.org/abs/2602.04480)
*JunDong Zhong,ZhaoMing Wang*

Main category: quant-ph

TL;DR: 基于LSTM神经网络预测开放量子系统动力学，提出快速高效的最优控制框架，应用于非马尔可夫环境中两能级系统的绝热加速控制设计。


<details>
  <summary>Details</summary>
Motivation: 在噪声环境中实现高保真量子控制对量子信息处理至关重要，传统最优控制设计需要数值计算系统动力学，计算成本高。需要开发更快速高效的控制设计方法。

Method: 利用LSTM神经网络准确预测开放量子系统的时间演化，基于预测的动力学建立最优控制框架。以两能级系统在非马尔可夫环境中的绝热加速为例，采用两步优化：驱动轨迹优化和零面积脉冲优化。

Result: 在两个优化步骤中都获得了保真度提升，证明了方案的有效性。基于预测动力学生成优化控制，计算效率高。

Conclusion: 提出的最优控制设计方案利用预测动力学生成优化控制，在量子计算、通信和传感等领域具有广泛的应用潜力，为噪声环境中的量子控制提供了高效解决方案。

Abstract: The realization of high-fidelity quantum control is crucial for quantum information processing, particularly in noisy environments where control strategies must simultaneously achieve precise manipulation and effective noise suppression. Conventional optimal control designs typically requires numerical calculations of the system dynamics. Recent studies have demonstrated that long short-term memory neural networks (LSTM-NNs) can accurately predict the time evolution of open quantum systems. Based on LSTM-NN predicted dynamics, we propose an optimal control framework for rapid and efficient optimal control design in open quantum systems. As an exemplary example, we apply our scheme to design an optimal control for the adiabatic speedup in a two-level system under a non-Markovian environment. Our optimization procedure entails two steps: driving trajectory optimization and zero-area pulse optimization. Fidelity improvement for both steps have been obtained, showing the effectiveness of the scheme. Our optimal control design scheme utilizes predicted dynamics to generate optimized controls, offering broad application potential in quantum computing, communication, and sensing.

</details>


### [23] [Squeezing-Enhanced Rotational Doppler Metrology](https://arxiv.org/abs/2602.04508)
*Javier Navarro,Mateo Casariego,Gabriel Molina-Terriza,Íñigo Luis Egusquiza,Mikel Sanz*

Main category: quant-ph

TL;DR: 提出了一种基于旋转多普勒效应的连续变量量子协议，利用压缩和位移的拉盖尔-高斯模式作为量子资源，通过零差探测测量频率偏移，实现角速度估计的Heisenberg标度。


<details>
  <summary>Details</summary>
Motivation: 旋转表面通过改变角动量会引起入射光的频率偏移（旋转多普勒效应），这为估计旋转角速度提供了方法。传统方法存在精度限制，需要开发量子增强方案来提高测量精度。

Method: 使用压缩和位移的拉盖尔-高斯模式作为量子探针，让这些模式与具有表面粗糙度的旋转金属盘相互作用。通过旋转多普勒效应产生的频率偏移，采用零差探测方案进行测量。通过分析Fisher信息评估协议性能。

Result: 在理想无噪声条件下，提出的压缩增强协议实现了Heisenberg标度。在存在噪声的情况下，Heisenberg标度会降低，但通过优化位移和压缩之间的能量分配比例，量子策略始终优于经典对应方案。

Conclusion: 该连续变量量子协议利用旋转多普勒效应实现了角速度的量子增强估计，在无噪声条件下达到Heisenberg标度，在噪声环境中通过优化参数仍能保持量子优势。

Abstract: A rotating surface can induce a frequency shift in incident light by changing its angular momentum, a phenomenon known as the rotational Doppler effect. This effect provides a means to estimate the angular velocity of the rotating surface. In this work, we develop a continuous-variable quantum protocol for estimating the angular velocity of a rotating surface via the rotational Doppler effect. Our approach exploits squeezed and displaced Laguerre-Gaussian modes as quantum resources, which interact with a rotating metallic disc with surface roughness. The frequency shift induced by the rotational Doppler effect is then measured using a homodyne detection scheme. By analyzing the Fisher information, we demonstrate that the proposed squeezing-enhanced protocol achieves Heisenberg scaling in the ideal noiseless regime. Furthermore, we investigate the influence of noise and consider different surface models to assess their impact on the protocol's performance. While Heisenberg scaling is degraded in the presence of noise, we show that optimizing the energy allocation ratio between displacement and squeezing of the probe ensures that the quantum strategy consistently outperforms its classical counterpart.

</details>


### [24] [A simple means for deriving quantum mechanics](https://arxiv.org/abs/2602.04524)
*Eric Tesse*

Main category: quant-ph

TL;DR: 提出一种新型力学，具有直观的经典描述，却能完全重现非相对论量子力学的所有可观测预测，包括纠缠、自旋等量子现象。


<details>
  <summary>Details</summary>
Motivation: 旨在构建一种既具有经典直观性（粒子有确定位置和连续轨迹）又能完全符合量子力学所有可观测预测的理论框架，弥合经典直观与量子形式主义之间的鸿沟。

Method: 提出一种新型力学，其中粒子存在于空间点，遵循连续、分段可微的路径，动量等于质量乘以速度，但位置和动量的概率分布（以粒子环境状态为条件）遵循量子理论规则。

Result: 该理论能够重现量子力学的所有可观测后果：粒子可以纠缠、具有内禀自旋（自旋非定域）、粒子身份影响概率等，所有量子力学规则都得到遵守并以直接方式出现。

Conclusion: 这种力学与玻姆力学、随机力学、多世界诠释和物理塌缩等其他量子世界理论存在联系，并可以扩展到相对论版本，提供了一种连接经典直观与量子实在的新框架。

Abstract: A type of mechanics will be presented that possesses some distinctive properties. On the one hand, its physical description & rules of operation are readily comprehensible & intuitively clear. On the other, it fully satisfies all observable predictions of non-relativistic quantum mechanics. Within it, particles exist at points in space, follow continuous, piecewise differentiable paths, and their linear momentum is equal to their mass times their velocity along their path. Yet the probabilities for position and momentum, conditioned on the state of the particle's environment, follow the rules of quantum theory. Indeed, all observable consequences of quantum theory are satisfied; particles can be entangled, have intrinsic spin, this spin is not local to the particle, particle identity can effect probabilities, and so forth. All the rules of quantum mechanics are obeyed, and all arise in a straightforward fashion. After this is established, connections will be drawn out between this type of mechanics and other types of quantum worlds; those that obey Bohmian mechanics, stochastic mechanics, the many worlds interpretation, and physical collapse. In the final section, a relativistic version of the mechanics will be presented.

</details>


### [25] [Thermodynamic Cost of Regeneration in a Quantum Stirling Cycle](https://arxiv.org/abs/2602.04538)
*Ferdi Altintas*

Main category: quant-ph

TL;DR: 该研究分析了量子斯特林热机循环中的再生过程，指出再生过程在开放系统描述中并非热力学免费，需要计入再生成本。研究发现，计入再生成本后，之前报道的超卡诺效率消失，修正后的效率保持在卡诺界限以下。


<details>
  <summary>Details</summary>
Motivation: 研究动机是澄清量子斯特林热机循环中再生过程的热力学成本。先前研究假设再生过程是免费的，这可能导致报告超卡诺效率，但作者认为在开放系统描述中再生过程需要工作输入，应计入成本。

Method: 采用弱耦合、马尔可夫开放量子系统框架，研究两种工作物质：单自旋-1/2粒子和相互作用的自旋-1/2粒子对。通过将再生所需的工作输入作为显式成本，修改循环效率公式。将再生成本设为其最小值（由卡诺热泵极限设定），并与相同条件下无再生的传统斯特林循环进行比较。

Result: 1. 计入再生成本后，先前假设免费再生时报告的超卡诺效率消失；2. 修正后的效率保持在卡诺界限以下，但仍高于传统斯特林循环效率；3. 为传统斯特林循环提供了基于量子相对熵的严格卡诺界限；4. 为再生循环推导了保证热力学一致性的再生成本充分下界。

Conclusion: 量子斯特林热机中的再生过程需要计入热力学成本，计入成本后效率不会超过卡诺界限。研究为量子热机的热力学一致性分析提供了框架，并提出了三种量子再生器模型供未来研究。

Abstract: We study the regenerative quantum Stirling heat engine cycle within the standard weak-coupling, Markovian open quantum system framework. We point out that the regeneration process is not thermodynamically free in a reduced open-system description, and we treat the required work input as an explicit regeneration cost by modifying the cycle efficiency accordingly. We consider two working substances--a single spin-$1/2$ and a pair of interacting spin-$1/2$ particles--and investigate the cycle performance by taking the regeneration cost at its minimum value set by the Carnot heat-pump limit. For comparison, we also analyze the conventional Stirling cycle without regeneration under the same conditions. The super-Carnot efficiencies reported under the cost-free regeneration assumption disappear once the regeneration cost is included: the modified efficiency stays below the Carnot bound, while still remaining higher than the efficiency of the conventional Stirling cycle. For the conventional Stirling cycle, we provide a rigorous Carnot bound using quantum relative entropy, whereas for the regenerative cycle we derive a sufficient lower bound on the regeneration cost that guarantees thermodynamic consistency. Finally, we suggest three candidate quantum regenerator models for future work.

</details>


### [26] [Effect of initial intrasystem entanglement on entropy growth in generalized Jaynes-Cummings models](https://arxiv.org/abs/2602.04543)
*Daria Gaidukevich*

Main category: quant-ph

TL;DR: 初始系统内纠缠与熵增长的正相关关系研究


<details>
  <summary>Details</summary>
Motivation: 研究初始系统内纠缠如何影响量子系统与光子环境相互作用中的熵生成，探索纠缠在量子信息过程中的作用

Method: 使用多个广义Jaynes-Cummings模型（包含两个或更多子系统），考虑不同初始状态集合：纯态和混合态的Haar随机态、固定平均能量或固定混合度的集合、不同初始光子数的环境

Result: 在所有情况下都观察到初始纠缠与熵增长之间的正相关关系，尽管初始纠缠的贡献比例有所不同

Conclusion: 系统内关联是量子信息过程中熵增长的一个重要因素，初始纠缠对熵生成有积极影响

Abstract: We investigate how initial intrasystem entanglement influences the entropy generated in atomic systems interacting with a photonic environment in several generalizations of the Jaynes-Cummings model with two or more subsystems. Since the initial entanglement does not uniquely determine the final entropy, we focus on ensemble-averaged behavior. We consider ensembles of initial system states including pure and mixed Haar-random states, ensembles with fixed average energy or fixed mixedness, and varying initial photon numbers in the environment. In all cases, we observe a positive correlation between the initial entanglement and the entropy growth, although the fractional contribution of the initial entanglement varies. Our results emphasize the role of intrasystem correlations as a factor contributing to entropy growth in quantum informational processes.

</details>


### [27] [Locally Gentle State Certification for High Dimensional Quantum Systems](https://arxiv.org/abs/2602.04550)
*Cristina Butucea,Jan Johannes,Henning Stein*

Main category: quant-ph

TL;DR: 该论文研究了局部温和量子态认证的基本极限，推导了在扰动约束下的极小极大样本复杂度，发现α-温和性引入了d/α²的样本惩罚，总复杂度为Θ(d³/ε²α²)。


<details>
  <summary>Details</summary>
Motivation: 传统量子统计推断方法依赖于导致波函数坍缩的测量，这会消耗量子态。本研究旨在探索在局部温和约束下的量子态认证基本极限，即学习算法对状态的扰动不超过α迹范数，从而允许样本重用。

Method: 通过构建显式测量算子，分析区分未知态ρ等于参考态ρ₀还是ε远离它的假设检验问题，推导极小极大样本复杂度，量化非破坏性测量的信息理论代价。

Result: α-温和性约束引入了d/α²的样本大小惩罚，总样本复杂度为n = Θ(d³/ε²α²)。与典型高维私有估计的d²-1参数相比，温和性惩罚随希尔伯特空间维度d线性增长。

Conclusion: 研究阐明了信息提取与状态扰动之间的权衡，揭示了物理测量约束与量子学习中隐私机制的深层联系。温和性约束的样本惩罚随维度d线性增长而非平方增长，这对量子学习中的隐私保护具有重要意义。

Abstract: Standard approaches to quantum statistical inference rely on measurements that induce a collapse of the wave function, effectively consuming the quantum state to extract information. In this work, we investigate the fundamental limits of \emph{locally-gentle} quantum state certification, where the learning algorithm is constrained to perturb the state by at most $α$ in trace norm, thereby allowing for the reuse of samples. We analyze the hypothesis testing problem of distinguishing whether an unknown state $ρ$ is equal to a reference $ρ_0$ or $ε$-far from it. We derive the minimax sample complexity for this problem, quantifying the information-theoretic price of non-destructive measurements. Specifically, by constructing explicit measurement operators, we show that the constraint of $α$-gentleness imposes a sample size penalty of $\frac{d}{α^2}$, yielding a total sample complexity of $n = Θ(\frac{d^3}{ε^2 α^2})$. Our results clarify the trade-off between information extraction and state disturbance, and highlight deep connections between physical measurement constraints and privacy mechanisms in quantum learning. Crucially, we find that the sample size penalty incurred by enforcing $α$-gentleness scales linearly with the Hilbert-space dimension $d$ rather than the number of parameters $d^2-1$ typical for high-dimensional private estimation.

</details>


### [28] [Entanglement improves coordination in distributed systems](https://arxiv.org/abs/2602.04588)
*Francisco Ferreira da Silva,Stephanie Wehner*

Main category: quant-ph

TL;DR: 量子纠缠辅助的分布式调度策略在双工作优化问题中，相比无通信经典策略实现了帕累托更优性能


<details>
  <summary>Details</summary>
Motivation: 分布式系统中的协调常受通信延迟影响，量子纠缠提供比经典方法更强的瞬时相关性，可用于改善分布式调度性能

Method: 建立严格分析模型，通过排队论分析、非局域博弈公式化和经典边界计算认证，研究共享纠缠在双服务器分布式系统中的应用

Result: 当基线任务吞吐量函数严格凸时，纠缠辅助路由策略相比最优无通信经典策略实现帕累托更优性能

Conclusion: 分布式调度和协调是近期纠缠量子网络的新应用领域，量子纠缠可显著提升分布式系统性能

Abstract: Coordination in distributed systems is often hampered by communication latency, which degrades performance. Quantum entanglement offers fundamentally stronger correlations than classically achievable without communication. Crucially, these correlations manifest instantaneously upon measurement, irrespective of the physical distance separating the systems. We investigate the application of shared entanglement to a dual-work optimization problem in a distributed system comprising two servers. The system must process both a continuously available, preemptible baseline task and incoming customer requests arriving in pairs. System performance is characterized by the trade-off between baseline task throughput and customer waiting time. We present a rigorous analytical model demonstrating that when the baseline task throughput function is strictly convex, rewarding longer uninterrupted processing periods, entanglement-assisted routing strategies achieve Pareto-superior performance compared to optimal communication-free classical strategies. We prove this advantage through queueing-theoretic analysis, non-local game formulation, and computational certification of classical bounds. Our results identify distributed scheduling and coordination as a novel application domain for near-term entanglement-based quantum networks.

</details>


### [29] [Dicke Superradiance in Extended 2D Quantum Arrays Coupled to Metasurface Bound States in the Continuum](https://arxiv.org/abs/2602.04627)
*Daniel Eyles,Emmanuel Lassalle,Adam Stokes,Ahsan Nazir,Ramón Paniagua-Domínguez*

Main category: quant-ph

TL;DR: 利用介电超表面的BIC模式实现量子发射体阵列的Dicke超辐射，克服传统亚波长间距限制，可达理想Dicke极限。


<details>
  <summary>Details</summary>
Motivation: 传统Dicke超辐射需要量子发射体在亚波长间距内紧密排列，这在实际实验中难以实现。本文旨在寻找一种能够克服这一限制的平台，使远距离量子发射体也能实现相干增强的集体辐射。

Method: 提出利用介电超表面支持的束缚态连续体（BIC）模式作为平台。BIC模式具有高Q值和非局域特性，能够介导相距数个波长的量子发射体之间的相互作用。通过β因子量化发射体与BIC模式的耦合效率。

Result: BIC介导的发射体相互作用可跨越数个波长，突破传统自由空间所需的亚波长间距限制。当发射体与BIC模式耦合效率足够高时，系统可达理想Dicke极限。分析表明该方案对实际实验缺陷具有鲁棒性。

Conclusion: 支持BIC模式的光学超表面是实现扩展量子发射体阵列中合作辐射上限的物理可行平台，为实验实现理想Dicke超辐射提供了新途径。

Abstract: Dicke superradiance is a collective phenomenon where the emission from ensembles of quantum emitters is coherently enhanced beyond the sum of each emitter's independent emission. Here, we propose a platform that exploits the delocalised nature of a high-Q, non-local mode supported by a dielectric metasurface (a so-called bound-state-in-the-continuum or BIC) to induce superradiant behaviour within an extended two-dimensional array of distant quantum emitters. We show that these BIC-mediated emitter interactions can span several wavelengths, thus overcoming the traditional subwavelength separation between emitters required in free space. We further show that reaching the idealised Dicke limit is possible in this system, provided that the emitters are coupled to the BIC mode efficiently enough, as quantified through the $β$-factor. Moreover, we demonstrate its experimental viability by analysing its robustness to realistic experimental imperfections. This work puts forward optical metasurfaces supporting BICs as a physically viable platform for realising the upper limits of cooperative emission in physically extended quantum emitter arrays.

</details>


### [30] [On the emergence of classical stochasticity](https://arxiv.org/abs/2602.04633)
*Xuan Du Trinh,Ismaël Septembre,Hai-Chau Nguyen*

Main category: quant-ph

TL;DR: 该论文探讨了量子系统如何从泡利型主方程中涌现出经典随机性，特别关注了中间时间确定状态假设对计算随机时间的重要性，并通过单粒子、玻色子和费米子在超退相干极限下的例子进行说明。


<details>
  <summary>Details</summary>
Motivation: 虽然泡利型主方程描述概率演化，但它们不能自动证明基于"系统在中间时间处于确定状态"假设的经典推理。该论文旨在澄清这种经典随机性从量子力学中涌现的逻辑结构，特别是中间状态假设对计算随机时间（如持续时间和首次到达时间）的重要性。

Method: 论文采用理论分析方法，首先分析泡利型主方程的逻辑结构，然后展示中间时间确定状态假设对标准随机时间计算的关键作用。最后通过具体例子（单粒子、玻色子和费米子）在超退相干极限下说明经典随机性如何从量子力学中涌现。

Result: 研究表明，虽然泡利型主方程描述概率演化，但经典随机推理需要额外的中间时间确定状态假设。在超退相干极限下，量子系统可以展现出经典随机行为，其中单粒子、玻色子和费米子系统的具体例子说明了这种涌现过程。

Conclusion: 经典随机性从量子力学中的涌现需要超越单纯概率演化的额外假设，特别是系统在中间时间处于确定状态的假设。超退相干极限为理解这种涌现提供了具体框架，展示了量子系统如何表现出经典随机行为。

Abstract: We examine the logical structure of the emergence of classical stochasticity for a quantum system governed by a Pauli-type master equation. It is well-known that while such equations describe the evolution of probabilities, they do not automatically justify classical reasoning based on the assumption that the system exists in a definite state at intermediate times. On the other hand, we show that this assumption is crucial for the standard calculation of stochastic times such as the persistent time and the time of first arrivals. We then consider examples of single particles, bosons, and fermions in the so-called ultradecoherence limit to illustrate how classical stochasticity may emerge from quantum mechanics.

</details>


### [31] [Pure narrowband photon-pair generation in a monolithic cavity](https://arxiv.org/abs/2602.04646)
*Xavier Barcons Planas,Helen M. Chrzanowski,Janik Wolters*

Main category: quant-ph

TL;DR: 该论文展示了一种基于SPDC的腔增强单光子源，在1540nm波长下实现了84%的预示效率和96.2%的频谱纯度，同时保持多光子污染低于3%。


<details>
  <summary>Details</summary>
Motivation: 光子量子技术需要高效且纯净的单光子源，现有单光子源在频谱纯度、空间纯度和效率方面存在挑战，需要开发优化的单光子源以满足量子技术应用需求。

Method: 采用基于自发参量下转换（SPDC）的单片腔增强设计，通过优化腔结构实现高频谱和空间纯度，在1540nm波长下工作，带宽为168MHz。

Result: 实现了84%的最大预示效率，多光子污染低于3%，频谱纯度达到96.2%，光子主要分布在中心腔模中。

Conclusion: 该单片腔增强SPDC单光子源在效率、纯度和多光子抑制方面表现出色，为光子量子技术提供了高质量的单光子源解决方案。

Abstract: Photonic quantum technologies require efficient sources of pure single photons. Here we present a heralded SPDC single-photon source in a monolithic cavity optimized for high spectral and spatial purity. The source heralds single-photons at a wavelength of 1540 nm and a spectral bandwidth of 168 MHz with a maximum heralding efficiency of 84%, while keeping the multi-photon contamination below 3%. The cavity enhancement generates photons mainly in the central cavity mode with 96.2% spectral purity.

</details>


### [32] [Pre-optimization of quantum circuits, barren plateaus and classical simulability: tensor networks to unlock the variational quantum eigensolver](https://arxiv.org/abs/2602.04676)
*Baptiste Anselme Martin,Thomas Ayral*

Main category: quant-ph

TL;DR: 使用可微分二维张量网络优化参数化量子电路，以制备横场伊辛模型的基态，缓解贫瘠高原问题，并识别量子硬件优于经典模拟的体系。


<details>
  <summary>Details</summary>
Motivation: 变分量子算法在制备基态方面具有实用性，但其量子优势潜力尚不明确。需要解决贫瘠高原问题（梯度随系统尺寸指数衰减），并探索量子硬件相对于经典模拟的优势区域。

Method: 采用可微分二维张量网络（TN）来优化参数化量子电路，用于制备横场伊辛模型的基态。通过TN预优化获得"热启动"初始参数，进入梯度不随系统尺寸指数衰减的增强梯度区域。

Result: 该方法能够以高能量精度制备基态，即使对于超越一维的大系统也有效。TN预优化可以缓解贫瘠高原问题，提供不随系统尺寸指数收缩的梯度区域。评估了在这些热启动下的经典模拟成本，并识别出量子硬件比TN模拟具有更好标度性的体系。

Conclusion: 张量网络预优化是缓解变分量子算法中贫瘠高原问题的有效策略，同时能够识别量子硬件相对于经典模拟具有优势的物理体系，为量子优势的实现提供了新途径。

Abstract: Variational quantum algorithms are practical approaches to prepare ground states, but their potential for quantum advantage remains unclear. Here, we use differentiable 2D tensor networks (TN) to optimize parameterized quantum circuits that prepare the ground state of the transverse field Ising model (TFIM). Our method enables the preparation of states with high energy accuracy, even for large systems beyond 1D. We show that TN pre-optimization can mitigate the barren plateau issue by giving access to enhanced gradient zones that do not shrink exponentially with system size. We evaluate the classical simulation cost evaluating energies at these warm-starts, and identify regimes where quantum hardware offers better scaling than TN simulations.

</details>


### [33] [Quantum Advantage in Decision Trees: A Weighted Graph and $L_1$ Norm Approach](https://arxiv.org/abs/2602.04700)
*Sebastian Alberto Grillo,Bernardo Daniel Dávalos,Rodney Fabian Franco Torres,Franklin de Lima Marquezino,Edgar López Pezoa*

Main category: quant-ph

TL;DR: 本文提出将单查询量子决策树建模为加权图，通过分析输出矩阵的L1谱范数来研究量子优势，并展示了指数级量子优势的函数实例。


<details>
  <summary>Details</summary>
Motivation: 分析单查询量子算法的计算能力很重要，因为它们必须从一次oracle调用中提取最大信息，这揭示了量子优势的基本限制，并有助于实现资源高效的量子计算。

Method: 将单查询量子决策树表述为加权图，这种表述便于分析算法输出的L1谱范数。提出最大化L1谱范数的启发式方法，展示如何组合加权图生成范数严格递增的序列。

Result: 展示了具有指数级量子优势的函数实例，建立了单查询量子优势与测量投影仪维度渐近增长之间的必要条件联系。

Conclusion: 加权图表述为分析单查询量子算法的计算能力提供了有效框架，L1谱范数是衡量量子优势的关键指标，该研究揭示了量子决策树的基本限制。

Abstract: The analysis of the computational power of single-query quantum algorithms is important because they must extract maximal information from one oracle call, revealing fundamental limits of quantum advantage and enabling optimal, resource-efficient quantum computation. This paper proposes a formulation of single-query quantum decision trees as weighted graphs. This formulation has the advantage that it facilitates the analysis of the $L_1$ spectral norm of the algorithm output. This advantage is based on the fact that a high $L_1$ spectral norm of the output of a quantum decision tree is a necessary condition to outperform its classical counterpart. We propose heuristics for maximizing the $L_{1}$ spectral norm, show how to combine weighted graphs to generate sequences with strictly increasing norm, and present functions exhibiting exponential quantum advantage. Finally, we establish a necessary condition linking single-query quantum advantage to the asymptotic growth of measurement projector dimensions.

</details>


### [34] [Enabling large-scale digital quantum simulations with superconducting qubits](https://arxiv.org/abs/2602.04719)
*Laurin E. Fischer*

Main category: quant-ph

TL;DR: 这篇博士论文探索量子模拟的全栈方法，包括硬件创新、噪声建模、错误缓解和高效测量处理，旨在在当前有噪声的中等规模量子设备上实现实用量子优势。


<details>
  <summary>Details</summary>
Motivation: 量子计算在量子化学、凝聚态物理和材料科学等领域有革命性潜力，但当前量子设备存在噪声和错误问题，而容错量子计算所需的量子比特数量过大，因此需要开发能在有噪声中等规模量子设备上工作的实用方法。

Method: 采用全栈方法：硬件层面创新、噪声建模与错误缓解技术改进、通过高效测量处理实现算法优化，结合量子与经典资源，利用硬件原生操作。

Result: 论文未提供具体实验结果，但提出了一个全面的研究框架，旨在推进量子模拟技术，使其能在当前有噪声量子硬件上产生有意义的计算结果。

Conclusion: 通过跨计算堆栈的协同创新，可以在当前有噪声中等规模量子设备上实现实用量子优势，为量子模拟的实际应用铺平道路。

Abstract: Quantum computing promises to revolutionize several scientific and technological domains through fundamentally new ways of processing information. Among its most compelling applications is digital quantum simulation, where quantum computers are used to replicate the behavior of other quantum systems. This could enable the study of problems that are otherwise intractable on classical computers, transforming fields such as quantum chemistry, condensed matter physics, and materials science. Despite this potential, realizations of practical quantum advantage for relevant problems are hindered by imperfections of current devices. This also affects quantum hardware based on superconducting circuits which is among the most advanced and scalable platforms. The envisaged long-term solution of fault-tolerant quantum computers that correct their own errors remains out of reach mainly due to the associated qubit number overhead. As a result, the field has developed strategies that combine quantum and classical resources, exploit hardware-native operations, and employ error mitigation techniques to extract meaningful results from noisy data. This doctoral thesis contributes to this broader effort by exploring methods for advancing quantum simulation across the full computational stack, including hardware-level innovations, refined techniques for noise modeling and error mitigation, and algorithmic improvements enabled by efficient measurement processing.

</details>


### [35] [Ising-Induced Spectral Broadening Resolves the Relaxation Bottleneck in Superradiant Masers](https://arxiv.org/abs/2602.04721)
*Hongze Ding,Jiuqing Liang*

Main category: quant-ph

TL;DR: 论文揭示了高密度自旋系统中弛豫瓶颈的微观机制：对角Ising相互作用导致强烈的不均匀展宽，抑制了自旋翻转交换，从而减慢了集体弛豫过程。


<details>
  <summary>Details</summary>
Motivation: 实验观察到自诱导超辐射激射现象中的集体弛豫时间尺度比标准相干输运模型预测的要慢得多，需要解释这种"弛豫瓶颈"的微观起源。

Method: 通过分析高密度自旋系统中的对角Ising相互作用，证明这些通常被视为微扰的相互作用会产生超过本征单粒子退相干的强烈不均匀展宽，从而抑制共振翻转交换。

Result: 无参数解析理论定量地重现了实验观测到的微秒级动力学，确定Ising诱导的展宽是稠密固态自旋系综中能量输运的主导机制。

Conclusion: 对角Ising相互作用导致的强烈不均匀展宽是造成高密度自旋系统中弛豫瓶颈的关键机制，这重新定义了稠密固态自旋系综中能量输运的理解。

Abstract: The recent observation of self-induced superradiant masing [[W. Kersten et al., Nat. Phys. 22, 158 (2026)]] revealed a collective relaxation timescale significantly slower than predicted by standard coherent transport models. Here, we elucidate the microscopic origin of this ``relaxation bottleneck.'' We show that in the high-density regime relevant to the experiment, diagonal Ising interactions -- often treated as perturbative -- generate profound inhomogeneous broadening that exceeds the intrinsic single-particle dephasing. This intense diagonal disorder suppresses resonant flip-flop exchange, effectively renormalizing the density of states available for spectral diffusion. Our parameter-free analytic theory quantitatively reproduces the experimentally observed microsecond dynamics, identifying Ising-induced broadening as the governing mechanism for energy transport in dense solid-state spin ensembles.

</details>


### [36] [Resource-Efficient Digitized Adiabatic Quantum Factorization](https://arxiv.org/abs/2602.04740)
*Felip Pellicer,Juan José García-Ripoll,Alan C. Santos*

Main category: quant-ph

TL;DR: 本文提出一种基于QUBO的数字绝热量子分解算法，通过将问题编码在哈密顿量核子空间中（而非传统基态编码），显著降低了电路复杂度和门需求，在8位整数分解中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 传统绝热量子分解算法（Peng等人提出）使用基态编码和PUBO（多项式无约束二进制优化）方法，导致电路复杂、门需求高。本文旨在利用数字化量子计算机的优势，开发更高效的绝热分解算法。

Method: 提出新的绝热分解方法：将问题解编码在问题哈密顿量的核子空间中，而非传统基态编码。这使得算法属于QUBO（二次无约束二进制优化）类，而非传统的PUBO类。利用数字化量子计算机的灵活性实现门分解。

Result: 实现了8位整数的分解，结果显示相比PUBO公式有显著改进：电路复杂度降低，识别正确解的保真度提高。

Conclusion: 提出的QUBO编码方法为绝热量子分解提供了更高效的实现途径，通过减少门需求和电路复杂度，在数字化量子计算机上展现出优越性能，为大规模整数分解问题提供了有前景的解决方案。

Abstract: Digitized adiabatic quantum factorization is a hybrid algorithm that exploits the advantage of digitized quantum computers to implement efficient adiabatic algorithms for factorization through gate decompositions of analog evolutions. In this paper, we harness the flexibility of digitized computers to derive a digitized adiabatic algorithm able to reduce the gate-demanding costs of implementing factorization. To this end, we propose a new approach for adiabatic factorization by encoding the solution of the problem in the kernel subspace of the problem Hamiltonian, instead of using ground-state encoding considered in the standard adiabatic factorization proposed by Peng $et$ $al$. [Phys. Rev. Lett. 101, 220405 (2008)]. Our encoding enables the design of adiabatic factorization algorithms belonging to the class of Quadratic Unconstrained Binary Optimization (QUBO) methods, instead the Polinomial Unconstrained Binary Optimization (PUBO) used by standard adiabatic factorization. We illustrate the performance of our QUBO algorithm by implementing the factorization of integers $N$ up to 8 bits. The results demonstrate a substantial improvement over the PUBO formulation, both in terms of reduced circuit complexity and increased fidelity in identifying the correct solution.

</details>


### [37] [Generalized quantum theory for accessing nonlinear systems: the case of Levinson-Smith equations](https://arxiv.org/abs/2602.04747)
*Bijan Bagchi,Anindya Ghose-Choudhury*

Main category: quant-ph

TL;DR: 论文探讨了广义量子力学方案与Levinson-Smith非线性系统（特别是Liénard微分方程族）的联系，分析了平衡点稳定性、Jacobi椭圆函数解、位置依赖质量系统以及孤子状解的出现。


<details>
  <summary>Details</summary>
Motivation: 受最近发展的广义量子力学方案启发，研究Levinson-Smith类非线性系统与Liénard微分方程族的联系，探索这些系统的数学特性和物理意义。

Method: 将Liénard方程转换为Abel形式获得闭式解，分析控制条件确定平衡点稳定性，研究参数组合下的Jacobi椭圆函数解，并探讨位置依赖质量系统的相关性。

Result: 发现其中一个非平凡平衡点是稳定的；某些参数组合下获得Jacobi椭圆函数解；不同参数集与位置依赖质量系统相关；从系统能级面条件中出现了孤子状解。

Conclusion: 广义量子力学方案与Levinson-Smith非线性系统之间存在深刻联系，Liénard方程族具有丰富的数学结构和物理应用，包括稳定平衡点、椭圆函数解、位置依赖质量系统和孤子解等特性。

Abstract: Motivated by a recently developed generalized scheme of quantum mechanics, we touch upon connections with Levinson-Smith classes of nonlinear systems that contain as a particular case the Liénard family of differential equations. The latter, which has coefficients of odd and odd symmetry, admits a closed form solution when converted to the Abel form. Analysis of the governing condition shows that one of the nontrivial equilibrium points is stable in character. Other classes of differential equations that we encounter speak of solutions involving Jacobi elliptic functions for a certain combination of underlying parameters, while, for a different set, relevance to position-dependent mass systems is shown. In addition, an interesting off-shoot of our results is the emergence of solitonic-like solutions from the condition of the level surface in the system.

</details>


### [38] [Quantifying the Operational Cost of Multipartite Entanglement](https://arxiv.org/abs/2602.04760)
*Francois Payn,Michele Minervini,Davide Girolami*

Main category: quant-ph

TL;DR: 提出一种量化多体系统k-部纠缠的通用方法，通过最大化子系统内任意二分纠缠度量，并证明k-部纠缠态的实验制备至少需要k-1个两粒子纠缠门。


<details>
  <summary>Details</summary>
Motivation: 多体量子系统中的多部纠缠决定了相互作用的强度和范围，但由于量子态的复杂结构，评估多部纠缠非常困难。

Method: 通过最大化子系统内任意二分纠缠度量来量化N粒子系统的k≤N-部纠缠，其中子系统大小不超过k。

Result: 该方法能对多部态进行分类，揭示其实验成本：制备k-部纠缠态至少需要k-1个两粒子纠缠门。计算了包括任意维度W态在内的几类态中新定义的k-部纠缠形成量。

Conclusion: 提出了一种通用的多部纠缠量化框架，将重要二分纠缠度量推广到多部情形，为多体量子系统的纠缠特性分析提供了新工具。

Abstract: Multipartite entanglement determines the strength and range of interactions in many-body quantum systems. Yet, it is hard to evaluate it, due to the complex structures of quantum states. Here, we introduce a generic method to quantify the k <= N-partite entanglement of an N-particle system, by maximizing an arbitrary bipartite entanglement measure within subsystems of size up to k. The resulting classification of multipartite states captures their experimental cost: creating a k-partite entangled state requires at least k-1 two-particle entangling gates. Further, we analytically calculate the newly defined k-partite entanglement of formation, which generalizes an important bipartite entanglement measure, in several classes of states, including the W states of any dimension.

</details>


### [39] [Dynamical Quantum Phase Transitions in Boundary Time Crystals](https://arxiv.org/abs/2602.04792)
*Sukrut Mondkar,Priya Ghosh,Ujjwal Sen*

Main category: quant-ph

TL;DR: 在耗散集体自旋模型中，通过淬火或线性斜坡驱动跨越边界时间晶体相变，发现了动力学量子相变的存在，表现为Loschmidt回波的零点和非解析性特征。


<details>
  <summary>Details</summary>
Motivation: 研究耗散系统中边界时间晶体相变与动力学量子相变之间的关系，探索非平衡量子动力学中的相变现象。

Method: 使用耗散集体自旋模型，通过两种协议驱动系统跨越BTC相变：1）突然淬火，2）有限时间线性斜坡驱动。通过保真度Loschmidt回波的零点诊断DQPT，分析速率函数的非解析特征。

Result: 淬火到BTC相时，Loschmidt回波因时间周期稳态而出现重复零点；淬火到非BTC相时，重叠消失并保持为零。斜坡协议后继续幺正演化，DQPT仍然存在。有限尺寸标度显示临界时间在热力学极限下收敛为常数。

Conclusion: 耗散集体自旋模型中的边界时间晶体相变伴随着动力学量子相变，Loschmidt回波的零点特征揭示了非平衡量子动力学中的相变行为，且该现象在淬火和斜坡协议下均存在。

Abstract: We demonstrate the existence of a dynamical quantum phase transition (DQPT) in a dissipative collective-spin model that exhibits the boundary time crystal (BTC) phase. We initialize the system in the ground state of the Hamiltonian in either the BTC or the non-BTC phase, and drive it across the BTC transition. The driving is done by an abrupt quench or by a finite-time linear ramp of a Hamiltonian control parameter under Markovian Lindblad dynamics. We diagnose DQPTs through zeros of the fidelity-based Loschmidt echo between the initial state and the evolving mixed state, which induce nonanalytic cusp-like features in the associated rate function. For quenches into the BTC phase, the Loschmidt echo exhibits repeated zeros due to the emergent time-periodic steady state, whereas for quenches into the non-BTC phase, the overlap vanishes and remains zero once the dynamics relaxes to a stationary state. We further show that the DQPT persists under the ramp protocol followed by unitary evolution with the final Hamiltonian. Finally, we analyze the finite-size scaling of the first critical time and find convergence to a constant in the thermodynamic limit, with distinct power-law approaches for the quench and the ramp protocols.

</details>


### [40] [Review of Superconducting Qubit Devices and Their Large-Scale Integration](https://arxiv.org/abs/2602.04831)
*Hiu Yung Wong*

Main category: quant-ph

TL;DR: 本文综述了超导量子比特量子计算机的基础知识、关键技术、DiVincenzo准则的实现、大规模集成方案以及电子设计自动化在超导量子计算机设计中的应用。


<details>
  <summary>Details</summary>
Motivation: 超导量子比特量子计算机因其成熟度和与半导体制造基础设施的接近性，被认为是最有前景的大规模集成量子计算架构之一。从教育角度看，它连接了经典微波电子学和量子电动力学，需要系统性地梳理其基础理论、技术挑战和发展路径。

Method: 采用综述研究方法，首先回顾量子计算机、超导性和约瑟夫森结的基础知识；然后基于DiVincenzo准则分析超导量子比特的关键技术，包括不同类型量子比特设计、纠缠门操作方案、读出工程、两能级系统缺陷研究；最后探讨大规模集成方案和电子设计自动化应用。

Result: 系统梳理了超导量子比特量子计算机的完整技术体系，包括：1) 各种约瑟夫森结量子比特的设计权衡和噪声免疫性；2) 不同纠缠门操作方案及其在容错量子计算中的瓶颈；3) Purcell滤波器和量子极限放大器的读出工程技术；4) 限制量子比特相干时间的两能级系统缺陷研究；5) 大规模集成方案和电子设计自动化应用前景。

Conclusion: 超导量子比特量子计算机在实现DiVincenzo准则方面取得了显著进展，但要构建实用的量子计算机仍需解决大规模集成问题。通过借鉴半导体行业的电子设计自动化工具和方法，有望推动超导量子计算机向大规模集成方向发展，最终实现实用化量子计算。

Abstract: The superconducting qubit quantum computer is one of the most promising quantum computing architectures for large-scale integration due to its maturity and close proximity to the well-established semiconductor manufacturing infrastructure. From an education perspective, it also bridges classical microwave electronics and quantum electrodynamics. In this paper, we will review the basics of quantum computers, superconductivity, and Josephson junctions. We then introduce important technologies and concepts related to DiVincenzo's criteria, which are the necessary conditions for the superconducting qubits to work as a useful quantum computer. Firstly, we will discuss various types of superconducting qubits formed with Josephson junctions, from which we will understand the trade-off across multiple design parameters, including their noise immunity. Secondly, we will discuss different schemes to achieve entanglement gate operations, which are a major bottleneck in achieving more efficient fault-tolerant quantum computing. Thirdly, we will review readout engineering, including the implementations of the Purcell filters and quantum-limited amplifiers. Finally, we will discuss the nature and review the studies of two-level system defects, which are currently the limiting factor of qubit coherence time. DiVincenzo's criteria are only the necessary conditions for a technology to be eligible for quantum computing. To have a useful quantum computer, large-scale integration is required. We will review proposals and developments for the large-scale integration of superconducting qubit devices. By comparing with the application of electronic design automation (EDA) in semiconductors, we will also review the use of EDA in superconducting qubit quantum computer design, which is necessary for its large-scale integration.

</details>


### [41] [Digital signatures with classical shadows on near-term quantum computers](https://arxiv.org/abs/2602.04859)
*Pradeep Niroula,Minzhao Liu,Sivaprasad Omanakuttan,David Amaro,Shouvanik Chakrabarti,Soumik Ghosh,Zichang He,Yuwei Jin,Fatih Kaleoglu,Steven Kordonowy,Rohan Kumar,Michael A. Perlin,Akshay Seshadri,Matthew Steinberg,Joseph Sullivan,Jacob Watkins,Henry Yuen,Ruslan Shaydulin*

Main category: quant-ph

TL;DR: 提出一种仅需经典通信的量子数字签名方案，利用随机电路产生的经典影子作为公钥，通过改进的状态认证原语实现高噪声容忍度和低样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有量子密码方案需要低噪声量子通信和长寿命量子存储器，这些在实际中难以实现。需要开发更实用的量子密码方案，减少对量子资源的依赖。

Method: 使用随机电路产生的经典影子作为公钥，设计改进的状态认证原语，采用高率错误检测码适应随机电路集合，通过实验生成32量子比特状态的影子。

Result: 实验实现了≥80个逻辑门（≥582个物理门）的电路，达到0.90±0.01保真度。通过增加测量样本，实现了原理验证的量子数字签名，展示了方案的近期可行性。

Conclusion: 该方案仅需经典通信，降低了量子资源需求，通过改进的认证原语和实验验证，展示了量子数字签名在近期量子设备上的可行性。

Abstract: Quantum mechanics provides cryptographic primitives whose security is grounded in hardness assumptions independent of those underlying classical cryptography. However, existing proposals require low-noise quantum communication and long-lived quantum memory, capabilities which remain challenging to realize in practice. In this work, we introduce a quantum digital signature scheme that operates with only classical communication, using the classical shadows of states produced by random circuits as public keys. We provide theoretical and numerical evidence supporting the conjectured hardness of learning the private key (the circuit) from the public key (the shadow). A key technical ingredient enabling our scheme is an improved state-certification primitive that achieves higher noise tolerance and lower sample complexity than prior methods. We realize this certification by designing a high-rate error-detecting code tailored to our random-circuit ensemble and experimentally generating shadows for 32-qubit states using circuits with $\geq 80$ logical ($\geq 582$ physical) two-qubit gates, attaining 0.90 $\pm$ 0.01 fidelity. With increased number of measurement samples, our hardware-demonstrated primitives realize a proof-of-principle quantum digital signature, demonstrating the near-term feasibility of our scheme.

</details>


### [42] [Requirements for Teleportation in an Intercity Quantum Network](https://arxiv.org/abs/2602.04869)
*Soubhadra Maiti,Guus Avis,Sounak Kar,Stephanie Wehner*

Main category: quant-ph

TL;DR: 该研究分析了城际量子隐形传态网络的硬件需求，推导了保真度与速率的解析表达式，发现城域尺度传态已可用现有硬件实现，但扩展到城际尺度需要硬件性能的进一步提升。


<details>
  <summary>Details</summary>
Motivation: 研究动机是确定实现城际尺度量子隐形传态网络所需的最低硬件改进要求，特别是要达到超越经典极限（保真度2/3）的性能指标，为实际量子网络建设提供指导。

Method: 将硬件需求计算建模为优化问题，基于简化噪声模型推导了异构量子硬件（包括带存储截止的量子中继链）下隐形传态保真度和速率的闭式解析表达式，并通过NetSquid平台仿真验证。

Result: 研究结果表明：当数据量子比特在端到端纠缠建立后制备时，城域距离的隐形传态已可用现有硬件实现；但扩展到城际尺度需要硬件性能的额外改进，这些改进虽然具有挑战性但理论上可实现。

Conclusion: 该研究为城际量子隐形传态网络提供了实用的硬件需求分析框架，推导的解析表达式能高效探索参数空间，为未来量子网络的实际部署提供了重要指导。

Abstract: We investigate the hardware requirements for quantum teleportation in an intercity-scale network topology consisting of two metropolitan-scale networks connected via a long-distance backbone link. Specifically, we identify the minimal improvements required beyond the state-of-the-art to achieve an end-to-end expected teleportation fidelity of $2/3$, which represents the classical limit. To this end, we formulate the hardware requirements computation as optimisation problems, where the hardware parameters representing the underlying device capabilities serve as decision variables. Assuming a simplified noise model, we derive closed-form analytical expressions for the teleportation fidelity and rate when the network is realised using heterogeneous quantum hardware, including a quantum repeater chain with a memory cut-off. Our derivations are based on events defined by the order statistics of link generation durations in both the metropolitan networks and the backbone, and the resulting expressions are validated through simulations on the NetSquid platform. The analytical expressions facilitate efficient exploration of the optimisation parameter space without resorting to computationally intensive simulations. We then apply this framework to a representative realisation in which the metropolitan nodes are based on trapped-ion processors and the backbone is composed of ensemble-based quantum memories. Our results suggest that teleportation across metropolitan distances is already achievable with state-of-the-art hardware when the data qubit is prepared after end-to-end entanglement has already been established, whereas extending teleportation to intercity scales requires additional, though plausibly achievable, improvements in hardware performance.

</details>


### [43] [Thermal State Simulation with Pauli and Majorana Propagation](https://arxiv.org/abs/2602.04878)
*Manuel S. Rudolph,Armando Angrisani,Andrew Wright,Iwo Sanderski,Ricard Puig,Zoë Holmes*

Main category: quant-ph

TL;DR: 提出基于传播的热态模拟方法，通过将泡利和马约拉纳传播适应于薛定谔绘景中的虚时演化，利用高温态在泡利或马约拉纳基中稀疏的特性实现高效热态模拟。


<details>
  <summary>Details</summary>
Motivation: 传统热态模拟方法计算成本高，本文观察到高温态在泡利或马约拉纳基中具有稀疏性，无限高温时趋近于单位矩阵，这为高效热态模拟提供了新思路。

Method: 将虚时演化直接表述在泡利或马约拉纳算子基中，从最大混合态开始演化，通过小系数截断和泡利权重（马约拉纳长度）截断策略，量化误差增长和回流的影晌。

Result: 在一维J1-J2模型（能量）和三角晶格哈伯德模型（静态关联）上的大规模数值模拟验证了该方法在高温下的效率。

Conclusion: 该方法为热态模拟提供了高效的新途径，特别适用于高温区域，通过算子基中的稀疏表示和截断策略实现了计算效率的提升。

Abstract: We introduce a propagation-based approach to thermal state simulation by adapting Pauli and Majorana propagation to imaginary-time evolution in the Schrödinger picture. Our key observation is that high-temperature states can be sparse in the Pauli or Majorana bases, approaching the identity at infinite temperature. By formulating imaginary-time evolution directly in these operator bases and evolving from the maximally mixed state, we access a continuum of temperatures where the state remains efficiently representable. We provide analytic guarantees for small-coefficient truncation and Pauli-weight (Majorana-length) truncation strategies by quantifying the error growth and the impact of backflow. Large-scale numerics on the 1D J1-J2 model (energies) and the triangular-lattice Hubbard model (static correlations) validate efficiency at high temperatures.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [44] [Understanding the Impact of Differentially Private Training on Memorization of Long-Tailed Data](https://arxiv.org/abs/2602.03872)
*Jiaming Zhang,Huanyi Xie,Meng Ding,Shaopeng Fu,Jinyan Liu,Di Wang*

Main category: cs.LG

TL;DR: 论文分析了DP-SGD在长尾数据上的性能问题，发现隐私保护机制会显著降低模型对稀有样本的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习模型通过记忆训练样本来获得高预测精度，这引发了隐私担忧，促使广泛采用DP-SGD等差分隐私训练算法。然而，DP-SGD在长尾数据上往往表现不佳，特别是对稀有或非典型样本，但这种现象的理论理解仍然缺乏。

Method: 开发了首个从特征学习角度分析DP-SGD在长尾数据上的理论框架，分析了梯度裁剪和噪声注入如何联合影响模型对代表性不足样本的记忆能力，并通过合成和真实数据集进行了实验验证。

Result: DP-SGD训练模型在长尾子群体上的测试误差显著大于在整个数据集上的总体测试误差，梯度裁剪和噪声注入共同损害了模型记忆信息丰富但代表性不足样本的能力。

Conclusion: 差分隐私训练算法在保护隐私的同时，会严重损害模型对长尾数据中稀有样本的泛化性能，这为理解隐私保护与模型性能之间的权衡提供了理论框架。

Abstract: Recent research shows that modern deep learning models achieve high predictive accuracy partly by memorizing individual training samples. Such memorization raises serious privacy concerns, motivating the widespread adoption of differentially private training algorithms such as DP-SGD. However, a growing body of empirical work shows that DP-SGD often leads to suboptimal generalization performance, particularly on long-tailed data that contain a large number of rare or atypical samples. Despite these observations, a theoretical understanding of this phenomenon remains largely unexplored, and existing differential privacy analysis are difficult to extend to the nonconvex and nonsmooth neural networks commonly used in practice. In this work, we develop the first theoretical framework for analyzing DP-SGD on long-tailed data from a feature learning perspective. We show that the test error of DP-SGD-trained models on the long-tailed subpopulation is significantly larger than the overall test error over the entire dataset. Our analysis further characterizes the training dynamics of DP-SGD, demonstrating how gradient clipping and noise injection jointly adversely affect the model's ability to memorize informative but underrepresented samples. Finally, we validate our theoretical findings through extensive experiments on both synthetic and real-world datasets.

</details>


### [45] [Reversible Deep Learning for 13C NMR in Chemoinformatics: On Structures and Spectra](https://arxiv.org/abs/2602.03875)
*Stefan Kuhn,Vandana Dwarka,Przemyslaw Karol Grenda,Eero Vainikko*

Main category: cs.LG

TL;DR: 提出可逆深度學習模型，使用單一條件可逆神經網路實現分子結構與13C NMR譜圖的雙向轉換


<details>
  <summary>Details</summary>
Motivation: 傳統方法需要分別訓練結構預測譜圖和譜圖預測結構的模型，無法統一處理譜圖到結構的一對多映射關係

Method: 基於i-RevNet風格雙射塊構建條件可逆神經網路，訓練時從圖結構編碼預測128位元譜圖碼，其餘潛在維度捕捉殘差變異性

Result: 在過濾子集上模型數值可逆，譜圖碼預測優於隨機，驗證譜圖反轉時產生有意義的結構信號

Conclusion: 可逆架構能在單一端到端模型中統一譜圖預測和不確定性感知的候選結構生成

Abstract: We introduce a reversible deep learning model for 13C NMR that uses a single conditional invertible neural network for both directions between molecular structures and spectra. The network is built from i-RevNet style bijective blocks, so the forward map and its inverse are available by construction. We train the model to predict a 128-bit binned spectrum code from a graph-based structure encoding, while the remaining latent dimensions capture residual variability. At inference time, we invert the same trained network to generate structure candidates from a spectrum code, which explicitly represents the one-to-many nature of spectrum-to-structure inference. On a filtered subset, the model is numerically invertible on trained examples, achieves spectrum-code prediction above chance, and produces coarse but meaningful structural signals when inverted on validation spectra. These results demonstrate that invertible architectures can unify spectrum prediction and uncertainty-aware candidate generation within one end-to-end model.

</details>


### [46] [GOPO: Policy Optimization using Ranked Rewards](https://arxiv.org/abs/2602.03876)
*Kyuseong Choi,Dwaipayan Saha,Woojeong Kim,Anish Agarwal,Raaz Dwivedi*

Main category: cs.LG

TL;DR: GOPO是一种新的策略优化方法，仅使用奖励排名而忽略其幅度，在非可验证奖励场景中比GRPO表现更好。


<details>
  <summary>Details</summary>
Motivation: 传统RLHF方法存在奖励模型与策略优化之间的不匹配问题：奖励模型捕捉相对偏好，但策略优化依赖绝对奖励幅度。在摘要、指令遵循、聊天完成等非可验证奖励场景中，这种不匹配导致次优性能。

Method: 提出Group Ordinal Policy Optimization (GOPO)，一种仅使用奖励排名而忽略其幅度的策略优化方法。通过基于排名的奖励转换，在非可验证奖励设置中实现更优性能。

Result: 相比GRPO，GOPO在非可验证奖励设置中：(1) 训练/验证奖励轨迹持续更高；(2) 大多数中间训练步骤的LLM-as-judge评估更优；(3) 以更少训练步骤达到相当质量的政策。在不同任务和模型规模上均显示一致改进。

Conclusion: GOPO通过仅使用奖励排名而非幅度，解决了RLHF中奖励模型与策略优化的不匹配问题，在非可验证奖励场景中实现了更高效和更优的性能。

Abstract: Standard reinforcement learning from human feedback (RLHF) trains a reward model on pairwise preference data and then uses it for policy optimization. However, while reward models are optimized to capture relative preferences, existing policy optimization techniques rely on absolute reward magnitudes during training. In settings where the rewards are non-verifiable such as summarization, instruction following, and chat completion, this misalignment often leads to suboptimal performance. We introduce Group Ordinal Policy Optimization (GOPO), a policy optimization method that uses only the ranking of the rewards and discards their magnitudes. Our rank-based transformation of rewards provides several gains, compared to Group Relative Policy Optimization (GRPO), in settings with non-verifiable rewards: (1) consistently higher training/validation reward trajectories, (2) improved LLM-as-judge evaluations across most intermediate training steps, and (3) reaching a policy of comparable quality in substantially less training steps than GRPO. We demonstrate consistent improvements across a range of tasks and model sizes.

</details>


### [47] [Generative Neural Operators through Diffusion Last Layer](https://arxiv.org/abs/2602.04139)
*Sungwon Park,Anthony Zhou,Hongjoong Kim,Amir Barati Farimani*

Main category: cs.LG

TL;DR: 提出扩散最后一层(DLL)作为神经算子的轻量级概率头部，用于函数空间中的不确定性量化


<details>
  <summary>Details</summary>
Motivation: 许多实际系统本质上是随机的，需要原则性的不确定性量化来确保可靠部署。现有神经算子缺乏对预测不确定性的建模能力。

Method: DLL是一个轻量级概率头部，可附加到任意神经算子骨干上。它通过低秩Karhunen-Loève展开在函数空间中直接参数化条件输出分布，利用PDE解分布通常具有的相对平滑性和低维结构特性。

Result: 在随机PDE算子学习基准测试中，DLL改善了泛化能力和不确定性感知预测。即使在确定性长时程滚动设置中，DLL也增强了滚动稳定性，并为骨干神经算子提供了有意义的认知不确定性估计。

Conclusion: DLL是一种简单有效的附加组件，能够为神经算子提供函数空间中的不确定性量化能力，在随机和确定性场景中都能提升性能。

Abstract: Neural operators have emerged as a powerful paradigm for learning discretization-invariant function-to-function mappings in scientific computing. However, many practical systems are inherently stochastic, making principled uncertainty quantification essential for reliable deployment. To address this, we introduce a simple add-on, the diffusion last layer (DLL), a lightweight probabilistic head that can be attached to arbitrary neural operator backbones to model predictive uncertainty. Motivated by the relative smoothness and low-dimensional structure often exhibited by PDE solution distributions, DLL parameterizes the conditional output distribution directly in function space through a low-rank Karhunen-Loève expansion, enabling efficient and expressive uncertainty modeling. Across stochastic PDE operator learning benchmarks, DLL improves generalization and uncertainty-aware prediction. Moreover, even in deterministic long-horizon rollout settings, DLL enhances rollout stability and provides meaningful estimates of epistemic uncertainty for backbone neural operators.

</details>


### [48] [NeuroPareto: Calibrated Acquisition for Costly Many-Goal Search in Vast Parameter Spaces](https://arxiv.org/abs/2602.03901)
*Rong Fu,Wenxin Zhang,Chunlei Meng,Youjin Wang,Haoyu Zhao,Jiaxuan Lu,Kun Liu,JiaBao Dou,Simon James Fong*

Main category: cs.LG

TL;DR: NeuroPareto：一种集成排序过滤、不确定性解耦和历史条件采集策略的神经架构，用于高效多目标优化，在计算约束下生成高质量帕累托解。


<details>
  <summary>Details</summary>
Motivation: 在高维搜索空间中，在严格计算约束下寻找最优权衡解是多目标优化的核心挑战。传统方法在复杂目标景观中导航效率有限，需要更智能的方法来平衡收敛性和多样性。

Method: 1. 排序中心过滤：使用校准贝叶斯分类器估计非支配层级的认知不确定性，快速生成高质量候选解；2. 不确定性解耦：深度高斯过程代理模型将预测不确定性分解为可减少和不可减少部分，提供精确预测和风险感知信号；3. 历史条件采集：轻量级采集网络从历史超体积改进中在线训练，指导昂贵评估到平衡收敛与多样性的区域；4. 分层筛选和摊销代理更新，保持精度同时降低计算开销。

Result: 在DTLZ和ZDT测试套件以及地下能源提取任务上的实验表明，NeuroPareto在帕累托接近度和超体积指标上持续优于分类器增强和代理辅助的基线方法。

Conclusion: NeuroPareto通过集成排序过滤、不确定性解耦和历史条件采集策略，提供了一种计算高效的多目标优化框架，能够在严格计算约束下导航复杂目标景观，生成高质量的帕累托解。

Abstract: The pursuit of optimal trade-offs in high-dimensional search spaces under stringent computational constraints poses a fundamental challenge for contemporary multi-objective optimization. We develop NeuroPareto, a cohesive architecture that integrates rank-centric filtering, uncertainty disentanglement, and history-conditioned acquisition strategies to navigate complex objective landscapes. A calibrated Bayesian classifier estimates epistemic uncertainty across non-domination tiers, enabling rapid generation of high-quality candidates with minimal evaluation cost. Deep Gaussian Process surrogates further separate predictive uncertainty into reducible and irreducible components, providing refined predictive means and risk-aware signals for downstream selection. A lightweight acquisition network, trained online from historical hypervolume improvements, guides expensive evaluations toward regions balancing convergence and diversity. With hierarchical screening and amortized surrogate updates, the method maintains accuracy while keeping computational overhead low. Experiments on DTLZ and ZDT suites and a subsurface energy extraction task show that NeuroPareto consistently outperforms classifier-enhanced and surrogate-assisted baselines in Pareto proximity and hypervolume.

</details>


### [49] [GeoIB: Geometry-Aware Information Bottleneck via Statistical-Manifold Compression](https://arxiv.org/abs/2602.03906)
*Weiqi Wang,Zhiyi Tian,Chenhan Zhang,Shui Yu*

Main category: cs.LG

TL;DR: 提出GeoIB方法，通过信息几何视角直接控制信息瓶颈，无需互信息估计，使用Fisher-Rao距离和Jacobian-Frobenius项实现更好的压缩-预测权衡。


<details>
  <summary>Details</summary>
Motivation: 传统信息瓶颈方法在深度学习中通常使用变分界或互信息估计器作为代理，但这些方法的松弛性和估计器依赖偏差使得信息压缩只能间接控制，优化过程脆弱。

Method: 从信息几何角度重新审视IB问题，提出GeoIB方法：1) 将I(X;Z)和I(Z;Y)表示为最小KL距离到独立流形的投影形式；2) 使用Fisher-Rao距离作为分布级正则化；3) 使用Jacobian-Frobenius项作为几何级容量型上界；4) 推导与FR度量一致的自然梯度优化器。

Result: 在多个流行数据集上的实验表明，GeoIB在信息平面上实现了比主流IB基线更好的预测精度和压缩率权衡，提高了不变性和优化稳定性。

Conclusion: GeoIB通过将分布级和几何级正则化统一在单个瓶颈乘子下，避免了互信息估计，实现了更直接和稳定的信息瓶颈控制。

Abstract: Information Bottleneck (IB) is widely used, but in deep learning, it is usually implemented through tractable surrogates, such as variational bounds or neural mutual information (MI) estimators, rather than directly controlling the MI I(X;Z) itself. The looseness and estimator-dependent bias can make IB "compression" only indirectly controlled and optimization fragile.
  We revisit the IB problem through the lens of information geometry and propose a \textbf{Geo}metric \textbf{I}nformation \textbf{B}ottleneck (\textbf{GeoIB}) that dispenses with mutual information (MI) estimation. We show that I(X;Z) and I(Z;Y) admit exact projection forms as minimal Kullback-Leibler (KL) distances from the joint distributions to their respective independence manifolds. Guided by this view, GeoIB controls information compression with two complementary terms: (i) a distribution-level Fisher-Rao (FR) discrepancy, which matches KL to second order and is reparameterization-invariant; and (ii) a geometry-level Jacobian-Frobenius (JF) term that provides a local capacity-type upper bound on I(Z;X) by penalizing pullback volume expansion of the encoder. We further derive a natural-gradient optimizer consistent with the FR metric and prove that the standard additive natural-gradient step is first-order equivalent to the geodesic update. We conducted extensive experiments and observed that the GeoIB achieves a better trade-off between prediction accuracy and compression ratio in the information plane than the mainstream IB baselines on popular datasets. GeoIB improves invariance and optimization stability by unifying distributional and geometric regularization under a single bottleneck multiplier. The source code of GeoIB is released at "https://anonymous.4open.science/r/G-IB-0569".

</details>


### [50] [The Role of Target Update Frequencies in Q-Learning](https://arxiv.org/abs/2602.03911)
*Simon Weissmann,Tilman Aach,Benedikt Wille,Sebastian Kassing,Leif Döring*

Main category: cs.LG

TL;DR: 本文通过近似动态规划理论分析Q学习中的目标网络更新频率，证明恒定更新计划次优，提出几何增长的自适应更新计划可避免对数级样本复杂度开销。


<details>
  <summary>Details</summary>
Motivation: 目标网络更新频率是深度Q学习中的核心稳定机制，但当前对其选择缺乏理论理解，通常仅作为可调超参数而非原则性设计决策。需要从理论层面分析目标固定机制，为这一关键超参数提供原则性设置指导。

Method: 将周期性目标更新建模为嵌套优化方案：外层迭代应用不精确的Bellman最优算子，内层循环用通用优化器近似。在异步采样设置下进行有限时间收敛分析，特别针对内层循环使用随机梯度下降的情况。

Result: 理论分析明确刻画了目标更新周期引起的偏差-方差权衡，展示了如何最优设置这一关键超参数。证明恒定目标更新计划次优，会产生可避免的对数级样本复杂度开销。分析表明最优目标更新频率应在学习过程中几何增长。

Conclusion: 目标网络更新频率应作为原则性设计决策而非简单超参数。自适应几何增长更新计划优于恒定计划，能显著提升样本效率。理论框架为深度Q学习中的目标网络设计提供了严格指导。

Abstract: The target network update frequency (TUF) is a central stabilization mechanism in (deep) Q-learning. However, their selection remains poorly understood and is often treated merely as another tunable hyperparameter rather than as a principled design decision. This work provides a theoretical analysis of target fixing in tabular Q-learning through the lens of approximate dynamic programming. We formulate periodic target updates as a nested optimization scheme in which each outer iteration applies an inexact Bellman optimality operator, approximated by a generic inner loop optimizer. Rigorous theory yields a finite-time convergence analysis for the asynchronous sampling setting, specializing to stochastic gradient descent in the inner loop. Our results deliver an explicit characterization of the bias-variance trade-off induced by the target update period, showing how to optimally set this critical hyperparameter. We prove that constant target update schedules are suboptimal, incurring a logarithmic overhead in sample complexity that is entirely avoidable with adaptive schedules. Our analysis shows that the optimal target update frequency increases geometrically over the course of the learning process.

</details>


### [51] [Echo State Networks for Time Series Forecasting: Hyperparameter Sweep and Benchmarking](https://arxiv.org/abs/2602.03912)
*Alexander Häußer*

Main category: cs.LG

TL;DR: ESN在M4数据集上的预测性能评估显示，对于月度数据与ARIMA/TBATS相当，对于季度数据表现最佳，在预测精度和计算效率之间取得了良好平衡。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估完全自动化的纯反馈驱动的回声状态网络(ESN)是否能作为传统统计预测方法(如ARIMA、ETS、TBATS)的竞争性替代方案，特别是在自动化时间序列预测场景中。

Method: 采用严格的两阶段评估方法：1) 使用参数数据集进行超参数扫描(泄漏率、谱半径、储备池大小、正则化信息准则)，超过400万个ESN模型拟合；2) 使用不相交的预测数据集进行样本外精度评估，使用MASE和sMAPE指标，并与漂移、季节性朴素法以及ARIMA、ETS、TBATS等统计模型进行基准比较。

Result: 超参数分析显示一致且可解释的模式：月度序列偏好中等持续性储备池，季度序列偏好更收缩的动态特性；高泄漏率在两个频率上都受偏好。样本外评估中，ESN在月度数据上与ARIMA和TBATS表现相当，在季度数据上获得最低平均MASE，且计算成本低于更复杂的统计模型。

Conclusion: ESN在预测精度、鲁棒性和计算效率之间提供了有吸引力的平衡，使其成为自动化时间序列预测的实用选择，特别是对于季度数据表现优异。

Abstract: This paper investigates the forecasting performance of Echo State Networks (ESNs) for univariate time series forecasting using a subset of the M4 Forecasting Competition dataset. Focusing on monthly and quarterly time series with at most 20 years of historical data, we evaluate whether a fully automatic, purely feedback-driven ESN can serve as a competitive alternative to widely used statistical forecasting methods. The study adopts a rigorous two-stage evaluation approach: a Parameter dataset is used to conduct an extensive hyperparameter sweep covering leakage rate, spectral radius, reservoir size, and information criteria for regularization, resulting in over four million ESN model fits; a disjoint Forecast dataset is then used for out-of-sample accuracy assessment. Forecast accuracy is measured using MASE and sMAPE and benchmarked against simple benchmarks like drift and seasonal naive and statistical models like ARIMA, ETS, and TBATS. The hyperparameter analysis reveals consistent and interpretable patterns, with monthly series favoring moderately persistent reservoirs and quarterly series favoring more contractive dynamics. Across both frequencies, high leakage rates are preferred, while optimal spectral radii and reservoir sizes vary with temporal resolution. In the out-of-sample evaluation, the ESN performs on par with ARIMA and TBATS for monthly data and achieves the lowest mean MASE for quarterly data, while requiring lower computational cost than the more complex statistical models. Overall, the results demonstrate that ESNs offer a compelling balance between predictive accuracy, robustness, and computational efficiency, positioning them as a practical option for automated time series forecasting.

</details>


### [52] [Causal Discovery for Cross-Sectional Data Based on Super-Structure and Divide-and-Conquer](https://arxiv.org/abs/2602.03914)
*Wenyu Wang,Yaping Wan*

Main category: cs.LG

TL;DR: 提出轻量级框架，通过放宽超结构构建要求，结合高效图分割与合并策略，大幅降低条件独立性测试开销，同时保持因果发现准确性。


<details>
  <summary>Details</summary>
Motivation: 解决基于超结构的因果发现方法中，构建准确超结构计算成本高的问题，特别是在条件独立性测试昂贵且缺乏领域知识的情况下。

Method: 提出新颖的轻量级框架，整合弱约束超结构与高效图分割和合并策略，实例化为具体因果发现算法。

Result: 在合成数据（Gaussian Bayesian networks）上匹配或接近PC和FCI的结构准确性，同时大幅减少CI测试数量；在真实世界CHARLS数据集上验证了实用性。

Conclusion: 即使在超结构假设最小的情况下，也能实现准确、可扩展的因果发现，为生物医学和社会科学等大规模、知识稀缺领域的应用开辟了新途径。

Abstract: This paper tackles a critical bottleneck in Super-Structure-based divide-and-conquer causal discovery: the high computational cost of constructing accurate Super-Structures--particularly when conditional independence (CI) tests are expensive and domain knowledge is unavailable. We propose a novel, lightweight framework that relaxes the strict requirements on Super-Structure construction while preserving the algorithmic benefits of divide-and-conquer. By integrating weakly constrained Super-Structures with efficient graph partitioning and merging strategies, our approach substantially lowers CI test overhead without sacrificing accuracy. We instantiate the framework in a concrete causal discovery algorithm and rigorously evaluate its components on synthetic data. Comprehensive experiments on Gaussian Bayesian networks, including magic-NIAB, ECOLI70, and magic-IRRI, demonstrate that our method matches or closely approximates the structural accuracy of PC and FCI while drastically reducing the number of CI tests. Further validation on the real-world China Health and Retirement Longitudinal Study (CHARLS) dataset confirms its practical applicability. Our results establish that accurate, scalable causal discovery is achievable even under minimal assumptions about the initial Super-Structure, opening new avenues for applying divide-and-conquer methods to large-scale, knowledge-scarce domains such as biomedical and social science research.

</details>


### [53] [SpecMD: A Comprehensive Study On Speculative Expert Prefetching](https://arxiv.org/abs/2602.03921)
*Duc Hoang,Ajay Jaiswal,Mohammad Samragh,Minsik Cho*

Main category: cs.LG

TL;DR: SpecMD框架用于标准化评估MoE模型专家缓存策略，研究发现传统缓存假设不适用于MoE，提出Least-Stale策略显著提升缓存命中率和推理性能。


<details>
  <summary>Details</summary>
Motivation: MoE模型通过稀疏专家激活实现高效推理，但需要缓存机制来发挥其优势。现有硬件中心化缓存策略在不同硬件配置下的交互效果不明确，缺乏标准化评估框架。

Method: 开发SpecMD标准化框架，在多种硬件配置下对MoE缓存策略进行系统化基准测试。基于发现MoE专家访问不符合时间局部性假设，提出Least-Stale驱逐策略，利用MoE可预测的专家访问模式减少冲突缺失。

Result: 实验表明Least-Stale策略相比LRU减少85倍冲突缺失，在仅5%或0.6GB VRAM缓存容量下实现超过88%命中率，TTFT减少高达34.7%。

Conclusion: MoE专家访问模式具有特殊性，传统缓存假设不适用。Least-Stale策略能有效利用MoE的可预测访问模式，显著提升缓存效率和推理性能，为MoE模型部署提供优化方案。

Abstract: Mixture-of-Experts (MoE) models enable sparse expert activation, meaning that only a subset of the model's parameters is used during each inference. However, to translate this sparsity into practical performance, an expert caching mechanism is required. Previous works have proposed hardware-centric caching policies, but how these various caching policies interact with each other and different hardware specification remains poorly understood. To address this gap, we develop \textbf{SpecMD}, a standardized framework for benchmarking ad-hoc cache policies on various hardware configurations. Using SpecMD, we perform an exhaustive benchmarking of several MoE caching strategies, reproducing and extending prior approaches in controlled settings with realistic constraints. Our experiments reveal that MoE expert access is not consistent with temporal locality assumptions (e.g LRU, LFU). Motivated by this observation, we propose \textbf{Least-Stale}, a novel eviction policy that exploits MoE's predictable expert access patterns to reduce collision misses by up to $85\times$ over LRU. With such gains, we achieve over $88\%$ hit rates with up to $34.7\%$ Time-to-first-token (TTFT) reduction on OLMoE at only $5\%$ or $0.6GB$ of VRAM cache capacity.

</details>


### [54] [Online Vector Quantized Attention](https://arxiv.org/abs/2602.03922)
*Nick Alonso,Tomas Figliolia,Beren Millidge*

Main category: cs.LG

TL;DR: OVQ注意力机制：一种在线向量量化注意力层，在保持线性计算和恒定内存的同时，通过稀疏内存更新提高长上下文处理能力，在64k序列长度内达到与自注意力相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有序列混合层在效率与性能之间存在权衡：自注意力在长上下文任务中表现良好但计算和内存成本高（二次计算和线性内存），而线性注意力和SSM虽然计算和内存成本低（线性计算和恒定内存），但在长上下文处理上表现不佳。

Method: 提出在线向量量化（OVQ）注意力机制，基于高斯混合回归理论，使用稀疏内存更新策略，在保持线性计算成本和恒定内存的同时，显著增加内存状态大小和容量。

Result: 在多种合成长上下文任务和长上下文语言建模中，OVQ注意力相比线性注意力基线和原始VQ注意力有显著改进，在64k序列长度内与强自注意力基线表现相当甚至相同，同时仅使用自注意力的一小部分内存。

Conclusion: OVQ注意力在计算效率与长上下文处理能力之间找到了更好的平衡点，为语言模型提供了一种既高效又能处理长序列的序列混合层方案。

Abstract: Standard sequence mixing layers used in language models struggle to balance efficiency and performance. Self-attention performs well on long context tasks but has expensive quadratic compute and linear memory costs, while linear attention and SSMs use only linear compute and constant memory but struggle with long context processing. In this paper, we develop a sequence mixing layer that aims to find a better compromise between memory-compute costs and long-context processing, which we call online vector-quantized (OVQ) attention. OVQ-attention requires linear compute costs and constant memory, but, unlike linear attention and SSMs, it uses a sparse memory update that allows it to greatly increase the size of its memory state and, consequently, memory capacity. We develop a theoretical basis for OVQ-attention based on Gaussian mixture regression, and we test it on a variety of synthetic long context tasks and on long context language modeling. OVQ-attention shows significant improvements over linear attention baselines and the original VQ-attention, on which OVQ-attention was inspired. It demonstrates competitive, and sometimes identical, performance to strong self-attention baselines up 64k sequence length, despite using a small fraction of the memory of full self-attention.

</details>


### [55] [WIND: Weather Inverse Diffusion for Zero-Shot Atmospheric Modeling](https://arxiv.org/abs/2602.03924)
*Michael Aich,Andreas Fürst,Florian Sestak,Carlos Ruiz-Gonzalez,Niklas Boers,Johannes Brandstetter*

Main category: cs.LG

TL;DR: WIND是一个统一的气象气候基础模型，无需任务特定微调即可处理多种任务，通过视频扩散模型和逆问题求解实现。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习气象模型高度专业化且分散，需要为不同任务单独训练模型。作者希望建立一个统一的基础模型来替代这些专门化基线。

Method: 使用无条件视频扩散模型进行自监督视频重建预训练，学习大气稳健先验。推理时将各种任务统一为逆问题，通过后验采样求解。

Result: WIND能够处理概率预报、时空降尺度、稀疏重建、守恒定律执行等多种任务，还能生成全球变暖情景下的极端天气反事实情景。

Conclusion: 结合生成视频建模和逆问题求解，WIND为AI大气建模提供了计算高效的新范式，实现了统一的基础模型方法。

Abstract: Deep learning has revolutionized weather and climate modeling, yet the current landscape remains fragmented: highly specialized models are typically trained individually for distinct tasks. To unify this landscape, we introduce WIND, a single pre-trained foundation model capable of replacing specialized baselines across a vast array of tasks. Crucially, in contrast to previous atmospheric foundation models, we achieve this without any task-specific fine-tuning. To learn a robust, task-agnostic prior of the atmosphere, we pre-train WIND with a self-supervised video reconstruction objective, utilizing an unconditional video diffusion model to iteratively reconstruct atmospheric dynamics from a noisy state. At inference, we frame diverse domain-specific problems strictly as inverse problems and solve them via posterior sampling. This unified approach allows us to tackle highly relevant weather and climate problems, including probabilistic forecasting, spatial and temporal downscaling, sparse reconstruction and enforcing conservation laws purely with our pre-trained model. We further demonstrate the model's capacity to generate physically consistent counterfactual storylines of extreme weather events under global warming scenarios. By combining generative video modeling with inverse problem solving, WIND offers a computationally efficient paradigm shift in AI-based atmospheric modeling.

</details>


### [56] [Autonomous AI Agents for Real-Time Affordable Housing Site Selection: Multi-Objective Reinforcement Learning Under Regulatory Constraints](https://arxiv.org/abs/2602.03940)
*Olaf Yunus Laitinen Imanov,Duygu Erisken,Derya Umut Kulali,Taner Yilmaz,Rana Irem Turhan*

Main category: cs.LG

TL;DR: AURA是一个用于经济适用房选址的层次化多智能体强化学习系统，能够在硬性法规约束下实时优化选址，显著提高效率和质量。


<details>
  <summary>Details</summary>
Motivation: 全球面临经济适用房短缺问题，而传统选址过程受限于土地稀缺和复杂法规，导致选址过程缓慢（通常需要18个月）。需要一种能够实时处理大量法规约束并优化多个社会目标的自动化系统。

Method: 将任务建模为约束多目标马尔可夫决策过程，使用层次化多智能体强化学习系统。关键技术包括：法规感知状态编码（127个联邦和地方约束）、帕累托约束策略梯度（保证可行性）、奖励分解（分离即时成本和长期社会结果）。

Result: 在8个美国大都市区（47,392个候选地块）的数据集上，AURA达到94.3%的法规合规率，帕累托超体积比基线提高37.2%。在纽约市2026年案例研究中，将选址时间从18个月缩短到72小时，识别出比专家多23%的可行地块，所选地块的交通便利性提高31%，环境影响降低19%。

Conclusion: AURA系统能够显著加速经济适用房选址过程，在严格遵守法规约束的同时，优化多个社会目标，为城市规划和住房政策提供有效的决策支持工具。

Abstract: Affordable housing shortages affect billions, while land scarcity and regulations make site selection slow. We present AURA (Autonomous Urban Resource Allocator), a hierarchical multi-agent reinforcement learning system for real-time affordable housing site selection under hard regulatory constraints (QCT, DDA, LIHTC). We model the task as a constrained multi-objective Markov decision process optimizing accessibility, environmental impact, construction cost, and social equity while enforcing feasibility. AURA uses a regulatory-aware state encoding 127 federal and local constraints, Pareto-constrained policy gradients with feasibility guarantees, and reward decomposition separating immediate costs from long-term social outcomes. On datasets from 8 U.S. metros (47,392 candidate parcels), AURA attains 94.3% regulatory compliance and improves Pareto hypervolume by 37.2% over strong baselines. In a New York City 2026 case study, it reduces selection time from 18 months to 72 hours and identifies 23% more viable sites; chosen sites have 31% better transit access and 19% lower environmental impact than expert picks.

</details>


### [57] [Grables: Tabular Learning Beyond Independent Rows](https://arxiv.org/abs/2602.03945)
*Tamara Cucumides,Floris Geerts*

Main category: cs.LG

TL;DR: 论文提出grables框架，将表格学习从独立行预测扩展到图结构预测，以捕捉行间依赖关系，在交易数据和临床试验数据上验证了其优势。


<details>
  <summary>Details</summary>
Motivation: 当前表格学习主要采用独立行预测器，这在i.i.d.基准测试中有效，但在事务性、时间性和关系性表格中失败，因为标签往往依赖于其他行。行预测排除了基于全局计数、重叠和关系模式的自然目标。

Method: 提出grables框架：模块化接口，将表格转换为图（构造器）与在图节点上计算预测（节点预测器）分离，明确表达能力的来源。通过消息传递捕捉行间依赖，并采用混合方法显式提取行间结构并输入给强大的表格学习器。

Result: 在合成任务、交易数据和RelBench临床试验数据集上的实验证实了预测的分离：消息传递捕捉了行局部模型忽略的行间依赖，显式提取行间结构并输入给表格学习器的混合方法带来了持续增益。

Conclusion: 表格学习需要超越独立行预测，考虑行间依赖关系。grables框架为理解和设计捕捉表格结构的方法提供了系统方法，在关系性表格任务中表现出优越性能。

Abstract: Tabular learning is still dominated by row-wise predictors that score each row independently, which fits i.i.d. benchmarks but fails on transactional, temporal, and relational tables where labels depend on other rows. We show that row-wise prediction rules out natural targets driven by global counts, overlaps, and relational patterns. To make "using structure" precise across architectures, we introduce grables: a modular interface that separates how a table is lifted to a graph (constructor) from how predictions are computed on that graph (node predictor), pinpointing where expressive power comes from. Experiments on synthetic tasks, transaction data, and a RelBench clinical-trials dataset confirm the predicted separations: message passing captures inter-row dependencies that row-local models miss, and hybrid approaches that explicitly extract inter-row structure and feed it to strong tabular learners yield consistent gains.

</details>


### [58] [Representation Geometry as a Diagnostic for Out-of-Distribution Robustness](https://arxiv.org/abs/2602.03951)
*Ali Zia,Farid Hazratian*

Main category: cs.LG

TL;DR: 提出基于几何的诊断框架，通过嵌入的类条件互k近邻图提取全局谱复杂度和局部平滑度指标，用于无标签预测模型在分布偏移下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在缺乏目标域标签的情况下，监控和优化分布偏移下的鲁棒泛化很困难，因为具有相似分布内精度的模型可能表现出显著不同的分布外性能。现有工作主要关注训练时正则化和低阶表示统计，而嵌入几何结构是否能提供可靠的鲁棒性后验信号尚不清楚。

Method: 提出几何诊断框架：从分布内嵌入构建类条件互k近邻图，提取两个互补不变量：1) 基于归一化拉普拉斯矩阵缩减对数行列式的全局谱复杂度代理；2) 基于Ollivier-Ricci曲率的局部平滑度度量。

Result: 在多种架构、训练机制和损坏基准测试中，发现较低的谱复杂度和较高的平均曲率一致地预测更强的分布外准确性。受控扰动和拓扑分析进一步表明这些信号反映了有意义的表示结构而非表面嵌入统计。

Conclusion: 表示几何能够实现可解释、无标签的鲁棒性诊断，并支持在分布偏移下进行可靠的无人监督检查点选择。

Abstract: Robust generalization under distribution shift remains difficult to monitor and optimize in the absence of target-domain labels, as models with similar in-distribution accuracy can exhibit markedly different out-of-distribution (OOD) performance. While prior work has focused on training-time regularization and low-order representation statistics, little is known about whether the geometric structure of learned embeddings provides reliable post-hoc signals of robustness. We propose a geometry-based diagnostic framework that constructs class-conditional mutual k-nearest-neighbor graphs from in-distribution embeddings and extracts two complementary invariants: a global spectral complexity proxy based on the reduced log-determinant of the normalized Laplacian, and a local smoothness measure based on Ollivier--Ricci curvature. Across multiple architectures, training regimes, and corruption benchmarks, we find that lower spectral complexity and higher mean curvature consistently predict stronger OOD accuracy across checkpoints. Controlled perturbations and topological analyses further show that these signals reflect meaningful representation structure rather than superficial embedding statistics. Our results demonstrate that representation geometry enables interpretable, label-free robustness diagnosis and supports reliable unsupervised checkpoint selection under distribution shift.

</details>


### [59] [Child Mortality Prediction in Bangladesh: A Decade-Long Validation Study](https://arxiv.org/abs/2602.03957)
*Md Muhtasim Munif Fahim,Md Rezaul Karim*

Main category: cs.LG

TL;DR: 使用遗传算法神经网络搜索开发儿童死亡率预测模型，在时间分割验证中表现优于XGBoost，并发现模型在贫困地区预测效果更好，可用于精准公共卫生干预。


<details>
  <summary>Details</summary>
Motivation: 传统儿童死亡率预测模型存在前瞻性偏差，在应用于未来人群时准确性下降，需要开发更稳健的预测方法。

Method: 使用孟加拉国2011-2022年DHS数据，采用时间分割验证（2011-2014训练，2017验证，2022测试），通过遗传算法进行神经网络架构搜索，并与XGBoost对比，使用SHAP值和Platt校准进行验证。

Result: 遗传算法找到的单层神经网络（64个单元）AUROC为0.76，显著优于XGBoost的0.73。模型在贫困地区表现更好（AUC 0.74），在富裕地区较差（AUC 0.66），存在社会经济预测梯度。每年可比梯度提升模型多识别1300名高危儿童。

Conclusion: 该方法提供了稳健、可部署的计算表型，能够更准确地识别需要干预的高危儿童，特别在贫困地区效果显著，可用于精准母婴健康干预。

Abstract: The predictive machine learning models for child mortality tend to be inaccurate when applied to future populations, since they suffer from look-ahead bias due to the randomization used in cross-validation. The Demographic and Health Surveys (DHS) data from Bangladesh for 2011-2022, with n = 33,962, are used in this paper. We trained the model on (2011-2014) data, validated it on 2017 data, and tested it on 2022 data. Eight years after the initial test of the model, a genetic algorithm-based Neural Architecture Search found a single-layer neural architecture (with 64 units) to be superior to XGBoost (AUROC = 0.76 vs. 0.73; p < 0.01). Additionally, through a detailed fairness audit, we identified an overall "Socioeconomic Predictive Gradient," with a positive correlation between regional poverty level (r = -0.62) and the algorithm's AUC. In addition, we found that the model performed at its highest levels in the least affluent divisions (AUC 0.74) and decreased dramatically in the wealthiest divisions (AUC 0.66). These findings suggest that the model is identifying areas with the greatest need for intervention. Our model would identify approximately 1300 additional at-risk children annually than a Gradient Boosting model when screened at the 10% level and validated using SHAP values and Platt Calibration, and therefore provide a robust, production-ready computational phenotype for targeted maternal and child health interventions.

</details>


### [60] [Non-linear PCA via Evolution Strategies: a Novel Objective Function](https://arxiv.org/abs/2602.03967)
*Thomas Uriot,Elise Chung*

Main category: cs.LG

TL;DR: 提出一个结合PCA可解释性与神经网络灵活性的非线性PCA框架，使用进化策略优化神经网络的变量变换，通过最大化每个变量的个体方差贡献来提升学习效果，能处理分类和有序变量，在解释方差上显著优于线性和核PCA。


<details>
  <summary>Details</summary>
Motivation: 传统PCA是线性方法，无法捕捉真实数据的复杂结构；核PCA虽然能处理非线性但牺牲了可解释性且超参数选择困难。需要一种既能保持PCA可解释性又能处理非线性关系的降维方法。

Method: 使用神经网络参数化变量变换，通过进化策略优化（处理特征分解的非可微性）。提出新颖的粒度化目标函数，最大化每个变量的个体方差贡献而非全局方差，能原生处理分类和有序变量而无需独热编码。

Result: 在合成和真实数据集上，该方法在解释方差方面显著优于线性PCA和核PCA，同时保持了PCA的可解释性，可以使用双标图等标准工具进行特征贡献的可视化和分析。

Conclusion: 提出的非线性PCA框架成功统一了PCA的可解释性与神经网络的灵活性，通过进化策略和粒度化目标函数实现了更好的降维效果，同时保持了分析的可解释性。

Abstract: Principal Component Analysis (PCA) is a powerful and popular dimensionality reduction technique. However, due to its linear nature, it often fails to capture the complex underlying structure of real-world data. While Kernel PCA (kPCA) addresses non-linearity, it sacrifices interpretability and struggles with hyperparameter selection. In this paper, we propose a robust non-linear PCA framework that unifies the interpretability of PCA with the flexibility of neural networks. Our method parametrizes variable transformations via neural networks, optimized using Evolution Strategies (ES) to handle the non-differentiability of eigendecomposition. We introduce a novel, granular objective function that maximizes the individual variance contribution of each variable providing a stronger learning signal than global variance maximization. This approach natively handles categorical and ordinal variables without the dimensional explosion associated with one-hot encoding. We demonstrate that our method significantly outperforms both linear PCA and kPCA in explained variance across synthetic and real-world datasets. At the same time, it preserves PCA's interpretability, enabling visualization and analysis of feature contributions using standard tools such as biplots. The code can be found on GitHub.

</details>


### [61] [DeXposure-FM: A Time-series, Graph Foundation Model for Credit Exposures and Stability on Decentralized Financial Networks](https://arxiv.org/abs/2602.03981)
*Aijie Shu,Wenbin Wu,Gbenga Ibikunle,Fengxiang He*

Main category: cs.LG

TL;DR: DeXposure-FM是首个用于测量和预测DeFi网络协议间信用风险暴露的时序图基础模型，通过图-表格编码器和多任务头架构，在包含4300+协议、24300+代币的大规模数据集上训练，显著优于现有方法，并提供系统性风险评估工具。


<details>
  <summary>Details</summary>
Motivation: DeFi中的信用风险暴露通常是隐性的、代币中介的，形成了密集的协议间依赖网络。单一代币的冲击可能导致显著且不受控制的传染效应。随着DeFi与传统金融基础设施的日益连接，这种动态风险需要更强大的量化工具来评估。

Method: 提出DeXposure-FM时序图基础模型，采用图-表格编码器架构，使用预训练权重初始化，配备多个任务特定头。在包含43.7百万数据条目、覆盖602条区块链上4300+协议和24300+代币的DeXposure数据集上进行训练，用于信用风险暴露预测，包括协议级流动以及信用风险暴露链接的拓扑和权重联合动态预测。

Result: 在两个机器学习基准测试中，DeXposure-FM持续优于现有最先进方法，包括图基础模型和时序图神经网络。模型进一步提供金融经济学工具，支持宏观审慎监控和基于场景的DeFi压力测试，通过预测-测量流程实现协议级系统性重要性评分、行业级溢出和集中度度量。

Conclusion: DeXposure-FM是首个用于DeFi信用风险暴露量化和预测的时序图基础模型，在预测性能和风险评估能力方面显著优于现有方法，为DeFi系统性风险监控提供了有效的量化工具，模型和代码已公开可用。

Abstract: Credit exposure in Decentralized Finance (DeFi) is often implicit and token-mediated, creating a dense web of inter-protocol dependencies. Thus, a shock to one token may result in significant and uncontrolled contagion effects. As the DeFi ecosystem becomes increasingly linked with traditional financial infrastructure through instruments, such as stablecoins, the risk posed by this dynamic demands more powerful quantification tools. We introduce DeXposure-FM, the first time-series, graph foundation model for measuring and forecasting inter-protocol credit exposure on DeFi networks, to the best of our knowledge. Employing a graph-tabular encoder, with pre-trained weight initialization, and multiple task-specific heads, DeXposure-FM is trained on the DeXposure dataset that has 43.7 million data entries, across 4,300+ protocols on 602 blockchains, covering 24,300+ unique tokens. The training is operationalized for credit-exposure forecasting, predicting the joint dynamics of (1) protocol-level flows, and (2) the topology and weights of credit-exposure links. The DeXposure-FM is empirically validated on two machine learning benchmarks; it consistently outperforms the state-of-the-art approaches, including a graph foundation model and temporal graph neural networks. DeXposure-FM further produces financial economics tools that support macroprudential monitoring and scenario-based DeFi stress testing, by enabling protocol-level systemic-importance scores, sector-level spillover and concentration measures via a forecast-then-measure pipeline. Empirical verification fully supports our financial economics tools. The model and code have been publicly available. Model: https://huggingface.co/EVIEHub/DeXposure-FM.
Code: https://github.com/EVIEHub/DeXposure-FM.

</details>


### [62] [eCP: Informative uncertainty quantification via Equivariantized Conformal Prediction with pre-trained models](https://arxiv.org/abs/2602.03986)
*Nikolaos Bousias,Lars Lindemann,George Pappas*

Main category: cs.LG

TL;DR: 通过群对称化预训练模型改进共形预测，利用几何信息分布非共形质量，缩小预测集并提高统计保证


<details>
  <summary>Details</summary>
Motivation: 传统共形预测在长时任务中不确定性区域显著增大，导致统计保证变得无意义。需要利用几何对称性信息来改进不确定性量化

Method: 通过群平均预训练预测器，将非共形质量分布在轨道上，将每个样本视为轨道的代表，利用对称群元素纠缠的样本来缓解不确定性

Result: 理论上证明该方法能按递增凸序收缩非共形分数，改善指数尾界，在期望上获得更尖锐的共形预测集，特别是在高置信水平下

Conclusion: 提出了一种融合几何对称性的共形预测方法，理论上能改善不确定性量化，并设计了行人轨迹预测实验来验证理论主张

Abstract: We study the effect of group symmetrization of pre-trained models on conformal prediction (CP), a post-hoc, distribution-free, finite-sample method of uncertainty quantification that offers formal coverage guarantees under the assumption of data exchangeability. Unfortunately, CP uncertainty regions can grow significantly in long horizon missions, rendering the statistical guarantees uninformative. To that end, we propose infusing CP with geometric information via group-averaging of the pretrained predictor to distribute the non-conformity mass across the orbits. Each sample now is treated as a representative of an orbit, thus uncertainty can be mitigated by other samples entangled to it via the orbit inducing elements of the symmetry group. Our approach provably yields contracted non-conformity scores in increasing convex order, implying improved exponential-tail bounds and sharper conformal prediction sets in expectation, especially at high confidence levels. We then propose an experimental design to test these theoretical claims in pedestrian trajectory prediction.

</details>


### [63] [When Chains of Thought Don't Matter: Causal Bypass in Large Language Models](https://arxiv.org/abs/2602.03994)
*Anish Sathyanarayanan,Aditya Nagarsekar,Aarush Rathore*

Main category: cs.LG

TL;DR: 研究发现CoT提示并不能保证模型答案真正依赖推理过程，即使表面看起来合理，答案往往通过绕过推理的电路独立产生。


<details>
  <summary>Details</summary>
Motivation: 挑战CoT提示能暴露模型推理过程并提高透明性的普遍假设，探究表面合规的推理是否真正因果影响模型答案。

Method: 提出诊断框架：包含(i)可解释行为模块评估CoT文本中的操纵信号，(ii)因果探针通过隐藏状态修补测量CoT介导影响，计算绕过分数量化答案独立于推理的程度。

Result: 即使审计感知提示增加可检测的操纵信号，因果探针显示任务依赖的调节：许多QA项目几乎完全绕过推理，而某些逻辑问题显示更强的调节作用。层分析揭示即使平均CMI低，也存在狭窄的任务依赖"推理窗口"。

Conclusion: CoT提示的表面合规性不能保证模型真正依赖推理过程，需要因果分析来验证推理的真实影响，这对AI透明性和可解释性有重要启示。

Abstract: Chain-of-thought (CoT) prompting is widely assumed to expose a model's reasoning process and improve transparency. We attempted to enforce this assumption by penalizing unfaithful reasoning, but found that surface-level compliance does not guarantee causal reliance. Our central finding is negative: even when CoT is verbose, strategic, and flagged by surface-level manipulation detectors, model answers are often causally independent of the CoT content. We present a diagnostic framework for auditing this failure mode: it combines (i) an interpretable behavioral module that scores manipulation-relevant signals in CoT text and (ii) a causal probe that measures CoT-mediated influence (CMI) via hidden-state patching and reports a bypass score ($1-\mathrm{CMI}$), quantifying the degree to which the answer is produced by a bypass circuit independent of the rationale. In pilot evaluations, audit-aware prompting increases detectable manipulation signals (mean risk-score delta: $+5.10$), yet causal probes reveal task-dependent mediation: many QA items exhibit near-total bypass (CMI $\approx 0$), while some logic problems show stronger mediation (CMI up to $0.56$). Layer-wise analysis reveals narrow and task-dependent ``reasoning windows'' even when mean CMI is low.

</details>


### [64] [Rational ANOVA Networks](https://arxiv.org/abs/2602.04006)
*Jusheng Zhang,Ningyuan Liu,Qinhan Lyu,Jing Yang,Keze Wang*

Main category: cs.LG

TL;DR: RAN提出了一种基于函数ANOVA分解和Padé有理逼近的新型神经网络架构，用可学习的稳定有理单元替代传统固定非线性激活函数，在保持计算效率的同时提升了解释性和外推能力。


<details>
  <summary>Details</summary>
Motivation: 传统深度神经网络将非线性激活函数（如ReLU）视为固定原语，这限制了解释性和对函数类的精细控制。现有的加性模型（如KANs）虽然尝试使用样条函数解决这一问题，但存在计算效率低和边界不稳定的问题。

Method: RAN基于函数ANOVA分解和Padé风格有理逼近，将函数f(x)建模为主效应和稀疏成对交互作用的组合，每个组件由稳定的可学习有理单元参数化。关键创新是强制分母严格为正，避免极点和数值不稳定性，同时比多项式基更有效地捕捉尖锐过渡和近奇异行为。

Result: 在受控函数基准测试和视觉分类任务（如CIFAR-10）中，在匹配参数和计算预算的条件下，RAN达到或超越了参数匹配的MLP和可学习激活基线，具有更好的稳定性和吞吐量。

Conclusion: RAN提供了一种基础架构，通过ANOVA结构实现显式的低阶交互偏置以提高数据效率和解释性，同时有理参数化显著改善了外推能力，为神经网络的非线性建模提供了更灵活、稳定和可解释的替代方案。

Abstract: Deep neural networks typically treat nonlinearities as fixed primitives (e.g., ReLU), limiting both interpretability and the granularity of control over the induced function class. While recent additive models (like KANs) attempt to address this using splines, they often suffer from computational inefficiency and boundary instability. We propose the Rational-ANOVA Network (RAN), a foundational architecture grounded in functional ANOVA decomposition and Padé-style rational approximation. RAN models f(x) as a composition of main effects and sparse pairwise interactions, where each component is parameterized by a stable, learnable rational unit. Crucially, we enforce a strictly positive denominator, which avoids poles and numerical instability while capturing sharp transitions and near-singular behaviors more efficiently than polynomial bases. This ANOVA structure provides an explicit low-order interaction bias for data efficiency and interpretability, while the rational parameterization significantly improves extrapolation. Across controlled function benchmarks and vision classification tasks (e.g., CIFAR-10) under matched parameter and compute budgets, RAN matches or surpasses parameter-matched MLPs and learnable-activation baselines, with better stability and throughput. Code is available at https://github.com/jushengzhang/Rational-ANOVA-Networks.git.

</details>


### [65] [PromptSplit: Revealing Prompt-Level Disagreement in Generative Models](https://arxiv.org/abs/2602.04009)
*Mehdi Lotfian,Mohammad Jalali,Farzan Farnia*

Main category: cs.LG

TL;DR: PromptSplit：基于核方法的框架，用于检测和分析生成模型之间的提示依赖分歧，通过张量积嵌入和核协方差矩阵识别不同提示下的模型行为差异。


<details>
  <summary>Details</summary>
Motivation: 随着提示引导的生成式AI模型在视觉和语言领域的快速发展，不同模型在训练数据和架构上的差异导致需要系统方法来识别哪些类型的提示会导致模型行为差异。现有方法缺乏对提示依赖分歧的检测和分析工具。

Method: 提出PromptSplit框架：1）为每对模型构建联合提示-输出表示，通过提示特征和图像/文本特征的张量积嵌入；2）计算相应的核协方差矩阵；3）利用这些矩阵加权差异的特征空间识别提示间行为差异的主要方向；4）采用随机投影近似将计算复杂度降至O(nr² + r³)；5）提供理论分析证明近似误差界限为O(1/r²)。

Result: 在文本到图像、文本到文本和图像描述任务上的实验表明，PromptSplit能够准确检测真实的行为差异，并识别导致分歧的提示，为生成模型分歧检测提供了可解释的工具。

Conclusion: PromptSplit提供了一个可扩展且理论上有保证的框架，用于检测和分析生成模型之间的提示依赖分歧，有助于理解不同模型在哪些提示下会产生不同行为，为模型比较和评估提供了新工具。

Abstract: Prompt-guided generative AI models have rapidly expanded across vision and language domains, producing realistic and diverse outputs from textual inputs. The growing variety of such models, trained with different data and architectures, calls for principled methods to identify which types of prompts lead to distinct model behaviors. In this work, we propose PromptSplit, a kernel-based framework for detecting and analyzing prompt-dependent disagreement between generative models. For each compared model pair, PromptSplit constructs a joint prompt--output representation by forming tensor-product embeddings of the prompt and image (or text) features, and then computes the corresponding kernel covariance matrix. We utilize the eigenspace of the weighted difference between these matrices to identify the main directions of behavioral difference across prompts. To ensure scalability, we employ a random-projection approximation that reduces computational complexity to $O(nr^2 + r^3)$ for projection dimension $r$. We further provide a theoretical analysis showing that this approximation yields an eigenstructure estimate whose expected deviation from the full-dimensional result is bounded by $O(1/r^2)$. Experiments across text-to-image, text-to-text, and image-captioning settings demonstrate that PromptSplit accurately detects ground-truth behavioral differences and isolates the prompts responsible, offering an interpretable tool for detecting where generative models disagree.

</details>


### [66] [Understanding and Guiding Layer Placement in Parameter-Efficient Fine-Tuning of Large Language Models](https://arxiv.org/abs/2602.04019)
*Yichen Xu,Yuyang Liang,Shan Dai,Tianyang Hu,Tsz Nam Chan,Chenhao Ma*

Main category: cs.LG

TL;DR: 该论文提出了一种基于投影残差视角的参数高效微调层选择方法，通过Layer Card诊断工具指导选择要微调的层，在保持性能的同时显著降低微调成本。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模增长，全参数微调成本高昂，参数高效微调成为主流。然而当前实践通常在所有层上均匀应用PEFT，缺乏对层选择的深入理解和利用，无法根据推理延迟、边缘设备部署等约束灵活选择微调层。

Method: 提出统一的投影残差视角来分析PEFT，在局部二次近似下，层适应由三个量控制：投影残差范数、激活能量和层耦合。基于这些洞察，引入Layer Card诊断工具，总结每层的残差信号强度、计算成本和性能，指导层选择策略。

Result: 在Qwen3-8B模型上，选择性微调部分层可以达到接近全层LoRA的性能，同时显著降低微调成本和推理时的适配器层数，提供更具成本效益的替代方案。

Conclusion: 通过Layer Card指导的层选择策略，可以在保持性能的同时灵活优化不同目标（如最大化性能或最小化成本），为参数高效微调提供了更智能、更经济的方法。

Abstract: As large language models (LLMs) continue to grow, the cost of full-parameter fine-tuning has made parameter-efficient fine-tuning (PEFT) the default strategy for downstream adaptation. Constraints from inference latency in scalable serving and fine-tuning cost in edge or rapid-deployment settings make the choice of which layers to fine-tune unavoidable. Yet current practice typically applies PEFT uniformly across all layers, with limited understanding or leverage of layer selection. This paper develops a unified projected residual view of PEFT on top of a frozen base model. Under a local quadratic approximation, layerwise adaptation is governed by three quantities: (i) the projected residual norm (resnorm), which measures how much correctable bias a layer can capture; (ii) the activation energy, which determines feature conditioning; and (iii) layer coupling, which quantifies how strongly residuals interact across layers. We show that, for squared loss and linear adapters, the resnorm equals a normalized gradient norm, activation energy controls ill-conditioning and noise amplification, and weak coupling yields approximately additive layerwise contributions. Building on these insights, we introduce the Layer Card, a reusable diagnostic that summarizes residual signal strength, compute cost, and performance for each layer of a given model. With an identical model and LoRA configuration, Layer Card-guided placement refines the choice of adapted layers to flexibly prioritize different objectives, such as maximizing performance or reducing fine-tuning cost. Moreover, on Qwen3-8B, we show that selectively adapting a subset of layers can achieve performance close to full-layer LoRA while substantially reducing fine-tuning cost and the number of adapter-augmented layers during inference, offering a more cost-performance-aware alternative to full-layer insertion.

</details>


### [67] [Group Contrastive Learning for Weakly Paired Multimodal Data](https://arxiv.org/abs/2602.04021)
*Aditya Gorla,Hugues Van Assel,Jan-Christian Huetter,Heming Yao,Kyunghyun Cho,Aviv Regev,Russell Littman*

Main category: cs.LG

TL;DR: GROOVE：一种用于弱配对多模态扰动数据的半监督表示学习方法，通过GroupCLIP损失函数和组合评估框架，在跨模态匹配和插补任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理高含量扰动数据时面临挑战，这些数据中不同模态的样本仅通过共享的扰动标签弱配对，缺乏直接对应关系。需要开发能够有效利用这种弱配对信息的表示学习方法。

Method: 提出GROOVE框架，包含：1) GroupCLIP损失函数，将CLIP的跨模态对比学习与SupCon的单模态监督对比学习相结合；2) 即时反向翻译自编码器框架，促进跨模态纠缠表示同时保持组级一致性；3) 组合评估框架，系统评估不同最优传输对齐器的性能。

Result: 在模拟数据和两个真实单细胞遗传扰动数据集上，GROOVE在跨模态匹配和插补任务中与现有方法相当或更优。消融研究表明GroupCLIP是性能提升的关键因素。组合基准测试显示目前没有对齐器在所有设置或模态对中均占优势。

Conclusion: 在只有弱配对可用的场景中，利用组级约束对于有效的多模态表示学习至关重要。GROOVE通过GroupCLIP损失函数和综合评估框架，为这类问题提供了有效的解决方案。

Abstract: We present GROOVE, a semi-supervised multi-modal representation learning approach for high-content perturbation data where samples across modalities are weakly paired through shared perturbation labels but lack direct correspondence. Our primary contribution is GroupCLIP, a novel group-level contrastive loss that bridges the gap between CLIP for paired cross-modal data and SupCon for uni-modal supervised contrastive learning, addressing a fundamental gap in contrastive learning for weakly-paired settings. We integrate GroupCLIP with an on-the-fly backtranslating autoencoder framework to encourage cross-modally entangled representations while maintaining group-level coherence within a shared latent space. Critically, we introduce a comprehensive combinatorial evaluation framework that systematically assesses representation learners across multiple optimal transport aligners, addressing key limitations in existing evaluation strategies. This framework includes novel simulations that systematically vary shared versus modality-specific perturbation effects enabling principled assessment of method robustness. Our combinatorial benchmarking reveals that there is not yet an aligner that uniformly dominates across settings or modality pairs. Across simulations and two real single-cell genetic perturbation datasets, GROOVE performs on par with or outperforms existing approaches for downstream cross-modal matching and imputation tasks. Our ablation studies demonstrate that GroupCLIP is the key component driving performance gains. These results highlight the importance of leveraging group-level constraints for effective multi-modal representation learning in scenarios where only weak pairing is available.

</details>


### [68] [A Consensus-Bayesian Framework for Detecting Malicious Activity in Enterprise Directory Access Graphs](https://arxiv.org/abs/2602.04027)
*Pratyush Uppuluri,Shilpa Noushad,Sajan Kumar*

Main category: cs.LG

TL;DR: 提出基于共识的贝叶斯框架，通过建模目录访问图中的用户行为，使用意见动力学检测恶意访问行为


<details>
  <summary>Details</summary>
Motivation: 企业目录访问图中存在恶意用户行为，传统方法难以检测逻辑不一致的跨组件访问异常

Method: 将目录建模为主题，用户建模为多级交互图中的智能体，使用影响加权的意见动力学模拟访问演化，通过动态矩阵编码逻辑依赖关系，引入贝叶斯异常评分机制

Result: 在合成访问图上的模拟验证了方法的有效性，能够敏感检测逻辑不一致性，并在动态扰动下保持鲁棒性

Conclusion: 该框架成功结合共识机制和贝叶斯推理，为检测企业目录访问图中的恶意行为提供了有效解决方案

Abstract: This work presents a consensus-based Bayesian framework to detect malicious user behavior in enterprise directory access graphs. By modeling directories as topics and users as agents within a multi-level interaction graph, we simulate access evolution using influence-weighted opinion dynamics. Logical dependencies between users are encoded in dynamic matrices Ci, and directory similarity is captured via a shared influence matrix W. Malicious behavior is injected as cross-component logical perturbations that violate structural norms of strongly connected components(SCCs). We apply theoretical guarantees from opinion dynamics literature to determine topic convergence and detect anomaly via scaled opinion variance. To quantify uncertainty, we introduce a Bayesian anomaly scoring mechanism that evolves over time, using both static and online priors. Simulations over synthetic access graphs validate our method, demonstrating its sensitivity to logical inconsistencies and robustness under dynamic perturbation.

</details>


### [69] [The Illusion of Generalization: Re-examining Tabular Language Model Evaluation](https://arxiv.org/abs/2602.04031)
*Aditya Gorla,Ratish Puduppully*

Main category: cs.LG

TL;DR: 对Tabula-8B作为代表性表格语言模型的系统再评估显示，其声称的泛化能力可能源于评估缺陷而非真正的表格推理学习


<details>
  <summary>Details</summary>
Motivation: 重新评估表格语言模型声称的"涌现泛化"能力，检验这些声称是否真实反映了模型学习到的表格推理能力

Method: 使用UniPredict基准的165个数据集对Tabula-8B进行系统评估，分析其在不同类型分类任务上的表现，并检查数据污染问题

Result: 1) 二元和分类分类任务中位数提升接近零，强聚合性能完全由四分位数分类任务驱动；2) 表现最佳的数据集存在普遍的数据污染，包括完整的训练-测试重叠和规避标准去重的任务级泄漏；3) 无表格暴露的指令调优恢复了92.2%的标准分类性能，在四分位数分类中，格式熟悉度填补了71.3%的性能差距

Conclusion: 表格语言模型声称的泛化能力可能反映了评估缺陷而非学习到的表格推理能力，需要加强TLM评估方法

Abstract: Tabular Language Models (TLMs) have been claimed to achieve emergent generalization for tabular prediction. We conduct a systematic re-evaluation of Tabula-8B as a representative TLM, utilizing 165 datasets from the UniPredict benchmark. Our investigation reveals three findings. First, binary and categorical classification achieve near-zero median lift over majority-class baselines and strong aggregate performance is driven entirely by quartile classification tasks. Second, top-performing datasets exhibit pervasive contamination, including complete train-test overlap and task-level leakage that evades standard deduplication. Third, instruction-tuning without tabular exposure recovers 92.2% of standard classification performance and on quartile classification, format familiarity closes 71.3% of the gap with the residual attributable to contaminated datasets. These findings suggest claimed generalization likely reflects evaluation artifacts rather than learned tabular reasoning. We conclude with recommendations for strengthening TLM evaluation.

</details>


### [70] [DADP: Domain Adaptive Diffusion Policy](https://arxiv.org/abs/2602.04037)
*Pengcheng Wang,Qinghang Liu,Haotian Lin,Yiheng Li,Guojian Zhan,Masayoshi Tomizuka,Yixiao Wang*

Main category: cs.LG

TL;DR: DADP通过滞后上下文动态预测实现无监督解耦，将静态域信息与动态特性分离，并通过域感知扩散注入提升零样本适应能力


<details>
  <summary>Details</summary>
Motivation: 现有域自适应方法在泛化到未见过的动态系统时面临挑战，域表示学习容易将静态域信息与变化的动态特性纠缠，限制了零样本适应能力

Method: 提出DADP：1) 滞后上下文动态预测，通过历史偏移上下文预测未来状态，无监督解耦静态域表示；2) 域感知扩散注入，将学习到的域表示直接整合到生成过程中，通过偏置先验分布和重构扩散目标

Result: 在运动和操作等挑战性基准测试中表现出优越性能，相比先前方法具有更好的泛化能力

Conclusion: DADP通过解耦静态域表示和域感知扩散注入，实现了鲁棒的零样本域自适应，为学习泛化控制策略提供了有效解决方案

Abstract: Learning domain adaptive policies that can generalize to unseen transition dynamics, remains a fundamental challenge in learning-based control. Substantial progress has been made through domain representation learning to capture domain-specific information, thus enabling domain-aware decision making. We analyze the process of learning domain representations through dynamical prediction and find that selecting contexts adjacent to the current step causes the learned representations to entangle static domain information with varying dynamical properties. Such mixture can confuse the conditioned policy, thereby constraining zero-shot adaptation. To tackle the challenge, we propose DADP (Domain Adaptive Diffusion Policy), which achieves robust adaptation through unsupervised disentanglement and domain-aware diffusion injection. First, we introduce Lagged Context Dynamical Prediction, a strategy that conditions future state estimation on a historical offset context; by increasing this temporal gap, we unsupervisedly disentangle static domain representations by filtering out transient properties. Second, we integrate the learned domain representations directly into the generative process by biasing the prior distribution and reformulating the diffusion target. Extensive experiments on challenging benchmarks across locomotion and manipulation demonstrate the superior performance, and the generalizability of DADP over prior methods. More visualization results are available on the https://outsider86.github.io/DomainAdaptiveDiffusionPolicy/.

</details>


### [71] [Partition Trees: Conditional Density Estimation over General Outcome Spaces](https://arxiv.org/abs/2602.04042)
*Felipe Angelim,Alessandro Leite*

Main category: cs.LG

TL;DR: 提出Partition Trees框架，用于一般结果空间的非参数条件密度估计，通过最小化条件负对数似然学习数据自适应划分，支持连续和分类变量统一处理。


<details>
  <summary>Details</summary>
Motivation: 现有概率树方法通常对目标分布做出参数假设，缺乏一个统一的非参数框架来处理连续和分类变量的条件密度估计问题。

Method: 基于数据自适应划分建模条件分布为分段常数密度，通过直接最小化条件负对数似然学习树结构，并引入Partition Forests集成方法平均条件密度。

Result: 相比CART风格树有明显改进，与最先进的概率树方法和随机森林相比具有竞争性或更优性能，对冗余特征和异方差噪声具有鲁棒性。

Conclusion: Partition Trees提供了一个可扩展的非参数替代方案，统一处理连续和分类变量的条件密度估计，无需对目标分布做参数假设。

Abstract: We propose Partition Trees, a tree-based framework for conditional density estimation over general outcome spaces, supporting both continuous and categorical variables within a unified formulation. Our approach models conditional distributions as piecewise-constant densities on data adaptive partitions and learns trees by directly minimizing conditional negative log-likelihood. This yields a scalable, nonparametric alternative to existing probabilistic trees that does not make parametric assumptions about the target distribution. We further introduce Partition Forests, an ensemble extension obtained by averaging conditional densities. Empirically, we demonstrate improved probabilistic prediction over CART-style trees and competitive or superior performance compared to state-of-the-art probabilistic tree methods and Random Forests, along with robustness to redundant features and heteroscedastic noise.

</details>


### [72] [SEIS: Subspace-based Equivariance and Invariance Scores for Neural Representations](https://arxiv.org/abs/2602.04054)
*Huahua Lin,Katayoun Farrahi,Xiaohao Cai*

Main category: cs.LG

TL;DR: SEIS是一种基于子空间的度量方法，用于分析神经网络层在几何变换下的特征表示，能够区分等变性和不变性，无需标签或变换先验知识。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要通过比较变换输入下的模型输出来评估鲁棒性，但无法深入了解几何信息在内部表示中的组织方式，也无法区分信息丢失与重新编码。

Method: 提出SEIS（基于子空间的等变性和不变性评分），这是一种子空间度量方法，用于分析层间特征表示在几何变换下的行为，能够解耦等变性和不变性。

Result: 合成验证表明SEIS能正确恢复已知变换；应用于训练好的分类网络显示：早期层呈现等变性，深层呈现不变性；数据增强增加不变性同时保持等变性；多任务学习在共享编码器中协同提升两种属性；跳跃连接能恢复解码过程中丢失的等变性。

Conclusion: SEIS提供了一种无需监督的方法来分析神经网络表示中的几何结构，揭示了网络如何组织几何信息，并为理解表示学习中的等变性和不变性提供了新工具。

Abstract: Understanding how neural representations respond to geometric transformations is essential for evaluating whether learned features preserve meaningful spatial structure. Existing approaches primarily assess robustness by comparing model outputs under transformed inputs, offering limited insight into how geometric information is organized within internal representations and failing to distinguish between information loss and re-encoding. In this work, we introduce SEIS (Subspace-based Equivariance and Invariance Scores), a subspace metric for analyzing layer-wise feature representations under geometric transformations, disentangling equivariance from invariance without requiring labels or explicit knowledge of the transformation. Synthetic validation confirms that SEIS correctly recovers known transformations. Applied to trained classification networks, SEIS reveals a transition from equivariance in early layers to invariance in deeper layers, and that data augmentation increases invariance while preserving equivariance. We further show that multi-task learning induces synergistic gains in both properties at the shared encoder, and skip connections restore equivariance lost during decoding.

</details>


### [73] [An Empirical Survey and Benchmark of Learned Distance Indexes for Road Networks](https://arxiv.org/abs/2602.04068)
*Gautam Choudhary,Libin Zhou,Yeasir Rayhan,Walid G. Aref*

Main category: cs.LG

TL;DR: 本文首次对基于机器学习的道路网络距离索引进行系统性实证评估，比较了10种ML技术和经典非ML基线在训练时间、查询延迟、存储和准确性四个维度的表现。


<details>
  <summary>Details</summary>
Motivation: 道路网络中最短路径距离计算是导航系统和空间分析的核心操作，虽然经典算法能提供精确答案，但其延迟无法满足现代实时大规模部署需求。近年来基于ML的距离索引被提出以高效回答近似最短路径查询，但缺乏对这些ML方法的全面系统评估。

Method: 使用7个真实世界道路网络和基于轨迹数据的工作负载驱动查询数据集，对10种代表性ML技术进行基准测试，并与强大的经典非ML基线进行比较。评估维度包括训练时间、查询延迟、存储和准确性。

Result: 研究提供了对ML距离索引的全面实证评估，揭示了关键见解和实际权衡，并发布了统一的开源代码库以支持可重复性和未来研究。

Conclusion: 这是首个对基于机器学习的道路网络距离索引的实证调查，为研究人员和实践者提供了系统评估框架和基准测试结果，推动了学习型距离索引领域的发展。

Abstract: The calculation of shortest-path distances in road networks is a core operation in navigation systems, location-based services, and spatial analytics. Although classical algorithms, e.g., Dijkstra's algorithm, provide exact answers, their latency is prohibitive for modern real-time, large-scale deployments. Over the past two decades, numerous distance indexes have been proposed to speed up query processing for shortest distance queries. More recently, with the advancement in machine learning (ML), researchers have designed and proposed ML-based distance indexes to answer approximate shortest path and distance queries efficiently. However, a comprehensive and systematic evaluation of these ML-based approaches is lacking. This paper presents the first empirical survey of ML-based distance indexes on road networks, evaluating them along four key dimensions: Training time, query latency, storage, and accuracy. Using seven real-world road networks and workload-driven query datasets derived from trajectory data, we benchmark ten representative ML techniques and compare them against strong classical non-ML baselines, highlighting key insights and practical trade-offs. We release a unified open-source codebase to support reproducibility and future research on learned distance indexes.

</details>


### [74] [Agentic AI-Empowered Dynamic Survey Framework](https://arxiv.org/abs/2602.04071)
*Furkan Mumcu,Lokman Bekit,Michael J. Jones,Anoop Cherian,Yasin Yilmaz*

Main category: cs.LG

TL;DR: 提出动态调查框架，将综述论文视为活文档而非一次性生成任务，通过智能体持续更新现有综述，整合新研究同时保持结构完整性。


<details>
  <summary>Details</summary>
Motivation: 研究产出快速增长导致综述论文迅速过时，造成文献冗余和碎片化。传统综述作为一次性生成任务无法跟上研究进展，需要新的方法来解决这一维护问题。

Method: 提出基于智能体的动态调查框架，将综述写作重构为长期维护问题。该框架支持现有综述的持续更新，通过增量方式整合新研究，同时保持综述结构并最小化不必要的干扰。

Result: 通过回顾性实验设置证明，该框架能有效识别和整合新兴研究，同时保持现有综述的连贯性和结构完整性。

Conclusion: 综述论文应被视为活文档，动态调查框架为解决综述过时问题提供了有效方法，支持综述与研究同步演进，减少文献冗余和碎片化。

Abstract: Survey papers play a central role in synthesizing and organizing scientific knowledge, yet they are increasingly strained by the rapid growth of research output. As new work continues to appear after publication, surveys quickly become outdated, contributing to redundancy and fragmentation in the literature. We reframe survey writing as a long-horizon maintenance problem rather than a one-time generation task, treating surveys as living documents that evolve alongside the research they describe. We propose an agentic Dynamic Survey Framework that supports the continuous updating of existing survey papers by incrementally integrating new work while preserving survey structure and minimizing unnecessary disruption. Using a retrospective experimental setup, we demonstrate that the proposed framework effectively identifies and incorporates emerging research while preserving the coherence and structure of existing surveys.

</details>


### [75] [Stroke Lesions as a Rosetta Stone for Language Model Interpretability](https://arxiv.org/abs/2602.04074)
*Julius Fridriksson,Roger D. Newman-Norlund,Saeed Ahmadi,Regan Willis,Nadra Salman,Kalil Warren,Xiang Guan,Yong Yang,Srihari Nelakuditi,Rutvik Desai,Leonardo Bonilha,Jeff Charney,Chris Rorden*

Main category: cs.LG

TL;DR: 研究者提出BLUM框架，利用脑损伤-症状映射作为外部验证标准，通过比较语言模型扰动后的错误模式与中风失语症患者的脑损伤模式，验证了语言模型与人类语言处理在神经层面的相似性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的解释性方法主要依赖内部指标，缺乏外部验证。研究者希望建立一个能够从外部验证语言模型功能必要性的框架，借鉴临床神经科学中已建立因果关系的脑损伤-症状映射方法。

Method: 提出BLUM框架：1) 使用410名中风后失语症患者数据训练症状-损伤模型；2) 对transformer层进行系统性扰动；3) 对扰动后的语言模型和人类患者进行相同的临床评估；4) 将语言模型的错误模式投射到人类脑损伤空间中。

Result: 语言模型的错误模式与人类足够相似，预测的脑损伤位置在67%的图片命名条件和68.3%的句子完成条件下显著高于随机水平。语义主导的错误映射到腹侧流损伤模式，语音主导的错误映射到背侧流模式。

Conclusion: 该研究为语言模型解释性开辟了新方法，将临床神经科学作为外部验证标准，建立了人类脑损伤-症状映射作为评估人工语言系统的参考框架，并促使进一步研究行为对齐是否反映了共享的计算原理。

Abstract: Large language models (LLMs) have achieved remarkable capabilities, yet methods to verify which model components are truly necessary for language function remain limited. Current interpretability approaches rely on internal metrics and lack external validation. Here we present the Brain-LLM Unified Model (BLUM), a framework that leverages lesion-symptom mapping, the gold standard for establishing causal brain-behavior relationships for over a century, as an external reference structure for evaluating LLM perturbation effects. Using data from individuals with chronic post-stroke aphasia (N = 410), we trained symptom-to-lesion models that predict brain damage location from behavioral error profiles, applied systematic perturbations to transformer layers, administered identical clinical assessments to perturbed LLMs and human patients, and projected LLM error profiles into human lesion space. LLM error profiles were sufficiently similar to human error profiles that predicted lesions corresponded to actual lesions in error-matched humans above chance in 67% of picture naming conditions (p < 10^{-23}) and 68.3% of sentence completion conditions (p < 10^{-61}), with semantic-dominant errors mapping onto ventral-stream lesion patterns and phonemic-dominant errors onto dorsal-stream patterns. These findings open a new methodological avenue for LLM interpretability in which clinical neuroscience provides external validation, establishing human lesion-symptom mapping as a reference framework for evaluating artificial language systems and motivating direct investigation of whether behavioral alignment reflects shared computational principles.

</details>


### [76] [Principles of Lipschitz continuity in neural networks](https://arxiv.org/abs/2602.04078)
*Róisín Luo*

Main category: cs.LG

TL;DR: 该论文探讨了神经网络中Lipschitz连续性的原理，从内部训练动态和外部频率信号传播两个互补视角，旨在建立对鲁棒性和泛化性的理论理解。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习取得了显著成功，但在面对输入扰动和分布外数据时仍存在鲁棒性和泛化性挑战。现有研究主要关注基于Lipschitz约束的经验正则化方法，而对其基本原理探索不足。需要从理论层面理解Lipschitz连续性如何支配神经网络的鲁棒性和泛化性。

Method: 采用两个互补视角：1）内部视角：研究神经网络训练过程中Lipschitz连续性的时间演化（训练动态）；2）外部视角：探究Lipschitz连续性如何调节神经网络对输入数据特征的行为，特别是其在控制频率信号传播中的作用。

Result: 论文未提供具体实验结果，但提出了一个系统性的理论框架，旨在揭示Lipschitz连续性在神经网络训练动态和频率信号传播中的基本原理。

Conclusion: 通过从内部训练动态和外部频率调节两个角度系统研究Lipschitz连续性，该论文旨在建立对神经网络鲁棒性和泛化性的更深刻理论理解，为设计更可靠的深度学习系统提供理论基础。

Abstract: Deep learning has achieved remarkable success across a wide range of domains, significantly expanding the frontiers of what is achievable in artificial intelligence. Yet, despite these advances, critical challenges remain -- most notably, ensuring robustness to small input perturbations and generalization to out-of-distribution data. These critical challenges underscore the need to understand the underlying fundamental principles that govern robustness and generalization. Among the theoretical tools available, Lipschitz continuity plays a pivotal role in governing the fundamental properties of neural networks related to robustness and generalization. It quantifies the worst-case sensitivity of network's outputs to small input perturbations. While its importance is widely acknowledged, prior research has predominantly focused on empirical regularization approaches based on Lipschitz constraints, leaving the underlying principles less explored. This thesis seeks to advance a principled understanding of the principles of Lipschitz continuity in neural networks within the paradigm of machine learning, examined from two complementary perspectives: an internal perspective -- focusing on the temporal evolution of Lipschitz continuity in neural networks during training (i.e., training dynamics); and an external perspective -- investigating how Lipschitz continuity modulates the behavior of neural networks with respect to features in the input data, particularly its role in governing frequency signal propagation (i.e., modulation of frequency signal propagation).

</details>


### [77] [A Probabilistic Framework for Solving High-Frequency Helmholtz Equations via Diffusion Models](https://arxiv.org/abs/2602.04082)
*Yicheng Zou,Samuel Lanthaler,Hossein Salahshoor*

Main category: cs.LG

TL;DR: 提出基于分数条件扩散算子的概率神经算子框架，用于解决高频波动方程近似问题，相比确定性方法在L2、H1和能量范数上表现更优，并能捕捉输入不确定性。


<details>
  <summary>Details</summary>
Motivation: 确定性神经算子在处理高频波动现象时存在困难，因为强输入输出敏感性和谱偏差会使振荡模糊。需要概率方法来处理高频波动方程的挑战。

Method: 采用基于分数条件扩散算子的概率框架，首先对亥姆霍兹算子进行稳定性分析，然后通过数值实验验证方法在不同频率下的有效性。

Result: 概率神经算子在L2、H1和能量范数上产生最稳健的预测和最低误差，且能捕捉输入声速图传播到解场的不确定性，而确定性方法无法做到。

Conclusion: 概率算子学习是解决亥姆霍兹等复杂PDE在高频挑战性区域的有原则且有效的方法，为高频波动现象提供了新的解决方案。

Abstract: Deterministic neural operators perform well on many PDEs but can struggle with the approximation of high-frequency wave phenomena, where strong input-to-output sensitivity makes operator learning challenging, and spectral bias blurs oscillations. We argue for adopting a probabilistic approach for approximating waves in high-frequency regime, and develop our probabilistic framework using a score-based conditional diffusion operator. After demonstrating a stability analysis of the Helmholtz operator, we present our numerical experiments across a wide range of frequencies, benchmarked against other popular data-driven and machine learning approaches for waves. We show that our probabilistic neural operator consistently produces robust predictions with the lowest errors in $L^2$, $H^1$, and energy norms. Moreover, unlike all the other tested deterministic approaches, our framework remarkably captures uncertainties in the input sound speed map propagated to the solution field. We envision that our results position probabilistic operator learning as a principled and effective approach for solving complex PDEs such as Helmholtz in the challenging high-frequency regime.

</details>


### [78] [Federated Concept-Based Models: Interpretable models with distributed supervision](https://arxiv.org/abs/2602.04093)
*Dario Fenoglio,Arianna Casanova,Francesco De Santis,Mohan Li,Gabriele Dominici,Johannes Schneider,Martin Gjoreski,Marc Langheinrich,Pietro Barbiero,Giovanni De Felice*

Main category: cs.LG

TL;DR: 提出Federated Concept-based Models (F-CMs)，在联邦学习环境中部署概念模型，解决概念标注稀缺和机构异质性问题


<details>
  <summary>Details</summary>
Motivation: 概念模型能增强深度学习可解释性，但概念标注获取成本高且单一数据源中难以大规模获取。联邦学习可利用跨机构概念标注，但缺乏可解释建模范式。现有概念模型假设固定概念空间和预定义架构，而真实联邦学习环境是异质、非平稳的

Method: 提出F-CMs方法：跨机构聚合概念级信息，根据可用概念监督变化高效调整模型架构，同时保护机构隐私。支持动态适应新加入机构和新增监督

Result: F-CMs在保持全概念监督训练准确性和干预有效性的同时，优于非自适应联邦基线。关键创新：使机构能够对自身未拥有的概念进行可解释推理

Conclusion: F-CMs为联邦学习环境中的可解释AI提供了有效解决方案，解决了概念标注稀缺和机构异质性问题，实现了跨机构概念知识的共享和适应

Abstract: Concept-based models (CMs) enhance interpretability in deep learning by grounding predictions in human-understandable concepts. However, concept annotations are expensive to obtain and rarely available at scale within a single data source. Federated learning (FL) could alleviate this limitation by enabling cross-institutional training that leverages concept annotations distributed across multiple data owners. Yet, FL lacks interpretable modeling paradigms. Integrating CMs with FL is non-trivial: CMs assume a fixed concept space and a predefined model architecture, whereas real-world FL is heterogeneous and non-stationary, with institutions joining over time and bringing new supervision. In this work, we propose Federated Concept-based Models (F-CMs), a new methodology for deploying CMs in evolving FL settings. F-CMs aggregate concept-level information across institutions and efficiently adapt the model architecture in response to changes in the available concept supervision, while preserving institutional privacy. Empirically, F-CMs preserve the accuracy and intervention effectiveness of training settings with full concept supervision, while outperforming non-adaptive federated baselines. Notably, F-CMs enable interpretable inference on concepts not available to a given institution, a key novelty with respect to existing approaches.

</details>


### [79] [CoRe: Context-Robust Remasking for Diffusion Language Models](https://arxiv.org/abs/2602.04096)
*Kevin Zhai,Sabbir Mollah,Zhenyi Wang,Mubarak Shah*

Main category: cs.LG

TL;DR: CoRe是一种无需训练的推理时修正框架，通过探测token对上下文扰动的敏感性来识别并修正上下文脆弱的token，从而提升掩码扩散模型的解码质量。


<details>
  <summary>Details</summary>
Motivation: 标准掩码扩散模型解码存在上下文刚性：token基于瞬态高置信度被保留，但早期预测缺乏完整上下文，导致初始不一致性误导后续生成。现有修正策略依赖静态置信度分数，但这些信号本质上是短视的，不一致的token可能对模型本身显得很自信。

Method: 提出Context-Robust Remasking (CoRe)框架，不依赖静态token概率，而是通过探测token对针对性掩码上下文扰动的敏感性来识别上下文脆弱的token。将修正形式化为对上下文偏移的鲁棒优化目标，并高效近似该目标以优先修正不稳定的token。

Result: 在LLaDA-8B-Base模型上，CoRe在推理和代码基准测试中带来一致改进，优于计算匹配的基线方法，并将MBPP性能提升高达9.2个百分点。

Conclusion: CoRe通过动态评估token对上下文变化的敏感性而非依赖静态置信度，有效解决了掩码扩散模型中的上下文刚性问题和级联错误，实现了无需训练的推理时修正。

Abstract: Standard decoding in Masked Diffusion Models (MDMs) is hindered by context rigidity: tokens are retained based on transient high confidence, often ignoring that early predictions lack full context. This creates cascade effects where initial inconsistencies misguide the remaining generation. Existing revision strategies attempt to mitigate this by relying on static confidence scores, but these signals are inherently myopic; inconsistent tokens can appear confident to the model itself. We propose Context-Robust Remasking (CoRe), a training-free framework for inference-time revision. Rather than trusting static token probabilities, CoRe identifies context-brittle tokens by probing their sensitivity to targeted masked-context perturbations. We formalize revision as a robust optimization objective over context shifts and efficiently approximate this objective to prioritize unstable tokens for revision. On LLaDA-8B-Base, CoRe delivers consistent improvements across reasoning and code benchmarks, outperforming compute-matched baselines and improving MBPP by up to 9.2 percentage points.

</details>


### [80] [Rethinking Perplexity: Revealing the Impact of Input Length on Perplexity Evaluation in LLMs](https://arxiv.org/abs/2602.04099)
*Letian Cheng,Junyan Wang,Yan Gao,Elliott Wen,Ting Dang,Hong Jia*

Main category: cs.LG

TL;DR: 本文提出LengthBenchmark框架，系统研究输入长度对LLM困惑度评估的影响，发现滑动窗口评估会夸大短输入性能，且模型性能随评估片段长度增加而提升。


<details>
  <summary>Details</summary>
Motivation: 现有困惑度评估方法存在不可靠问题，尤其是在处理无关长输入时，但输入长度对评估的影响尚未从系统角度进行系统研究，也很少将输入长度作为影响公平性和效率的一等系统变量来对待。

Method: 提出LengthBenchmark框架，明确整合输入长度、评估协议设计和系统级成本，在变化上下文长度下使用两种评分协议（直接累积和固定窗口滑动）评估代表性LLM，同时测量延迟、内存占用和评估成本。

Result: 发现两个关键观察：(1)滑动窗口评估会持续夸大短输入的性能；(2)全精度和量化模型都随着评估片段长度的增加而表现出性能提升，表明长度偏差是普遍现象，会破坏跨模型公平比较。

Conclusion: 输入长度对LLM评估有系统性影响，长度偏差是普遍现象，需要在评估框架中明确考虑输入长度作为关键变量，以进行公平的跨模型比较和实际部署决策。

Abstract: Perplexity is a widely adopted metric for assessing the predictive quality of large language models (LLMs) and often serves as a reference metric for downstream evaluations. However, recent evidence shows that perplexity can be unreliable, especially when irrelevant long inputs are used, raising concerns for both benchmarking and system deployment. While prior efforts have employed selective input filtering and curated datasets, the impact of input length on perplexity has not been systematically studied from a systems perspective and input length has rarely been treated as a first-class system variable affecting both fairness and efficiency. In this work, we close this gap by introducing LengthBenchmark, a system-conscious evaluation framework that explicitly integrates input length, evaluation protocol design, and system-level costs, evaluating representative LLMs under two scoring protocols (direct accumulation and fixed window sliding) across varying context lengths. Unlike prior work that focuses solely on accuracy-oriented metrics, LengthBenchmark additionally measures latency, memory footprint, and evaluation cost, thereby linking predictive metrics to deployment realities. We further incorporate quantized variants not as a main contribution, but as robustness checks, showing that length-induced biases persist across both full-precision and compressed models. This design disentangles the effects of evaluation logic, quantization, and input length, and demonstrates that length bias is a general phenomenon that undermines fair cross-model comparison. Our analysis yields two key observations: (i) sliding window evaluation consistently inflates performance on short inputs, and (ii) both full-precision and quantized models appear to realise gains as the evaluated segment length grows.

</details>


### [81] [Supervised Learning as Lossy Compression: Characterizing Generalization and Sample Complexity via Finite Blocklength Analysis](https://arxiv.org/abs/2602.04107)
*Kosuke Sugiyama,Masato Uchida*

Main category: cs.LG

TL;DR: 该论文提出了一种基于信息论和有限块长分析的机器学习泛化新视角，将学习问题框架化为有损压缩过程，推导出样本复杂度和泛化误差的下界。


<details>
  <summary>Details</summary>
Motivation: 现有泛化理论框架通常无法清晰分离过拟合程度与归纳偏置不匹配这两个关键因素，需要一个新的理论框架来统一信息论界限和稳定性理论，并提供更精细的泛化分析。

Method: 采用信息论视角，将机器学习形式化为有损压缩问题：训练数据采样对应编码过程，模型构建对应解码过程。应用有限块长分析技术，为固定随机化学习算法及其最优采样策略推导样本复杂度和泛化误差下界。

Result: 推导出的界限明确地将学习算法的过拟合程度与其归纳偏置与任务不匹配程度分离为两个独立项，这种分离优于现有框架。同时将过拟合项分解，展示了其与信息论界限和稳定性理论中现有度量的理论联系。

Conclusion: 该研究提出了一个统一的信息论框架，能够更精细地分析机器学习泛化问题，清晰区分过拟合和归纳偏置不匹配，为理解泛化行为提供了新的理论工具。

Abstract: This paper presents a novel information-theoretic perspective on generalization in machine learning by framing the learning problem within the context of lossy compression and applying finite blocklength analysis. In our approach, the sampling of training data formally corresponds to an encoding process, and the model construction to a decoding process. By leveraging finite blocklength analysis, we derive lower bounds on sample complexity and generalization error for a fixed randomized learning algorithm and its associated optimal sampling strategy. Our bounds explicitly characterize the degree of overfitting of the learning algorithm and the mismatch between its inductive bias and the task as distinct terms. This separation provides a significant advantage over existing frameworks. Additionally, we decompose the overfitting term to show its theoretical connection to existing metrics found in information-theoretic bounds and stability theory, unifying these perspectives under our proposed framework.

</details>


### [82] [Rate-Optimal Noise Annealing in Semi-Dual Neural Optimal Transport: Tangential Identifiability, Off-Manifold Ambiguity, and Guaranteed Recovery](https://arxiv.org/abs/2602.04110)
*Raymond Chu,Jaewoong Choi,Dohyun Kwon*

Main category: cs.LG

TL;DR: 论文分析了半对偶神经最优传输在低维流形数据上的虚假解问题，提出了基于加性噪声平滑的解决方案，并推导了最优统计速率的可计算终止噪声水平。


<details>
  <summary>Details</summary>
Motivation: 半对偶神经最优传输通过最大-最小目标学习传输映射，但在低维流形数据上训练可能收敛到错误或退化解。需要理解这些虚假解的特性并找到有效的解决方案。

Method: 采用加性噪声平滑作为补救措施，分析最优计划的定量稳定性、平滑诱导偏差和有限样本误差，推导出由数据内在维度m控制的最优统计速率。

Result: 提出了可计算的终止噪声水平ε_stat(N)，该水平达到最优统计速率，并发现当噪声ε趋近于0时，约简的半对偶目标变得越来越病态。

Conclusion: 加性噪声平滑能有效解决虚假解问题，但需要设置适当的终止噪声水平。低于ε_stat(N)的退火会恶化优化条件而不改善统计精度，这提供了一个原则性的停止规则。

Abstract: Semi-dual neural optimal transport learns a transport map via a max-min objective, yet training can converge to incorrect or degenerate maps. We fully characterize these spurious solutions in the common regime where data concentrate on low-dimensional manifold: the objective is underconstrained off the data manifold, while the on-manifold transport signal remains identifiable. Following Choi, Choi, and Kwon (2025), we study additive-noise smoothing as a remedy and prove new map recovery guarantees as the noise vanishes. Our main practical contribution is a computable terminal noise level $\varepsilon_{\mathrm{stat}}(N)$ that attains the optimal statistical rate, with scaling governed by the intrinsic dimension $m$ of the data. The formula arises from a theoretical unified analysis of (i) quantitative stability of optimal plans, (ii) smoothing-induced bias, and (iii) finite-sample error, yielding rates that depend on $m$ rather than the ambient dimension. Finally, we show that the reduced semi-dual objective becomes increasingly ill-conditioned as $\varepsilon \downarrow 0$. This provides a principled stopping rule: annealing below $\varepsilon_{\mathrm{stat}}(N)$ can $\textit{worsen}$ optimization conditioning without improving statistical accuracy.

</details>


### [83] [Turning mechanistic models into forecasters by using machine learning](https://arxiv.org/abs/2602.04114)
*Amit K. Chakraborty,Hao Wang,Pouria Ramazi*

Main category: cs.LG

TL;DR: 提出一种结合时变参数的数据驱动微分方程发现方法，既能学习系统方程又能用于预测，相比传统时不变系数方法能更好捕捉动态演化


<details>
  <summary>Details</summary>
Motivation: 复杂动力系统的方程可能难以通过专家知识识别，特别是当底层机制未知时。传统数据驱动方法通常假设时不变系数，这限制了捕捉系统动态演化的能力

Method: 允许部分参数随时间变化，直接从数据中学习其时间演化，推断包含常参数和时变参数的系统方程。然后将此框架转化为预测模型，通过预测时变参数并将其代入学习到的方程中进行预测

Result: 在SIR、消费者-资源、温室气体浓度和蓝藻细胞计数数据集上验证，动态适应时间变化，学习时间序列的平均绝对误差低于3%，提前一个月预测的误差低于6%。相比CNN-LSTM和梯度提升机，在大多数数据集上表现更优

Conclusion: 将时变参数整合到数据驱动的微分方程发现中，既能提高建模精度，又能提升预测性能，能更好地捕捉系统动态演化

Abstract: The equations of complex dynamical systems may not be identified by expert knowledge, especially if the underlying mechanisms are unknown. Data-driven discovery methods address this challenge by inferring governing equations from time-series data using a library of functions constructed from the measured variables. However, these methods typically assume time-invariant coefficients, which limits their ability to capture evolving system dynamics. To overcome this limitation, we allow some of the parameters to vary over time, learn their temporal evolution directly from data, and infer a system of equations that incorporates both constant and time-varying parameters. We then transform this framework into a forecasting model by predicting the time-varying parameters and substituting these predictions into the learned equations. The model is validated using datasets for Susceptible-Infected-Recovered, Consumer--Resource, greenhouse gas concentration, and Cyanobacteria cell count. By dynamically adapting to temporal shifts, our proposed model achieved a mean absolute error below 3\% for learning a time series and below 6\% for forecasting up to a month ahead. We additionally compare forecasting performance against CNN-LSTM and Gradient Boosting Machine (GBM), and show that our model outperforms these methods across most datasets. Our findings demonstrate that integrating time-varying parameters into data-driven discovery of differential equations improves both modeling accuracy and forecasting performance.

</details>


### [84] [Toward Effective Multimodal Graph Foundation Model: A Divide-and-Conquer Based Approach](https://arxiv.org/abs/2602.04116)
*Sicheng Liu,Xunkai Li,Daohan Su,Ru Zhang,Hongchao Qin,Ronghua Li,Guoren Wang*

Main category: cs.LG

TL;DR: PLANET提出了一种新的多模态图基础模型框架，通过分治策略解决现有方法在模态交互和对齐方面的不足，显著提升了多模态图任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有图基础模型主要关注文本属性图，而多模态属性图潜力未充分挖掘。现有多模态图基础模型存在两个根本缺陷：1) 未能显式建模模态交互，无法捕捉复杂的跨模态语义；2) 模态对齐效果不佳，难以弥合不同模态空间间的语义鸿沟。

Method: PLANET采用分治策略，在不同粒度上解耦模态交互和对齐：1) 嵌入粒度上，使用嵌入级域门控(EDG)通过自适应注入拓扑感知的跨模态上下文实现局部语义增强和模态交互；2) 节点粒度上，使用节点级离散化检索(NDR)构建离散化语义表示空间(DSRS)确保全局模态对齐。

Result: 大量实验表明，PLANET在多种图中心任务和多模态生成任务上显著优于现有最先进基线方法。

Conclusion: PLANET通过创新的分治策略有效解决了多模态图基础模型中的模态交互和对齐问题，为多模态属性图的分析和应用提供了强大框架。

Abstract: Graph Foundation Models (GFMs) have achieved remarkable success in generalizing across diverse domains. However, they mainly focus on Text-Attributed Graphs (TAGs), leaving Multimodal-Attributed Graphs (MAGs) largely untapped. Developing Multimodal Graph Foundation Models (MGFMs) allows for leveraging the rich multimodal information in MAGs, and extends applicability to broader types of downstream tasks. While recent MGFMs integrate diverse modality information, our empirical investigation reveals two fundamental limitations of existing MGFMs: (1)they fail to explicitly model modality interaction, essential for capturing intricate cross-modal semantics beyond simple aggregation, and (2)they exhibit sub-optimal modality alignment, which is critical for bridging the significant semantic disparity between distinct modal spaces. To address these challenges, we propose PLANET (graPh topoLogy-aware modAlity iNteraction and alignmEnT), a novel framework employing a Divide-and-Conquer strategy to decouple modality interaction and alignment across distinct granularities. At the embedding granularity, (1)Embedding-wise Domain Gating (EDG) performs local semantic enrichment by adaptively infusing topology-aware cross-modal context, achieving modality interaction. At the node granularity, (2)Node-wise Discretization Retrieval (NDR) ensures global modality alignment by constructing a Discretized Semantic Representation Space (DSRS) to bridge modality gaps. Extensive experiments demonstrate that PLANET significantly outperforms state-of-the-art baselines across diverse graph-centric and multimodal generative tasks.

</details>


### [85] [Learning to Reason in 13 Parameters](https://arxiv.org/abs/2602.04118)
*John X. Morris,Niloofar Mireshghallah,Mark Ibrahim,Saeed Mahloujifar*

Main category: cs.LG

TL;DR: TinyLoRA：一种超低秩适配器方法，仅用13个参数就能让8B模型在GSM8K上达到91%准确率，比传统LoRA参数少1000倍


<details>
  <summary>Details</summary>
Motivation: 传统LoRA无法扩展到低于模型维度的秩，作者质疑是否真的需要秩=1的LoRA来学习推理能力，因此探索更极端的低秩参数化方法

Method: 提出TinyLoRA方法，将低秩适配器扩展到小至单个参数的规模，在强化学习框架下训练超低秩参数

Result: 仅用13个参数（26字节）就能让8B Qwen2.5在GSM8K上达到91%准确率；在AIME、AMC、MATH500等更难基准上，用1000倍更少参数恢复90%性能提升；RL训练效果远超SFT

Conclusion: TinyLoRA展示了极端低秩参数化的可行性，强化学习是实现这种高效参数利用的关键，为模型推理能力的高效学习提供了新方向

Abstract: Recent research has shown that language models can learn to \textit{reason}, often via reinforcement learning. Some work even trains low-rank parameterizations for reasoning, but conventional LoRA cannot scale below the model dimension. We question whether even rank=1 LoRA is necessary for learning to reason and propose TinyLoRA, a method for scaling low-rank adapters to sizes as small as one parameter. Within our new parameterization, we are able to train the 8B parameter size of Qwen2.5 to 91\% accuracy on GSM8K with only 13 trained parameters in bf16 (26 total bytes). We find this trend holds in general: we are able to recover 90\% of performance improvements while training $1000x$ fewer parameters across a suite of more difficult learning-to-reason benchmarks such as AIME, AMC, and MATH500. Notably, we are only able to achieve such strong performance with RL: models trained using SFT require $100-1000x$ larger updates to reach the same performance.

</details>


### [86] [Synthesizable Molecular Generation via Soft-constrained GFlowNets with Rich Chemical Priors](https://arxiv.org/abs/2602.04119)
*Hyeonah Kim,Minsu Kim,Celine Roget,Dionessa Biton,Louis Vaillancourt,Yves V. Brun,Yoshua Bengio,Alex Hernandez-Garcia*

Main category: cs.LG

TL;DR: S3-GFN：通过软正则化的序列GFlowNet生成可合成SMILES分子，避免硬约束限制


<details>
  <summary>Details</summary>
Motivation: 现有基于反应模板和构建块的GFlowNet方法在药物发现中缺乏灵活性和可扩展性，硬约束限制了分子生成能力

Method: 提出S3-GFN，通过软正则化序列GFlowNet，利用大规模SMILES语料库学习分子先验，通过对比学习信号进行离策略回放训练

Result: S3-GFN能生成高可合成性分子（≥95%），在多样任务中获得更高奖励

Conclusion: 软正则化方法比硬约束更灵活有效，能引导分子生成到高奖励、可合成的化学空间

Abstract: The application of generative models for experimental drug discovery campaigns is severely limited by the difficulty of designing molecules de novo that can be synthesized in practice. Previous works have leveraged Generative Flow Networks (GFlowNets) to impose hard synthesizability constraints through the design of state and action spaces based on predefined reaction templates and building blocks. Despite the promising prospects of this approach, it currently lacks flexibility and scalability. As an alternative, we propose S3-GFN, which generates synthesizable SMILES molecules via simple soft regularization of a sequence-based GFlowNet. Our approach leverages rich molecular priors learned from large-scale SMILES corpora to steer molecular generation towards high-reward, synthesizable chemical spaces. The model induces constraints through off-policy replay training with a contrastive learning signal based on separate buffers of synthesizable and unsynthesizable samples. Our experiments show that S3-GFN learns to generate synthesizable molecules ($\geq 95\%$) with higher rewards in diverse tasks.

</details>


### [87] [Scalable Explainability-as-a-Service (XaaS) for Edge AI Systems](https://arxiv.org/abs/2602.04120)
*Samaresh Kumar Singh,Joyjit Roy*

Main category: cs.LG

TL;DR: 提出Explainability-as-a-Service (XaaS)分布式架构，将可解释性作为系统服务而非模型特定功能，通过解耦推理与解释生成来优化边缘AI系统的效率。


<details>
  <summary>Details</summary>
Motivation: 当前XAI在边缘和物联网系统中的集成通常是临时且低效的，现有方法将解释生成与模型推理耦合，导致冗余计算、高延迟和可扩展性差的问题。

Method: 提出XaaS架构，包含三个创新：(1)基于语义相似性的分布式解释缓存系统；(2)轻量级验证协议确保缓存和新生成解释的保真度；(3)自适应解释引擎根据设备能力和用户需求选择解释方法。

Result: 在三个真实边缘AI用例（制造质量控制、自动驾驶感知、医疗诊断）上评估，XaaS将延迟降低38%，同时保持高质量的解释输出。

Conclusion: XaaS实现了透明和可问责AI在大规模异构物联网系统中的部署，弥合了XAI研究与边缘实践之间的差距。

Abstract: Though Explainable AI (XAI) has made significant advancements, its inclusion in edge and IoT systems is typically ad-hoc and inefficient. Most current methods are "coupled" in such a way that they generate explanations simultaneously with model inferences. As a result, these approaches incur redundant computation, high latency and poor scalability when deployed across heterogeneous sets of edge devices. In this work we propose Explainability-as-a-Service (XaaS), a distributed architecture for treating explainability as a first-class system service (as opposed to a model-specific feature). The key innovation in our proposed XaaS architecture is that it decouples inference from explanation generation allowing edge devices to request, cache and verify explanations subject to resource and latency constraints. To achieve this, we introduce three main innovations: (1) A distributed explanation cache with a semantic similarity based explanation retrieval method which significantly reduces redundant computation; (2) A lightweight verification protocol that ensures the fidelity of both cached and newly generated explanations; and (3) An adaptive explanation engine that chooses explanation methods based upon device capability and user requirement. We evaluated the performance of XaaS on three real-world edge-AI use cases: (i) manufacturing quality control; (ii) autonomous vehicle perception; and (iii) healthcare diagnostics. Experimental results show that XaaS reduces latency by 38\% while maintaining high explanation quality across three real-world deployments. Overall, this work enables the deployment of transparent and accountable AI across large scale, heterogeneous IoT systems, and bridges the gap between XAI research and edge-practicality.

</details>


### [88] [Decoupling Time and Risk: Risk-Sensitive Reinforcement Learning with General Discounting](https://arxiv.org/abs/2602.04131)
*Mehrdad Moghimi,Anthony Coache,Hyejin Ku*

Main category: cs.LG

TL;DR: 提出支持灵活折扣和风险度量的分布强化学习新框架，解决传统指数折扣的局限性


<details>
  <summary>Details</summary>
Motivation: 传统分布强化学习中折扣因子常被忽视，通常作为固定参数处理，但折扣函数对表征智能体时间偏好至关重要，指数折扣无法完全捕捉复杂的时间偏好

Method: 提出支持未来奖励灵活折扣和风险度量优化的分布强化学习新框架，包含多时间范围扩展以解决现有方法的问题

Result: 通过广泛实验验证了方法的鲁棒性，证明灵活折扣在捕捉更丰富的时间和风险偏好方面的重要性

Conclusion: 折扣是决策问题中捕捉更丰富时间和风险偏好的关键因素，对现实世界安全关键应用具有潜在影响

Abstract: Distributional reinforcement learning (RL) is a powerful framework increasingly adopted in safety-critical domains for its ability to optimize risk-sensitive objectives. However, the role of the discount factor is often overlooked, as it is typically treated as a fixed parameter of the Markov decision process or tunable hyperparameter, with little consideration of its effect on the learned policy. In the literature, it is well-known that the discounting function plays a major role in characterizing time preferences of an agent, which an exponential discount factor cannot fully capture. Building on this insight, we propose a novel framework that supports flexible discounting of future rewards and optimization of risk measures in distributional RL. We provide a technical analysis of the optimality of our algorithms, show that our multi-horizon extension fixes issues raised with existing methodologies, and validate the robustness of our methods through extensive experiments. Our results highlight that discounting is a cornerstone in decision-making problems for capturing more expressive temporal and risk preferences profiles, with potential implications for real-world safety-critical applications.

</details>


### [89] [Training Data Efficiency in Multimodal Process Reward Models](https://arxiv.org/abs/2602.04145)
*Jinyuan Li,Chengsong Huang,Langlin Huang,Shaoyang Xu,Haolin Liu,Wenxuan Zhang,Jiaxin Huang*

Main category: cs.LG

TL;DR: 本文提出平衡信息分数（BIS）方法，通过优先考虑正负步骤的标签混合和标签可靠性，仅用10%的训练数据就能达到全数据性能，显著提升多模态过程奖励模型的数据效率。


<details>
  <summary>Details</summary>
Motivation: 训练多模态过程奖励模型（MPRMs）通常需要大规模蒙特卡洛标注语料，成本高昂。研究发现MPRM训练在随机子采样下很快饱和，表明现有标注语料存在大量冗余，需要提高数据效率。

Method: 提出平衡信息分数（BIS）方法，基于理论框架分析发现信息梯度更新取决于两个因素：正负步骤的标签混合和标签可靠性（正步骤的平均MC分数）。BIS在rollout级别优先考虑这两个因素，无需额外成本。

Result: 在两个骨干模型（InternVL2.5-8B和Qwen2.5-VL-7B）上，BIS选择的子集在VisualProcessBench上仅用10%训练数据就能达到全数据性能，相对随机子采样提升4.1%。

Conclusion: BIS方法通过智能数据选择显著提高MPRM训练的数据效率，仅需少量数据就能达到全数据性能，为视觉推理中的步骤级监督提供了高效解决方案。

Abstract: Multimodal Process Reward Models (MPRMs) are central to step-level supervision for visual reasoning in MLLMs. Training MPRMs typically requires large-scale Monte Carlo (MC)-annotated corpora, incurring substantial training cost. This paper studies the data efficiency for MPRM training.Our preliminary experiments reveal that MPRM training quickly saturates under random subsampling of the training data, indicating substantial redundancy within existing MC-annotated corpora.To explain this, we formalize a theoretical framework and reveal that informative gradient updates depend on two factors: label mixtures of positive/negative steps and label reliability (average MC scores of positive steps). Guided by these insights, we propose the Balanced-Information Score (BIS), which prioritizes both mixture and reliability based on existing MC signals at the rollout level, without incurring any additional cost. Across two backbones (InternVL2.5-8B and Qwen2.5-VL-7B) on VisualProcessBench, BIS-selected subsets consistently match and even surpass the full-data performance at small fractions. Notably, the BIS subset reaches full-data performance using only 10% of the training data, improving over random subsampling by a relative 4.1%.

</details>


### [90] [Pruning for Generalization: A Transfer-Oriented Spatiotemporal Graph Framework](https://arxiv.org/abs/2602.04153)
*Zihao Jing,Yuxi Long,Ganlin Feng*

Main category: cs.LG

TL;DR: TL-GPSTGN：一种通过结构感知上下文选择增强样本效率和跨域泛化的图时空预测框架


<details>
  <summary>Details</summary>
Motivation: 现有图时空模型在数据稀缺和跨域偏移情况下性能下降，需要提高样本效率和跨域泛化能力

Method: 采用信息论和相关性准则选择性剪枝非优化图上下文，提取结构信息子图和特征，集成到时空卷积架构

Result: 在大规模交通基准测试中，TL-GPSTGN在低数据迁移场景下持续优于基线方法

Conclusion: 显式上下文剪枝作为强大的归纳偏置，能提升基于图的预测模型的鲁棒性

Abstract: Multivariate time series forecasting in graph-structured domains is critical for real-world applications, yet existing spatiotemporal models often suffer from performance degradation under data scarcity and cross-domain shifts. We address these challenges through the lens of structure-aware context selection. We propose TL-GPSTGN, a transfer-oriented spatiotemporal framework that enhances sample efficiency and out-of-distribution generalization by selectively pruning non-optimized graph context. Specifically, our method employs information-theoretic and correlation-based criteria to extract structurally informative subgraphs and features, resulting in a compact, semantically grounded representation. This optimized context is subsequently integrated into a spatiotemporal convolutional architecture to capture complex multivariate dynamics. Evaluations on large-scale traffic benchmarks demonstrate that TL-GPSTGN consistently outperforms baselines in low-data transfer scenarios. Our findings suggest that explicit context pruning serves as a powerful inductive bias for improving the robustness of graph-based forecasting models.

</details>


### [91] [BPDQ: Bit-Plane Decomposition Quantization on a Variable Grid for Large Language Models](https://arxiv.org/abs/2602.04163)
*Junyu Chen,Jungang Li,Jing Xiong,Wenjie Wang,Qingyao Yang,He Xiao,Zhen Li,Taiqiang Wu,Mengzhao Chen,Zhen Peng,Chaofan Tao,Long Shi,Hongxia Yang,Ngai Wong*

Main category: cs.LG

TL;DR: BPDQ是一种新的量化方法，通过位平面分解和可变量化网格，在2-3位低比特量化中保持LLM推理精度，使Qwen2.5-72B能在单张RTX 3090上运行。


<details>
  <summary>Details</summary>
Motivation: 现有后训练量化方法在4位时效果良好，但在2-3位低比特量化时性能显著下降。根本原因是传统方法使用固定均匀间隔的量化网格，限制了误差最小化的可行解空间。

Method: 提出位平面分解量化(BPDQ)，通过位平面和标量系数构建可变量化网格，利用近似二阶信息迭代优化，并逐步补偿量化误差以最小化输出差异。

Result: 在2位量化下，BPDQ能使Qwen2.5-72B在单张RTX 3090上运行，GSM8K准确率达到83.85%（16位时为90.83%）。理论分析表明可变网格扩展了可行解空间，量化过程与Hessian诱导几何中的优化目标一致。

Conclusion: BPDQ通过可变量化网格和迭代优化，显著提升了低比特量化的性能，为资源受限环境中的大语言模型高效推理提供了有效解决方案。

Abstract: Large language model (LLM) inference is often bounded by memory footprint and memory bandwidth in resource-constrained deployments, making quantization a fundamental technique for efficient serving. While post-training quantization (PTQ) maintains high fidelity at 4-bit, it deteriorates at 2-3 bits. Fundamentally, existing methods enforce a shape-invariant quantization grid (e.g., the fixed uniform intervals of UINT2) for each group, severely restricting the feasible set for error minimization. To address this, we propose Bit-Plane Decomposition Quantization (BPDQ), which constructs a variable quantization grid via bit-planes and scalar coefficients, and iteratively refines them using approximate second-order information while progressively compensating quantization errors to minimize output discrepancy. In the 2-bit regime, BPDQ enables serving Qwen2.5-72B on a single RTX 3090 with 83.85% GSM8K accuracy (vs. 90.83% at 16-bit). Moreover, we provide theoretical analysis showing that the variable grid expands the feasible set, and that the quantization process consistently aligns with the optimization objective in Hessian-induced geometry. Code: github.com/KingdalfGoodman/BPDQ.

</details>


### [92] [Topology-Aware Revival for Efficient Sparse Training](https://arxiv.org/abs/2602.04166)
*Meiling Jin,Fei Wang,Xiaoyun Yuan,Chen Qian,Yuan Cheng*

Main category: cs.LG

TL;DR: TAR是一种轻量级的一次性后剪枝方法，通过拓扑感知的复活步骤改善静态稀疏训练，在强化学习中显著提升性能


<details>
  <summary>Details</summary>
Motivation: 静态稀疏训练虽然高效，但固定的掩码模式限制了鲁棒性。早期剪枝决策会将网络锁定在脆弱结构中，难以调整，特别是在深度强化学习中，不断演化的策略会持续改变训练分布

Method: TAR在静态剪枝后执行一次性复活步骤：1）根据拓扑需求在各层分配小规模预算；2）在每层内随机均匀地重新激活一些先前被剪枝的连接；3）保持结果连接性固定进行后续训练

Result: 在多个连续控制任务中使用SAC和TD3算法，TAR相比静态稀疏基线最终回报提升高达+37.9%，相比动态稀疏训练基线中位数增益为+13.5%

Conclusion: TAR通过轻量级的一次性拓扑感知复活步骤，有效改善了静态稀疏训练的鲁棒性和性能，在强化学习中取得了显著优势

Abstract: Static sparse training is a promising route to efficient learning by committing to a fixed mask pattern, yet the constrained structure reduces robustness. Early pruning decisions can lock the network into a brittle structure that is difficult to escape, especially in deep reinforcement learning (RL) where the evolving policy continually shifts the training distribution. We propose Topology-Aware Revival (TAR), a lightweight one-shot post-pruning procedure that improves static sparsity without dynamic rewiring. After static pruning, TAR performs a single revival step by allocating a small reserve budget across layers according to topology needs, randomly uniformly reactivating a few previously pruned connections within each layer, and then keeping the resulting connectivity fixed for the remainder of training. Across multiple continuous-control tasks with SAC and TD3, TAR improves final return over static sparse baselines by up to +37.9% and also outperforms dynamic sparse training baselines with a median gain of +13.5%.

</details>


### [93] [Benchmarking Uncertainty Quantification of Plug-and-Play Diffusion Priors for Inverse Problems Solving](https://arxiv.org/abs/2602.04189)
*Xiaoyu Qiu,Taewon Yang,Zhanhao Liu,Guanyang Wang,Liyue Shen*

Main category: cs.LG

TL;DR: 该论文系统评估了即插即用扩散先验（PnPDP）方法在逆问题求解中的不确定性量化能力，提出了基于不确定性的分类方法，并通过实验验证了分类的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前对PnPDP重建质量的评估主要关注单样本的点估计精度指标，忽略了逆问题的随机性和内在不确定性，这与科学任务中需要后验分布输出的实际需求存在根本性不匹配。

Method: 设计了严格的玩具模型仿真来评估各种PnPDP求解器的不确定性行为，提出了基于不确定性量化的分类方法，并在玩具仿真和多样化的真实世界科学逆问题上进行了广泛实验。

Result: 实验观察到的不确定性行为与提出的分类方法和理论解释一致，为评估和理解PnPDP的不确定性提供了新的见解。

Conclusion: 该研究填补了PnPDP不确定性量化评估的空白，提出的分类框架和评估方法有助于更好地理解和评估扩散逆求解器的不确定性特性。

Abstract: Plug-and-play diffusion priors (PnPDP) have become a powerful paradigm for solving inverse problems in scientific and engineering domains. Yet, current evaluations of reconstruction quality emphasize point-estimate accuracy metrics on a single sample, which do not reflect the stochastic nature of PnPDP solvers and the intrinsic uncertainty of inverse problems, critical for scientific tasks. This creates a fundamental mismatch: in inverse problems, the desired output is typically a posterior distribution and most PnPDP solvers induce a distribution over reconstructions, but existing benchmarks only evaluate a single reconstruction, ignoring distributional characterization such as uncertainty. To address this gap, we conduct a systematic study to benchmark the uncertainty quantification (UQ) of existing diffusion inverse solvers. Specifically, we design a rigorous toy model simulation to evaluate the uncertainty behavior of various PnPDP solvers, and propose a UQ-driven categorization. Through extensive experiments on toy simulations and diverse real-world scientific inverse problems, we observe uncertainty behaviors consistent with our taxonomy and theoretical justification, providing new insights for evaluating and understanding the uncertainty for PnPDPs.

</details>


### [94] [LORE: Jointly Learning the Intrinsic Dimensionality and Relative Similarity Structure From Ordinal Data](https://arxiv.org/abs/2602.04192)
*Vivek Anand,Alec Helbling,Mark Davenport,Gordon Berman,Sankar Alagapan,Christopher Rozell*

Main category: cs.LG

TL;DR: LORE是一个从三元组比较数据中联合学习内在维度和序数嵌入的可扩展框架，使用非凸Schatten-p拟范数进行正则化，无需预先设定嵌入维度。


<details>
  <summary>Details</summary>
Motivation: 从序数数据（如"是A更相似于B还是C？"的三元组比较）中学习主观感知空间（如味觉、嗅觉、美学）的内在维度是一个挑战性问题。现有方法需要预先设定嵌入维度，而无法自动推断内在维度。

Method: 提出LORE框架，使用非凸Schatten-p拟范数进行正则化，通过迭代重加权算法优化联合目标函数，同时学习序数嵌入和其内在维度，并建立了收敛性保证。

Result: 在合成数据集、模拟感知空间和真实世界众包序数判断上的广泛实验表明，LORE能够学习紧凑、可解释且高精度的低维嵌入，恢复主观感知的潜在几何结构。

Conclusion: LORE通过同时推断内在维度和序数嵌入，为心理物理学提供了更可解释和数据高效的感知建模，并为从序数数据中可扩展地发现低维结构开辟了新方向。

Abstract: Learning the intrinsic dimensionality of subjective perceptual spaces such as taste, smell, or aesthetics from ordinal data is a challenging problem. We introduce LORE (Low Rank Ordinal Embedding), a scalable framework that jointly learns both the intrinsic dimensionality and an ordinal embedding from noisy triplet comparisons of the form, "Is A more similar to B than C?". Unlike existing methods that require the embedding dimension to be set apriori, LORE regularizes the solution using the nonconvex Schatten-$p$ quasi norm, enabling automatic joint recovery of both the ordinal embedding and its dimensionality. We optimize this joint objective via an iteratively reweighted algorithm and establish convergence guarantees. Extensive experiments on synthetic datasets, simulated perceptual spaces, and real world crowdsourced ordinal judgements show that LORE learns compact, interpretable and highly accurate low dimensional embeddings that recover the latent geometry of subjective percepts. By simultaneously inferring both the intrinsic dimensionality and ordinal embeddings, LORE enables more interpretable and data efficient perceptual modeling in psychophysics and opens new directions for scalable discovery of low dimensional structure from ordinal data in machine learning.

</details>


### [95] [From Sparse Sensors to Continuous Fields: STRIDE for Spatiotemporal Reconstruction](https://arxiv.org/abs/2602.04201)
*Yanjie Tong,Peng Chen*

Main category: cs.LG

TL;DR: STRIDE是一个两阶段框架，通过时间编码器将稀疏传感器测量映射到潜在状态，并使用调制隐式神经表示解码器在任意查询位置重建时空场，在稀疏传感下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 从稀疏点传感器测量重建高维时空场是学习参数化PDE动力学的核心挑战。现有方法难以在不同轨迹和参数设置间泛化，或依赖于与离散化绑定的解码器，无法自然地在不同网格和分辨率间迁移。

Method: 提出STRIDE两阶段框架：1) 时间编码器将短窗口传感器测量映射到潜在状态；2) 调制隐式神经表示（INR）解码器在任意查询位置重建场。使用FMMNN作为INR骨干，比基于正弦的INR能更好地表示复杂空间场并提供更稳定的优化。

Result: 在四个涵盖混沌动力学和波传播的挑战性基准测试中，STRIDE在极端稀疏传感下优于强基线，支持超分辨率，并对噪声保持鲁棒性。

Conclusion: STRIDE通过将时间编码与调制隐式神经表示相结合，为从稀疏传感器测量重建时空场提供了有效的解决方案，具有理论依据并在实验中表现出优越性能。

Abstract: Reconstructing high-dimensional spatiotemporal fields from sparse point-sensor measurements is a central challenge in learning parametric PDE dynamics. Existing approaches often struggle to generalize across trajectories and parameter settings, or rely on discretization-tied decoders that do not naturally transfer across meshes and resolutions. We propose STRIDE (Spatio-Temporal Recurrent Implicit DEcoder), a two-stage framework that maps a short window of sensor measurements to a latent state with a temporal encoder and reconstructs the field at arbitrary query locations with a modulated implicit neural representation (INR) decoder. Using the Fourier Multi-Component and Multi-Layer Neural Network (FMMNN) as the INR backbone improves representation of complex spatial fields and yields more stable optimization than sine-based INRs. We provide a conditional theoretical justification: under stable delay observability of point measurements on a low-dimensional parametric invariant set, the reconstruction operator factors through a finite-dimensional embedding, making STRIDE-type architectures natural approximators. Experiments on four challenging benchmarks spanning chaotic dynamics and wave propagation show that STRIDE outperforms strong baselines under extremely sparse sensing, supports super-resolution, and remains robust to noise.

</details>


### [96] [RAPO: Risk-Aware Preference Optimization for Generalizable Safe Reasoning](https://arxiv.org/abs/2602.04224)
*Zeming Wei,Qiaosheng Zhang,Xia Hu,Xingcheng Xu*

Main category: cs.LG

TL;DR: 本文提出RAPO框架，通过风险感知的偏好优化，让大型推理模型能够自适应地识别和处理安全风险，提升对复杂越狱攻击的防御能力。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型虽然具备链式思维推理能力，但仍面临与基础语言模型类似的安全问题。现有的安全拒绝机制在面对多样化和复杂的越狱攻击时泛化能力不足，需要更充分的安全推理过程来防御高级攻击提示。

Method: 提出风险感知偏好优化（RAPO）框架，使LRM能够自适应地识别安全风险，并在思维内容中以适当的粒度处理这些风险。该方法通过理论分析和实证证据支持更充分的安全推理过程的必要性。

Result: 大量实验表明，RAPO成功使多个LRM能够自适应地将安全推理泛化到不同的攻击提示，同时保持通用效用，为LRM安全提供了鲁棒的对齐技术。

Conclusion: RAPO框架通过风险感知的偏好优化，有效提升了大型推理模型对复杂越狱攻击的防御能力，同时保持了模型的通用性能，为解决LRM安全问题提供了有效的对齐技术。

Abstract: Large Reasoning Models (LRMs) have achieved tremendous success with their chain-of-thought (CoT) reasoning, yet also face safety issues similar to those of basic language models. In particular, while algorithms are designed to guide them to deliberately refuse harmful prompts with safe reasoning, this process often fails to generalize against diverse and complex jailbreak attacks. In this work, we attribute these failures to the generalization of the safe reasoning process, particularly their insufficiency against complex attack prompts. We provide both theoretical and empirical evidence to show the necessity of a more sufficient safe reasoning process to defend against advanced attack prompts. Building on this insight, we propose a Risk-Aware Preference Optimization (RAPO) framework that enables LRM to adaptively identify and address the safety risks with appropriate granularity in its thinking content. Extensive experiments demonstrate that RAPO successfully generalizes multiple LRMs' safe reasoning adaptively across diverse attack prompts whilst preserving general utility, contributing a robust alignment technique for LRM safety. Our code is available at https://github.com/weizeming/RAPO.

</details>


### [97] [Cascading Robustness Verification: Toward Efficient Model-Agnostic Certification](https://arxiv.org/abs/2602.04236)
*Mohammadreza Maleki,Rushendra Sidibomma,Arman Adibi,Reza Samavi*

Main category: cs.LG

TL;DR: 提出级联鲁棒性验证框架CRV，通过多验证器级联提升对抗样本验证的可靠性和效率


<details>
  <summary>Details</summary>
Motivation: 现有神经网络鲁棒性验证方法存在局限性：单一验证器可能因近似松弛或与训练方法不匹配而低估鲁棒性；完整验证器计算成本高，不完整验证器虽高效但可能不准确

Method: 提出级联鲁棒性验证框架CRV：1) 模型无关验证器，不依赖训练过程；2) 多验证器级联，从最便宜方法开始，一旦认证为鲁棒即停止；3) 引入逐步松弛算法SR，对昂贵方法增量添加约束，避免不必要计算

Result: 理论分析显示CRV达到与级联中强大但计算昂贵的验证器相同或更高的验证准确率；实证结果表明CRV认证至少与基准方法相同数量的输入，同时将运行时效率提升高达90%

Conclusion: CRV框架通过多验证器级联策略，在保持高验证准确率的同时显著降低计算开销，为神经网络对抗鲁棒性验证提供了可靠高效的解决方案

Abstract: Certifying neural network robustness against adversarial examples is challenging, as formal guarantees often require solving non-convex problems. Hence, incomplete verifiers are widely used because they scale efficiently and substantially reduce the cost of robustness verification compared to complete methods. However, relying on a single verifier can underestimate robustness because of loose approximations or misalignment with training methods. In this work, we propose Cascading Robustness Verification (CRV), which goes beyond an engineering improvement by exposing fundamental limitations of existing robustness metric and introducing a framework that enhances both reliability and efficiency. CRV is a model-agnostic verifier, meaning that its robustness guarantees are independent of the model's training process. The key insight behind the CRV framework is that, when using multiple verification methods, an input is certifiably robust if at least one method certifies it as robust. Rather than relying solely on a single verifier with a fixed constraint set, CRV progressively applies multiple verifiers to balance the tightness of the bound and computational cost. Starting with the least expensive method, CRV halts as soon as an input is certified as robust; otherwise, it proceeds to more expensive methods. For computationally expensive methods, we introduce a Stepwise Relaxation Algorithm (SR) that incrementally adds constraints and checks for certification at each step, thereby avoiding unnecessary computation. Our theoretical analysis demonstrates that CRV achieves equal or higher verified accuracy compared to powerful but computationally expensive incomplete verifiers in the cascade, while significantly reducing verification overhead. Empirical results confirm that CRV certifies at least as many inputs as benchmark approaches, while improving runtime efficiency by up to ~90%.

</details>


### [98] [Training A Foundation Model to Represent Graphs as Vectors](https://arxiv.org/abs/2602.04244)
*Qi Feng,Jicong Fan*

Main category: cs.LG

TL;DR: 该论文提出了一种图基础模型，通过多图特征对齐和密度最大化均值对齐算法，学习跨领域图的通用表示，在少样本图分类和图聚类任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 训练一个能够将任何图表示为向量的图基础模型，保留对下游图级任务（如图分类和图聚类）有用的结构和语义信息，同时保持对新领域的强泛化能力。

Method: 1. 多图特征对齐方法：利用每个数据集中所有节点的属性构建加权图，生成一致的节点嵌入；2. 密度最大化均值对齐算法：增强不同数据集特征的一致性；3. 对比学习框架：将原始图和生成的节点嵌入输入图神经网络获得判别性图表示；4. 多层参考分布模块：无需池化操作，增强从节点级表示到图级表示的信息保留；5. 提供理论泛化界支持模型有效性。

Result: 实验结果表明，在少样本图分类和图聚类任务中，该模型优于强基线方法。

Conclusion: 提出的图基础模型能够有效学习跨领域图的通用表示，通过创新的特征对齐和表示学习技术，在少样本场景下展现出优越性能，并有理论保证。

Abstract: This paper aims to train a graph foundation model that is able to represent any graph as a vector preserving structural and semantic information useful for downstream graph-level tasks such as graph classification and graph clustering. To learn the features of graphs from diverse domains while maintaining strong generalization ability to new domains, we propose a multi-graph-based feature alignment method, which constructs weighted graphs using the attributes of all nodes in each dataset and then generates consistent node embeddings. To enhance the consistency of the features from different datasets, we propose a density maximization mean alignment algorithm with guaranteed convergence. The original graphs and generated node embeddings are fed into a graph neural network to achieve discriminative graph representations in contrastive learning. More importantly, to enhance the information preservation from node-level representations to the graph-level representation, we construct a multi-layer reference distribution module without using any pooling operation. We also provide a theoretical generalization bound to support the effectiveness of the proposed model. The experimental results of few-shot graph classification and graph clustering show that our model outperforms strong baselines.

</details>


### [99] [From Ambiguity to Action: A POMDP Perspective on Partial Multi-Label Ambiguity and Its Horizon-One Resolution](https://arxiv.org/abs/2602.04255)
*Hanlin Pan,Yuhao Tang,Wanfu Gao*

Main category: cs.LG

TL;DR: 提出POMDP框架解决部分多标签学习中的标签消歧问题，通过强化学习生成高质量伪标签并进行特征选择


<details>
  <summary>Details</summary>
Motivation: 部分多标签学习中真实标签不可观测，模糊的候选标签会将错误传播到下游任务如特征工程中，需要解决标签消歧难题

Method: 将标签消歧和特征选择联合建模为部分可观测马尔可夫决策过程，第一阶段通过强化学习训练transformer策略生成高质量硬伪标签，第二阶段将特征选择建模为序列强化学习问题，逐步选择特征并输出可解释的全局排名

Result: 提供了PML-POMDP对应关系的理论分析和超额风险界，将误差分解为伪标签质量项和样本大小项，在多个指标和数据集上的实验验证了框架优势

Conclusion: 提出的POMDP框架有效解决了部分多标签学习中的标签消歧问题，通过强化学习方法生成高质量伪标签并进行可解释的特征选择，理论分析和实验验证了方法的有效性

Abstract: In partial multi-label learning (PML), the true labels are unobserved, which makes label disambiguation important but difficult. A key challenge is that ambiguous candidate labels can propagate errors into downstream tasks such as feature engineering. To solve this issue, we jointly model the disambiguation and feature selection tasks as Partially Observable Markov Decision Processes (POMDP) to turn PML risk minimization into expected-return maximization. Stage 1 trains a transformer policy via reinforcement learning to produce high-quality hard pseudo-labels; Stage 2 describes feature selection as a sequential reinforcement learning problem, selecting features step by step and outputting an interpretable global ranking. We further provide the theoretical analysis of PML-POMDP correspondence and the excess-risk bound that decompose the error into pseudo label quality term and sample size. Experiments in multiple metrics and data sets verify the advantages of the framework.

</details>


### [100] [From Dead Neurons to Deep Approximators: Deep Bernstein Networks as a Provable Alternative to Residual Layers](https://arxiv.org/abs/2602.04264)
*Ibrahim Albool,Malak Gamal El-Din,Salma Elmalaki,Yasser Shoukry*

Main category: cs.LG

TL;DR: Deep Bernstein Networks使用Bernstein多项式作为激活函数，无需残差连接即可解决梯度消失问题，在训练性和表达能力上优于传统激活函数。


<details>
  <summary>Details</summary>
Motivation: 残差连接虽然是缓解梯度消失的事实标准，但存在结构约束且无法解决分段线性激活函数的固有低效问题。需要一种既能优化训练性又能提升表达能力且无需残差连接的架构。

Method: 使用Bernstein多项式作为激活函数构建深度网络。理论证明：1) 局部导数有严格非零下界，解决梯度停滞；2) Bernstein网络的逼近误差随深度指数衰减，优于ReLU的多项式衰减。

Result: 将"死亡"神经元从标准深度网络的90%减少到不足5%，优于ReLU、Leaky ReLU、SeLU和GeLU。在HIGGS和MNIST数据集上实现无需跳跃连接的高性能训练。

Conclusion: Bernstein激活函数为函数逼近和信号流提供了更优机制，为实现具有增强表达能力且无需残差连接的深度架构提供了原则性路径。

Abstract: Residual connections are the de facto standard for mitigating vanishing gradients, yet they impose structural constraints and fail to address the inherent inefficiencies of piecewise linear activations. We show that Deep Bernstein Networks (which utilizes Bernstein polynomials as activation functions) can act as residual-free architecture while simultaneously optimize trainability and representation power. We provide a two-fold theoretical foundation for our approach. First, we derive a theoretical lower bound on the local derivative, proving it remains strictly bounded away from zero. This directly addresses the root cause of gradient stagnation; empirically, our architecture reduces ``dead'' neurons from 90\% in standard deep networks to less than 5\%, outperforming ReLU, Leaky ReLU, SeLU, and GeLU. Second, we establish that the approximation error for Bernstein-based networks decays exponentially with depth, a significant improvement over the polynomial rates of ReLU-based architectures. By unifying these results, we demonstrate that Bernstein activations provide a superior mechanism for function approximation and signal flow. Our experiments on HIGGS and MNIST confirm that Deep Bernstein Networks achieve high-performance training without skip-connections, offering a principled path toward deep, residual-free architectures with enhanced expressive capacity.

</details>


### [101] [Thickening-to-Thinning: Reward Shaping via Human-Inspired Learning Dynamics for LLM Reasoning](https://arxiv.org/abs/2602.04265)
*Wenze Lin,Zhen Yang,Xitai Jiang,Pony Ma,Gao Huang*

Main category: cs.LG

TL;DR: T2T(Thickening-to-Thinning)是一个动态奖励框架，通过"增厚"阶段鼓励探索和"减薄"阶段惩罚冗余，解决了RLVR中的熵崩溃、冗长和探索不足问题，在数学基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法存在熵崩溃、过度冗长和对难题探索不足的问题，且现有奖励方案无法区分问题解决过程中的广泛搜索需求与已掌握知识所需的效率。

Method: T2T受人类学习过程启发，采用双阶段机制：1) 错误尝试时"增厚"阶段，奖励较长轨迹以扩大搜索空间；2) 正确时"减薄"阶段，施加长度惩罚以减少冗余，培养模型信心和固化推理能力。

Result: 在MATH-500、AIME、AMC等数学基准测试中，T2T在Qwen系列和Deepseek模型上显著优于标准GRPO和近期基线方法，取得了优越性能。

Conclusion: T2T框架通过动态调整奖励策略，有效解决了RLVR中的关键问题，为增强LLM推理能力提供了有前景的方向。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a promising paradigm for enhancing reasoning in Large Language Models (LLMs). However, it frequently encounters challenges such as entropy collapse, excessive verbosity, and insufficient exploration for hard problems. Crucially, existing reward schemes fail to distinguish between the need for extensive search during problem-solving and the efficiency required for mastered knowledge. In this work, we introduce T2T(Thickening-to-Thinning), a dynamic reward framework inspired by human learning processes. Specifically, it implements a dual-phase mechanism: (1) On incorrect attempts, T2T incentivizes "thickening" (longer trajectories) to broaden the search space and explore novel solution paths; (2) Upon achieving correctness, it shifts to "thinning", imposing length penalties to discourage redundancy, thereby fostering model confidence and crystallizing reasoning capabilities. Extensive experiments on mathematical benchmarks (MATH-500, AIME, AMC) across Qwen-series and Deepseek models demonstrate that T2T significantly outperforms standard GRPO and recent baselines, achieving superior performance.

</details>


### [102] [Multi-Integration of Labels across Categories for Component Identification (MILCCI)](https://arxiv.org/abs/2602.04270)
*Noga Mudrik,Yuxi Chen,Gal Mishne,Adam S. Charles*

Main category: cs.LG

TL;DR: MILCCI是一种新颖的数据驱动方法，用于分析带有多类别元数据标签的重复测量时间序列数据，能够识别可解释的组件、捕捉跨试验变异性，并整合标签信息来理解每个类别在数据中的表示。


<details>
  <summary>Details</summary>
Motivation: 许多领域通过重复测量收集大规模时间序列数据，每个试验都带有跨越多个类别的元数据变量。关键挑战是理解这些标签如何在多试验观测中被编码，并区分每个标签条目在不同类别中的独特影响。

Method: MILCCI扩展了稀疏的每试验分解方法，利用每个类别内的标签相似性来实现细微的、标签驱动的跨试验组件组成调整，并区分每个类别的贡献。该方法还学习每个组件对应的时间轨迹，这些轨迹在每个试验内随时间演化并在不同试验间灵活变化。

Result: 通过合成和真实世界示例（包括投票模式、在线页面浏览趋势和神经元记录）展示了MILCCI的性能。

Conclusion: MILCCI提供了一种有效的方法来分析带有多类别元数据标签的时间序列数据，能够识别可解释的组件并理解不同类别标签在数据中的表示方式。

Abstract: Many fields collect large-scale temporal data through repeated measurements (trials), where each trial is labeled with a set of metadata variables spanning several categories. For example, a trial in a neuroscience study may be linked to a value from category (a): task difficulty, and category (b): animal choice. A critical challenge in time-series analysis is to understand how these labels are encoded within the multi-trial observations, and disentangle the distinct effect of each label entry across categories. Here, we present MILCCI, a novel data-driven method that i) identifies the interpretable components underlying the data, ii) captures cross-trial variability, and iii) integrates label information to understand each category's representation within the data. MILCCI extends a sparse per-trial decomposition that leverages label similarities within each category to enable subtle, label-driven cross-trial adjustments in component compositions and to distinguish the contribution of each category. MILCCI also learns each component's corresponding temporal trace, which evolves over time within each trial and varies flexibly across trials. We demonstrate MILCCI's performance through both synthetic and real-world examples, including voting patterns, online page view trends, and neuronal recordings.

</details>


### [103] [Multi Objective Design Optimization of Non Pneumatic Passenger Car Tires Using Finite Element Modeling, Machine Learning, and Particle swarm Optimization and Bayesian Optimization Algorithms](https://arxiv.org/abs/2602.04277)
*Priyankkumar Dhrangdhariya,Soumyadipta Maiti,Venkataramana Runkana*

Main category: cs.LG

TL;DR: 提出集成生成式设计和机器学习的框架，优化UPTIS型非充气轮胎辐条几何结构，实现刚度可调性、耐久性和振动性能的显著提升


<details>
  <summary>Details</summary>
Motivation: 非充气轮胎作为充气轮胎的有前景替代品，但其不连续的辐条结构在刚度调节、耐久性和高速振动方面存在挑战，需要系统化的优化方法

Method: 采用高阶多项式参数化辐条轮廓，通过PCHIP几何变异生成约250个设计；使用KRR预测刚度、XGBoost预测耐久性和振动；结合粒子群优化和贝叶斯优化进行性能优化

Result: 优化设计实现53%刚度可调性、最高50%耐久性提升和43%振动减少；PSO提供快速收敛，贝叶斯优化有效探索多目标权衡

Conclusion: 提出的集成框架能够系统化开发高性能下一代UPTIS辐条结构，显著提升非充气轮胎的综合性能

Abstract: Non Pneumatic tires offer a promising alternative to pneumatic tires. However, their discontinuous spoke structures present challenges in stiffness tuning, durability, and high speed vibration. This study introduces an integrated generative design and machine learning driven framework to optimize UPTIS type spoke geometries for passenger vehicles. Upper and lower spoke profiles were parameterized using high order polynomial representations, enabling the creation of approximately 250 generative designs through PCHIP based geometric variation. Machine learning models like KRR for stiffness and XGBoost for durability and vibration achieved strong predictive accuracy, reducing the reliance on computationally intensive FEM simulations. Optimization using Particle Swarm Optimization and Bayesian Optimization further enabled extensive performance refinement. The resulting designs demonstrate 53% stiffness tunability, up to 50% durability improvement, and 43% reduction in vibration compared to the baseline. PSO provided fast, targeted convergence, while Bayesian Optimization effectively explored multi objective tradeoffs. Overall, the proposed framework enables systematic development of high performance, next generation UPTIS spoke structures.

</details>


### [104] [Convolution Operator Network for Forward and Inverse Problems (FI-Conv): Application to Plasma Turbulence Simulations](https://arxiv.org/abs/2602.04287)
*Xingzhuo Chen,Anthony Poole,Ionut-Gabriel Farcas,David R. Hatch,Ulisses Braga-Neto*

Main category: cs.LG

TL;DR: FI-Conv是一个基于U-Net架构的卷积算子网络，用于复杂时空动力学（如湍流）的前向预测和参数反演。它用ConvNeXt V2块替换大部分卷积层，在保持计算效率的同时处理高频变化输入。


<details>
  <summary>Details</summary>
Motivation: 现有物理信息机器学习方法在处理复杂时空动力学系统（如湍流等离子体）时面临挑战，需要既能准确预测系统演化又能从数据中推断参数的统一框架。

Method: 基于U-Net架构，用ConvNeXt V2块替换大部分卷积层。前向预测使用自回归预测程序，输入初始状态、PDE参数和时间来预测未来状态。参数反演使用基于梯度下降的逆估计方法，不修改训练好的模型权重。

Result: 在Hasegawa-Wakatani方程描述的湍流等离子体场预测中，FI-Conv在短时间（t~3）内准确预测等离子体状态演化，在长时间（t~100）内捕捉到导出物理量的统计特性。参数反演方法能准确从等离子体演化数据推断PDE参数。

Conclusion: FI-Conv是处理复杂时空动力学系统的有效替代方案，既能进行准确的前向预测，又能实现参数反演，为物理信息机器学习方法提供了新思路。

Abstract: We propose the Convolutional Operator Network for Forward and Inverse Problems (FI-Conv), a framework capable of predicting system evolution and estimating parameters in complex spatio-temporal dynamics, such as turbulence. FI-Conv is built on a U-Net architecture, in which most convolutional layers are replaced by ConvNeXt V2 blocks. This design preserves U-Net performance on inputs with high-frequency variations while maintaining low computational complexity. FI-Conv uses an initial state, PDE parameters, and evolution time as input to predict the system future state. As a representative example of a system exhibiting complex dynamics, we evaluate the performance of FI-Conv on the task of predicting turbulent plasma fields governed by the Hasegawa-Wakatani (HW) equations. The HW system models two-dimensional electrostatic drift-wave turbulence and exhibits strongly nonlinear behavior, making accurate approximation and long-term prediction particularly challenging. Using an autoregressive forecasting procedure, FI-Conv achieves accurate forward prediction of the plasma state evolution over short times (t ~ 3) and captures the statistic properties of derived physical quantities of interest over longer times (t ~ 100). Moreover, we develop a gradient-descent-based inverse estimation method that accurately infers PDE parameters from plasma state evolution data, without modifying the trained model weights. Collectively, our results demonstrate that FI-Conv can be an effective alternative to existing physics-informed machine learning methods for systems with complex spatio-temporal dynamics.

</details>


### [105] [Disentangling Causal Importance from Emergent Structure in Multi-Expert Orchestration](https://arxiv.org/abs/2602.04291)
*Sudipto Ghosh,Sujoy Nath,Sunny Manchanda,Tanmoy Chakraborty*

Main category: cs.LG

TL;DR: INFORM是一个可解释性分析框架，将多专家系统的编排视为可分析的显式计算，揭示了路由主导性与功能必要性之间的差异，以及关系重要性和内在重要性之间的分歧。


<details>
  <summary>Details</summary>
Motivation: 多专家系统中多个大语言模型协作解决复杂任务，但编排策略（专家交互和顺序）通常不透明，需要可解释性分析来理解编排机制。

Method: INFORM将编排视为显式可分析计算，解耦专家交互结构、执行顺序和因果归因。在GSM8K、HumanEval和MMLU任务上评估编排器，使用同质和异质专家集合，包括LLaMA-3.1 8B、Qwen-3 8B和DeepSeek-R1 8B等模型。

Result: 路由主导性是功能必要性的不良代理；频繁选择的专家常作为交互枢纽但因果影响有限，而稀疏路由的专家可能结构关键；编排行为异步出现，专家集中化先于稳定路由置信度；针对性消融显示屏蔽内在重要专家比屏蔽频繁交互专家对交互结构破坏更大。

Conclusion: INFORM揭示了编排中的因果和结构依赖关系，超越了准确性指标，表明路由模式不能完全反映专家功能重要性，为理解多专家系统编排提供了新的分析视角。

Abstract: Multi-expert systems, where multiple Large Language Models (LLMs) collaborate to solve complex tasks, are increasingly adopted for high-performance reasoning and generation. However, the orchestration policies governing expert interaction and sequencing remain largely opaque. We introduce INFORM, an interpretability analysis that treats orchestration as an explicit, analyzable computation, enabling the decoupling of expert interaction structure, execution order, and causal attribution. We use INFORM to evaluate an orchestrator on GSM8K, HumanEval, and MMLU using a homogeneous consortium of ten instruction-tuned experts drawn from LLaMA-3.1 8B, Qwen-3 8B, and DeepSeek-R1 8B, with controlled decoding-temperature variation, and a secondary heterogeneous consortium spanning 1B-7B parameter models. Across tasks, routing dominance is a poor proxy for functional necessity. We reveal a divergence between relational importance, captured by routing mass and interaction topology, and intrinsic importance, measured via gradient-based causal attribution: frequently selected experts often act as interaction hubs with limited causal influence, while sparsely routed experts can be structurally critical. Orchestration behaviors emerge asynchronously, with expert centralization preceding stable routing confidence and expert ordering remaining non-deterministic. Targeted ablations show that masking intrinsically important experts induces disproportionate collapse in interaction structure compared to masking frequent peers, confirming that INFORM exposes causal and structural dependencies beyond accuracy metrics alone.

</details>


### [106] [Efficient Equivariant High-Order Crystal Tensor Prediction via Cartesian Local-Environment Many-Body Coupling](https://arxiv.org/abs/2602.04323)
*Dian Jin,Yancheng Yuan,Xiaoming Tao*

Main category: cs.LG

TL;DR: CEITNet：一种高效的笛卡尔环境交互张量网络，用于从原子结构端到端预测高阶晶体张量性质，在保持高精度的同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有球谐函数等变模型虽然表达能力强大，但其Clebsch-Gordan张量积在预测高阶目标时计算和内存成本过高，需要更高效的端到端预测方法。

Method: 提出笛卡尔环境交互张量网络（CEITNet）：为每个原子构建多通道笛卡尔局部环境张量，通过可学习的通道空间交互进行灵活的多体混合，在通道空间学习并使用笛卡尔张量基组装等变输出。

Result: 在二阶介电张量、三阶压电张量和四阶弹性张量预测的基准数据集上，CEITNet在关键精度指标上超越了先前的高阶预测方法，同时提供高计算效率。

Conclusion: CEITNet通过笛卡尔张量表示和通道空间交互，实现了高效的高阶晶体张量性质预测，为材料科学中的张量性质预测提供了更实用的解决方案。

Abstract: End-to-end prediction of high-order crystal tensor properties from atomic structures remains challenging: while spherical-harmonic equivariant models are expressive, their Clebsch-Gordan tensor products incur substantial compute and memory costs for higher-order targets. We propose the Cartesian Environment Interaction Tensor Network (CEITNet), an approach that constructs a multi-channel Cartesian local environment tensor for each atom and performs flexible many-body mixing via a learnable channel-space interaction. By performing learning in channel space and using Cartesian tensor bases to assemble equivariant outputs, CEITNet enables efficient construction of high-order tensor. Across benchmark datasets for order-2 dielectric, order-3 piezoelectric, and order-4 elastic tensor prediction, CEITNet surpasses prior high-order prediction methods on key accuracy criteria while offering high computational efficiency.

</details>


### [107] [RISE: Interactive Visual Diagnosis of Fairness in Machine Learning Models](https://arxiv.org/abs/2602.04339)
*Ray Chen,Christan Grant*

Main category: cs.LG

TL;DR: RISE是一个可视化工具，通过排序残差分析模型公平性，帮助识别领域转移下的局部差异和隐藏的公平性问题。


<details>
  <summary>Details</summary>
Motivation: 在领域转移下评估公平性具有挑战性，因为标量指标常常掩盖了差异出现的确切位置和方式。需要更精细的分析工具来揭示隐藏的公平性问题。

Method: RISE（通过排序评估的残差检查）是一个交互式可视化工具，将排序后的残差转换为可解释的模式。它通过连接残差曲线结构与形式化公平概念，实现局部差异诊断、跨环境子组比较和隐藏公平性问题的检测。

Result: RISE能够暴露聚合统计量遗漏的准确性与公平性权衡，支持更明智的模型选择。通过事后分析，工具能够识别传统指标无法发现的公平性问题。

Conclusion: RISE为领域转移下的公平性评估提供了更精细的分析框架，通过可视化残差模式帮助研究人员和从业者更好地理解和解决模型公平性问题。

Abstract: Evaluating fairness under domain shift is challenging because scalar metrics often obscure exactly where and how disparities arise. We introduce \textit{RISE} (Residual Inspection through Sorted Evaluation), an interactive visualization tool that converts sorted residuals into interpretable patterns. By connecting residual curve structures to formal fairness notions, RISE enables localized disparity diagnosis, subgroup comparison across environments, and the detection of hidden fairness issues. Through post-hoc analysis, RISE exposes accuracy-fairness trade-offs that aggregate statistics miss, supporting more informed model selection.

</details>


### [108] [UnMaskFork: Test-Time Scaling for Masked Diffusion via Deterministic Action Branching](https://arxiv.org/abs/2602.04344)
*Kou Misaki,Takuya Akiba*

Main category: cs.LG

TL;DR: 提出UnMaskFork框架，利用蒙特卡洛树搜索优化掩码扩散语言模型的生成路径，在推理时通过确定性部分解掩码提升复杂任务性能


<details>
  <summary>Details</summary>
Motivation: 现有测试时扩展策略主要针对自回归大语言模型，而掩码扩散语言模型因其迭代和非自回归的生成特性，天然适合更高级的搜索策略

Method: 提出UnMaskFork框架，将解掩码轨迹建模为搜索树，使用蒙特卡洛树搜索优化生成路径，通过多个MDLM执行确定性部分解掩码动作来探索搜索空间

Result: 在复杂编码基准测试中一致优于现有测试时扩展基线，在数学推理任务上也展现出强大的可扩展性

Conclusion: 掩码扩散语言模型通过UnMaskFork框架能够有效利用推理时计算资源，为复杂推理任务提供了新的优化路径

Abstract: Test-time scaling strategies have effectively leveraged inference-time compute to enhance the reasoning abilities of Autoregressive Large Language Models. In this work, we demonstrate that Masked Diffusion Language Models (MDLMs) are inherently amenable to advanced search strategies, owing to their iterative and non-autoregressive generation process. To leverage this, we propose UnMaskFork (UMF), a framework that formulates the unmasking trajectory as a search tree and employs Monte Carlo Tree Search to optimize the generation path. In contrast to standard scaling methods relying on stochastic sampling, UMF explores the search space through deterministic partial unmasking actions performed by multiple MDLMs. Our empirical evaluation demonstrates that UMF consistently outperforms existing test-time scaling baselines on complex coding benchmarks, while also exhibiting strong scalability on mathematical reasoning tasks.

</details>


### [109] [MirrorLA: Reflecting Feature Map for Vision Linear Attention](https://arxiv.org/abs/2602.04346)
*Weikang Meng,Liangyu Huo,Yadan Luo,Yaowei Wang,Yingjian Li,Zheng Zhang*

Main category: cs.LG

TL;DR: MirrorLA提出了一种几何框架，通过可学习的Householder反射将特征几何旋转到非负象限，解决了线性注意力因非负约束导致性能下降的问题，在保持线性效率的同时实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 线性注意力将Transformer的计算复杂度从二次降低到线性，但性能始终落后于基于softmax的注意力。研究发现性能下降的根本原因是核特征映射的非负约束：像ReLU这样的标准投影作为"被动截断"操作，不加区分地丢弃负域中的语义信息。

Method: MirrorLA采用几何框架，用主动重定向替代被动截断。通过可学习的Householder反射将特征几何旋转到非负象限以最大化信息保留。采用多尺度设计：1) 通过块级等距优化局部可区分性；2) 使用方差感知调制稳定长上下文动态以多样化激活；3) 通过跨头反射整合分散子空间以诱导全局协方差混合。

Result: MirrorLA在标准基准测试中实现了最先进的性能，证明严格线性效率可以在不牺牲表示保真度的情况下实现。

Conclusion: 线性注意力的性能下降源于非负约束导致的被动信息截断。MirrorLA通过几何重定向框架解决了这一问题，在保持线性复杂度的同时恢复了表示密度，为高效Transformer架构提供了新方向。

Abstract: Linear attention significantly reduces the computational complexity of Transformers from quadratic to linear, yet it consistently lags behind softmax-based attention in performance. We identify the root cause of this degradation as the non-negativity constraint imposed on kernel feature maps: standard projections like ReLU act as "passive truncation" operators, indiscriminately discarding semantic information residing in the negative domain. We propose MirrorLA, a geometric framework that substitutes passive truncation with active reorientation. By leveraging learnable Householder reflections, MirrorLA rotates the feature geometry into the non-negative orthant to maximize information retention. Our approach restores representational density through a cohesive, multi-scale design: it first optimizes local discriminability via block-wise isometries, stabilizes long-context dynamics using variance-aware modulation to diversify activations, and finally, integrates dispersed subspaces via cross-head reflections to induce global covariance mixing. MirrorLA achieves state-of-the-art performance across standard benchmarks, demonstrating that strictly linear efficiency can be achieved without compromising representational fidelity.

</details>


### [110] [Mosaic Learning: A Framework for Decentralized Learning with Model Fragmentation](https://arxiv.org/abs/2602.04352)
*Sayan Biswas,Davide Frey,Romaric Gaudel,Nirupam Gupta,Anne-Marie Kermarrec,Dimitri Lerévérend,Rafael Pires,Rishi Sharma,François Taïani,Martijn de Vos*

Main category: cs.LG

TL;DR: Mosaic Learning是一个去中心化学习框架，通过将模型分解为片段并在网络中独立传播，减少冗余通信并提升信息多样性，在保持通信成本不变的情况下提高性能。


<details>
  <summary>Details</summary>
Motivation: 去中心化学习（DL）允许在没有中央服务器的情况下进行协作机器学习，适用于无法集中托管训练数据的场景。现有方法存在冗余通信和信息传播多样性不足的问题。

Method: Mosaic Learning将模型分解为片段，并在网络中独立传播这些片段。通过利用模型参数之间的相关性，减少冗余通信，同时在不增加通信成本的情况下实现更多样化的信息传播。

Result: 理论分析显示Mosaic Learning具有最先进的最坏情况收敛率，并通过降低简化系统的最高特征值来改善收缩性。在四个学习任务上的实验表明，相比最先进的流行病学习（EL）基线，节点级测试准确率最高提升12个百分点。

Conclusion: Mosaic Learning在不牺牲效用或效率的情况下提高了去中心化学习的性能，有望成为新的去中心化学习标准。

Abstract: Decentralized learning (DL) enables collaborative machine learning (ML) without a central server, making it suitable for settings where training data cannot be centrally hosted. We introduce Mosaic Learning, a DL framework that decomposes models into fragments and disseminates them independently across the network. Fragmentation reduces redundant communication across correlated parameters and enables more diverse information propagation without increasing communication cost. We theoretically show that Mosaic Learning (i) shows state-of-the-art worst-case convergence rate, and (ii) leverages parameter correlation in an ML model, improving contraction by reducing the highest eigenvalue of a simplified system. We empirically evaluate Mosaic Learning on four learning tasks and observe up to 12 percentage points higher node-level test accuracy compared to epidemic learning (EL), a state-of-the-art baseline. In summary, Mosaic Learning improves DL performance without sacrificing its utility or efficiency, and positions itself as a new DL standard.

</details>


### [111] [Counterfactual Explanations for Hypergraph Neural Networks](https://arxiv.org/abs/2602.04360)
*Fabiano Veglianti,Lorenzo Antonelli,Gabriele Tolomei*

Main category: cs.LG

TL;DR: CF-HyperGNNExplainer：一种用于超图神经网络的反事实解释方法，通过最小结构修改生成可解释的预测解释


<details>
  <summary>Details</summary>
Motivation: 超图神经网络（HGNNs）能有效建模现实系统中的高阶交互，但缺乏可解释性限制了其在高风险场景中的应用。需要开发能够解释HGNN决策的方法。

Method: 提出CF-HyperGNNExplainer反事实解释方法，通过可操作的编辑（移除节点-超边关联或删除超边）生成最小结构修改，创建反事实超图来解释模型预测。

Result: 在三个基准数据集上的实验表明，CF-HyperGNNExplainer能生成有效且简洁的反事实解释，突出显示对HGNN决策最关键的高阶关系。

Conclusion: 该方法为超图神经网络提供了结构上有意义的解释，增强了模型的可解释性，有助于在高风险场景中部署HGNNs。

Abstract: Hypergraph neural networks (HGNNs) effectively model higher-order interactions in many real-world systems but remain difficult to interpret, limiting their deployment in high-stakes settings.
  We introduce CF-HyperGNNExplainer, a counterfactual explanation method for HGNNs that identifies the minimal structural changes required to alter a model's prediction. The method generates counterfactual hypergraphs using actionable edits limited to removing node-hyperedge incidences or deleting hyperedges, producing concise and structurally meaningful explanations. Experiments on three benchmark datasets show that CF-HyperGNNExplainer generates valid and concise counterfactuals, highlighting the higher-order relations most critical to HGNN decisions.

</details>


### [112] [EXaMCaP: Subset Selection with Entropy Gain Maximization for Probing Capability Gains of Large Chart Understanding Training Sets](https://arxiv.org/abs/2602.04365)
*Jiapeng Liu,Liang Li,Bing Li,Peng Fu,Xiyan Gao,Chengyang Fang,Xiaoshuai Hao,Can Ma*

Main category: cs.LG

TL;DR: EXaMCaP：基于熵增益最大化的图表理解数据集子集选择方法，用于高效评估MLLMs能力增益


<details>
  <summary>Details</summary>
Motivation: 现有方法通过全量微调评估图表理解训练集的效果耗时巨大，阻碍了数据集的迭代优化。研究发现子集可以探测MLLMs从全量微调中获得的能力增益，而数据多样性对提升MLLMs性能至关重要。

Method: 提出EXaMCaP方法，通过熵增益最大化选择高多样性子集。该方法从大型图表理解数据集中选择最大熵子集，采用迭代选择策略：每次选择能最大化当前集合熵增益的样本，近似获得全数据集的最大熵子集。

Result: 实验表明EXaMCaP在探测图表理解训练集能力增益方面优于基线方法，在不同子集大小下都表现出强有效性，并且与多种MLLM架构兼容。

Conclusion: EXaMCaP提供了一种高效评估图表理解数据集效果的方法，通过选择高多样性子集来近似全量微调的能力增益评估，显著降低了评估时间成本，促进了数据集的迭代优化。

Abstract: Recent works focus on synthesizing Chart Understanding (ChartU) training sets to inject advanced chart knowledge into Multimodal Large Language Models (MLLMs), where the sufficiency of the knowledge is typically verified by quantifying capability gains via the fine-tune-then-evaluate paradigm. However, full-set fine-tuning MLLMs to assess such gains incurs significant time costs, hindering the iterative refinement cycles of the ChartU dataset. Reviewing the ChartU dataset synthesis and data selection domains, we find that subsets can potentially probe the MLLMs' capability gains from full-set fine-tuning. Given that data diversity is vital for boosting MLLMs' performance and entropy reflects this feature, we propose EXaMCaP, which uses entropy gain maximization to select a subset. To obtain a high-diversity subset, EXaMCaP chooses the maximum-entropy subset from the large ChartU dataset. As enumerating all possible subsets is impractical, EXaMCaP iteratively selects samples to maximize the gain in set entropy relative to the current set, approximating the maximum-entropy subset of the full dataset. Experiments show that EXaMCaP outperforms baselines in probing the capability gains of the ChartU training set, along with its strong effectiveness across diverse subset sizes and compatibility with various MLLM architectures.

</details>


### [113] [Multi-scale hypergraph meets LLMs: Aligning large language models for time series analysis](https://arxiv.org/abs/2602.04369)
*Zongjiang Shang,Dongliang Cui,Binqing Wu,Ling Chen*

Main category: cs.LG

TL;DR: MSH-LLM提出了一种多尺度超图方法，通过超边机制增强时间序列语义空间的多尺度信息，结合跨模态对齐和混合提示机制，将大语言模型与时间序列分析对齐，在27个真实世界数据集上取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 当前利用预训练大语言模型进行时间序列分析时，虽然取得了成功，但未能充分考虑自然语言和时间序列的多尺度结构，导致LLM能力利用不足。需要更有效地对齐这两种模态的多尺度特征。

Method: 1. 超边机制增强时间序列语义空间的多尺度语义信息；2. 跨模态对齐模块在不同尺度上对齐自然语言和时间序列的模态；3. 混合提示机制提供上下文信息，增强LLM理解时间序列多尺度时序模式的能力。

Result: 在5个不同应用的27个真实世界数据集上的实验结果表明，MSH-LLM达到了最先进的性能。

Conclusion: MSH-LLM通过多尺度超图方法有效对齐大语言模型与时间序列分析，解决了现有方法未能充分利用LLM能力的问题，在多个应用领域取得了优异性能。

Abstract: Recently, there has been great success in leveraging pre-trained large language models (LLMs) for time series analysis. The core idea lies in effectively aligning the modality between natural language and time series. However, the multi-scale structures of natural language and time series have not been fully considered, resulting in insufficient utilization of LLMs capabilities. To this end, we propose MSH-LLM, a Multi-Scale Hypergraph method that aligns Large Language Models for time series analysis. Specifically, a hyperedging mechanism is designed to enhance the multi-scale semantic information of time series semantic space. Then, a cross-modality alignment (CMA) module is introduced to align the modality between natural language and time series at different scales. In addition, a mixture of prompts (MoP) mechanism is introduced to provide contextual information and enhance the ability of LLMs to understand the multi-scale temporal patterns of time series. Experimental results on 27 real-world datasets across 5 different applications demonstrate that MSH-LLM achieves the state-of-the-art results.

</details>


### [114] [Reducing the labeling burden in time-series mapping using Common Ground: a semi-automated approach to tracking changes in land cover and species over time](https://arxiv.org/abs/2602.04373)
*Geethen Singh,Jasper A Slingsby,Tamara B Robinson,Glenn Moncrieff*

Main category: cs.LG

TL;DR: 论文提出"Common Ground"方法，利用时间稳定区域作为隐式监督，实现无需更新标签的多时相遥感分类，在入侵树种制图中比传统方法提升21-40%精度。


<details>
  <summary>Details</summary>
Motivation: 遥感分类依赖最新参考标签，但动态或偏远生态系统中持续收集新标签数据成本高、难度大。需要开发能在初始时间步标签基础上实现有效时间泛化的方法。

Method: 结合变化检测和半监督学习，提出"Common Ground"方法：利用时间稳定区域（光谱或语义特征变化小的区域）作为动态区域的隐式监督源，构建半监督框架。

Result: 在入侵树种制图中，Common Ground比简单时间迁移方法提升21-40%分类精度，比黄金标准方法（各时相单独训练）提升10-16%精度；在欧洲广域土地覆盖制图中提升2%精度。

Conclusion: 结合稳定参考筛选与半监督学习可实现可扩展、标签高效的多时相遥感分类，无需在初始时间步后手动更新参考标签，有效解决动态生态系统的标签获取难题。

Abstract: Reliable classification of Earth Observation data depends on consistent, up-to-date reference labels. However, collecting new labelled data at each time step remains expensive and logistically difficult, especially in dynamic or remote ecological systems. As a response to this challenge, we demonstrate that a model with access to reference data solely from time step t0 can perform competitively on both t0 and a future time step t1, outperforming models trained separately on time-specific reference data (the gold standard). This finding suggests that effective temporal generalization can be achieved without requiring manual updates to reference labels beyond the initial time step t0. Drawing on concepts from change detection and semi-supervised learning (SSL), the most performant approach, "Common Ground", uses a semi-supervised framework that leverages temporally stable regions-areas with little to no change in spectral or semantic characteristics between time steps-as a source of implicit supervision for dynamic regions. We evaluate this strategy across multiple classifiers, sensors (Landsat-8, Sentinel-2 satellite multispectral and airborne imaging spectroscopy), and ecological use cases. For invasive tree species mapping, we observed a 21-40% improvement in classification accuracy using Common Ground compared to naive temporal transfer, where models trained at a single time step are directly applied to a future time step. We also observe a 10 -16% higher accuracy for the introduced approach compared to a gold-standard approach. In contrast, when broad land cover categories were mapped across Europe, we observed a more modest 2% increase in accuracy compared to both the naive and gold-standard approaches. These results underscore the effectiveness of combining stable reference screening with SSL for scalable and label-efficient multi-temporal remote sensing classification.

</details>


### [115] [Beyond KL Divergence: Policy Optimization with Flexible Bregman Divergences for LLM Reasoning](https://arxiv.org/abs/2602.04380)
*Rui Yuan,Mykola Khandoga,Vinay Kumar Sankarapu*

Main category: cs.LG

TL;DR: GBMPO扩展了基于组的策略优化框架，引入灵活的Bregman散度（包括手工设计的概率空间L2散度和学习的神经镜像映射），在数学推理和代码生成任务上显著优于仅使用KL散度的现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于组的策略优化方法（如GRPO）仅使用KL散度进行策略正则化，散度函数的选择这一关键设计维度尚未被探索。本文旨在研究不同散度函数对基于组策略优化性能的影响。

Method: 提出Group-Based Mirror Policy Optimization (GBMPO)框架，将基于组的策略优化扩展到灵活的Bregman散度，包括手工设计的概率空间L2散度（ProbL2）和学习的神经镜像映射。使用进化策略元学习来优化神经镜像映射。

Result: 在GSM8K数学推理任务上，手工设计的ProbL2-GRPO达到86.7%准确率，比Dr. GRPO基线提升5.5个百分点。在MBPP代码生成任务上，神经镜像映射达到60.1-60.8% pass@1，随机初始化已能获得大部分收益。进化策略元学习主要带来方差减少（±0.2 vs ±0.6）和效率提升（MBPP上响应缩短15%）。

Conclusion: 散度选择是基于组策略优化中一个关键且先前未被探索的设计维度。神经镜像映射的随机初始化已足够大多数实际应用，而进化策略元学习主要提供方差减少和效率改进。

Abstract: Policy optimization methods like Group Relative Policy Optimization (GRPO) and its variants have achieved strong results on mathematical reasoning and code generation tasks. Despite extensive exploration of reward processing strategies and training dynamics, all existing group-based methods exclusively use KL divergence for policy regularization, leaving the choice of divergence function unexplored. We introduce Group-Based Mirror Policy Optimization (GBMPO), a framework that extends group-based policy optimization to flexible Bregman divergences, including hand-designed alternatives (L2 in probability space) and learned neural mirror maps. On GSM8K mathematical reasoning, hand-designed ProbL2-GRPO achieves 86.7% accuracy, improving +5.5 points over the Dr. GRPO baseline. On MBPP code generation, neural mirror maps reach 60.1-60.8% pass@1, with random initialization already capturing most of the benefit. While evolutionary strategies meta-learning provides marginal accuracy improvements, its primary value lies in variance reduction ($\pm$0.2 versus $\pm$0.6) and efficiency gains (15% shorter responses on MBPP), suggesting that random initialization of neural mirror maps is sufficient for most practical applications. These results establish divergence choice as a critical, previously unexplored design dimension in group-based policy optimization for LLM reasoning.

</details>


### [116] [Blockchain Federated Learning for Sustainable Retail: Reducing Waste through Collaborative Demand Forecasting](https://arxiv.org/abs/2602.04384)
*Fabio Turazza,Alessandro Neri,Marcello Pietri,Maria Angela Butturi,Marco Picone,Marco Mamei*

Main category: cs.LG

TL;DR: 该研究探索了联邦学习在可持续供应链管理中的应用，通过区块链技术支持的多零售商协作需求预测模型，在不共享原始数据的情况下显著减少了食品浪费。


<details>
  <summary>Details</summary>
Motivation: 食品需求预测对减少浪费至关重要，但零售商之间因数据隐私顾虑难以合作，限制了预测准确性的提升潜力。

Method: 首先建立单个零售商独立场景下的基线预测模型，然后引入基于区块链的联邦学习模型，让多个零售商在不直接共享数据的情况下协作训练。

Result: 联邦学习模型的性能几乎等同于理想情况下各方共享数据的设置，且明显优于单个零售商独立构建的模型，有效减少了浪费并提高了效率。

Conclusion: 基于区块链的联邦学习为可持续供应链管理提供了可行的解决方案，能够在保护数据隐私的同时实现协作预测，显著减少食品浪费。

Abstract: Effective demand forecasting is crucial for reducing food waste. However, data privacy concerns often hinder collaboration among retailers, limiting the potential for improved predictive accuracy. In this study, we explore the application of Federated Learning (FL) in Sustainable Supply Chain Management (SSCM), with a focus on the grocery retail sector dealing with perishable goods. We develop a baseline predictive model for demand forecasting and waste assessment in an isolated retailer scenario. Subsequently, we introduce a Blockchain-based FL model, trained collaboratively across multiple retailers without direct data sharing. Our preliminary results show that FL models have performance almost equivalent to the ideal setting in which parties share data with each other, and are notably superior to models built by individual parties without sharing data, cutting waste and boosting efficiency.

</details>


### [117] [On the use of LLMs to generate a dataset of Neural Networks](https://arxiv.org/abs/2602.04388)
*Nadia Daoudi,Jordi Cabot*

Main category: cs.LG

TL;DR: 利用大语言模型自动生成包含608个样本的神经网络数据集，用于验证神经网络工具的有效性，填补了公开多样化数据集缺乏的空白。


<details>
  <summary>Details</summary>
Motivation: 神经网络在决策支持中应用日益广泛，需要验证其可靠性和适应性。现有验证、重构和迁移工具缺乏公开的多样化数据集进行系统性评估，难以有效评估这些工具的实际效果。

Method: 利用大语言模型自动生成神经网络数据集，覆盖多样化的架构组件、输入数据类型和任务。通过静态分析和符号追踪验证生成网络的正确性，确保数据集的一致性和可靠性。

Result: 成功生成了包含608个样本的神经网络数据集，每个样本都符合精确的设计选择。数据集已公开可用，支持社区在神经网络可靠性和适应性方面的研究。

Conclusion: 通过LLM自动生成的神经网络数据集填补了评估神经网络工具有效性的空白，为系统化评估验证、重构和迁移工具提供了基准，有助于推动神经网络可靠性和适应性研究。

Abstract: Neural networks are increasingly used to support decision-making. To verify their reliability and adaptability, researchers and practitioners have proposed a variety of tools and methods for tasks such as NN code verification, refactoring, and migration. These tools play a crucial role in guaranteeing both the correctness and maintainability of neural network architectures, helping to prevent implementation errors, simplify model updates, and ensure that complex networks can be reliably extended and reused. Yet, assessing their effectiveness remains challenging due to the lack of publicly diverse datasets of neural networks that would allow systematic evaluation. To address this gap, we leverage large language models (LLMs) to automatically generate a dataset of neural networks that can serve as a benchmark for validation. The dataset is designed to cover diverse architectural components and to handle multiple input data types and tasks. In total, 608 samples are generated, each conforming to a set of precise design choices. To further ensure their consistency, we validate the correctness of the generated networks using static analysis and symbolic tracing. We make the dataset publicly available to support the community in advancing research on neural network reliability and adaptability.

</details>


### [118] [LoRDO: Distributed Low-Rank Optimization with Infrequent Communication](https://arxiv.org/abs/2602.04396)
*Andrej Jovanović,Alex Iacob,Mher Safaryan,Ionut-Vlad Modoranu,Lorenzo Sani,William F. Shen,Xinchi Qiu,Dan Alistarh,Nicholas D. Lane*

Main category: cs.LG

TL;DR: LoRDO框架将低秩优化与不频繁同步相结合，在减少通信开销的同时保持模型性能，在125M-720M模型规模上实现约10倍通信减少。


<details>
  <summary>Details</summary>
Motivation: 分布式训练中的DDP受限于互联带宽，不频繁通信策略虽然减少了同步频率，但仍受限于优化器状态的内存和通信需求。低秩优化器可以缓解这些约束，但在本地更新机制中，工作节点缺乏计算低秩投影所需的全批次梯度，这会降低性能。

Method: 提出LoRDO框架，统一低秩优化与不频繁同步。首先分析基于伪梯度的全局投影理论上更优，但会永久限制优化轨迹于低秩子空间。为恢复子空间探索，引入全秩准双曲更新。

Result: 在125M-720M模型规模的语言建模和下游任务中，LoRDO实现了与低秩DDP近乎相当的性能，同时减少约10倍通信。在极低内存设置（小秩/小批次大小）中，LoRDO性能提升更明显。

Conclusion: LoRDO框架成功解决了分布式训练中通信瓶颈与低秩优化在本地更新机制下的性能下降问题，通过结合低秩优化与不频繁同步，在显著减少通信的同时保持模型性能。

Abstract: Distributed training of foundation models via $\texttt{DDP}$ is limited by interconnect bandwidth. While infrequent communication strategies reduce synchronization frequency, they remain bottlenecked by the memory and communication requirements of optimizer states. Low-rank optimizers can alleviate these constraints; however, in the local-update regime, workers lack access to the full-batch gradients required to compute low-rank projections, which degrades performance. We propose $\texttt{LoRDO}$, a principled framework unifying low-rank optimization with infrequent synchronization. We first demonstrate that, while global projections based on pseudo-gradients are theoretically superior, they permanently restrict the optimization trajectory to a low-rank subspace. To restore subspace exploration, we introduce a full-rank quasi-hyperbolic update. $\texttt{LoRDO}$ achieves near-parity with low-rank $\texttt{DDP}$ in language modeling and downstream tasks at model scales of $125$M--$720$M, while reducing communication by $\approx 10 \times$. Finally, we show that $\texttt{LoRDO}$ improves performance even more in very low-memory settings with small rank/batch size.

</details>


### [119] [Theory of Speciation Transitions in Diffusion Models with General Class Structure](https://arxiv.org/abs/2602.04404)
*Beatrice Achilli,Marco Benedetti,Giulio Biroli,Marc Mézard*

Main category: cs.LG

TL;DR: 该论文提出了扩散模型中物种形成转变的通用理论，适用于任意具有明确定义类别的目标分布，超越了仅能处理一阶矩可区分类别（如高斯混合）的现有理论限制。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型理论分析局限于类别可通过一阶矩（如均值分离的高斯混合）识别的场景，无法处理类别通过高阶或集体特征区分的更一般情况。需要发展适用于任意目标分布的通用物种形成理论。

Method: 通过贝叶斯分类形式化类别结构概念，用类别间的自由熵差来表征物种形成时间。该框架可容纳多类别，预测与逐步细粒度类别承诺相关的连续物种形成时间。在可解析处理的示例（不同温度的一维Ising模型混合、零均值不同协方差结构的高斯混合）上验证理论。

Result: 理论框架恢复了先前高斯混合模型的已知结果，同时扩展到类别不能通过一阶矩区分的情况。在Ising模型案例中，通过映射到随机场Ising模型并使用复本方法求解，获得了物种形成时间的显式表达式。

Conclusion: 该研究为扩散生成模型中的物种形成转变提供了统一且广泛适用的描述，超越了现有理论的限制，能够处理类别通过高阶或集体特征区分的更一般分布。

Abstract: Diffusion Models generate data by reversing a stochastic diffusion process, progressively transforming noise into structured samples drawn from a target distribution. Recent theoretical work has shown that this backward dynamics can undergo sharp qualitative transitions, known as speciation transitions, during which trajectories become dynamically committed to data classes. Existing theoretical analyses, however, are limited to settings where classes are identifiable through first moments, such as mixtures of Gaussians with well-separated means. In this work, we develop a general theory of speciation in diffusion models that applies to arbitrary target distributions admitting well-defined classes. We formalize the notion of class structure through Bayes classification and characterize speciation times in terms of free-entropy difference between classes. This criterion recovers known results in previously studied Gaussian-mixture models, while extending to situations in which classes are not distinguishable by first moments and may instead differ through higher-order or collective features. Our framework also accommodates multiple classes and predicts the existence of successive speciation times associated with increasingly fine-grained class commitment. We illustrate the theory on two analytically tractable examples: mixtures of one-dimensional Ising models at different temperatures and mixtures of zero-mean Gaussians with distinct covariance structures. In the Ising case, we obtain explicit expressions for speciation times by mapping the problem onto a random-field Ising model and solving it via the replica method. Our results provide a unified and broadly applicable description of speciation transitions in diffusion-based generative models.

</details>


### [120] [Separation-Utility Pareto Frontier: An Information-Theoretic Characterization](https://arxiv.org/abs/2602.04408)
*Shizhou Xu*

Main category: cs.LG

TL;DR: 该研究通过信息论视角分析效用与分离公平性之间的帕累托前沿，提出基于条件互信息的正则化方法，在多个数据集上有效减少分离违规同时保持或提升模型效用。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索机器学习公平性中效用与分离公平性之间的权衡。分离公平性要求预测独立于敏感属性（给定真实结果），但实践中这种公平性要求往往以牺牲模型效用为代价。需要理论分析这种权衡关系并提供实用的实现方法。

Method: 1. 通过信息论框架理论分析效用-分离帕累托前沿，证明其凹性；2. 提出基于条件互信息（CMI）的正则化方法，监控预测与敏感属性之间的条件依赖关系；3. 开发与基于梯度的深度模型兼容的训练框架；4. 在COMPAS、UCI Adult、UCI Bank和CelebA数据集上进行实验验证。

Result: 1. 理论证明了效用-分离帕累托前沿的凹性，表明分离的边际成本递增；2. 提出的CMI正则化方法在四个数据集上显著减少分离违规；3. 在保持或超过基准方法效用的同时实现更好的公平性；4. 方法提供了训练过程中的可追踪保证。

Conclusion: 该研究提供了理论可证明、稳定且灵活的方法来在深度学习中实施分离公平性。通过信息论分析揭示了效用与分离之间的权衡关系，提出的CMI正则化方法在实践中有效平衡了公平性与模型性能。

Abstract: We study the Pareto frontier (optimal trade-off) between utility and separation, a fairness criterion requiring predictive independence from sensitive attributes conditional on the true outcome. Through an information-theoretic lens, we prove a characterization of the utility-separation Pareto frontier, establish its concavity, and thereby prove the increasing marginal cost of separation in terms of utility. In addition, we characterize the conditions under which this trade-off becomes strict, providing a guide for trade-off selection in practice. Based on the theoretical characterization, we develop an empirical regularizer based on conditional mutual information (CMI) between predictions and sensitive attributes given the true outcome. The CMI regularizer is compatible with any deep model trained via gradient-based optimization and serves as a scalar monitor of residual separation violations, offering tractable guarantees during training. Finally, numerical experiments support our theoretical findings: across COMPAS, UCI Adult, UCI Bank, and CelebA, the proposed method substantially reduces separation violations while matching or exceeding the utility of established baseline methods. This study thus offers a provable, stable, and flexible approach to enforcing separation in deep learning.

</details>


### [121] [EMA Policy Gradient: Taming Reinforcement Learning for LLMs with EMA Anchor and Top-k KL](https://arxiv.org/abs/2602.04417)
*Lunjun Zhang,Jimmy Ba*

Main category: cs.LG

TL;DR: 提出EMA-PG方法，通过EMA锚点策略和Top-k KL估计器改进LLM的强化学习，在数学推理和智能体任务上显著提升性能


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习方法在大型语言模型应用中存在稳定性问题，需要更有效的策略梯度算法来提升推理和智能体行为的学习效果

Method: 1. 使用指数移动平均(EMA)替代固定锚点策略，类似深度Q学习中的目标网络；2. 引入Top-k KL估计器，在精确KL和采样KL之间灵活插值

Result: 在数学推理任务上，R1蒸馏的Qwen-1.5B在OlympiadBench达到53.9%（GRPO为50.8%）；在智能体任务上，Qwen-3B在7个搜索问答数据集上平均提升33.3%，HotpotQA从29.7%提升到44.1%

Conclusion: EMA-PG是一种简单、有理论基础且强大的方法，能够有效扩展LLM的强化学习能力，在数学推理和智能体任务上均取得显著改进

Abstract: Reinforcement Learning (RL) has enabled Large Language Models (LLMs) to acquire increasingly complex reasoning and agentic behaviors. In this work, we propose two simple techniques to improve policy gradient algorithms for LLMs. First, we replace the fixed anchor policy during RL with an Exponential Moving Average (EMA), similar to a target network in deep Q-learning. Second, we introduce Top-k KL estimator, which allows for flexible interpolation between exact KL and sampled KL. We derive the stability conditions for using EMA anchor; moreover, we show that our Top-k KL estimator yields both unbiased KL values and unbiased gradients at any k, while bringing the benefits of exact KL. When combined with GRPO, the two techniques (EMA-PG) lead to a significant performance boost. On math reasoning, it allows R1-distilled Qwen-1.5B to reach 53.9% on OlympiadBench compared to 50.8% by GRPO. On agentic RL domains, with Qwen-3B base, EMA-PG improves GRPO by an average of 33.3% across 7 datasets of Q&A with search engines, including 29.7% $\rightarrow$ 44.1% on HotpotQA, 27.4% $\rightarrow$ 40.1% on 2WikiMultiHopQA. Overall, we show that EMA-PG is a simple, principled, and powerful approach to scaling RL for LLMs. Code: https://github.com/LunjunZhang/ema-pg

</details>


### [122] [MaMa: A Game-Theoretic Approach for Designing Safe Agentic Systems](https://arxiv.org/abs/2602.04431)
*Jonathan Nöther,Adish Singla,Goran Radanovic*

Main category: cs.LG

TL;DR: 提出MaMa算法，通过元代理与元对抗者的博弈自动设计安全的LLM多智能体系统，即使部分智能体被攻陷也能保持安全。


<details>
  <summary>Details</summary>
Motivation: LLM多智能体系统虽然能力强大，但当单个智能体失效或表现出对抗行为时会带来严重安全风险。需要设计即使部分智能体被攻陷也能保持安全的系统。

Method: 将安全设计问题形式化为Stackelberg安全博弈，提出MaMa算法：元代理迭代提出系统设计，元对抗者发现最强攻击并提供反馈，通过LLM驱动的对抗搜索近似求解博弈。

Result: 在不同环境中，MaMa设计的系统能持续抵御最坏情况攻击，同时保持与仅优化任务成功的系统相当的性能。系统还能泛化到更强的对抗者、不同攻击目标和底层LLM。

Conclusion: MaMa算法能自动设计出具有鲁棒安全性的智能体系统，在训练设置之外也能保持安全，为解决LLM多智能体系统的安全挑战提供了有效方法。

Abstract: LLM-based multi-agent systems have demonstrated impressive capabilities, but they also introduce significant safety risks when individual agents fail or behave adversarially. In this work, we study the automated design of agentic systems that remain safe even when a subset of agents is compromised. We formalize this challenge as a Stackelberg security game between a system designer (the Meta-Agent) and a best-responding Meta-Adversary that selects and compromises a subset of agents to minimize safety. We propose Meta-Adversary-Meta-Agent (MaMa), a novel algorithm for approximately solving this game and automatically designing safe agentic systems. Our approach uses LLM-based adversarial search, where the Meta-Agent iteratively proposes system designs and receives feedback based on the strongest attacks discovered by the Meta-Adversary. Empirical evaluations across diverse environments show that systems designed with MaMa consistently defend against worst-case attacks while maintaining performance comparable to systems optimized solely for task success. Moreover, the resulting systems generalize to stronger adversaries, as well as ones with different attack objectives or underlying LLMs, demonstrating robust safety beyond the training setting.

</details>


### [123] [Hand Gesture Recognition from Doppler Radar Signals Using Echo State Networks](https://arxiv.org/abs/2602.04436)
*Towa Sano,Gouhei Tanaka*

Main category: cs.LG

TL;DR: 提出基于回声状态网络（ESN）的雷达手势识别方法，使用多储备池并行处理不同特征图，在保持低计算成本的同时实现高性能识别。


<details>
  <summary>Details</summary>
Motivation: 基于多普勒雷达的手势识别在车载界面和机器人系统中应用广泛，但现有深度学习方法计算成本高，不适合资源受限环境。需要开发轻量高效的识别技术。

Method: 将原始雷达数据转换为距离-时间和多普勒-时间特征图，输入到一个或多个基于循环神经网络的储备池中，通过岭回归、支持向量机或随机森林等读出分类器进行处理。

Result: 在Soli数据集的11类手势识别任务中优于现有方法，在Dop-NET数据集的4类手势识别任务中超越现有深度学习模型，同时保持低计算成本。

Conclusion: 多储备池ESN方法能有效识别时空和时频域中的时序模式，在低计算成本下实现高性能手势识别，在资源受限环境中具有巨大潜力。

Abstract: Hand gesture recognition (HGR) is a fundamental technology in human computer interaction (HCI).In particular, HGR based on Doppler radar signals is suited for in-vehicle interfaces and robotic systems, necessitating lightweight and computationally efficient recognition techniques. However, conventional deep learning-based methods still suffer from high computational costs. To address this issue, we propose an Echo State Network (ESN) approach for radar-based HGR, using frequency-modulated-continuous-wave (FMCW) radar signals. Raw radar data is first converted into feature maps, such as range-time and Doppler-time maps, which are then fed into one or more recurrent neural network-based reservoirs. The obtained reservoir states are processed by readout classifiers, including ridge regression, support vector machines, and random forests. Comparative experiments demonstrate that our method outperforms existing approaches on an 11-class HGR task using the Soli dataset and surpasses existing deep learning models on a 4-class HGR task using the Dop-NET dataset. The results indicate that parallel processing using multi-reservoir ESNs are effective for recognizing temporal patterns from the multiple different feature maps in the time-space and time-frequency domains. Our ESN approaches achieve high recognition performance with low computational cost in HGR, showing great potential for more advanced HCI technologies, especially in resource-constrained environments.

</details>


### [124] [Mixture of Masters: Sparse Chess Language Models with Player Routing](https://arxiv.org/abs/2602.04447)
*Giacomo Frisoni,Lorenzo Molfetta,Davide Freddi,Gianluca Moro*

Main category: cs.LG

TL;DR: 提出MoM（专家混合）架构，通过多个小型GPT专家模拟不同国际象棋大师风格，动态选择最适合当前棋局状态的专家，解决传统密集模型风格同质化问题。


<details>
  <summary>Details</summary>
Motivation: 传统国际象棋语言模型是密集Transformer，在大量对局数据上训练，但容易陷入"模式平均"行为，模糊风格边界，抑制罕见但有效的策略。需要解决模型同质化问题，保持风格多样性和策略特异性。

Method: 1. 设计Mixture-of-Masters（MoM）架构，包含多个小型GPT专家，每个专家模拟世界级国际象棋大师风格；2. 专家训练结合自监督学习和强化学习（使用国际象棋特定奖励）；3. 引入可学习的门控网络，根据棋局状态动态选择最合适的专家；4. 支持动态切换风格（如塔尔进攻风格或彼得罗相防守风格）。

Result: 在未见过的标准对局中评估，MoM表现优于：1. 密集的单个专家网络；2. 在聚合数据上训练的流行GPT基线。同时确保生成多样性、可控性和可解释性。

Conclusion: MoM架构有效解决了国际象棋语言模型的同质化问题，通过专家混合方法实现了风格多样性、动态适应性和性能提升，为专业领域AI模型设计提供了新思路。

Abstract: Modern chess language models are dense transformers trained on millions of games played by thousands of high-rated individuals. However, these monolithic networks tend to collapse into mode-averaged behavior, where stylistic boundaries are blurred, and rare but effective strategies are suppressed. To counteract homogenization, we introduce Mixture-of-Masters (MoM), the first chess mixture-of-experts model with small-sized GPT experts emulating world-class grandmasters. Each expert is trained with a combination of self-supervised learning and reinforcement learning guided by chess-specific rewards. For each move, a post-hoc learnable gating network selects the most appropriate persona to channel depending on the game state, allowing MoM to switch its style dynamically$--$e.g., Tal's offensive vocation or Petrosian's defensive solidity. When evaluated against Stockfish on unseen standard games, MoM outperforms both dense individual expert networks and popular GPT baselines trained on aggregated data, while ensuring generation variety, control, and interpretability.

</details>


### [125] [RASA: Routing-Aware Safety Alignment for Mixture-of-Experts Models](https://arxiv.org/abs/2602.04448)
*Jiacheng Liang,Yuhui Wang,Tanqiu Jiang,Ting Wang*

Main category: cs.LG

TL;DR: RASA：针对MoE语言模型的路由感知专家级对齐框架，通过修复安全关键专家而非全局参数更新，实现鲁棒的安全对齐


<details>
  <summary>Details</summary>
Motivation: MoE语言模型的稀疏路由机制在标准全参数微调下可能导致退化优化行为，现有方法通过路由或专家主导效应降低攻击成功率，而非直接修复安全关键专家

Method: RASA框架：1)识别被成功越狱攻击过度激活的专家；2)在固定路由下选择性微调这些专家；3)强制与安全对齐上下文的路由一致性

Result: 在两个代表性MoE架构和多样化越狱攻击上，RASA实现近乎完美的鲁棒性、强大的跨攻击泛化能力、显著减少过度拒绝，同时在MMLU、GSM8K、TruthfulQA等基准上保持通用能力

Conclusion: 鲁棒的MoE安全对齐受益于有针对性的专家修复而非全局参数更新，RASA为先前方法提供了实用且保持架构的替代方案

Abstract: Mixture-of-Experts (MoE) language models introduce unique challenges for safety alignment due to their sparse routing mechanisms, which can enable degenerate optimization behaviors under standard full-parameter fine-tuning. In our preliminary experiments, we observe that naively applying full-parameter safety fine-tuning to MoE models can reduce attack success rates through routing or expert dominance effects, rather than by directly repairing Safety-Critical Experts. To address this challenge, we propose RASA, a routing-aware expert-level alignment framework that explicitly repairs Safety-Critical Experts while preventing routing-based bypasses. RASA identifies experts disproportionately activated by successful jailbreaks, selectively fine-tunes only these experts under fixed routing, and subsequently enforces routing consistency with safety-aligned contexts. Across two representative MoE architectures and a diverse set of jailbreak attacks, RASA achieves near-perfect robustness, strong cross-attack generalization, and substantially reduced over-refusal, while preserving general capabilities on benchmarks such as MMLU, GSM8K, and TruthfulQA. Our results suggest that robust MoE safety alignment benefits from targeted expert repair rather than global parameter updates, offering a practical and architecture-preserving alternative to prior approaches.

</details>


### [126] [Greedy-Gnorm: A Gradient Matrix Norm-Based Alternative to Attention Entropy for Head Pruning](https://arxiv.org/abs/2602.04491)
*Yuxi Guo,Paul Sheridan*

Main category: cs.LG

TL;DR: 提出Greedy-Gnorm算法，通过动态重新计算注意力头重要性分数来改进Transformer模型剪枝，在保持准确性的同时显著减少模型大小。


<details>
  <summary>Details</summary>
Motivation: 现有注意力头剪枝方法依赖静态重要性分数，无法捕捉剪枝过程中注意力头角色的动态变化，导致剪枝效果不佳。

Method: 提出Greedy-Gnorm算法：1) 使用验证集估计每个注意力头的Q/K/V梯度块的L2范数；2) 计算这些范数的逐元素乘积作为重要性分数；3) 在每次贪婪剪枝迭代后动态重新计算所有剩余头的重要性分数。

Result: 在BERT、ALBERT、RoBERTa和XLM-RoBERTa上的实验表明，Greedy-Gnorm在大量剪除注意力头的情况下能更好地保持模型准确性，优于基于注意力熵的方法。

Conclusion: Greedy-Gnorm通过动态梯度重要性评估实现了更有效的Transformer模型压缩，为部署更节能的Transformer模型提供了有前景的方法。

Abstract: Attention head pruning has emerged as an effective technique for transformer model compression, an increasingly important goal in the era of Green AI. However, existing pruning methods often rely on static importance scores, which fail to capture the evolving role of attention heads during iterative removal. We propose Greedy-Gradient norm (Greedy-Gnorm), a novel head pruning algorithm that dynamically recalculates head importance after each pruning step. Specifically, each head is scored by the elementwise product of the l2-norms of its Q/K/V gradient blocks, as estimated from a hold-out validation set and updated at every greedy iteration. This dynamic approach to scoring mitigates against stale rankings and better reflects gradient-informed importance as pruning progresses. Extensive experiments on BERT, ALBERT, RoBERTa, and XLM-RoBERTa demonstrate that Greedy-Gnorm consistently preserves accuracy under substantial head removal, outperforming attention entropy. By effectively reducing model size while maintaining task performance, Greedy-Gnorm offers a promising step toward more energy-efficient transformer model deployment.

</details>


### [127] [Forget to Generalize: Iterative Adaptation for Generalization in Federated Learning](https://arxiv.org/abs/2602.04536)
*Abdulrahman Alotaibi,Irene Tenison,Miriam Kim,Isaac Lee,Lalana Kagal*

Main category: cs.LG

TL;DR: 提出迭代联邦适应(IFA)方法，通过逐代遗忘和进化策略提升异构联邦学习中的泛化性能，在非IID数据分布下平均提升21.5%准确率。


<details>
  <summary>Details</summary>
Motivation: 现实Web系统中存在高度异构的用户设备、地理区域和浏览模式，导致非IID数据分布普遍存在。传统联邦学习在非IID环境下性能严重下降，需要新的训练范式来提升泛化能力。

Method: 提出迭代联邦适应(IFA)训练范式：将训练分为多个世代，每代结束时选择部分模型参数（随机或从后层）进行重新初始化。这种迭代遗忘和进化策略帮助模型逃离局部最优，保留全局相关表示。

Result: 在CIFAR-10、MIT-Indoors和Stanford Dogs数据集上的实验表明，该方法显著提升全局准确率，尤其在非IID数据分布下效果明显。平均提升21.5%，且可与现有联邦算法结合使用。

Conclusion: IFA方法通过迭代遗忘和进化策略有效提升异构联邦学习中的泛化性能，为实现可扩展、隐私保护的现实Web系统智能提供了新思路。

Abstract: The Web is naturally heterogeneous with user devices, geographic regions, browsing patterns, and contexts all leading to highly diverse, unique datasets. Federated Learning (FL) is an important paradigm for the Web because it enables privacy-preserving, collaborative machine learning across diverse user devices, web services and clients without needing to centralize sensitive data. However, its performance degrades severely under non-IID client distributions that is prevalent in real-world web systems. In this work, we propose a new training paradigm - Iterative Federated Adaptation (IFA) - that enhances generalization in heterogeneous federated settings through generation-wise forget and evolve strategy. Specifically, we divide training into multiple generations and, at the end of each, select a fraction of model parameters (a) randomly or (b) from the later layers of the model and reinitialize them. This iterative forget and evolve schedule allows the model to escape local minima and preserve globally relevant representations. Extensive experiments on CIFAR-10, MIT-Indoors, and Stanford Dogs datasets show that the proposed approach improves global accuracy, especially when the data cross clients are Non-IID. This method can be implemented on top any federated algorithm to improve its generalization performance. We observe an average of 21.5%improvement across datasets. This work advances the vision of scalable, privacy-preserving intelligence for real-world heterogeneous and distributed web systems.

</details>


### [128] [Continual Learning through Control Minimization](https://arxiv.org/abs/2602.04542)
*Sander de Haan,Yassine Taoudi-Benchekroun,Pau Vilimelis Aceituno,Benjamin F. Grewe*

Main category: cs.LG

TL;DR: 将持续学习重构为控制问题，通过保护信号与学习信号的竞争来避免灾难性遗忘，无需显式存储曲率信息


<details>
  <summary>Details</summary>
Motivation: 解决神经网络在顺序任务训练中的灾难性遗忘问题，传统方法需要显式存储曲率信息，计算成本高

Method: 将持续学习建模为控制问题，将正则化惩罚转换为保护信号来保护先前任务的表示，通过最小化控制努力来整合新任务，在平衡时产生隐含编码完整先前任务曲率的权重更新

Result: 方法能够恢复真实的先前任务曲率，实现任务区分，在标准基准测试中优于现有方法且无需重放机制

Conclusion: 通过控制理论框架将持续学习重新概念化，提供了一种无需显式存储曲率信息的有效解决方案，在避免灾难性遗忘方面表现出色

Abstract: Catastrophic forgetting remains a fundamental challenge for neural networks when tasks are trained sequentially. In this work, we reformulate continual learning as a control problem where learning and preservation signals compete within neural activity dynamics. We convert regularization penalties into preservation signals that protect prior-task representations. Learning then proceeds by minimizing the control effort required to integrate new tasks while competing with the preservation of prior tasks. At equilibrium, the neural activities produce weight updates that implicitly encode the full prior-task curvature, a property we term the continual-natural gradient, requiring no explicit curvature storage. Experiments confirm that our learning framework recovers true prior-task curvature and enables task discrimination, outperforming existing methods on standard benchmarks without replay.

</details>


### [129] [Gradient Flow Through Diagram Expansions: Learning Regimes and Explicit Solutions](https://arxiv.org/abs/2602.04548)
*Dmitry Yarotsky,Eugene Golikov,Yaroslav Gusev*

Main category: cs.LG

TL;DR: 本文开发了一个分析大规模学习问题中梯度流缩放机制的数学框架，使用类似费曼图的图示方法，揭示了不同学习阶段，并在张量CP分解模型中发现了多种极端懒惰和丰富梯度流机制。


<details>
  <summary>Details</summary>
Motivation: 需要理解大规模学习问题中梯度流的缩放机制和动力学行为，特别是在不同参数化程度下的学习阶段变化，这对于理论理解和实际应用都很重要。

Method: 采用形式幂级数展开损失演化，系数用类似费曼图的图示编码，在大型极限下定义良好。通过将形式损失展开约化为PDE（通常是一阶的），用特征线法求解。特别关注高阶张量的CP分解模型。

Result: 发现了CP分解模型的多种极端梯度流机制：自由演化、NTK、欠参数化和过参数化平均场。这些机制依赖于参数缩放、张量阶数和模型对称性的特定微妙方式。理论预测与实验高度吻合。

Conclusion: 提出的数学框架能有效分析梯度流缩放机制，揭示了不同学习阶段的存在和特性，为理解大规模学习动力学提供了新工具，在张量分解等具体模型中得到了验证。

Abstract: We develop a general mathematical framework to analyze scaling regimes and derive explicit analytic solutions for gradient flow (GF) in large learning problems. Our key innovation is a formal power series expansion of the loss evolution, with coefficients encoded by diagrams akin to Feynman diagrams. We show that this expansion has a well-defined large-size limit that can be used to reveal different learning phases and, in some cases, to obtain explicit solutions of the nonlinear GF. We focus on learning Canonical Polyadic (CP) decompositions of high-order tensors, and show that this model has several distinct extreme lazy and rich GF regimes such as free evolution, NTK and under- and over-parameterized mean-field. We show that these regimes depend on the parameter scaling, tensor order, and symmetry of the model in a specific and subtle way. Moreover, we propose a general approach to summing the formal loss expansion by reducing it to a PDE; in a wide range of scenarios, it turns out to be 1st order and solvable by the method of characteristics. We observe a very good agreement of our theoretical predictions with experiment.

</details>


### [130] [Finding Structure in Continual Learning](https://arxiv.org/abs/2602.04555)
*Pourya Shamsolmoali,Masoumeh Zareapoor*

Main category: cs.LG

TL;DR: 论文提出使用Douglas-Rachford Splitting方法重新构建持续学习目标，将学习过程视为可塑性（新任务）与稳定性（旧知识）两个解耦目标之间的协商，而非直接权衡，从而提供更稳定高效的学习动态。


<details>
  <summary>Details</summary>
Motivation: 传统持续学习方法通常通过求和竞争性损失项来处理可塑性与稳定性之间的冲突，这会产生梯度冲突，需要复杂低效的策略（如外部记忆回放或参数正则化）来管理。需要更原则性、更稳定的学习动态。

Method: 使用Douglas-Rachford Splitting（DRS）重新构建持续学习目标，将学习过程视为两个解耦目标之间的协商：一个促进新任务的可塑性，另一个强制执行旧知识的稳定性。通过迭代地使用它们的近端算子寻找共识，DRS提供了更原则性的学习动态。

Result: 该方法在稳定性和可塑性之间实现了高效平衡，无需辅助模块或复杂附加组件，为持续学习系统提供了更简单但更强大的范式。

Conclusion: DRS方法为持续学习提供了一个更原则性的框架，通过解耦可塑性和稳定性目标并迭代协商，避免了传统方法中的梯度冲突问题，实现了更高效稳定的学习。

Abstract: Learning from a stream of tasks usually pits plasticity against stability: acquiring new knowledge often causes catastrophic forgetting of past information. Most methods address this by summing competing loss terms, creating gradient conflicts that are managed with complex and often inefficient strategies such as external memory replay or parameter regularization. We propose a reformulation of the continual learning objective using Douglas-Rachford Splitting (DRS). This reframes the learning process not as a direct trade-off, but as a negotiation between two decoupled objectives: one promoting plasticity for new tasks and the other enforcing stability of old knowledge. By iteratively finding a consensus through their proximal operators, DRS provides a more principled and stable learning dynamic. Our approach achieves an efficient balance between stability and plasticity without the need for auxiliary modules or complex add-ons, providing a simpler yet more powerful paradigm for continual learning systems.

</details>


### [131] [Probabilistic Label Spreading: Efficient and Consistent Estimation of Soft Labels with Epistemic Uncertainty on Graphs](https://arxiv.org/abs/2602.04574)
*Jonathan Klees,Tobias Riedlinger,Peter Stehr,Bennet Böddecker,Daniel Kondermann,Matthias Rottmann*

Main category: cs.LG

TL;DR: 提出一种概率标签传播方法，通过图扩散传播单标注来估计标签的偶然性和认知不确定性，显著减少标注预算并达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 感知任务的安全人工智能面临挑战，主要因为缺乏高质量标注数据。标注本身存在偶然性和认知不确定性，但通常在标注和评估中被忽略。虽然众包可以收集多个标注来估计这些不确定性，但由于所需标注工作量巨大，这种方法在大规模应用中不切实际。

Method: 引入概率标签传播方法，假设标签在特征空间上具有平滑性，使用基于图的扩散方法传播单个标注。该方法即使在每个数据点的标注数量趋近于零时，也能产生一致的概率估计器。提出了可扩展的实现方案。

Result: 实验结果表明，相比基线方法，该方法在常见图像数据集上显著减少了达到所需标签质量所需的标注预算，并在Data-Centric图像分类基准上达到了新的最先进水平。

Conclusion: 提出的概率标签传播方法能够可靠地估计标签的偶然性和认知不确定性，即使每个数据点只有一个标注，也能提供一致的概率估计，为大规模安全人工智能系统提供了实用的标注解决方案。

Abstract: Safe artificial intelligence for perception tasks remains a major challenge, partly due to the lack of data with high-quality labels. Annotations themselves are subject to aleatoric and epistemic uncertainty, which is typically ignored during annotation and evaluation. While crowdsourcing enables collecting multiple annotations per image to estimate these uncertainties, this approach is impractical at scale due to the required annotation effort. We introduce a probabilistic label spreading method that provides reliable estimates of aleatoric and epistemic uncertainty of labels. Assuming label smoothness over the feature space, we propagate single annotations using a graph-based diffusion method. We prove that label spreading yields consistent probability estimators even when the number of annotations per data point converges to zero. We present and analyze a scalable implementation of our method. Experimental results indicate that, compared to baselines, our approach substantially reduces the annotation budget required to achieve a desired label quality on common image datasets and achieves a new state of the art on the Data-Centric Image Classification benchmark.

</details>


### [132] [Stochastic Decision Horizons for Constrained Reinforcement Learning](https://arxiv.org/abs/2602.04599)
*Nikola Milosevic,Leonard Franz,Daniel Haeufle,Georg Martius,Nico Scherf,Pavel Kolev*

Main category: cs.LG

TL;DR: 提出基于随机决策视野的控制即推理框架，通过生存加权目标实现约束MDP的离策略学习，提出两种违反语义（吸收终止和虚拟终止），在标准基准和高维肌肉骨骼系统上展示优越性能。


<details>
  <summary>Details</summary>
Motivation: 传统CMDP方法使用加性成本约束和对偶变量，阻碍了离策略可扩展性。需要一种能保持离策略兼容性的约束处理框架。

Method: 基于随机决策视野的控制即推理框架，约束违反会衰减奖励贡献并通过状态-动作依赖的延续缩短有效规划视野，产生生存加权目标。提出两种违反语义：吸收终止和虚拟终止，分别对应不同的优化结构，实现SAC/MPO风格的策略改进。

Result: 实验显示在标准基准上提高了样本效率，获得了更好的回报-违反权衡。虚拟终止MPO（VT-MPO）在高维肌肉骨骼Hyfydy设置上有效扩展。

Conclusion: 提出的生存加权框架为约束强化学习提供了可扩展的离策略解决方案，通过控制即推理视角统一处理约束，在复杂环境中保持高效学习能力。

Abstract: Constrained Markov decision processes (CMDPs) provide a principled model for handling constraints, such as safety and other auxiliary objectives, in reinforcement learning. The common approach of using additive-cost constraints and dual variables often hinders off-policy scalability. We propose a Control as Inference formulation based on stochastic decision horizons, where constraint violations attenuate reward contributions and shorten the effective planning horizon via state-action-dependent continuation. This yields survival-weighted objectives that remain replay-compatible for off-policy actor-critic learning. We propose two violation semantics, absorbing and virtual termination, that share the same survival-weighted return but result in distinct optimization structures that lead to SAC/MPO-style policy improvement. Experiments demonstrate improved sample efficiency and favorable return-violation trade-offs on standard benchmarks. Moreover, MPO with virtual termination (VT-MPO) scales effectively to our high-dimensional musculoskeletal Hyfydy setup.

</details>


### [133] [Jacobian Regularization Stabilizes Long-Term Integration of Neural Differential Equations](https://arxiv.org/abs/2602.04608)
*Maya Janvier,Julien Salomon,Etienne Meunier*

Main category: cs.LG

TL;DR: 提出两种正则化方法，通过约束神经微分方程模型的雅可比矩阵方向导数来提升长期积分稳定性，降低训练成本


<details>
  <summary>Details</summary>
Motivation: 混合模型和神经微分方程在物理系统建模中日益重要，但在长期积分时面临稳定性和精度问题。传统基于展开轨迹的训练方法虽然能限制发散，但计算梯度成本过高

Method: 设计了两种正则化方法：1）已知动力学情况下直接推导动态方向导数；2）未知动力学情况下使用有限差分近似方向导数。两种方法都在训练过程中正则化NDE模型的雅可比矩阵

Result: 两种方法在多个常微分方程和偏微分方程上成功提升了长期模拟的稳定性，相比训练时的长展开轨迹成本大幅降低

Conclusion: 该方法为训练大规模系统长期积分的神经微分方程方法打开了大门，通过低成本的正则化策略有效解决了长期积分稳定性问题

Abstract: Hybrid models and Neural Differential Equations (NDE) are getting increasingly important for the modeling of physical systems, however they often encounter stability and accuracy issues during long-term integration. Training on unrolled trajectories is known to limit these divergences but quickly becomes too expensive due to the need for computing gradients over an iterative process. In this paper, we demonstrate that regularizing the Jacobian of the NDE model via its directional derivatives during training stabilizes long-term integration in the challenging context of short training rollouts. We design two regularizations, one for the case of known dynamics where we can directly derive the directional derivatives of the dynamic and one for the case of unknown dynamics where they are approximated using finite differences. Both methods, while having a far lower cost compared to long rollouts during training, are successful in improving the stability of long-term simulations for several ordinary and partial differential equations, opening up the door to training NDE methods for long-term integration of large scale systems.

</details>


### [134] [Resilient Load Forecasting under Climate Change: Adaptive Conditional Neural Processes for Few-Shot Extreme Load Forecasting](https://arxiv.org/abs/2602.04609)
*Chenxi Hu,Yue Ma,Yifan Wu,Yunhe Hou*

Main category: cs.LG

TL;DR: AdaCNP是一个用于极端天气下电力负荷概率预测的模型，通过共享嵌入空间学习相似性，对历史上下文信息进行重新加权，能够在极端样本稀缺的情况下实现少样本适应，提高预测鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 极端天气会显著改变电力消费行为，导致负荷曲线出现剧烈波动。如果预测不准确，电力系统可能面临供电短缺或局部过载，需要采取紧急措施如减载，增加服务中断和公共安全风险。这个问题本质上是困难的，因为极端事件会触发负荷模式的突然变化，而相关的极端样本稀少且不规则，使得可靠学习和校准具有挑战性。

Method: AdaCNP是一个用于数据稀缺条件下的概率预测模型。它在共享嵌入空间中学习相似性，对每个目标数据评估每个历史上下文段与当前条件的相关性，并相应地对上下文信息进行重新加权。这种设计即使在极端样本稀少的情况下也能突出最有信息量的历史证据，实现对先前未见极端模式的少样本适应。模型还能产生预测分布，用于风险感知决策，而无需在目标域上进行昂贵的微调。

Result: 在真实世界电力系统负荷数据上的评估表明，AdaCNP在极端时期更加鲁棒，相对于最强基线将均方误差降低了22%，同时实现了最低的负对数似然，表明其概率输出更加可靠。

Conclusion: AdaCNP能够有效缓解突然分布变化和极端样本稀缺的综合影响，为极端事件下的弹性电力系统运行提供更可信的预测。

Abstract: Extreme weather can substantially change electricity consumption behavior, causing load curves to exhibit sharp spikes and pronounced volatility. If forecasts are inaccurate during those periods, power systems are more likely to face supply shortfalls or localized overloads, forcing emergency actions such as load shedding and increasing the risk of service disruptions and public-safety impacts. This problem is inherently difficult because extreme events can trigger abrupt regime shifts in load patterns, while relevant extreme samples are rare and irregular, making reliable learning and calibration challenging. We propose AdaCNP, a probabilistic forecasting model for data-scarce condition. AdaCNP learns similarity in a shared embedding space. For each target data, it evaluates how relevant each historical context segment is to the current condition and reweights the context information accordingly. This design highlights the most informative historical evidence even when extreme samples are rare. It enables few-shot adaptation to previously unseen extreme patterns. AdaCNP also produces predictive distributions for risk-aware decision-making without expensive fine-tuning on the target domain. We evaluate AdaCNP on real-world power-system load data and compare it against a range of representative baselines. The results show that AdaCNP is more robust during extreme periods, reducing the mean squared error by 22\% relative to the strongest baseline while achieving the lowest negative log-likelihood, indicating more reliable probabilistic outputs. These findings suggest that AdaCNP can effectively mitigate the combined impact of abrupt distribution shifts and scarce extreme samples, providing a more trustworthy forecasting for resilient power system operation under extreme events.

</details>


### [135] [QUATRO: Query-Adaptive Trust Region Policy Optimization for LLM Fine-tuning](https://arxiv.org/abs/2602.04620)
*Doyeon Lee,Eunyi Lyou,Hyunsoo Cho,Sookyung Kim,Joonseok Lee,Jaemoo Choi*

Main category: cs.LG

TL;DR: QUATRO是一种新的强化学习LLM微调方法，通过精确的信任区域约束解决现有GRPO方法中启发式近似导致的优化不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO风格的RL微调方法依赖启发式的信任区域近似，导致优化行为脆弱。全局重要性比率裁剪和组归一化无法有效调节超出裁剪范围的样本，需要更精确的信任区域约束方法。

Method: 提出QUATRO方法，通过原则性优化直接强制执行信任区域约束。该方法产生清晰可解释的目标函数，能够显式控制策略更新，实现稳定的熵控制优化，稳定项从精确的信任区域公式中自然产生。

Result: 在多种数学推理基准测试中验证，QUATRO在策略陈旧性增加和激进学习率下仍能保持稳定训练，整个训练过程中熵得到良好控制。

Conclusion: QUATRO通过精确的信任区域约束解决了现有RL微调方法的不稳定问题，提供了更稳定、可控的优化框架，特别适用于LLM的强化学习微调。

Abstract: GRPO-style reinforcement learning (RL)-based LLM fine-tuning algorithms have recently gained popularity. Relying on heuristic trust-region approximations, however, they can lead to brittle optimization behavior, as global importance-ratio clipping and group-wise normalization fail to regulate samples whose importance ratios fall outside the clipping range. We propose Query-Adaptive Trust-Region policy Optimization (QUATRO), which directly enforces trust-region constraints through a principled optimization. This yields a clear and interpretable objective that enables explicit control over policy updates and stable, entropy-controlled optimization, with a stabilizer terms arising intrinsically from the exact trust-region formulation. Empirically verified on diverse mathematical reasoning benchmarks, QUATRO shows stable training under increased policy staleness and aggressive learning rates, maintaining well-controlled entropy throughout training.

</details>


### [136] [RIGA-Fold: A General Framework for Protein Inverse Folding via Recurrent Interaction and Geometric Awareness](https://arxiv.org/abs/2602.04637)
*Sisi Yuan,Jiehuang Chen,Junchuang Cai,Dong Xu,Xueliang Li,Zexuan Zhu,Junkai Ji*

Main category: cs.LG

TL;DR: RIGA-Fold是一个用于蛋白质逆折叠的几何感知循环交互框架，通过几何注意力更新和全局上下文桥接解决现有GNN方法的感受野限制和错误累积问题，RIGA-Fold*变体结合ESM先验知识，在序列恢复和结构一致性方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于GNN的蛋白质逆折叠方法存在两个主要瓶颈：1）受限的感受野无法捕捉长程依赖关系；2）"单次通过"的推理范式导致错误累积。需要解决这些限制以提高蛋白质设计的准确性和可靠性。

Method: 提出RIGA-Fold框架，包含：1）微观层面的几何注意力更新模块，使用边特征作为注意力键确保SE(3)不变性；2）宏观层面的注意力全局上下文桥接，动态注入全局拓扑信息；3）RIGA-Fold*变体结合可训练几何特征和ESM-2/ESM-IF的冻结进化先验；4）生物启发的"预测-循环-精炼"迭代去噪策略。

Result: 在CATH 4.2、TS50和TS500基准测试中，几何框架具有高度竞争力，RIGA-Fold*在序列恢复和结构一致性方面显著优于最先进的基线方法。

Conclusion: RIGA-Fold通过几何感知的循环交互解决了蛋白质逆折叠中的关键瓶颈，结合进化先验的RIGA-Fold*变体在多个基准测试中表现出色，为蛋白质设计提供了更准确可靠的解决方案。

Abstract: Protein inverse folding, the task of predicting amino acid sequences for desired structures, is pivotal for de novo protein design. However, existing GNN-based methods typically suffer from restricted receptive fields that miss long-range dependencies and a "single-pass" inference paradigm that leads to error accumulation. To address these bottlenecks, we propose RIGA-Fold, a framework that synergizes Recurrent Interaction with Geometric Awareness. At the micro-level, we introduce a Geometric Attention Update (GAU) module where edge features explicitly serve as attention keys, ensuring strictly SE(3)-invariant local encoding. At the macro-level, we design an attention-based Global Context Bridge that acts as a soft gating mechanism to dynamically inject global topological information. Furthermore, to bridge the gap between structural and sequence modalities, we introduce an enhanced variant, RIGA-Fold*, which integrates trainable geometric features with frozen evolutionary priors from ESM-2 and ESM-IF via a dual-stream architecture. Finally, a biologically inspired ``predict-recycle-refine'' strategy is implemented to iteratively denoise sequence distributions. Extensive experiments on CATH 4.2, TS50, and TS500 benchmarks demonstrate that our geometric framework is highly competitive, while RIGA-Fold* significantly outperforms state-of-the-art baselines in both sequence recovery and structural consistency.

</details>


### [137] [MTS-JEPA: Multi-Resolution Joint-Embedding Predictive Architecture for Time-Series Anomaly Prediction](https://arxiv.org/abs/2602.04643)
*Yanan He,Yunshi Wen,Xin Wang,Tengfei Ma*

Main category: cs.LG

TL;DR: MTS-JEPA：一种用于多元时间序列异常预测的架构，通过多分辨率预测目标和软码本瓶颈解决JEPA中的表示崩溃问题，能有效分离瞬态冲击和长期趋势，实现早期预警。


<details>
  <summary>Details</summary>
Motivation: 多元时间序列对关键基础设施至关重要，需要主动预测异常以降低风险。现有的联合嵌入预测架构（JEPA）存在表示崩溃问题，且无法捕捉不同时间尺度的前兆信号。

Method: 提出MTS-JEPA架构，整合多分辨率预测目标和软码本瓶颈，显式解耦瞬态冲击和长期趋势，利用码本捕捉离散状态转换，同时作为内在正则化器确保优化稳定性。

Result: 在标准基准测试中，该方法有效防止退化解，并在早期预警协议下实现了最先进的性能。

Conclusion: MTS-JEPA通过多分辨率预测和软码本瓶颈解决了JEPA在多元时间序列异常预测中的关键限制，为关键基础设施的风险缓解提供了有效的早期预警解决方案。

Abstract: Multivariate time series underpin modern critical infrastructure, making the prediction of anomalies a vital necessity for proactive risk mitigation. While Joint-Embedding Predictive Architectures (JEPA) offer a promising framework for modeling the latent evolution of these systems, their application is hindered by representation collapse and an inability to capture precursor signals across varying temporal scales. To address these limitations, we propose MTS-JEPA, a specialized architecture that integrates a multi-resolution predictive objective with a soft codebook bottleneck. This design explicitly decouples transient shocks from long-term trends, and utilizes the codebook to capture discrete regime transitions. Notably, we find this constraint also acts as an intrinsic regularizer to ensure optimization stability. Empirical evaluations on standard benchmarks confirm that our approach effectively prevents degenerate solutions and achieves state-of-the-art performance under the early-warning protocol.

</details>


### [138] [SAFE: Stable Alignment Finetuning with Entropy-Aware Predictive Control for RLHF](https://arxiv.org/abs/2602.04651)
*Dipan Maity*

Main category: cs.LG

TL;DR: SAFE是一种新的RLHF算法，通过双重软最小批评家、熵门控KL调节和PID控制自适应阈值，解决了PPO在语言模型RLHF中的不稳定问题，实现了更稳定高效的训练。


<details>
  <summary>Details</summary>
Motivation: PPO作为RLHF的标准方法存在诸多问题：启发式设计、KL散度约束处理随意、奖励振荡、熵崩溃、价值函数漂移、策略突然发散等，需要频繁重启和大量超参数调优。

Method: SAFE算法结合了双重软最小批评家进行悲观价值估计，以及多层稳定化框架，包括熵门控KL调节和PID控制的自适应阈值。与PPO的对称KL惩罚不同，SAFE区分高熵探索和低熵模式崩溃，并根据奖励速度动态调整惩罚。

Result: 在30亿参数模型上的实验显示，SAFE比PPO获得+5.15%的训练平均奖励（0.725 vs 0.689），奖励崩溃可忽略不计，KL控制优于PPO，计算开销最小。

Conclusion: SAFE提供了一个可解释、抗崩溃的RLHF框架，在保持快速学习速度的同时确保稳定的长期优化，适合生产部署。

Abstract: Optimization (PPO) has been positioned by recent literature as the canonical method for the RL part of RLHF. PPO performs well empirically but has a heuristic motivation and handles the KL-divergence constraint used in LM-RLHF in an ad-hoc manner and suffers form reward oscillations, entropy collapse, value function drift, and sudden policy divergence that require frequent restarts and extensive hyperparameter tuning. In this paper, we develop a new pure on policy actor-critic RL method for the LM-RLHF setting. We present SAFE (Stable Alignment Finetuning with Entropy-aware control),a novel RLHF algorithm that combines a Double Soft-Min Critic for pessimistic value estimation with a new multi-layer stabilization framework combining entropy-gated KL regulation, and PID-controlled adaptive thresholds. Unlike standard PPO's symmetric KL penalties, SAFE distinguishes high-entropy exploration from low-entropy mode collapse and adjusts penalties dynamically based on reward velocity. Experiments on a 3B parameter model show SAFE achieves +5.15\% training-average reward than PPO (0.725 vs 0.689), negligible reward crashes, and superior KL control than ppo . Our method adds minimal computational overhead and provides an interpretable, crash-resistant RLHF framework that maintains aggressive learning speed while ensuring stable long-horizon optimization suitable for production deployment. Code is available at https://github.com/ryyzn9/SAFE

</details>


### [139] [Rethinking the Design Space of Reinforcement Learning for Diffusion Models: On the Importance of Likelihood Estimation Beyond Loss Design](https://arxiv.org/abs/2602.04663)
*Jaemoo Choi,Yuchen Zhu,Wei Guo,Petr Molodyk,Bo Yuan,Jinbin Bai,Yi Xin,Molei Tao,Yongxin Chen*

Main category: cs.LG

TL;DR: 该论文系统分析了扩散模型强化学习的设计空间，发现基于ELBO的模型似然估计是高效稳定RL优化的关键因素，显著提升了生成质量与效率。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在文本到图像生成等视觉任务中应用广泛，但由于其难以处理的似然函数，直接应用流行的策略梯度方法存在障碍。现有方法主要基于已经大量工程化的LLM目标构建新目标，使用临时性的似然估计器，缺乏对这些估计如何影响整体算法性能的系统研究。

Method: 通过解耦三个因素进行系统分析：1) 策略梯度目标，2) 似然估计器，3) 轨迹采样方案。研究发现采用基于证据下界(ELBO)的模型似然估计器（仅从最终生成样本计算）是实现有效、高效和稳定RL优化的主导因素。

Result: 在多个奖励基准测试中使用SD 3.5 Medium验证了方法的有效性。GenEval分数从0.24提升到0.95，仅需90 GPU小时，比FlowGRPO效率高4.6倍，比当前最先进的DiffusionNFT方法效率高2倍，且没有奖励黑客问题。

Conclusion: 基于ELBO的模型似然估计是扩散模型强化学习优化的关键，其重要性超过了特定策略梯度损失函数的影响。该方法在多个任务中表现一致，为扩散模型的RL优化提供了系统性的设计指导。

Abstract: Reinforcement learning has been widely applied to diffusion and flow models for visual tasks such as text-to-image generation. However, these tasks remain challenging because diffusion models have intractable likelihoods, which creates a barrier for directly applying popular policy-gradient type methods. Existing approaches primarily focus on crafting new objectives built on already heavily engineered LLM objectives, using ad hoc estimators for likelihood, without a thorough investigation into how such estimation affects overall algorithmic performance. In this work, we provide a systematic analysis of the RL design space by disentangling three factors: i) policy-gradient objectives, ii) likelihood estimators, and iii) rollout sampling schemes. We show that adopting an evidence lower bound (ELBO) based model likelihood estimator, computed only from the final generated sample, is the dominant factor enabling effective, efficient, and stable RL optimization, outweighing the impact of the specific policy-gradient loss functional. We validate our findings across multiple reward benchmarks using SD 3.5 Medium, and observe consistent trends across all tasks. Our method improves the GenEval score from 0.24 to 0.95 in 90 GPU hours, which is $4.6\times$ more efficient than FlowGRPO and $2\times$ more efficient than the SOTA method DiffusionNFT without reward hacking.

</details>


### [140] [Delving into Muon and Beyond: Deep Analysis and Extensions](https://arxiv.org/abs/2602.04669)
*Xianbiao Qi,Marco Chen,Jiaquan Ye,Yelin He,Rong Xiao*

Main category: cs.LG

TL;DR: 论文通过谱视角分析Muon优化器，将其视为谱变换族(p=0)的端点，并与Adam等自适应优化器比较，发现Muon本质上是有效的谱归一化方法而非普遍更优的优化器。


<details>
  <summary>Details</summary>
Motivation: Muon优化器因其强经验性能和正交化更新而受关注，但其底层机制与Adam等自适应优化器的关系尚未充分理解，需要从统一谱视角进行研究。

Method: 将Muon视为谱变换族UΣ^pV'的p=0端点，考虑p=1/2、1/4、1等变体，应用于动量SGD和RMS归一化梯度更新，开发耦合牛顿迭代避免显式SVD计算。

Result: RMS归一化更新比一阶矩更新更稳定；谱压缩在一阶矩更新下提供强稳定化效果，但Muon(p=0)并不始终优于Adam。

Conclusion: Muon应被理解为有效的谱归一化形式，而非普遍更优的优化方法，其优势在于谱归一化而非超越Adam的优化性能。

Abstract: The Muon optimizer has recently attracted considerable attention for its strong empirical performance and use of orthogonalized updates on matrix-shaped parameters, yet its underlying mechanisms and relationship to adaptive optimizers such as Adam remain insufficiently understood. In this work, we aim to address these questions through a unified spectral perspective. Specifically, we view Muon as the p = 0 endpoint of a family of spectral transformations of the form U \boldsymbolΣ^{p} V' , and consider additional variants with p = 1/2 , p = 1/4 , and p = 1 . These transformations are applied to both first-moment updates, as in momentum SGD, and to root-mean-square (RMS) normalized gradient updates as in Adam. To enable efficient computation, we develop a coupled Newton iteration that avoids explicit singular value decomposition. Across controlled experiments, we find that RMS-normalized updates yield more stable optimization than first-moment updates. Moreover, while spectral compression provides strong stabilization benefits under first-moment updates, the Muon update (p = 0) does not consistently outperform Adam. These results suggest that Muon is best understood as an effective form of spectral normalization, but not a universally superior optimization method. Our source code will be released at https://github.com/Ocram7/BeyondMuon.

</details>


### [141] [Generalized Schrödinger Bridge on Graphs](https://arxiv.org/abs/2602.04675)
*Panagiotis Theodoropoulos,Juno Nam,Evangelos Theodorou,Jaemoo Choi*

Main category: cs.LG

TL;DR: 提出GSBoG框架，用于在任意图上学习可执行的连续时间马尔可夫链策略，通过似然优化方法实现可扩展的轨迹级策略学习


<details>
  <summary>Details</summary>
Motivation: 现有图传输方法存在局限性：缺乏可执行策略表达能力、依赖限制性假设、在稀疏拓扑上泛化能力差、随着图规模和时间范围扩大而扩展性差

Method: 引入广义薛定谔桥框架，通过似然优化方法学习轨迹级策略，避免使用密集全局求解器，在满足端点边际分布的同时优化中间状态相关运行成本

Result: 在具有挑战性的真实世界图拓扑上，GSBoG能够可靠地学习准确、尊重拓扑的策略，同时优化应用特定的中间状态成本

Conclusion: GSBoG为通用图上的成本感知动态传输开辟了新途径，展示了广泛的适用性

Abstract: Transportation on graphs is a fundamental challenge across many domains, where decisions must respect topological and operational constraints. Despite the need for actionable policies, existing graph-transport methods lack this expressivity. They rely on restrictive assumptions, fail to generalize across sparse topologies, and scale poorly with graph size and time horizon. To address these issues, we introduce Generalized Schrödinger Bridge on Graphs (GSBoG), a novel scalable data-driven framework for learning executable controlled continuous-time Markov chain (CTMC) policies on arbitrary graphs under state cost augmented dynamics. Notably, GSBoG learns trajectory-level policies, avoiding dense global solvers and thereby enhancing scalability. This is achieved via a likelihood optimization approach, satisfying the endpoint marginals, while simultaneously optimizing intermediate behavior under state-dependent running costs. Extensive experimentation on challenging real-world graph topologies shows that GSBoG reliably learns accurate, topology-respecting policies while optimizing application-specific intermediate state costs, highlighting its broad applicability and paving new avenues for cost-aware dynamical transport on general graphs.

</details>


### [142] [REDistill: Robust Estimator Distillation for Balancing Robustness and Efficiency](https://arxiv.org/abs/2602.04677)
*Ondrej Tybl,Lukas Neumann*

Main category: cs.LG

TL;DR: REDistill提出了一种基于鲁棒统计的知识蒸馏框架，使用幂散度损失替代传统KL散度，自适应降低不可靠教师输出的权重，无需特定超参数调优即可提升学生模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏方法基于KL散度，假设教师模型提供可靠的软目标。但实际上教师预测往往存在噪声或过度自信，现有校正方法依赖启发式规则和大量超参数调优，泛化能力有限。

Method: 提出REDistill框架，使用幂散度损失替代标准KD目标，这是KL散度的泛化形式，能够自适应降低不可靠教师输出的权重，同时保留信息丰富的logit关系。该方法仅需logits，可无缝集成到现有KD流程中。

Result: 在CIFAR-100和ImageNet-1k上的大量实验表明，REDistill在不同师生架构中持续提升学生模型准确率。值得注意的是，该方法无需模型特定的超参数调优，展现了强大的鲁棒性和对未见师生对的泛化能力。

Conclusion: REDistill提供了一个基于鲁棒统计的简单而原则性的知识蒸馏框架，通过幂散度损失有效处理教师噪声，无需复杂调优即可实现性能提升，具有良好的通用性和实用性。

Abstract: Knowledge Distillation (KD) transfers knowledge from a large teacher model to a smaller student by aligning their predictive distributions. However, conventional KD formulations - typically based on Kullback-Leibler divergence - assume that the teacher provides reliable soft targets. In practice, teacher predictions are often noisy or overconfident, and existing correction-based approaches rely on ad-hoc heuristics and extensive hyper-parameter tuning, which hinders generalization. We introduce REDistill (Robust Estimator Distillation), a simple yet principled framework grounded in robust statistics. REDistill replaces the standard KD objective with a power divergence loss, a generalization of KL divergence that adaptively downweights unreliable teacher output while preserving informative logit relationships. This formulation provides a unified and interpretable treatment of teacher noise, requires only logits, integrates seamlessly into existing KD pipelines, and incurs negligible computational overhead. Extensive experiments on CIFAR-100 and ImageNet-1k demonstrate that REDistill consistently improves student accuracy in diverse teacher-student architectures. Remarkably, it achieves these gains without model-specific hyper-parameter tuning, underscoring its robustness and strong generalization to unseen teacher-student pairs.

</details>


### [143] [Let Experts Feel Uncertainty: A Multi-Expert Label Distribution Approach to Probabilistic Time Series Forecasting](https://arxiv.org/abs/2602.04678)
*Zhen Zhou,Zhirui Wang,Qi Hong,Yunyang Shi,Ziyuan Gu,Zhiyuan Liu*

Main category: cs.LG

TL;DR: 提出Multi-Expert LDL框架，通过混合专家架构和分布学习实现时间序列预测，平衡预测精度与可解释的不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 现实世界时间序列预测需要高预测精度和可解释的不确定性量化。传统点预测方法无法捕捉数据固有不确定性，而现有概率方法难以平衡计算效率与可解释性。

Method: 提出两种方法：1) Multi-Expert LDL：使用多个具有不同学习参数的专家捕捉多样时间模式；2) Pattern-Aware LDL-MoE：通过专门子专家将时间序列显式分解为可解释组件（趋势、季节性、变点、波动性）。两种框架都通过最大均值差异实现分布学习。

Result: 在M5数据集衍生的聚合销售数据上评估，相比基线方法表现更优。连续Multi-Expert LDL获得最佳整体性能，Pattern-Aware LDL-MoE通过组件分析提供增强的可解释性。

Conclusion: 该框架成功平衡了预测精度与可解释性，适用于需要性能和可操作洞察的现实世界预测应用。

Abstract: Time series forecasting in real-world applications requires both high predictive accuracy and interpretable uncertainty quantification. Traditional point prediction methods often fail to capture the inherent uncertainty in time series data, while existing probabilistic approaches struggle to balance computational efficiency with interpretability. We propose a novel Multi-Expert Learning Distributional Labels (LDL) framework that addresses these challenges through mixture-of-experts architectures with distributional learning capabilities. Our approach introduces two complementary methods: (1) Multi-Expert LDL, which employs multiple experts with different learned parameters to capture diverse temporal patterns, and (2) Pattern-Aware LDL-MoE, which explicitly decomposes time series into interpretable components (trend, seasonality, changepoints, volatility) through specialized sub-experts. Both frameworks extend traditional point prediction to distributional learning, enabling rich uncertainty quantification through Maximum Mean Discrepancy (MMD). We evaluate our methods on aggregated sales data derived from the M5 dataset, demonstrating superior performance compared to baseline approaches. The continuous Multi-Expert LDL achieves the best overall performance, while the Pattern-Aware LDL-MoE provides enhanced interpretability through component-wise analysis. Our frameworks successfully balance predictive accuracy with interpretability, making them suitable for real-world forecasting applications where both performance and actionable insights are crucial.

</details>


### [144] [Static and auto-regressive neural emulation of phytoplankton biomass dynamics from physical predictors in the global ocean](https://arxiv.org/abs/2602.04689)
*Mahima Lakra,Ronan Fablet,Lucas Drumetz,Etienne Pauthenet,Elodie Martinez*

Main category: cs.LG

TL;DR: 使用深度学习模型（特别是UNet架构）基于卫星观测和环境条件预测全球海洋浮游植物生物量的时空分布，UNet在季节和年际模式预测上表现最佳，自回归版本可用于短期预测。


<details>
  <summary>Details</summary>
Motivation: 浮游植物是海洋食物网的基础，对生态过程和全球生物地球化学循环至关重要，但现有数值模型由于参数化有限、观测数据稀疏和海洋过程复杂性，难以准确模拟浮游植物动态。

Method: 研究比较了多种深度学习架构（CNN、ConvLSTM、4CastNet、UNet），使用卫星观测和环境数据作为输入，并测试了自回归版本的UNet，利用模型自身先前预测来预测未来条件。

Result: UNet在再现浮游植物生物量季节和年际模式方面表现最佳，使用1-2个月环境数据输入时效果更好，但会低估低频变化的幅度。自回归UNet在短期预测（最多5个月）表现良好，但长期预测性能下降。

Conclusion: 结合海洋物理预测因子与深度学习可以重建和短期预测浮游植物动态，这些模型可能成为监测海洋健康和支持海洋生态系统管理的强大工具，特别是在气候变化背景下。

Abstract: Phytoplankton is the basis of marine food webs, driving both ecological processes and global biogeochemical cycles. Despite their ecological and climatic significance, accurately simulating phytoplankton dynamics remains a major challenge for biogeochemical numerical models due to limited parameterizations, sparse observational data, and the complexity of oceanic processes. Here, we explore how deep learning models can be used to address these limitations predicting the spatio-temporal distribution of phytoplankton biomass in the global ocean based on satellite observations and environmental conditions. First, we investigate several deep learning architectures. Among the tested models, the UNet architecture stands out for its ability to reproduce the seasonal and interannual patterns of phytoplankton biomass more accurately than other models like CNNs, ConvLSTM, and 4CastNet. When using one to two months of environmental data as input, UNet performs better, although it tends to underestimate the amplitude of low-frequency changes in phytoplankton biomass. Thus, to improve predictions over time, an auto-regressive version of UNet was also tested, where the model uses its own previous predictions to forecast future conditions. This approach works well for short-term forecasts (up to five months), though its performance decreases for longer time scales. Overall, our study shows that combining ocean physical predictors with deep learning allows for reconstruction and short-term prediction of phytoplankton dynamics. These models could become powerful tools for monitoring ocean health and supporting marine ecosystem management, especially in the context of climate change.

</details>


### [145] [Towards Understanding and Avoiding Limitations of Convolutions on Graphs](https://arxiv.org/abs/2602.04709)
*Andreas Roth*

Main category: cs.LG

TL;DR: 该论文深入分析了MPNN的理论局限性，提出了SCA和CD两个关键问题，并开发了MRS、MIMO-GC/LMGC和基于PageRank的变体来解决这些问题，深化了对MPNN的理论理解。


<details>
  <summary>Details</summary>
Motivation: 尽管消息传递神经网络（MPNNs）显示出有前景的结果，但其实际应用仍然有限。现有研究对MPNN的理论基础理解不足，导致研究碎片化。论文旨在深入理论分析，识别限制MPNN性能的关键属性，并提出解决方案。

Method: 1. 识别MPNN的两个关键属性：共享组件放大（SCA）和组件主导（CD），这些导致节点表示秩崩溃；2. 提出多关系分割（MRS）框架，将现有MPNN转换为利用多边关系；3. 引入多特征通道的谱图卷积（MIMO-GC）及其局部变体LMGC；4. 基于个性化PageRank提出允许无限消息传递迭代的MPNN变体。

Result: 论文成功识别了MPNN的SCA和CD问题，这些导致节点表示秩崩溃（泛化了过平滑现象）。提出的MRS、MIMO-GC/LMGC和基于PageRank的变体能够有效解决这些问题，避免SCA和CD，同时保留初始节点特征。

Conclusion: 该研究深化了对MPNN的理论理解，通过识别SCA和CD问题并开发相应解决方案，为MPNN研究提供了更精确的理论框架，有助于更针对性的解决方案和更精确的学术交流。

Abstract: While message-passing neural networks (MPNNs) have shown promising results, their real-world impact remains limited. Although various limitations have been identified, their theoretical foundations remain poorly understood, leading to fragmented research efforts. In this thesis, we provide an in-depth theoretical analysis and identify several key properties limiting the performance of MPNNs. Building on these findings, we propose several frameworks that address these shortcomings. We identify two properties exhibited by many MPNNs: shared component amplification (SCA), where each message-passing iteration amplifies the same components across all feature channels, and component dominance (CD), where a single component gets increasingly amplified as more message-passing steps are applied. These properties lead to the observable phenomenon of rank collapse of node representations, which generalizes the established over-smoothing phenomenon. By generalizing and decomposing over-smoothing, we enable a deeper understanding of MPNNs, more targeted solutions, and more precise communication within the field. To avoid SCA, we show that utilizing multiple computational graphs or edge relations is necessary. Our multi-relational split (MRS) framework transforms any existing MPNN into one that leverages multiple edge relations. Additionally, we introduce the spectral graph convolution for multiple feature channels (MIMO-GC), which naturally uses multiple computational graphs. A localized variant, LMGC, approximates the MIMO-GC while inheriting its beneficial properties. To address CD, we demonstrate a close connection between MPNNs and the PageRank algorithm. Based on personalized PageRank, we propose a variant of MPNNs that allows for infinitely many message-passing iterations, while preserving initial node features. Collectively, these results deepen the theoretical understanding of MPNNs.

</details>


### [146] [Bounded-Abstention Multi-horizon Time-series Forecasting](https://arxiv.org/abs/2602.04714)
*Luca Stradiotti,Laurens Devos,Anna Monreale,Jesse Davis,Andrea Pugnana*

Main category: cs.LG

TL;DR: 论文提出了多时间步预测中的弃权学习框架，针对现有方法不适合多时间步预测的问题，定义了三种弃权策略并提出了相应算法。


<details>
  <summary>Details</summary>
Motivation: 多时间步预测在医疗和金融等高风险领域应用广泛，错误预测成本高且会降低信任度。现有的弃权学习框架只适用于单预测场景，忽略了多时间步预测的结构性和相关性特点。

Method: 形式化了多时间步预测中的弃权学习问题，提出了三种自然的弃权策略，通过理论分析推导出最优弃权策略，并设计了实现这些策略的算法。

Result: 在24个数据集上的广泛评估表明，提出的算法显著优于现有基线方法。

Conclusion: 多时间步预测的结构特性为弃权学习提供了更丰富的问题空间，提出的三种弃权策略和相应算法能有效处理多时间步预测中的不确定性，显著提升预测性能。

Abstract: Multi-horizon time-series forecasting involves simultaneously making predictions for a consecutive sequence of subsequent time steps. This task arises in many application domains, such as healthcare and finance, where mispredictions can have a high cost and reduce trust. The learning with abstention framework tackles these problems by allowing a model to abstain from offering a prediction when it is at an elevated risk of making a misprediction. Unfortunately, existing abstention strategies are ill-suited for the multi-horizon setting: they target problems where a model offers a single prediction for each instance. Hence, they ignore the structured and correlated nature of the predictions offered by a multi-horizon forecaster. We formalize the problem of learning with abstention for multi-horizon forecasting setting and show that its structured nature admits a richer set of abstention problems. Concretely, we propose three natural notions of how a model could abstain for multi-horizon forecasting. We theoretically analyze each problem to derive the optimal abstention strategy and propose an algorithm that implements it. Extensive evaluation on 24 datasets shows that our proposed algorithms significantly outperforms existing baselines.

</details>


### [147] [Identifying Intervenable and Interpretable Features via Orthogonality Regularization](https://arxiv.org/abs/2602.04718)
*Moritz Miller,Florent Draye,Bernhard Schölkopf*

Main category: cs.LG

TL;DR: 通过正交化惩罚改进稀疏自编码器，减少特征间的干扰和叠加，同时保持性能，提升特征可解释性和因果干预能力


<details>
  <summary>Details</summary>
Motivation: 现有稀疏自编码器特征存在干扰和叠加问题，影响特征可解释性和因果干预能力。正交化可以促进模块化表示，符合独立因果机制原理

Method: 在微调语言模型时，对解码器矩阵施加正交化惩罚，使特征近似正交，同时保持目标数据集性能基本不变

Result: 正交化惩罚减少了特征间的干扰和叠加，提高了特征可识别性和唯一性，特征解释之间的距离随正交化强度增加而增大，支持孤立干预

Conclusion: 正交化惩罚能产生更可解释、模块化的特征表示，支持因果干预，符合独立因果机制原理，为可解释AI提供了新方向

Abstract: With recent progress on fine-tuning language models around a fixed sparse autoencoder, we disentangle the decoder matrix into almost orthogonal features. This reduces interference and superposition between the features, while keeping performance on the target dataset essentially unchanged. Our orthogonality penalty leads to identifiable features, ensuring the uniqueness of the decomposition. Further, we find that the distance between embedded feature explanations increases with stricter orthogonality penalty, a desirable property for interpretability. Invoking the $\textit{Independent Causal Mechanisms}$ principle, we argue that orthogonality promotes modular representations amenable to causal intervention. We empirically show that these increasingly orthogonalized features allow for isolated interventions. Our code is available under $\texttt{https://github.com/mrtzmllr/sae-icm}$.

</details>


### [148] [Benchmarking and Enhancing PPG-Based Cuffless Blood Pressure Estimation Methods](https://arxiv.org/abs/2602.04725)
*Neville Mathew,Yidan Shen,Renjie Hu,Maham Rahimi,George Zouridakis*

Main category: cs.LG

TL;DR: PPG信号无袖带血压筛查研究：创建标准化基准数据集NBPDB，评估现有模型均未达到临床标准，加入人口统计学数据后模型性能显著提升，MInception模型接近AAMI/ISO标准。


<details>
  <summary>Details</summary>
Motivation: 现有基于PPG的血压估计模型在临床数值标准（AAMI/ISO 81060-2）上表现不一致，且缺乏在生理控制条件下的公平基准测试。公开数据集异质性强，缺乏生理控制条件，需要建立标准化基准。

Method: 1. 创建标准化基准数据集NBPDB：从MIMIC-III和VitalDB中提取101,453个高质量PPG片段（来自1,103名健康成人）；2. 系统评估多个SOTA PPG模型；3. 改进模型：修改模型架构并加入年龄、性别、BMI等人口统计学数据作为额外输入。

Result: 1. 所有评估模型均未达到AAMI/ISO 81060-2标准（平均误差<5 mmHg，标准差<8 mmHg）；2. 加入人口统计学数据后，所有模型性能一致提升；3. MInception模型误差降低23%，达到4.75 mmHg（SBP）和2.90 mmHg（DBP）的平均绝对误差，接近临床标准。

Conclusion: 现有PPG血压估计模型在标准化条件下缺乏临床实用性，但加入人口统计学信息能显著提高准确性和生理有效性，为可扩展心血管健康评估提供了改进方向。

Abstract: Cuffless blood pressure screening based on easily acquired photoplethysmography (PPG) signals offers a practical pathway toward scalable cardiovascular health assessment. Despite rapid progress, existing PPG-based blood pressure estimation models have not consistently achieved the established clinical numerical limits such as AAMI/ISO 81060-2, and prior evaluations often lack the rigorous experimental controls necessary for valid clinical assessment. Moreover, the publicly available datasets commonly used are heterogeneous and lack physiologically controlled conditions for fair benchmarking. To enable fair benchmarking under physiologically controlled conditions, we created a standardized benchmarking subset NBPDB comprising 101,453 high-quality PPG segments from 1,103 healthy adults, derived from MIMIC-III and VitalDB. Using this dataset, we systematically benchmarked several state-of-the-art PPG-based models. The results showed that none of the evaluated models met the AAMI/ISO 81060-2 accuracy requirements (mean error $<$ 5 mmHg and standard deviation $<$ 8 mmHg). To improve model accuracy, we modified these models and added patient demographic data such as age, sex, and body mass index as additional inputs. Our modifications consistently improved performance across all models. In particular, the MInception model reduced error by 23\% after adding the demographic data and yielded mean absolute errors of 4.75 mmHg (SBP) and 2.90 mmHg (DBP), achieves accuracy comparable to the numerical limits defined by AAMI/ISO accuracy standards. Our results show that existing PPG-based BP estimation models lack clinical practicality under standardized conditions, while incorporating demographic information markedly improves their accuracy and physiological validity.

</details>


### [149] [DMFlow: Disordered Materials Generation by Flow Matching](https://arxiv.org/abs/2602.04734)
*Liming Wu,Rui Jiao,Qi Li,Mingze Li,Songyou Li,Shifeng Jin,Wenbing Huang*

Main category: cs.LG

TL;DR: DMFlow是一个专门为无序晶体设计的生成框架，通过流匹配模型联合生成所有结构组件，在晶体结构预测和从头生成任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大多数深度生成模型只关注完美有序晶体，忽略了重要的无序材料类别。为了填补这一空白，需要专门为无序晶体设计生成框架。

Method: 引入统一表示方法处理有序、置换无序和位置无序晶体；采用黎曼流匹配框架和球面重参数化确保物理有效的无序权重；使用新颖的图神经网络学习向量场；通过两阶段离散化过程将连续权重转换为多热原子分配。

Result: 在从晶体学开放数据库整理的SD、PD和混合结构基准测试中，DMFlow在晶体结构预测和从头生成任务上显著优于最先进的基线方法。

Conclusion: DMFlow为AI驱动的无序材料发现提供了基础，填补了现有生成模型在无序晶体领域的空白。

Abstract: The design of materials with tailored properties is crucial for technological progress. However, most deep generative models focus exclusively on perfectly ordered crystals, neglecting the important class of disordered materials. To address this gap, we introduce DMFlow, a generative framework specifically designed for disordered crystals. Our approach introduces a unified representation for ordered, Substitutionally Disordered (SD), and Positionally Disordered (PD) crystals, and employs a flow matching model to jointly generate all structural components. A key innovation is a Riemannian flow matching framework with spherical reparameterization, which ensures physically valid disorder weights on the probability simplex. The vector field is learned by a novel Graph Neural Network (GNN) that incorporates physical symmetries and a specialized message-passing scheme. Finally, a two-stage discretization procedure converts the continuous weights into multi-hot atomic assignments. To support research in this area, we release a benchmark containing SD, PD, and mixed structures curated from the Crystallography Open Database. Experiments on Crystal Structure Prediction (CSP) and De Novo Generation (DNG) tasks demonstrate that DMFlow significantly outperforms state-of-the-art baselines adapted from ordered crystal generation. We hope our work provides a foundation for the AI-driven discovery of disordered materials.

</details>


### [150] [From Data to Behavior: Predicting Unintended Model Behaviors Before Training](https://arxiv.org/abs/2602.04735)
*Mengru Wang,Zhenqian Xu,Junfeng Fang,Yunzhi Yao,Shumin Deng,Huajun Chen,Ningyu Zhang*

Main category: cs.LG

TL;DR: 提出Data2Behavior任务和MDF方法，通过数据特征注入预测模型在微调前可能出现的意外行为，无需参数更新，节省80%GPU资源


<details>
  <summary>Details</summary>
Motivation: 大语言模型即使从看似良性的训练数据中也可能习得意外偏见，现有方法难以在微调前检测这些风险，事后评估成本高且效率低

Method: 提出Manipulating Data Features (MDF)方法：通过候选数据的平均表示总结数据特征，将其注入基础模型的前向传播中，让数据中的潜在统计信号塑造模型激活，从而揭示潜在偏见和安全风险，无需更新任何参数

Result: MDF在Qwen3-14B、Qwen2.5-32B-Instruct和Gemma-3-12b-it上的实验证实，能够可靠预测意外行为，并提供对预训练漏洞的洞察，同时仅消耗约20%的微调所需GPU资源

Conclusion: Data2Behavior任务和MDF方法为在模型训练前预测意外行为提供了有效解决方案，显著降低了风险评估的计算成本，有助于更早地发现和缓解模型偏见与安全问题

Abstract: Large Language Models (LLMs) can acquire unintended biases from seemingly benign training data even without explicit cues or malicious content. Existing methods struggle to detect such risks before fine-tuning, making post hoc evaluation costly and inefficient. To address this challenge, we introduce Data2Behavior, a new task for predicting unintended model behaviors prior to training. We also propose Manipulating Data Features (MDF), a lightweight approach that summarizes candidate data through their mean representations and injects them into the forward pass of a base model, allowing latent statistical signals in the data to shape model activations and reveal potential biases and safety risks without updating any parameters. MDF achieves reliable prediction while consuming only about 20% of the GPU resources required for fine-tuning. Experiments on Qwen3-14B, Qwen2.5-32B-Instruct, and Gemma-3-12b-it confirm that MDF can anticipate unintended behaviors and provide insight into pre-training vulnerabilities.

</details>


### [151] [Rationality Measurement and Theory for Reinforcement Learning Agents](https://arxiv.org/abs/2602.04737)
*Kejiang Qian,Amos Storkey,Fengxiang He*

Main category: cs.LG

TL;DR: 该论文提出了强化学习智能体的理性度量框架，定义了理性风险、理性风险缺口等概念，并理论分析了环境偏移和算法泛化性对理性行为的影响。


<details>
  <summary>Details</summary>
Motivation: 强化学习智能体的理性属性日益重要但很少被探索，需要建立理论框架来量化智能体在部署时的理性程度，并分析影响理性的因素。

Method: 定义完美理性行为（最大化隐藏真实价值函数），提出期望理性风险和理性风险缺口概念，将风险缺口分解为环境偏移引起的外在分量和算法泛化性引起的内在分量，并用Wasserstein距离和Rademacher复杂度进行理论上界分析。

Result: 理论分析表明：1）环境偏移（训练与部署环境差异）通过Wasserstein距离影响理性；2）算法泛化性通过价值函数类的Rademacher复杂度影响理性。实验验证了正则化技术（层归一化、L2正则化、权重归一化）和领域随机化的益处，以及环境偏移的危害。

Conclusion: 该研究建立了强化学习理性度量的理论框架，为理解智能体理性行为提供了理论基础，并验证了正则化和领域随机化对提升理性的有效性，为设计更理性的强化学习算法提供了指导。

Abstract: This paper proposes a suite of rationality measures and associated theory for reinforcement learning agents, a property increasingly critical yet rarely explored. We define an action in deployment to be perfectly rational if it maximises the hidden true value function in the steepest direction. The expected value discrepancy of a policy's actions against their rational counterparts, culminating over the trajectory in deployment, is defined to be expected rational risk; an empirical average version in training is also defined. Their difference, termed as rational risk gap, is decomposed into (1) an extrinsic component caused by environment shifts between training and deployment, and (2) an intrinsic one due to the algorithm's generalisability in a dynamic environment. They are upper bounded by, respectively, (1) the $1$-Wasserstein distance between transition kernels and initial state distributions in training and deployment, and (2) the empirical Rademacher complexity of the value function class. Our theory suggests hypotheses on the benefits from regularisers (including layer normalisation, $\ell_2$ regularisation, and weight normalisation) and domain randomisation, as well as the harm from environment shifts. Experiments are in full agreement with these hypotheses. The code is available at https://github.com/EVIEHub/Rationality.

</details>


### [152] [Decomposing Query-Key Feature Interactions Using Contrastive Covariances](https://arxiv.org/abs/2602.04752)
*Andrew Lee,Yonatan Belinkov,Fernanda Viégas,Martin Wattenberg*

Main category: cs.LG

TL;DR: 提出一种对比协方差方法来分解Transformer的查询-键空间为低秩可解释组件，揭示注意力机制的工作原理


<details>
  <summary>Details</summary>
Motivation: 尽管注意力头在Transformer中起核心作用，但缺乏理解模型为何关注特定token的工具，需要解释注意力机制的工作原理

Method: 研究查询-键(QK)空间，提出对比协方差方法将QK空间分解为低秩、人类可解释的组件，当键和查询的特征在这些低秩子空间中对齐时产生高注意力分数

Result: 在简化设置中进行了分析和实证研究，然后将方法应用于大语言模型，识别出分类语义特征和绑定特征的可解释QK子空间，并展示了如何将注意力分数归因于识别出的特征

Conclusion: 提出的方法能够有效分解QK空间为可解释组件，为理解Transformer注意力机制提供了新工具，有助于解释模型为何关注特定token

Abstract: Despite the central role of attention heads in Transformers, we lack tools to understand why a model attends to a particular token. To address this, we study the query-key (QK) space -- the bilinear joint embedding space between queries and keys. We present a contrastive covariance method to decompose the QK space into low-rank, human-interpretable components. It is when features in keys and queries align in these low-rank subspaces that high attention scores are produced. We first study our method both analytically and empirically in a simplified setting. We then apply our method to large language models to identify human-interpretable QK subspaces for categorical semantic features and binding features. Finally, we demonstrate how attention scores can be attributed to our identified features.

</details>


### [153] [A Dual-TransUNet Deep Learning Framework for Multi-Source Precipitation Merging and Improving Seasonal and Extreme Estimates](https://arxiv.org/abs/2602.04757)
*Yuchen Ye,Zixuan Qi,Shixuan Li,Wei Qi,Yanpeng Cai,Chaoxia Yuan*

Main category: cs.LG

TL;DR: 该研究提出了一个基于TransUNet的双阶段多源降水融合框架，用于整合六种多源降水产品和ERA5物理预测因子，以提高中国地区降水估计的精度，特别是在极端降水事件检测方面。


<details>
  <summary>Details</summary>
Motivation: 多源降水产品（卫星反演和再分析数据）在水文气候监测中广泛应用，但存在空间异质性偏差和极端降水检测能力有限的问题，限制了其水文应用价值。

Method: 开发了双阶段TransUNet多源降水融合框架：第一阶段分类器估计日降水发生概率，第二阶段回归器融合分类器输出和所有预测因子，在0.25度分辨率下估计2001-2020年中国日降水量。

Result: 该框架在季节性表现上最佳（R=0.75；RMSE=2.70 mm/day），提高了东部中国大部分地区强降水的公平威胁评分，更好地再现了2021年7月郑州暴雨的空间模式，在青藏高原数据稀缺区域也表现出适用性。

Conclusion: 提出的框架为降水融合和极端事件评估提供了可扩展且可解释的方法，通过SHAP分析强调了降水发生概率和地表压力的重要性，提供了物理可解释的诊断。

Abstract: Multi-source precipitation products (MSPs) from satellite retrievals and reanalysis are widely used for hydroclimatic monitoring, yet spatially heterogeneous biases and limited skill for extremes still constrain their hydrologic utility. Here we develop a dual-stage TransUNet-based multi-source precipitation merging framework (DDL-MSPMF) that integrates six MSPs with four ERA5 near-surface physical predictors. A first-stage classifier estimates daily precipitation occurrence probability, and a second-stage regressor fuses the classifier outputs together with all predictors to estimate daily precipitation amount at 0.25 degree resolution over China for 2001-2020. Benchmarking against multiple deep learning and hybrid baselines shows that the TransUNet - TransUNet configuration yields the best seasonal performance (R = 0.75; RMSE = 2.70 mm/day) and improves robustness relative to a single-regressor setting. For heavy precipitation (>25 mm/day), DDL-MSPMF increases equitable threat scores across most regions of eastern China and better reproduces the spatial pattern of the July 2021 Zhengzhou rainstorm, indicating enhanced extreme-event detection beyond seasonal-mean corrections. Independent evaluation over the Qinghai-Tibet Plateau using TPHiPr further supports its applicability in data-scarce regions. SHAP analysis highlights the importance of precipitation occurrence probabilities and surface pressure, providing physically interpretable diagnostics. The proposed framework offers a scalable and explainable approach for precipitation fusion and extreme-event assessment.

</details>


### [154] [Improved Dimension Dependence for Bandit Convex Optimization with Gradient Variations](https://arxiv.org/abs/2602.04761)
*Hang Yu,Yu-Hu Yan,Peng Zhao*

Main category: cs.LG

TL;DR: 该论文改进了带梯度变化的强盗凸优化（BCO）的遗憾界，特别是在两点反馈设置下，通过精化非连续梯度变化的分析，改善了维度依赖关系，并扩展到单点线性优化、动态/通用遗憾最小化以及强盗博弈等任务。


<details>
  <summary>Details</summary>
Motivation: 梯度变化在线学习在博弈论和优化中有重要应用，但在强盗反馈设置下研究不足。现有工作在两点反馈的强盗凸优化中，对梯度变化的分析（特别是非连续梯度变化）仍有改进空间，尤其是在维度依赖关系方面。

Method: 提出对非连续梯度变化的精化分析，这是强盗反馈中梯度变化的基本量。该方法首先改进两点反馈BCO的维度依赖，然后扩展到单点强盗线性优化（超矩形域），最后应用于动态/通用遗憾最小化和强盗博弈等任务。

Result: 1. 对凸函数和强凸函数，改进了Chiang等人（2013）已知最佳结果的维度依赖；2. 非连续梯度变化的改进分析还带来了梯度方差和小损失遗憾等其他有利的问题依赖保证；3. 首次为超矩形域上的单点强盗线性优化获得梯度变化界；4. 在两点BCO中建立了首个梯度变化的动态和通用遗憾界，并在强盗博弈中获得快速收敛率。

Conclusion: 该工作通过精化非连续梯度变化的分析，显著改进了强盗凸优化中的梯度变化界，展示了该技术的多功能性，能够扩展到多种在线学习任务，为梯度变化在线学习在强盗反馈设置下提供了更优的理论保证。

Abstract: Gradient-variation online learning has drawn increasing attention due to its deep connections to game theory, optimization, etc. It has been studied extensively in the full-information setting, but is underexplored with bandit feedback. In this work, we focus on gradient variation in Bandit Convex Optimization (BCO) with two-point feedback. By proposing a refined analysis on the non-consecutive gradient variation, a fundamental quantity in gradient variation with bandits, we improve the dimension dependence for both convex and strongly convex functions compared with the best known results (Chiang et al., 2013). Our improved analysis for the non-consecutive gradient variation also implies other favorable problem-dependent guarantees, such as gradient-variance and small-loss regrets. Beyond the two-point setup, we demonstrate the versatility of our technique by achieving the first gradient-variation bound for one-point bandit linear optimization over hyper-rectangular domains. Finally, we validate the effectiveness of our results in more challenging tasks such as dynamic/universal regret minimization and bandit games, establishing the first gradient-variation dynamic and universal regret bounds for two-point BCO and fast convergence rates in bandit games.

</details>


### [155] [Active Asymmetric Multi-Agent Multimodal Learning under Uncertainty](https://arxiv.org/abs/2602.04763)
*Rui Liu,Pratap Tokekar,Ming Lin*

Main category: cs.LG

TL;DR: 提出A2MAML框架，通过贝叶斯逆方差加权实现多智能体多模态不确定性感知协作，在自动驾驶事故检测中提升18.7%检测率


<details>
  <summary>Details</summary>
Motivation: 现有多智能体协作框架通常在智能体层面推理，假设同质传感，隐式处理不确定性，在传感器损坏时鲁棒性有限。多智能体系统配备异构多模态传感器带来丰富感知，但也引入模态特定和智能体依赖的不确定性。

Method: 提出A2MAML框架：1) 将每个模态特定特征建模为具有不确定性预测的随机估计；2) 主动选择可靠的智能体-模态对；3) 通过贝叶斯逆方差加权聚合信息。支持细粒度模态级融合和不对称模态可用性。

Result: 在联网自动驾驶场景的协作事故检测实验中，A2MAML持续优于单智能体和协作基线，实现高达18.7%更高的事故检测率。

Conclusion: A2MAML为不确定性感知的模态级协作提供了原则性方法，能够抑制损坏或噪声模态，支持不对称模态可用性，在多智能体多模态系统中表现出优越性能。

Abstract: Multi-agent systems are increasingly equipped with heterogeneous multimodal sensors, enabling richer perception but introducing modality-specific and agent-dependent uncertainty. Existing multi-agent collaboration frameworks typically reason at the agent level, assume homogeneous sensing, and handle uncertainty implicitly, limiting robustness under sensor corruption. We propose Active Asymmetric Multi-Agent Multimodal Learning under Uncertainty (A2MAML), a principled approach for uncertainty-aware, modality-level collaboration. A2MAML models each modality-specific feature as a stochastic estimate with uncertainty prediction, actively selects reliable agent-modality pairs, and aggregates information via Bayesian inverse-variance weighting. This formulation enables fine-grained, modality-level fusion, supports asymmetric modality availability, and provides a principled mechanism to suppress corrupted or noisy modalities. Extensive experiments on connected autonomous driving scenarios for collaborative accident detection demonstrate that A2MAML consistently outperforms both single-agent and collaborative baselines, achieving up to 18.7% higher accident detection rate.

</details>


### [156] [Billion-Scale Graph Foundation Models](https://arxiv.org/abs/2602.04768)
*Maya Bechler-Speicher,Yoel Gottlieb,Andrey Isakov,David Abensur,Ami Tavory,Daniel Haimovich,Ido Guy,Udi Weinsberg*

Main category: cs.LG

TL;DR: GraphBFF是首个用于构建十亿参数图基础模型的端到端框架，包含可扩展的Transformer架构、神经缩放定律和规模化训练方法，在未见图上实现优异的零样本和少样本性能。


<details>
  <summary>Details</summary>
Motivation: 基础模型在语言和视觉领域取得了巨大成功，但将其扩展到真实世界的通用图数据面临挑战。需要解决如何为异构、十亿级规模的图构建可扩展的基础模型的问题。

Method: 提出GraphBFF框架，包括：1）GraphBFF Transformer - 灵活可扩展的架构；2）神经缩放定律分析；3）数据批处理、预训练和微调的具体方法学；4）在10亿样本上预训练14亿参数模型。

Result: 在训练期间未见过的10个真实世界下游任务上（节点/链接分类和回归），GraphBFF实现了卓越的零样本和探测性能，在少样本设置中优势明显，PRAUC提升高达31个百分点。

Conclusion: GraphBFF为构建工业级图基础模型提供了实用框架，展示了图基础模型的可行性，并讨论了将GFM发展为图学习实用基础的关键挑战和开放机会。

Abstract: Graph-structured data underpins many critical applications. While foundation models have transformed language and vision via large-scale pretraining and lightweight adaptation, extending this paradigm to general, real-world graphs is challenging. In this work, we present Graph Billion- Foundation-Fusion (GraphBFF): the first end-to-end recipe for building billion-parameter Graph Foundation Models (GFMs) for arbitrary heterogeneous, billion-scale graphs. Central to the recipe is the GraphBFF Transformer, a flexible and scalable architecture designed for practical billion-scale GFMs. Using the GraphBFF, we present the first neural scaling laws for general graphs and show that loss decreases predictably as either model capacity or training data scales, depending on which factor is the bottleneck. The GraphBFF framework provides concrete methodologies for data batching, pretraining, and fine-tuning for building GFMs at scale. We demonstrate the effectiveness of the framework with an evaluation of a 1.4 billion-parameter GraphBFF Transformer pretrained on one billion samples. Across ten diverse, real-world downstream tasks on graphs unseen during training, spanning node- and link-level classification and regression, GraphBFF achieves remarkable zero-shot and probing performance, including in few-shot settings, with large margins of up to 31 PRAUC points. Finally, we discuss key challenges and open opportunities for making GFMs a practical and principled foundation for graph learning at industrial scale.

</details>


### [157] [NeuroCanvas: VLLM-Powered Robust Seizure Detection by Reformulating Multichannel EEG as Image](https://arxiv.org/abs/2602.04769)
*Yan Chen,Jie Peng,Moajjem Hossain Chowdhury,Tianlong Chen,Yunmei Liu*

Main category: cs.LG

TL;DR: NeuroCanvas框架通过熵引导通道选择器和神经元信号画布模块，将多通道EEG信号转换为结构化视觉表示，显著提升癫痫检测性能并降低计算延迟。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型处理EEG信号面临两个主要挑战：多通道异质性（癫痫相关信息在不同通道中差异显著）和计算效率低下（EEG信号需要编码为大量token）。需要一种能够有效处理多通道EEG信号并提高计算效率的癫痫检测方法。

Method: 提出NeuroCanvas框架，包含两个核心模块：1) 熵引导通道选择器(ECS)：选择与癫痫相关的通道输入LLM；2) 神经元信号画布(CNS)：将选定的多通道异质EEG信号转换为结构化视觉表示。ECS解决多通道异质性问题，CNS使用紧凑的视觉token表示EEG信号以提高计算效率。

Result: 在多个癫痫检测数据集上评估，F1分数显著提升20%，推理延迟降低88%。

Conclusion: NeuroCanvas为临床实践中实时且资源高效的癫痫检测提供了可扩展且有效的解决方案。

Abstract: Accurate and timely seizure detection from Electroencephalography (EEG) is critical for clinical intervention, yet manual review of long-term recordings is labor-intensive. Recent efforts to encode EEG signals into large language models (LLMs) show promise in handling neural signals across diverse patients, but two significant challenges remain: (1) multi-channel heterogeneity, as seizure-relevant information varies substantially across EEG channels, and (2) computing inefficiency, as the EEG signals need to be encoded into a massive number of tokens for the prediction. To address these issues, we draw the EEG signal and propose the novel NeuroCanvas framework. Specifically, NeuroCanvas consists of two modules: (i) The Entropy-guided Channel Selector (ECS) selects the seizure-relevant channels input to LLM and (ii) the following Canvas of Neuron Signal (CNS) converts selected multi-channel heterogeneous EEG signals into structured visual representations. The ECS module alleviates the multi-channel heterogeneity issue, and the CNS uses compact visual tokens to represent the EEG signals that improve the computing efficiency. We evaluate NeuroCanvas across multiple seizure detection datasets, demonstrating a significant improvement of $20\%$ in F1 score and reductions of $88\%$ in inference latency. These results highlight NeuroCanvas as a scalable and effective solution for real-time and resource-efficient seizure detection in clinical practice.The code will be released at https://github.com/Yanchen30247/seizure_detect.

</details>


### [158] [Generative Modeling via Drifting](https://arxiv.org/abs/2602.04770)
*Mingyang Deng,He Li,Tianhong Li,Yilun Du,Kaiming He*

Main category: cs.LG

TL;DR: 提出Drifting Models新范式，通过训练时演化pushforward分布实现一步推理，在ImageNet 256×256上达到SOTA效果


<details>
  <summary>Details</summary>
Motivation: 当前生成模型如扩散模型和流模型需要在推理时进行多步迭代，计算成本高。作者希望开发一种既能实现高质量生成又能进行一步推理的新方法。

Method: 提出Drifting Models范式，引入drifting field控制样本运动，当分布匹配时达到平衡。训练目标允许神经网络优化器演化分布，实现一步推理。

Result: 在ImageNet 256×256分辨率上达到最先进结果：潜在空间FID 1.54，像素空间FID 1.61，实现高质量一步生成。

Conclusion: Drifting Models为高质量一步生成开辟了新机会，通过训练时演化分布而非推理时迭代，实现了高效且高质量的生成。

Abstract: Generative modeling can be formulated as learning a mapping f such that its pushforward distribution matches the data distribution. The pushforward behavior can be carried out iteratively at inference time, for example in diffusion and flow-based models. In this paper, we propose a new paradigm called Drifting Models, which evolve the pushforward distribution during training and naturally admit one-step inference. We introduce a drifting field that governs the sample movement and achieves equilibrium when the distributions match. This leads to a training objective that allows the neural network optimizer to evolve the distribution. In experiments, our one-step generator achieves state-of-the-art results on ImageNet at 256 x 256 resolution, with an FID of 1.54 in latent space and 1.61 in pixel space. We hope that our work opens up new opportunities for high-quality one-step generation.

</details>


### [159] [Interval-Based AUC (iAUC): Extending ROC Analysis to Uncertainty-Aware Classification](https://arxiv.org/abs/2602.04775)
*Yuqi Li,Matthew M. Engelhard*

Main category: cs.LG

TL;DR: 提出用于区间值预测的不确定性感知ROC框架，引入AUC_L和AUC_U作为理论最优AUC的上下界，支持选择性预测和不确定性评估。


<details>
  <summary>Details</summary>
Motivation: 在高风险预测中，通过区间值预测量化不确定性对可靠决策至关重要。然而，标准评估工具如ROC曲线和AUC是为点值分数设计的，无法捕捉预测不确定性对排序性能的影响。

Method: 提出不确定性感知ROC框架，专门用于区间值预测，引入AUC_L和AUC_U两个新指标。该框架实现了ROC平面的三区域分解，将成对排序分为正确、错误和不确定排序。支持选择性预测，允许模型在区间重叠时放弃排序。

Result: 证明在有效的类条件覆盖下，AUC_L和AUC_U提供了理论最优AUC(AUC*)的正式上下界，刻画了可达到判别性能的物理极限。在真实基准数据集上的实验验证了框架的正确性和实用性。

Conclusion: 该框架广泛适用于区间值预测模型，无论区间构建方法如何，为不确定性感知评估和决策提供了实用工具，优化了弃权率与判别可靠性之间的权衡。

Abstract: In high-stakes risk prediction, quantifying uncertainty through interval-valued predictions is essential for reliable decision-making. However, standard evaluation tools like the receiver operating characteristic (ROC) curve and the area under the curve (AUC) are designed for point scores and fail to capture the impact of predictive uncertainty on ranking performance. We propose an uncertainty-aware ROC framework specifically for interval-valued predictions, introducing two new measures: $AUC_L$ and $AUC_U$. This framework enables an informative three-region decomposition of the ROC plane, partitioning pairwise rankings into correct, incorrect, and uncertain orderings. This approach naturally supports selective prediction by allowing models to abstain from ranking cases with overlapping intervals, thereby optimizing the trade-off between abstention rate and discriminative reliability. We prove that under valid class-conditional coverage, $AUC_L$ and $AUC_U$ provide formal lower and upper bounds on the theoretical optimal AUC ($AUC^*$), characterizing the physical limit of achievable discrimination. The proposed framework applies broadly to interval-valued prediction models, regardless of the interval construction method. Experiments on real-world benchmark datasets, using bootstrap-based intervals as one instantiation, validate the framework's correctness and demonstrate its practical utility for uncertainty-aware evaluation and decision-making.

</details>


### [160] [Dynamical Regimes of Multimodal Diffusion Models](https://arxiv.org/abs/2602.04780)
*Emil Albrychiewicz,Andrés Franco Valiente,Li-Ching Chen*

Main category: cs.LG

TL;DR: 本文提出了一个耦合扩散模型的理论框架，使用耦合Ornstein-Uhlenbeck过程作为可处理模型，揭示了多模态生成由相互作用时间尺度的谱层次结构而非同时分辨率控制，并预测了"同步间隙"现象。


<details>
  <summary>Details</summary>
Motivation: 尽管基于扩散的生成模型在高维数据合成方面取得了前所未有的保真度，但控制多模态生成的理论机制仍然知之甚少。需要建立一个理论框架来理解耦合扩散模型中的多模态生成机制。

Method: 使用耦合Ornstein-Uhlenbeck过程作为可处理模型，应用非平衡统计物理中的动态相变理论，分析耦合扩散模型。通过推导对称和各向异性耦合机制下的物种形成和崩溃时间的解析条件，建立耦合强度的严格界限。在MNIST数据集上训练扩散模型并进行精确分数采样器实验来验证理论预测。

Result: 发现多模态生成由相互作用时间尺度的谱层次结构控制，而非同时分辨率。预测了"同步间隙"现象，即在反向生成过程中不同特征模态以不同速率稳定的时间窗口，这为常见的去同步伪影提供了理论解释。耦合强度作为谱滤波器，对生成施加可调的时间层次结构。

Conclusion: 这些结果为针对模态特定时间尺度的时间依赖性耦合调度提供了理论基础，为替代临时性指导调整提供了潜在方案。建立了避免不稳定对称破缺的耦合强度严格界限，为理解和优化多模态生成提供了理论框架。

Abstract: Diffusion based generative models have achieved unprecedented fidelity in synthesizing high dimensional data, yet the theoretical mechanisms governing multimodal generation remain poorly understood. Here, we present a theoretical framework for coupled diffusion models, using coupled Ornstein-Uhlenbeck processes as a tractable model. By using the nonequilibrium statistical physics of dynamical phase transitions, we demonstrate that multimodal generation is governed by a spectral hierarchy of interaction timescales rather than simultaneous resolution. A key prediction is the ``synchronization gap'', a temporal window during the reverse generative process where distinct eigenmodes stabilize at different rates, providing a theoretical explanation for common desynchronization artifacts. We derive analytical conditions for speciation and collapse times under both symmetric and anisotropic coupling regimes, establishing strict bounds for coupling strength to avoid unstable symmetry breaking. We show that the coupling strength acts as a spectral filter that enforces a tunable temporal hierarchy on generation. We support these predictions through controlled experiments with diffusion models trained on MNIST datasets and exact score samplers. These results motivate time dependent coupling schedules that target mode specific timescales, offering a potential alternative to ad hoc guidance tuning.

</details>


### [161] [Legendre Memory Unit with A Multi-Slice Compensation Model for Short-Term Wind Speed Forecasting Based on Wind Farm Cluster Data](https://arxiv.org/abs/2602.04782)
*Mumin Zhang,Haochen Zhang,Xin Zhi Khoo,Yilin Zhang,Nuo Chen,Ting Zhang,Junjie Tang*

Main category: cs.LG

TL;DR: 提出WMF-CPK-MSLMU集成模型，用于风电场集群短期风速预测，结合加权均值滤波去噪、基于Kendall秩相关的补偿参数和多切片Legendre记忆单元，实现准确、快速、鲁棒的预测。


<details>
  <summary>Details</summary>
Motivation: 随着风电场集群化整合，集群短期风速预测对电力系统正常运行至关重要。需要充分利用具有时空相关性的集群数据，实现准确、快速、鲁棒的预测。

Method: 1) 使用加权均值滤波(WMF)对单风电场风速数据去噪；2) 创新应用Legendre记忆单元(LMU)进行风速预测，结合基于Kendall秩相关的补偿参数(CPK)构建多切片LMU(MSLMU)；3) 提出WMF-CPK-MSLMU集成模型，包含数据预处理、预测和多切片补偿三个关键模块。

Result: 在不同风电场集群上的测试结果表明，提出的WMF-CPK-MSLMU集成模型在短期预测方面相比现有模型具有有效性和优越性。

Conclusion: 该集成模型通过充分利用集群数据的时空相关性，实现了风电场集群短期风速的准确、快速、鲁棒预测，为电力系统运行提供了可靠支持。

Abstract: With more wind farms clustered for integration, the short-term wind speed prediction of such wind farm clusters is critical for normal operation of power systems. This paper focuses on achieving accurate, fast, and robust wind speed prediction by full use of cluster data with spatial-temporal correlation. First, weighted mean filtering (WMF) is applied to denoise wind speed data at the single-farm level. The Legendre memory unit (LMU) is then innovatively applied for the wind speed prediction, in combination with the Compensating Parameter based on Kendall rank correlation coefficient (CPK) of wind farm cluster data, to construct the multi-slice LMU (MSLMU). Finally, an innovative ensemble model WMF-CPK-MSLMU is proposed herein, with three key blocks: data pre-processing, forecasting, and multi-slice compensation. Advantages include: 1) LMU jointly models linear and nonlinear dependencies among farms to capture spatial-temporal correlations through backpropagation; 2) MSLMU enhances forecasting by using CPK-derived weights instead of random initialization, allowing spatial correlations to fully activate hidden nodes across clustered wind farms.; 3) CPK adaptively weights the compensation model in MSLMU and complements missing data spatially, to facilitate the whole model highly accurate and robust. Test results on different wind farm clusters indicate the effectiveness and superiority of proposed ensemble model WMF-CPK-MSLMU in the short-term prediction of wind farm clusters compared to the existing models.

</details>


### [162] [From independent patches to coordinated attention: Controlling information flow in vision transformers](https://arxiv.org/abs/2602.04784)
*Kieran A. Murphy*

Main category: cs.LG

TL;DR: 在视觉Transformer中引入变分信息瓶颈来显式控制注意力传递的信息量，实现从独立补丁处理到全局注意力的可控谱系


<details>
  <summary>Details</summary>
Motivation: 传统视觉Transformer中的注意力机制传递信息不透明，难以分析和控制。研究者希望使注意力传递的信息成为可测量、可控制的量，从而获得更易进行机制分析和控制的模型

Method: 在所有注意力对残差流的写入操作中插入变分信息瓶颈，不改变其他架构。通过信息成本训练模型，获得从独立补丁处理到全局注意力的可控谱系

Result: 在ImageNet-100上表征了分类行为和信息路由在谱系中的演化，分析了首批传递信息的注意力头如何从局部补丁处理中产生全局视觉表示。约束内部通信的偏置学习产生了更易进行机制分析和控制的模型

Conclusion: 通过显式控制注意力信息传输，该方法为视觉Transformer提供了可解释性和可控性，使模型更适合机制分析，并为理解全局表示如何从局部处理中涌现提供了新视角

Abstract: We make the information transmitted by attention an explicit, measurable quantity in vision transformers. By inserting variational information bottlenecks on all attention-mediated writes to the residual stream -- without other architectural changes -- we train models with an explicit information cost and obtain a controllable spectrum from independent patch processing to fully expressive global attention. On ImageNet-100, we characterize how classification behavior and information routing evolve across this spectrum, and provide initial insights into how global visual representations emerge from local patch processing by analyzing the first attention heads that transmit information. By biasing learning toward solutions with constrained internal communication, our approach yields models that are more tractable for mechanistic analysis and more amenable to control.

</details>


### [163] [Team, Then Trim: An Assembly-Line LLM Framework for High-Quality Tabular Data Generation](https://arxiv.org/abs/2602.04785)
*Congjing Zhang,Ryan Feng Lin,Ruoxuan Bao,Shuai Huang*

Main category: cs.LG

TL;DR: T²框架通过LLM团队协作生成高质量表格数据，并采用三阶段质量控制流程，在模拟和真实数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 表格数据对机器学习应用至关重要，但高质量表格数据获取成本高、难度大，现有数据集常存在类别不平衡、选择偏差、保真度低等问题，需要更有效的合成数据生成方法。

Method: 提出Team-then-Trim (T²)框架：1）由领域知识指导的专门化LLM团队协作生成不同数据组件；2）采用三阶段插件式数据质量控制管道，将表格数据生成视为制造过程进行系统评估。

Result: 在模拟和真实世界数据集上的实验结果表明，T²框架在生成高质量表格数据方面优于现有最先进方法，能够有效支持下游模型训练。

Conclusion: T²框架通过LLM团队协作和严格质量控制，能够生成高质量的合成表格数据，为直接数据收集不可行时的下游模型训练提供有效支持。

Abstract: While tabular data is fundamental to many real-world machine learning (ML) applications, acquiring high-quality tabular data is usually labor-intensive and expensive. Limited by the scarcity of observations, tabular datasets often exhibit critical deficiencies, such as class imbalance, selection bias, and low fidelity. To address these challenges, building on recent advances in Large Language Models (LLMs), this paper introduces Team-then-Trim (T$^2$), a framework that synthesizes high-quality tabular data through a collaborative team of LLMs, followed by a rigorous three-stage plug-in data quality control (QC) pipeline. In T$^2$, tabular data generation is conceptualized as a manufacturing process: specialized LLMs, guided by domain knowledge, are tasked with generating different data components sequentially, and the resulting products, i.e., the synthetic data, are systematically evaluated across multiple dimensions of QC. Empirical results on both simulated and real-world datasets demonstrate that T$^2$ outperforms state-of-the-art methods in producing high-quality tabular data, highlighting its potential to support downstream models when direct data collection is practically infeasible.

</details>


### [164] [Maximum-Volume Nonnegative Matrix Factorization](https://arxiv.org/abs/2602.04795)
*Olivier Vu Thanh,Nicolas Gillis*

Main category: cs.LG

TL;DR: 本文提出最大体积非负矩阵分解（MaxVol NMF），通过最大化因子H的体积来获得更稀疏、更可解释的分解，与最小体积NMF形成对偶方法。


<details>
  <summary>Details</summary>
Motivation: 传统最小体积NMF（MinVol NMF）虽然能获得可解释和唯一的解，但在存在噪声时可能产生秩不足的解。本文探索对偶方法，通过最大化H的体积来获得更好的稀疏分解和避免秩不足问题。

Method: 提出最大体积NMF（MaxVol NMF），最大化因子H的体积而不是最小化W的体积。开发了两种求解算法，并提出归一化变体，该变体在标准NMF和正交NMF之间形成连续谱。

Result: 在无噪声情况下，MaxVol NMF与MinVol NMF具有相同的可识别性，但在有噪声时表现更好：能提取更稀疏的分解，避免秩不足解，最大体积解对应将X的列聚类到不相交的簇中。

Conclusion: MaxVol NMF是MinVol NMF的有效对偶方法，在存在噪声时表现更优，归一化变体性能更好，在高光谱解混等应用中具有实用价值。

Abstract: Nonnegative matrix factorization (NMF) is a popular data embedding technique. Given a nonnegative data matrix $X$, it aims at finding two lower dimensional matrices, $W$ and $H$, such that $X\approx WH$, where the factors $W$ and $H$ are constrained to be element-wise nonnegative. The factor $W$ serves as a basis for the columns of $X$. In order to obtain more interpretable and unique solutions, minimum-volume NMF (MinVol NMF) minimizes the volume of $W$. In this paper, we consider the dual approach, where the volume of $H$ is maximized instead; this is referred to as maximum-volume NMF (MaxVol NMF). MaxVol NMF is identifiable under the same conditions as MinVol NMF in the noiseless case, but it behaves rather differently in the presence of noise. In practice, MaxVol NMF is much more effective to extract a sparse decomposition and does not generate rank-deficient solutions. In fact, we prove that the solutions of MaxVol NMF with the largest volume correspond to clustering the columns of $X$ in disjoint clusters, while the solutions of MinVol NMF with smallest volume are rank deficient. We propose two algorithms to solve MaxVol NMF. We also present a normalized variant of MaxVol NMF that exhibits better performance than MinVol NMF and MaxVol NMF, and can be interpreted as a continuum between standard NMF and orthogonal NMF. We illustrate our results in the context of hyperspectral unmixing.

</details>


### [165] [Evolving Afferent Architectures: Biologically-inspired Models for Damage-Avoidance Learning](https://arxiv.org/abs/2602.04807)
*Wolfgang Maass,Sabine Janzen,Prajvi Saxena,Sach Mukherjee*

Main category: cs.LG

TL;DR: 提出Afferent Learning框架，通过进化优化和强化学习相结合的方式，生成计算性传入痕迹作为内部风险信号，用于损伤避免学习，在生物力学数字孪生中实现高效且年龄鲁棒的行为适应。


<details>
  <summary>Details</summary>
Motivation: 受生物系统启发，传统损伤避免学习方法效率低下，需要开发能够自适应生成内部风险信号的框架，以在长期时间尺度上实现有效的损伤避免学习。

Method: 采用两层架构：外层进化优化发现能够支持有效策略学习的传入感知架构，内层强化学习使用这些信号训练损伤避免策略。将传入感知形式化为提供高效学习的归纳偏置。

Result: 在生物力学数字孪生长期模拟中，CAT进化架构比手工设计基线显著提高效率（23%高风险行为减少）和年龄鲁棒性，实现年龄依赖的行为适应。消融研究验证了CAT信号、进化和预测差异的必要性。

Conclusion: Afferent Learning框架通过进化优化的传入感知架构提供有效的内部风险信号，为损伤避免学习提供理论保证和实际应用价值，在生物力学数字孪生中展现出优越性能。

Abstract: We introduce Afferent Learning, a framework that produces Computational Afferent Traces (CATs) as adaptive, internal risk signals for damage-avoidance learning. Inspired by biological systems, the framework uses a two-level architecture: evolutionary optimization (outer loop) discovers afferent sensing architectures that enable effective policy learning, while reinforcement learning (inner loop) trains damage-avoidance policies using these signals. This formalizes afferent sensing as providing an inductive bias for efficient learning: architectures are selected based on their ability to enable effective learning (rather than directly minimizing damage). We provide theoretical convergence guarantees under smoothness and bounded-noise assumptions. We illustrate the general approach in the challenging context of biomechanical digital twins operating over long time horizons (multiple decades of the life-course). Here, we find that CAT-based evolved architectures achieve significantly higher efficiency and better age-robustness than hand-designed baselines, enabling policies that exhibit age-dependent behavioral adaptation (23% reduction in high-risk actions). Ablation studies validate CAT signals, evolution, and predictive discrepancy as essential. We release code and data for reproducibility.

</details>


### [166] [Beyond Rewards in Reinforcement Learning for Cyber Defence](https://arxiv.org/abs/2602.04809)
*Elizabeth Bates,Chris Hicks,Vasilios Mavroudis*

Main category: cs.LG

TL;DR: 稀疏奖励在网络安全强化学习代理中表现优于密集奖励，能产生更可靠、更有效、风险更低的防御策略，且无需显式成本惩罚。


<details>
  <summary>Details</summary>
Motivation: 当前网络安全防御代理通常使用密集、高度设计的奖励函数进行训练，这种函数结合了多种惩罚和激励。密集奖励有助于缓解复杂环境的探索挑战，但可能导致代理偏向次优且风险更高的解决方案，这在复杂的网络环境中尤为关键。

Method: 使用多种稀疏和密集奖励函数，在两个成熟的网络训练环境中，针对不同网络规模，结合策略梯度和基于价值的强化学习算法，全面评估奖励函数结构对学习和策略行为特征的影响。采用新颖的基准评估方法，直接比较不同奖励函数。

Result: 稀疏奖励（只要目标对齐且能频繁遇到）能提供更强的训练可靠性和更有效的网络防御代理，产生风险更低的策略。令人惊讶的是，稀疏奖励还能产生更符合网络防御者目标的策略，并且无需显式的奖励数值惩罚就能节约使用成本高昂的防御行动。

Conclusion: 稀疏奖励在网络防御强化学习中具有独特优势，能够产生更可靠、更有效、风险更低的防御策略，且无需依赖复杂的奖励工程。

Abstract: Recent years have seen an explosion of interest in autonomous cyber defence agents trained to defend computer networks using deep reinforcement learning. These agents are typically trained in cyber gym environments using dense, highly engineered reward functions which combine many penalties and incentives for a range of (un)desirable states and costly actions. Dense rewards help alleviate the challenge of exploring complex environments but risk biasing agents towards suboptimal and potentially riskier solutions, a critical issue in complex cyber environments. We thoroughly evaluate the impact of reward function structure on learning and policy behavioural characteristics using a variety of sparse and dense reward functions, two well-established cyber gyms, a range of network sizes, and both policy gradient and value-based RL algorithms. Our evaluation is enabled by a novel ground truth evaluation approach which allows directly comparing between different reward functions, illuminating the nuanced inter-relationships between rewards, action space and the risks of suboptimal policies in cyber environments. Our results show that sparse rewards, provided they are goal aligned and can be encountered frequently, uniquely offer both enhanced training reliability and more effective cyber defence agents with lower-risk policies. Surprisingly, sparse rewards can also yield policies that are better aligned with cyber defender goals and make sparing use of costly defensive actions without explicit reward-based numerical penalties.

</details>


### [167] [Robust Generalizable Heterogeneous Legal Link Prediction](https://arxiv.org/abs/2602.04812)
*Lorenz Wendlinger,Simon Alexander Nonn,Abdullah Al Zubaer,Michael Granitzer*

Main category: cs.LG

TL;DR: 该论文提出通过边丢弃和特征拼接改进法律引文网络的链接预测，并结合多语言节点特征与改进的非对称解码器，将模型扩展到新西兰等地理和语言上不相关的法律系统。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理大型异构法律引文网络时，尤其是在面对地理和语言上不相关的法律系统时，链接预测的鲁棒性和泛化能力不足。

Method: 1. 引入边丢弃和特征拼接来学习更鲁棒的表征；2. 提出基于多语言节点特征的改进非对称解码器，增强模型兼容性；3. 将方法扩展到新西兰等地理和语言上不相关的法律数据。

Result: 边丢弃和特征拼接将错误率降低了高达45%；改进的非对称解码器使模型能够更好地泛化到地理和语言上不相关的法律系统；提高了不同法律系统间的归纳迁移能力。

Conclusion: 通过边丢弃、特征拼接和多语言节点特征结合改进的非对称解码器，显著提升了法律引文网络链接预测的鲁棒性和跨法律系统的泛化能力。

Abstract: Recent work has applied link prediction to large heterogeneous legal citation networks \new{with rich meta-features}. We find that this approach can be improved by including edge dropout and feature concatenation for the learning of more robust representations, which reduces error rates by up to 45%. We also propose an approach based on multilingual node features with an improved asymmetric decoder for compatibility, which allows us to generalize and extend the prediction to more, geographically and linguistically disjoint, data from New Zealand. Our adaptations also improve inductive transferability between these disjoint legal systems.

</details>


### [168] [Safe Urban Traffic Control via Uncertainty-Aware Conformal Prediction and World-Model Reinforcement Learning](https://arxiv.org/abs/2602.04821)
*Joydeep Chandra,Satyam Kumar Navneet,Aleksandr Algazinov,Yong Zhang*

Main category: cs.LG

TL;DR: STREAM-RL是一个统一的城市交通管理框架，通过三种新算法实现预测、异常检测和安全策略学习的端到端理论保证，在真实交通数据上表现出色。


<details>
  <summary>Details</summary>
Motivation: 城市交通管理需要能够同时预测未来状况、检测异常并采取安全纠正措施的系统，同时需要提供可靠性保证。现有方法缺乏从预测到策略学习的端到端不确定性传播和理论保证。

Method: 提出STREAM-RL框架，包含三个核心算法：1) PU-GAT+：不确定性引导的自适应共形预测器，使用预测不确定性通过置信度单调注意力动态重新加权图注意力；2) CRFN-BY：共形残差流网络，通过具有Benjamini-Yekutieli FDR控制的归一化流建模不确定性归一化残差；3) LyCon-WRL+：不确定性引导的安全世界模型RL代理，具有Lyapunov稳定性证书、认证的Lipschitz边界和不确定性传播的想象推演。

Result: 在多个真实世界交通轨迹数据上的实验表明：STREAM-RL达到91.4%的覆盖效率，在已验证的依赖关系下控制FDR为4.1%，将安全率提高到95.2%（标准PPO为69%），同时获得更高奖励，端到端推理延迟为23ms。

Conclusion: STREAM-RL是首个从预测到异常检测再到安全策略学习传播校准不确定性的框架，具有端到端理论保证，在城市交通管理中实现了预测准确性、异常检测可靠性和策略安全性的统一。

Abstract: Urban traffic management demands systems that simultaneously predict future conditions, detect anomalies, and take safe corrective actions -- all while providing reliability guarantees. We present STREAM-RL, a unified framework that introduces three novel algorithmic contributions: (1) PU-GAT+, an Uncertainty-Guided Adaptive Conformal Forecaster that uses prediction uncertainty to dynamically reweight graph attention via confidence-monotonic attention, achieving distribution-free coverage guarantees; (2) CRFN-BY, a Conformal Residual Flow Network that models uncertainty-normalized residuals via normalizing flows with Benjamini-Yekutieli FDR control under arbitrary dependence; and (3) LyCon-WRL+, an Uncertainty-Guided Safe World-Model RL agent with Lyapunov stability certificates, certified Lipschitz bounds, and uncertainty-propagated imagination rollouts. To our knowledge, this is the first framework to propagate calibrated uncertainty from forecasting through anomaly detection to safe policy learning with end-to-end theoretical guarantees. Experiments on multiple real-world traffic trajectory data demonstrate that STREAM-RL achieves 91.4\% coverage efficiency, controls FDR at 4.1\% under verified dependence, and improves safety rate to 95.2\% compared to 69\% for standard PPO while achieving higher reward, with 23ms end-to-end inference latency.

</details>


### [169] [It's not a Lottery, it's a Race: Understanding How Gradient Descent Adapts the Network's Capacity to the Task](https://arxiv.org/abs/2602.04832)
*Hannah Pinson*

Main category: cs.LG

TL;DR: 该论文研究了梯度下降训练中神经网络理论容量如何缩减为有效容量的机制，通过分析单隐藏层ReLU网络中单个神经元的学习动力学，发现了三种动态原理：相互对齐、解锁和竞争。


<details>
  <summary>Details</summary>
Motivation: 神经网络的实证成功超越了理论理解，一个重要未解现象是：在梯度下降训练过程中，神经网络的理论容量如何缩减为适合任务的有效容量。本文旨在探究梯度下降实现这一过程的机制。

Method: 通过分析单隐藏层ReLU网络中单个神经元的学习动力学，识别了三种动态原理：相互对齐（神经元权重向量趋向对齐）、解锁（某些神经元从初始状态"解锁"开始学习）和竞争（神经元之间竞争权重增长）。

Result: 发现了三种动态原理共同解释了为什么训练后可以通过合并等效神经元或修剪低范数权重来成功缩减容量。特别解释了彩票票假设的机制，即为什么某些神经元的特定有利初始条件会导致它们获得更高的权重范数。

Conclusion: 梯度下降通过神经元间的相互对齐、解锁和竞争动态，自然地缩减了神经网络的理论容量，形成了有效的网络结构，这解释了训练后网络简化和彩票票现象的根本机制。

Abstract: Our theoretical understanding of neural networks is lagging behind their empirical success. One of the important unexplained phenomena is why and how, during the process of training with gradient descent, the theoretical capacity of neural networks is reduced to an effective capacity that fits the task. We here investigate the mechanism by which gradient descent achieves this through analyzing the learning dynamics at the level of individual neurons in single hidden layer ReLU networks. We identify three dynamical principles -- mutual alignment, unlocking and racing -- that together explain why we can often successfully reduce capacity after training through the merging of equivalent neurons or the pruning of low norm weights. We specifically explain the mechanism behind the lottery ticket conjecture, or why the specific, beneficial initial conditions of some neurons lead them to obtain higher weight norms.

</details>


### [170] [The Key to State Reduction in Linear Attention: A Rank-based Perspective](https://arxiv.org/abs/2602.04852)
*Philipp Nazari,T. Konstantin Rusch*

Main category: cs.LG

TL;DR: 论文提出了一种针对线性注意力模型的硬件感知结构化剪枝框架，通过修剪查询和键矩阵来减少状态大小，同时保持与现有CUDA内核的兼容性，能够在仅轻微增加困惑度的情况下移除50%的查询和键通道。


<details>
  <summary>Details</summary>
Motivation: 研究发现训练后的线性注意力模型状态通常呈现低秩结构，这表明模型在实践中未能充分利用其容量。低有效秩会通过放大查询噪声影响检索误差，同时低秩状态可以在训练后大幅减少而仅带来最小性能损失，从而获得更快、更内存高效的模型。

Method: 提出硬件感知的结构化剪枝方法，修剪查询和键矩阵以减少状态大小。适应现有剪枝策略并基于理论分析，提出基于秩揭示QR分解的新型结构化剪枝方法。框架与现有CUDA内核保持兼容。

Result: 在不同规模模型和各种下游任务上的实证结果表明，该状态缩减框架非常有效。能够在仅边际增加困惑度的情况下移除50%的查询和键通道，实现更快、更内存高效的模型。

Conclusion: 线性注意力模型存在低秩状态问题，通过提出的硬件感知结构化剪枝框架可以有效减少状态大小，在保持性能的同时显著提升计算效率和内存使用效率，为线性注意力模型的实用化提供了有效解决方案。

Abstract: Linear attention offers a computationally efficient yet expressive alternative to softmax attention. However, recent empirical results indicate that the state of trained linear attention models often exhibits a low-rank structure, suggesting that these models underexploit their capacity in practice. To illuminate this phenomenon, we provide a theoretical analysis of the role of rank in linear attention, revealing that low effective rank can affect retrieval error by amplifying query noise. In addition to these theoretical insights, we conjecture that the low-rank states can be substantially reduced post-training with only minimal performance degradation, yielding faster and more memory-efficient models. To this end, we propose a novel hardware-aware approach that structurally prunes key and query matrices, reducing the state size while retaining compatibility with existing CUDA kernels. We adapt several existing pruning strategies to fit our framework and, building on our theoretical analysis, propose a novel structured pruning method based on a rank-revealing QR decomposition. Our empirical results, evaluated across models of varying sizes and on various downstream tasks, demonstrate the effectiveness of our state reduction framework. We highlight that our framework enables the removal of 50% of the query and key channels at only a marginal increase in perplexity. The code for this project can be found at https://github.com/camail-official/LinearAttentionPruning.

</details>


### [171] [From Evaluation to Design: Using Potential Energy Surface Smoothness Metrics to Guide Machine Learning Interatomic Potential Architectures](https://arxiv.org/abs/2602.04861)
*Ryan Liu,Eric Qu,Tobias Kreiman,Samuel M. Blau,Aditi S. Krishnapriyan*

Main category: cs.LG

TL;DR: 提出BSCT测试方法，用于评估机器学习原子间势能函数的平滑性，相比传统分子动力学模拟更高效，能指导模型设计优化。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习原子间势能函数有时无法准确再现量子势能面的物理平滑性，导致下游模拟出现错误。传统评估方法如分子动力学模拟计算成本高且主要探测近平衡态。

Method: 提出键平滑性表征测试，通过受控的键变形探测势能面，检测非平滑性（包括不连续性、人工最小值和虚假力），既可在近平衡态也可在远离平衡态进行。

Result: BSCT与分子动力学稳定性强相关，但计算成本远低于分子动力学。通过BSCT指导模型设计，优化后的模型同时实现了低E/F回归误差、稳定的分子动力学模拟和稳健的原子性质预测。

Conclusion: BSCT既可作为验证指标，也可作为模型设计过程中的代理工具，帮助MLIP开发者识别当前基准测试无法高效评估的物理挑战。

Abstract: Machine Learning Interatomic Potentials (MLIPs) sometimes fail to reproduce the physical smoothness of the quantum potential energy surface (PES), leading to erroneous behavior in downstream simulations that standard energy and force regression evaluations can miss. Existing evaluations, such as microcanonical molecular dynamics (MD), are computationally expensive and primarily probe near-equilibrium states. To improve evaluation metrics for MLIPs, we introduce the Bond Smoothness Characterization Test (BSCT). This efficient benchmark probes the PES via controlled bond deformations and detects non-smoothness, including discontinuities, artificial minima, and spurious forces, both near and far from equilibrium. We show that BSCT correlates strongly with MD stability while requiring a fraction of the cost of MD. To demonstrate how BSCT can guide iterative model design, we utilize an unconstrained Transformer backbone as a testbed, illustrating how refinements such as a new differentiable $k$-nearest neighbors algorithm and temperature-controlled attention reduce artifacts identified by our metric. By optimizing model design systematically based on BSCT, the resulting MLIP simultaneously achieves a low conventional E/F regression error, stable MD simulations, and robust atomistic property predictions. Our results establish BSCT as both a validation metric and as an "in-the-loop" model design proxy that alerts MLIP developers to physical challenges that cannot be efficiently evaluated by current MLIP benchmarks.

</details>


### [172] [Subliminal Effects in Your Data: A General Mechanism via Log-Linearity](https://arxiv.org/abs/2602.04863)
*Ishaq Aden-Ali,Noah Golowich,Allen Liu,Abhishek Shetty,Ankur Moitra,Nika Haghtalab*

Main category: cs.LG

TL;DR: 论文提出Logit-Linear-Selection (LLS)方法，通过选择偏好数据集的子集来揭示隐藏的文本信号，这些信号在单个数据点中不可见，但能影响LLM的行为。


<details>
  <summary>Details</summary>
Motivation: 现代大语言模型训练使用多种算法和数据集来引发特定行为，但数据集可能传递无法从单个数据点直接观察到的信号，这给基于数据集理解LLM训练带来了概念挑战，需要理解这种隐藏效应的机制。

Method: 提出Logit-Linear-Selection (LLS)方法，基于LLM的线性结构理论，通过选择通用偏好数据集的特定子集来揭示隐藏的子文本效应。

Result: LLS方法成功从真实世界数据集中发现子集，使训练出的模型表现出特定偏好、用数据集中不存在的语言回应提示、或采用不同人格等行为。这种效应在不同架构的模型中都能持续存在。

Conclusion: LLS方法揭示了数据集隐藏子文本效应的通用机制，证明了数据集选择可以系统性地影响LLM行为，为理解数据集对模型属性的影响提供了新视角。

Abstract: Training modern large language models (LLMs) has become a veritable smorgasbord of algorithms and datasets designed to elicit particular behaviors, making it critical to develop techniques to understand the effects of datasets on the model's properties. This is exacerbated by recent experiments that show datasets can transmit signals that are not directly observable from individual datapoints, posing a conceptual challenge for dataset-centric understandings of LLM training and suggesting a missing fundamental account of such phenomena. Towards understanding such effects, inspired by recent work on the linear structure of LLMs, we uncover a general mechanism through which hidden subtexts can arise in generic datasets.
  We introduce Logit-Linear-Selection (LLS), a method that prescribes how to select subsets of a generic preference dataset to elicit a wide range of hidden effects. We apply LLS to discover subsets of real-world datasets so that models trained on them exhibit behaviors ranging from having specific preferences, to responding to prompts in a different language not present in the dataset, to taking on a different persona. Crucially, the effect persists for the selected subset, across models with varying architectures, supporting its generality and universality.

</details>


### [173] [CRoSS: A Continual Robotic Simulation Suite for Scalable Reinforcement Learning with High Task Diversity and Realistic Physics Simulation](https://arxiv.org/abs/2602.04868)
*Yannick Denker,Alexander Gepperth*

Main category: cs.LG

TL;DR: 提出CRoSS基准套件，用于在Gazebo模拟器中评估持续强化学习算法在机器人任务上的表现，包含轮式机器人和机械臂两种平台，支持多种传感器和快速运动学版本。


<details>
  <summary>Details</summary>
Motivation: 持续强化学习需要智能体在连续任务序列中学习而不遗忘先前策略，但缺乏基于真实机器人模拟的标准化基准。现有基准要么过于简单，要么缺乏物理真实性，难以评估算法在机器人应用中的实际表现。

Method: 基于Gazebo模拟器构建CRoSS基准套件，包含两种机器人平台：1）两轮差速驱动机器人，配备激光雷达、摄像头和碰撞传感器，用于线路跟随和物体推动任务；2）七关节机械臂，用于笛卡尔空间和关节空间的目标到达任务。提供运动学快速版本，支持容器化部署。

Result: CRoSS基准支持高物理真实性的机器人持续学习研究，运动学版本比物理模拟快两个数量级。展示了DQN和策略梯度等标准RL算法的性能，验证了基准的可扩展性和可复现性。

Conclusion: CRoSS为机器人持续强化学习研究提供了标准化、可扩展且易于使用的基准套件，支持高物理真实性和多种传感器配置，有助于推动该领域的发展。

Abstract: Continual reinforcement learning (CRL) requires agents to learn from a sequence of tasks without forgetting previously acquired policies. In this work, we introduce a novel benchmark suite for CRL based on realistically simulated robots in the Gazebo simulator. Our Continual Robotic Simulation Suite (CRoSS) benchmarks rely on two robotic platforms: a two-wheeled differential-drive robot with lidar, camera and bumper sensor, and a robotic arm with seven joints. The former represent an agent in line-following and object-pushing scenarios, where variation of visual and structural parameters yields a large number of distinct tasks, whereas the latter is used in two goal-reaching scenarios with high-level cartesian hand position control (modeled after the Continual World benchmark), and low-level control based on joint angles. For the robotic arm benchmarks, we provide additional kinematics-only variants that bypass the need for physical simulation (as long as no sensor readings are required), and which can be run two orders of magnitude faster. CRoSS is designed to be easily extensible and enables controlled studies of continual reinforcement learning in robotic settings with high physical realism, and in particular allow the use of almost arbitrary simulated sensors. To ensure reproducibility and ease of use, we provide a containerized setup (Apptainer) that runs out-of-the-box, and report performances of standard RL algorithms, including Deep Q-Networks (DQN) and policy gradient methods. This highlights the suitability as a scalable and reproducible benchmark for CRL research.

</details>


### [174] [Multi-Head LatentMoE and Head Parallel: Communication-Efficient and Deterministic MoE Parallelism](https://arxiv.org/abs/2602.04870)
*Chenwei Cui,Rockwell Jackson,Benjamin Joseph Herrera,Ana María Tárano,Hannah Kerner*

Main category: cs.LG

TL;DR: 提出Multi-Head LatentMoE和Head Parallel方法，解决稀疏专家混合模型训练中的通信成本、负载均衡和元数据交换问题，实现O(1)通信成本，训练速度提升1.61倍。


<details>
  <summary>Details</summary>
Motivation: 稀疏专家混合模型通过条件计算降低训练成本，但标准的专家并行方法存在三个主要问题：1) 通信成本随激活专家数k线性增长；2) 负载不均衡影响延迟和内存使用；3) 数据依赖的通信需要元数据交换。这些问题限制了大规模基础模型研究的可及性。

Method: 提出Multi-Head LatentMoE架构和Head Parallel并行策略。Multi-Head LatentMoE通过多头设计实现专家选择，Head Parallel实现O(1)通信成本、完全均衡的流量和确定性通信。还提出IO感知路由和专家计算来加速系统。

Result: 相比使用专家并行的MoE，Multi-Head LatentMoE与Head Parallel结合训练速度提升1.61倍，同时保持相同性能。当专家粒度加倍时，仍能实现1.11倍加速，且获得更高的整体性能。

Conclusion: 该方法有效解决了稀疏专家混合模型训练中的通信瓶颈问题，使数十亿参数基础模型的研究更加可及，为大规模语言模型训练提供了更高效的解决方案。

Abstract: Large language models have transformed many applications but remain expensive to train. Sparse Mixture of Experts (MoE) addresses this through conditional computation, with Expert Parallel (EP) as the standard distributed training method. However, EP has three limitations: communication cost grows linearly with the number of activated experts $k$, load imbalance affects latency and memory usage, and data-dependent communication requires metadata exchange. We propose Multi-Head LatentMoE and Head Parallel (HP), a new architecture and parallelism achieving $O(1)$ communication cost regardless of $k$, completely balanced traffic, and deterministic communication, all while remaining compatible with EP. To accelerate Multi-Head LatentMoE, we propose IO-aware routing and expert computation. Compared to MoE with EP, Multi-Head LatentMoE with HP trains up to $1.61\times$ faster while having identical performance. With doubled granularity, it achieves higher overall performance while still being $1.11\times$ faster. Our method makes multi-billion-parameter foundation model research more accessible.

</details>


### [175] [Rethinking the Trust Region in LLM Reinforcement Learning](https://arxiv.org/abs/2602.04879)
*Penghui Qi,Xiangxin Zhou,Zichen Liu,Tianyu Pang,Chao Du,Min Lin,Wee Sun Lee*

Main category: cs.LG

TL;DR: DPPO用直接策略散度估计替代PPO的启发式裁剪，解决了PPO在大词汇量LLM微调中的结构性问题，提高了训练稳定性和效率。


<details>
  <summary>Details</summary>
Motivation: PPO作为RL微调LLM的标准算法，其核心的概率比裁剪机制在大词汇量场景下存在结构性问题：对低概率token更新过度惩罚，而对高概率token的灾难性偏移约束不足，导致训练效率低下和不稳定。

Method: 提出DPPO，用更原则性的策略散度约束（如总变差或KL散度）替代启发式裁剪。为避免巨大内存开销，引入高效的Binary和Top-K近似来以可忽略的开销捕获核心散度。

Result: 广泛的实证评估表明，DPPO相比现有方法实现了更优的训练稳定性和效率，为基于RL的LLM微调提供了更稳健的基础。

Conclusion: DPPO通过用直接策略散度估计替代PPO的启发式裁剪，解决了PPO在大词汇量LLM微调中的结构性问题，为RL-based LLM fine-tuning提供了更稳健的算法基础。

Abstract: Reinforcement learning (RL) has become a cornerstone for fine-tuning Large Language Models (LLMs), with Proximal Policy Optimization (PPO) serving as the de facto standard algorithm. Despite its ubiquity, we argue that the core ratio clipping mechanism in PPO is structurally ill-suited for the large vocabularies inherent to LLMs. PPO constrains policy updates based on the probability ratio of sampled tokens, which serves as a noisy single-sample Monte Carlo estimate of the true policy divergence. This creates a sub-optimal learning dynamic: updates to low-probability tokens are aggressively over-penalized, while potentially catastrophic shifts in high-probability tokens are under-constrained, leading to training inefficiency and instability. To address this, we propose Divergence Proximal Policy Optimization (DPPO), which substitutes heuristic clipping with a more principled constraint based on a direct estimate of policy divergence (e.g., Total Variation or KL). To avoid huge memory footprint, we introduce the efficient Binary and Top-K approximations to capture the essential divergence with negligible overhead. Extensive empirical evaluations demonstrate that DPPO achieves superior training stability and efficiency compared to existing methods, offering a more robust foundation for RL-based LLM fine-tuning.

</details>


### [176] [Contrastive Continual Learning for Model Adaptability in Internet of Things](https://arxiv.org/abs/2602.04881)
*Ajesh Koyatan Chathoth*

Main category: cs.LG

TL;DR: 该论文综述了对比持续学习在物联网中的应用，连接算法设计与系统现实，提出统一框架和参考架构，并指出物联网领域的独特挑战。


<details>
  <summary>Details</summary>
Motivation: 物联网部署在非平稳动态环境中运行，传感器漂移、用户行为演变和隐私需求异质性等因素影响应用效用。持续学习通过适应模型而不灾难性遗忘来解决这一问题，而对比学习作为强大的表示学习范式，以自监督方式提高鲁棒性和样本效率。

Method: 提出统一的问题表述，推导结合对比和蒸馏损失的共同目标，提出面向物联网的参考架构（设备端、边缘和云端CCL），并提供评估协议和指标指导。

Result: 建立了对比持续学习在物联网应用中的系统框架，连接算法设计（重放、正则化、蒸馏、提示）与物联网系统现实（TinyML约束、间歇连接、隐私）。

Conclusion: 突出了物联网领域独特的开放挑战，包括处理表格和流式物联网数据、概念漂移、联邦设置和能量感知训练等问题。

Abstract: Internet of Things (IoT) deployments operate in nonstationary, dynamic environments where factors such as sensor drift, evolving user behavior, and heterogeneous user privacy requirements can affect application utility. Continual learning (CL) addresses this by adapting models over time without catastrophic forgetting. Meanwhile, contrastive learning has emerged as a powerful representation-learning paradigm that improves robustness and sample efficiency in a self-supervised manner. This paper reviews the usage of \emph{contrastive continual learning} (CCL) for IoT, connecting algorithmic design (replay, regularization, distillation, prompts) with IoT system realities (TinyML constraints, intermittent connectivity, privacy). We present a unifying problem formulation, derive common objectives that blend contrastive and distillation losses, propose an IoT-oriented reference architecture for on-device, edge, and cloud-based CCL, and provide guidance on evaluation protocols and metrics. Finally, we highlight open unique challenges with respect to the IoT domain, such as spanning tabular and streaming IoT data, concept drift, federated settings, and energy-aware training.

</details>


### [177] [Protein Autoregressive Modeling via Multiscale Structure Generation](https://arxiv.org/abs/2602.04883)
*Yanru Qu,Cheng-Yen Hsieh,Zaixiang Zheng,Ge Liu,Quanquan Gu*

Main category: cs.LG

TL;DR: PAR是首个多尺度自回归蛋白质骨架生成框架，通过从粗到细的逐尺度预测生成蛋白质结构，采用噪声上下文学习和计划采样缓解暴露偏差，在无条件生成基准上表现优异且具有零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有蛋白质结构生成方法通常缺乏多尺度建模能力，且自回归模型存在训练与生成过程不匹配导致的暴露偏差问题，影响生成质量。需要开发能够模拟蛋白质层次结构、从粗到细生成并解决暴露偏差的框架。

Method: 提出PAR框架，包含三个核心组件：1) 多尺度下采样操作，在训练中表示多尺度蛋白质结构；2) 自回归Transformer，编码多尺度信息并产生条件嵌入指导生成；3) 基于流的骨架解码器，根据嵌入生成骨架原子。采用噪声上下文学习和计划采样缓解暴露偏差。

Result: PAR有效学习蛋白质分布，生成高质量骨架结构，展现良好的缩放行为。在无条件生成基准上表现优异，具有强大的零样本泛化能力，支持灵活的人为提示条件生成和基序支架，无需微调。

Conclusion: PAR作为首个多尺度自回归蛋白质骨架生成框架，通过从粗到细的生成策略和暴露偏差缓解技术，在蛋白质结构生成任务中表现出色，为零样本条件生成提供了有前景的解决方案。

Abstract: We present protein autoregressive modeling (PAR), the first multi-scale autoregressive framework for protein backbone generation via coarse-to-fine next-scale prediction. Using the hierarchical nature of proteins, PAR generates structures that mimic sculpting a statue, forming a coarse topology and refining structural details over scales. To achieve this, PAR consists of three key components: (i) multi-scale downsampling operations that represent protein structures across multiple scales during training; (ii) an autoregressive transformer that encodes multi-scale information and produces conditional embeddings to guide structure generation; (iii) a flow-based backbone decoder that generates backbone atoms conditioned on these embeddings. Moreover, autoregressive models suffer from exposure bias, caused by the training and the generation procedure mismatch, and substantially degrades structure generation quality. We effectively alleviate this issue by adopting noisy context learning and scheduled sampling, enabling robust backbone generation. Notably, PAR exhibits strong zero-shot generalization, supporting flexible human-prompted conditional generation and motif scaffolding without requiring fine-tuning. On the unconditional generation benchmark, PAR effectively learns protein distributions and produces backbones of high design quality, and exhibits favorable scaling behavior. Together, these properties establish PAR as a promising framework for protein structure generation.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [178] [Thermodynamics and shadow of Simpson-Visser black hole with phantom global monopoles](https://arxiv.org/abs/2602.03888)
*Ahmad Al-Badawi*

Main category: gr-qc

TL;DR: 研究带有幻象整体单极子的非旋转Simpson Visser黑洞的热力学和阴影，分析三个参数（耦合常数ξ、对称破缺能标η、反弹参数a）对黑洞物理和几何性质的影响。


<details>
  <summary>Details</summary>
Motivation: 探索Simpson Visser黑洞在幻象整体单极子存在下的热力学行为和观测特征，理解反弹参数和幻象整体单极子如何改变黑洞的物理性质。

Method: 使用比热和自由能分析热力学稳定性；通过解析求解零测地线方程研究光子球半径和临界碰撞参数；分析三个参数（ξ, η, a）对黑洞结构和观测特征的影响。

Result: 小视界构型局部热力学稳定但从不全局有利；光子球半径依赖于反弹参数a和对称破缺能标η，但临界碰撞参数不受a影响；增加η会增大普通整体单极子的光子球半径和临界碰撞参数，但在幻象情况下会减小它们。

Conclusion: 反弹参数和幻象整体单极子显著改变了黑洞的物理和几何性质，特别是对热力学稳定性和观测特征（光子球半径和临界碰撞参数）产生重要影响。

Abstract: We investigate the thermodynamics and shadow of a non-rotating Simpson Visser black hole with a phantom global monopole. The model is governed by three parameters: the coupling constant $ξ$, the energy scale of symmetry breaking $η$, and the bounce parameter $a$, which jointly influence horizon structure and observational signatures. Using specific heat and free-energy analysis, we show that small-horizon configurations are locally thermodynamically stable but never globally favored. Analytical solutions of null geodesics reveal that the photon sphere radius depends on the bounce parameter $a$ and the energy scale of symmetry breaking $η$, while the critical impact parameter is still unaffected by $a$. Moreover, the photon sphere radius and critical impact parameter, showing that increasing $η$ enlarges both quantities for an ordinary global monopole, while reducing them in the phantom case. Our results highlight how the bounce parameter and phantom global monopole significantly alter the black hole's physical and geometric properties.

</details>


### [179] [Comparative Analysis of Holographic Dark Energy Models in $f(R,T^2)$ Gravity](https://arxiv.org/abs/2602.03898)
*M. Sharif,M. Zeeshan Gul,I. Hashim*

Main category: gr-qc

TL;DR: 该研究在f(R,T²)引力框架下，利用哈勃视界和里奇视界作为红外截断，重构了三种全息暗能量模型，并分析了宇宙演化相态和稳定性。


<details>
  <summary>Details</summary>
Motivation: 探索全息暗能量模型与修正引力理论之间的深层联系，研究f(R,T²)引力框架下不同全息暗能量模型如何描述宇宙加速膨胀。

Method: 在平坦FRW宇宙中，采用非相互作用场景，以哈勃视界和里奇视界为红外截断，重构了Renyi、Sharma-Mittal和广义全息暗能量模型对应的f(R,T²)引力模型。

Result: 重构的f(R,T²)模型能够有效描述宇宙演化的幻影相和精质相，与观测到的宇宙加速膨胀一致，所有模型都表现出稳定性。

Conclusion: 研究揭示了全息暗能量模型与修正引力理论之间的深刻联系，为理解宇宙大尺度动力学提供了有价值的见解。

Abstract: This study investigates the Renyi Holographic dark energy, Sharma-Mittal Holographic dark energy and Generalized Holographic dark energy models in the framework of $f(R,T^2)$ gravity, where $R$ denotes the Ricci scalar and $T^2$ represents the self-contraction of the stress-energy tensor. For this purpose we employed two horizons as infrared cut-offs, such as Hubble horizon and Ricci horizon. The analysis is conducted for a non-interacting scenario in a spatially flat Friedmann-Robertson-Walker universe. By considering a specific form of this modified gravity, we reconstruct the corresponding gravitational models based on these selected dark energy formulations. Additionally, a stability analysis is performed for all cases and the evolution of the equation of state parameter is examined. Our finding indicates that the reconstructed $f(R,T^2)$ models effectively describe both the phantom and quintessence phases of cosmic evolution, aligning with the observed accelerated expansion of the universe. This study highlights the deep interconnections between holographic dark energy models and modified gravity theories, offering valuable insights into the large scale dynamics of the cosmos.

</details>


### [180] [Strong field gravitational lensing of particles by a black-bounce-Schwarzschild black hole](https://arxiv.org/abs/2602.04218)
*Guansheng He,Jiaxu Huang,Zhongwen Feng,Ghulam Mustafa,Wenbin Lin*

Main category: gr-qc

TL;DR: 研究黑洞反弹-史瓦西时空中相对论性和非相对论性中性大质量粒子的强偏折引力透镜效应，分析粒子速度对透镜观测量的影响


<details>
  <summary>Details</summary>
Motivation: 探索在规则时空中大质量粒子的引力透镜效应，特别是粒子速度偏离光速时对强场透镜观测量的影响，为天文观测提供理论支持

Method: 从规则时空中大质量粒子的显式运动方程出发，推导粒子球方程和不稳定类时圆轨道半径；采用强场极限方法计算粒子偏折角，获得相对论性图像的透镜观测量；分析粒子速度效应并应用于银河系超大质量黑洞Sgr A*

Result: 发现粒子球方程在粒子初速度等于光速时可简化为光子球方程；获得了强偏折透镜观测量的解析表达式，包括视粒子球角半径、最外层相对论图像与堆积图像的角分离、以及流量放大比；量化了速度效应对透镜观测量的影响

Conclusion: 大质量粒子的速度效应对黑洞反弹-史瓦西时空的强场引力透镜观测量有显著影响，这些效应在银河系超大质量黑洞Sgr A*的建模中具有可探测性，为通过引力透镜观测区分不同粒子类型和时空参数提供了理论依据

Abstract: The gravitational lensing of relativistic and nonrelativistic neutral massive particles in the black-bounce-Schwarzschild black hole spacetime is investigated in the strong deflection limit. Beginning with the explicit equations of motion of a massive particle in the regular spacetime, we achieve the equation of the particle sphere and thus the radius of the unstable timelike circular orbit. It is interesting to find that the particle sphere equation can reduce to the well-known photon sphere equation, when the particle's initial velocity is equal to the speed of light. We adopt the strong field limit approach to calculate the black-bounce-Schwarzschild deflection angle of the particle subsequently, and obtain the strong-deflection lensing observables of the relativistic images of a pointlike particle source. The observables mainly include the apparent angular particle sphere radius, the angular separation between the outermost relativistic image and the other ones which are packed together, and the ratio between the particle-flux magnification of the outermost image and that of the packed ones (or equivalently, their resulted magnitudelike difference). The velocity effects induced by the deviation of the initial velocity of the particle from light speed on the corresponding strong-field lensing observables of the images of a pointlike light source in the regular geometry, along with these on the strong deflection limit coefficients and the critical impact parameter of the lightlike case, are then formulated. Serving as an application of the results, we finally concentrate on evaluating the astronomical detectability of the velocity effects on the lensing observables and analyzing their dependence on the parameters, by modeling the Galactic supermassive black hole (i.e., Sgr A$^{\ast}$) as the lens.

</details>


### [181] [Light deflection in the gravimagnetic dipole spacetime](https://arxiv.org/abs/2602.04376)
*Clémentine Dassy,Jan Govaerts*

Main category: gr-qc

TL;DR: 该论文研究了一个包含两个具有相反NUT电荷的旋转黑洞的引力磁偶极子解，通过调整黑洞间距可以消除Misner弦张力，形成稳定的相对旋转黑洞系统，并数值模拟了质量粒子测地线在赤道面和垂直轴上的引力透镜效应。


<details>
  <summary>Details</summary>
Motivation: 研究爱因斯坦广义相对论中的渐近平坦、稳态、轴对称真空解，探索包含两个具有相等质量和相反NUT电荷的非极端黑洞系统，特别关注如何通过调整黑洞间距消除Misner弦张力，构建稳定的相对旋转黑洞系统，并分析其引力透镜效应。

Method: 采用引力磁偶极子解作为理论框架，通过选择适当的黑洞间距使弦张力为零，构建稳定的相对旋转黑洞系统。使用数值模拟方法研究质量粒子测地线，分析在赤道面和垂直轴上来自无限远处扩展源的引力透镜效应。

Result: 成功构建了稳定的相对旋转黑洞系统，通过调整黑洞间距可以消除Misner弦张力。数值模拟揭示了在赤道面和垂直轴上来自无限远处扩展源的引力透镜效应，为理解这类黑洞系统的观测特征提供了理论基础。

Conclusion: 引力磁偶极子解描述了一个包含两个具有相反NUT电荷的旋转黑洞的稳定系统，通过适当选择黑洞间距可以消除Misner弦张力。该系统的引力透镜效应在赤道面和垂直轴上表现出不同的特征，为黑洞系统的观测研究提供了新的理论框架。

Abstract: The gravimagnetic dipole is an asymptotically flat, stationary, axisymmetric vacuum solution to Einstein's General Relativity describing two non-extreme black holes with equal masses and opposite NUT charges connected by a Misner string. The string tension's can be set to zero by choosing the black hole separation accordingly, yielding a stable system of oppositely rotating black holes at a fixed distance. Numerical simulations of massless particle geodesics reveal gravitational lensing effects for extended sources at infinity on the equatorial plane or on the vertical axis.

</details>


### [182] [Chaotic Dynamics in Extremal Black Holes: A Challenge to the Chaos Bound](https://arxiv.org/abs/2602.04423)
*Surojit Dalui,Chiranjeeb Singha,Krishnakanta Bhattacharya*

Main category: gr-qc

TL;DR: 极值黑洞中的混沌动力学研究：质量粒子在Reissner-Nordström和Kerr几何中的运动分析显示，即使温度为零，李雅普诺夫指数仍为正，违反MSS混沌界限。


<details>
  <summary>Details</summary>
Motivation: 研究极值黑洞中的混沌动力学，特别是检验Maldacena-Shenker-Stanford混沌界限在零温度下的适用性，探索极值黑洞作为引力独特动力学相的性质。

Method: 采用两种互补方法：(1) 取非极值解的极值极限；(2) 直接在极值背景中工作。分析质量粒子在Reissner-Nordström和Kerr几何中的运动。

Result: 发现李雅普诺夫指数在零温度下仍为正，违反MSS混沌界限。Reissner-Nordström黑洞中混沌减弱但仍存在，Kerr黑洞中混沌随自旋增加而增强。

Conclusion: 极值黑洞表现出残余混沌动力学，违反MSS界限，确立了它们作为引力独特动力学相的性质。

Abstract: We investigate chaotic dynamics in extremal black holes by analyzing the motion of massless particles in both Reissner-Nordström and Kerr geometries. Two complementary approaches (i) taking the extremal limit of non-extremal solutions and (ii) working directly in the extremal background, yield consistent results. We find that, contrary to naive extrapolation of the Maldacena-Shenker-Stanford (MSS) chaos bound, the Lyapunov exponent remains positive even at zero temperature. For Reissner-Nordström black holes, chaos diminishes but persists at extremality, while for Kerr black holes it strengthens with increasing spin. These results demonstrate that extremal black holes exhibit residual chaotic dynamics that violate the MSS bound, establishing them as qualitatively distinct dynamical phases of gravity.

</details>


### [183] [Trapped photon region in the phase space of sub-extremal Kerr-Newman and Kerr-Sen spacetimes](https://arxiv.org/abs/2602.04502)
*Carla Cederbaum,Karim Mosani*

Main category: gr-qc

TL;DR: 分析亚极端Kerr-Newman和Kerr-Sen时空外通信域中捕获光子区域的几何与拓扑结构，证明其在切/余切丛上的投影形成五维子流形，拓扑为SO(3)×ℝ²


<details>
  <summary>Details</summary>
Motivation: 研究黑洞时空几何中光子捕获区域的数学结构，扩展先前对Kerr时空的研究到更一般的带电旋转黑洞（Kerr-Newman和Kerr-Sen时空）

Method: 采用Cederbaum和Jahns为亚极端Kerr时空开发的方法，分析捕获光子区域在切丛和余切丛上的投影几何

Result: 证明在两种时空设置中，捕获光子区域的投影都形成五维子流形，拓扑结构为SO(3)×ℝ²

Conclusion: 亚极端Kerr-Newman和Kerr-Sen时空的捕获光子区域具有相同的拓扑结构，表明该方法可推广到更一般的黑洞时空

Abstract: We analyse the geometry and topology of the trapped photon region in the domain of outer communication of sub-extremal Kerr-Newman and Kerr-Sen spacetimes. Specifically, we show that its projection to the (co-)tangent bundle forms a five-dimensional submanifold with topology $SO(3)\times \mathbb{R}^2$ in each setup. The proof adapts the method of Cederbaum and Jahns for sub-extremal Kerr spacetime.

</details>


### [184] [A General Discussion on Photon Spheres in Different Categories of Spacetimes](https://arxiv.org/abs/2602.04573)
*Chen-Kai Qiao,Ping Su,Yang Huang*

Main category: gr-qc

TL;DR: 该论文对不同类别时空中的光子球进行几何分析，研究光子球的存在性、数量分布以及稳定与不稳定光子球的数量关系，这些结论仅依赖于时空光学几何性质，与具体度规形式无关。


<details>
  <summary>Details</summary>
Motivation: 光子球在黑洞和其他天体物理研究中具有重要意义，但不同时空类别中光子球的存在性和分布受到时空几何拓扑性质和引力场特性的显著影响，需要系统研究其普遍性质。

Method: 采用几何分析方法，研究不同类别时空（包括黑洞时空、超致密天体时空、规则时空和裸奇点时空）中光子球的光学几何性质，重点关注光子球的存在性、总数量n=n_stable+n_unstable以及稳定与不稳定光子球的数量差w=n_stable-n_unstable。

Result: 获得了这些时空类别中光子球的普遍性质和结论，这些结论仅依赖于时空光学几何性质，与具体时空度规形式无关，并成功恢复了近年来提出的关于光子球的一些重要定理。

Conclusion: 通过几何分析方法，建立了不同类别时空中光子球存在性和分布的普遍规律，为理解光子球与时空几何拓扑性质的关系提供了理论基础，并验证了现有相关定理的正确性。

Abstract: Photon spheres have attracted considerable interest in the studies of black holes and other astrophysical objects. For different categories of spacetimes (or gravitational sources), the existence of photon spheres and their distributions are dramatically influenced by the geometric and topological properties of spacetimes and characteristics of the corresponding gravitational fields. In this work, we carry out a geometric analysis on photon spheres for different categories of spacetime (including black hole spacetime, ultra-compact object's spacetime, regular spacetime, and naked singularity spacetime). Some universal properties and conclusions are obtained for these spacetimes. We mostly focus on the existence of photon spheres, the total number of photon spheres $n = n_{\text{stable}} + n_{\text{unstable}}$, the subtraction of stable photon sphere and unstable photon sphere $w = n_{\text{stable}} - n_{\text{unstable}}$ in different categories of spacetimes. These conclusions are derived solely from geometric properties of optical geometry of spacetimes, irrelevant to the specific spacetime metric forms. Besides, our results successfully recover some important theorems on photon spheres proposed in recent years.

</details>


### [185] [Impact of Higher-Order Modes on Eccentricity Measurement in Binary Black Hole Gravitational Waves](https://arxiv.org/abs/2602.04642)
*Honglue Tang,Jinzhao Yang,Baoxiang Wang,Tao Yang*

Main category: gr-qc

TL;DR: 研究发现在测量双黑洞并合轨道偏心率时，忽略引力波高阶模式不会对现有候选事件产生显著偏差，但对于大质量、不对称或侧向系统，未来测量需要包含高阶模式的波形以避免系统性误差。


<details>
  <summary>Details</summary>
Motivation: 研究引力波波形建模中忽略高阶模式是否会在测量双黑洞并合轨道偏心率时引入系统性偏差，特别是针对那些被提出具有偏心率的引力波事件。

Method: 使用包含高阶模式的有效单体模型SEOBNRv5EHM进行贝叶斯推断，重新分析六个先前被认为可能有偏心率的引力波事件。通过零噪声注入实验，系统性地改变探测器框架总质量、质量比、偏心率、倾角和网络信噪比，识别易受高阶模式忽略影响的参数区域。

Result: 对于六个被研究的引力波事件（包括有争议的GW190521），忽略高阶模式没有产生统计上显著的偏心率偏差。系统性偏差主要出现在高总质量（≳120M⊙）、高度不对称质量比（≳4）、近侧向取向（≳30°）和高信噪比（≈50）的系统中。对于准圆形双黑洞，忽略高阶模式可能导致对非零偏心率的错误证据。

Conclusion: 虽然当前偏心率候选事件不受高阶模式忽略的影响，但未来对大质量、不对称或侧向系统的偏心率测量必须使用包含高阶模式的波形，以避免严重的系统性误差。

Abstract: We investigate the systematic biases in measuring orbital eccentricity for binary black hole (BBH) mergers that arise when higher-order modes (HOMs) of gravitational waves are neglected in waveform modeling. Using Bayesian inference with the state-of-the-art eccentric, spin-aligned, higher-mode effective-one-body model SEOBNRv5EHM, we reanalyze six previously suggested eccentric gravitational-wave events--GW190521, GW190620, GW190701, GW191109, GW200129, and GW200208\_222617. Comparing results with its dominant-mode-only counterpart SEOBNRv5E, we find no statistically significant HOM-induced bias in eccentricity for any of these events, including GW190521, whose eccentricity has been debated in the literature. To identify parameter regimes vulnerable to HOM omission, we perform a broad zero-noise injection campaign varying detector-frame total mass, mass ratio, eccentricity, inclination, and network SNR. We find that significant systematic biases ($Δ_e/σ> 1$) arise predominantly in systems with high total mass ($M^{\rm det}\gtrsim120M_\odot$), highly asymmetric mass ratios ($q \gtrsim 4$), near edge-on orientations ($θ_\textrm{JN} \gtrsim 30^\circ$), and high SNRs ($ρ^N_\textrm{mf}\approx50$). Notably, for quasi-circular BBHs with $M^{\rm det}\gtrsim140M_\odot$, neglecting HOMs may lead to strong false-positive evidence for nonzero eccentricity. By contrast, for lower-mass systems ($M^{\rm det}\sim100 M_\odot$), HOM exclusion produces negligible eccentricity biases. Our results demonstrate that although current eccentric candidates are not impacted by HOM omission, future eccentricity measurements--particularly for massive, asymmetric, or edge-on systems--require HOM-inclusive waveforms to avoid substantial systematic errors.

</details>
