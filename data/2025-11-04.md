<div id=toc></div>

# Table of Contents

- [quant-ph](#quant-ph) [Total: 50]
- [physics.comp-ph](#physics.comp-ph) [Total: 5]
- [cs.LG](#cs.LG) [Total: 184]
- [gr-qc](#gr-qc) [Total: 35]


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [1] [Simulated outperforms quantum reverse annealing in mean-field models](https://arxiv.org/abs/2511.00150)
*Christopher L. Baldwin*

Main category: quant-ph

TL;DR: 本文研究了绝热反向退火(ARA)及其经典对应方法模拟反向退火(SRA)在解决优化问题中的性能，发现在无限范围p-自旋模型中，SRA不仅在所有ARA成功的情况下都能成功，甚至在某些ARA失败的参数范围内也能成功，表明ARA相对于SRA没有量子优势。


<details>
  <summary>Details</summary>
Motivation: 研究绝热反向退火(ARA)相对于传统量子退火的改进潜力，特别是利用初始解猜测来抑制问题相变的能力，并探讨ARA是否真正提供量子优势。

Method: 通过自由能景观分析ARA性能，引入经典对应方法模拟反向退火(SRA)，在无限范围p-自旋模型中对两种协议进行理论分析和动力学行为比较。

Result: 在无限范围p-自旋模型中，模拟反向退火(SRA)不仅在所有绝热反向退火(ARA)成功的情况下都能成功，甚至在某些ARA失败的参数范围内也能成功。

Conclusion: 量子算法相对于其经典对应方法没有优势，要声称ARA提供量子优势，不仅需要ARA成功，还需要相应的SRA失败，但在研究的模型中这种情况并未出现。

Abstract: Adiabatic reverse annealing (ARA) has been proposed as an improvement to
conventional quantum annealing for solving optimization problems, in which one
takes advantage of an initial guess at the solution to suppress problematic
phase transitions. Here we interpret the performance of ARA through its effects
on the free energy landscape, and use the intuition gained to introduce a
classical analogue to ARA termed ``simulated reverse annealing'' (SRA). This
makes it more difficult to claim that ARA provides a quantum advantage in
solving a given problem, as not only must ARA succeed but the corresponding SRA
must fail. As a solvable example, we analyze how both protocols behave in the
infinite-range (non-disordered) $p$-spin model. Through both the thermodynamic
phase diagrams and explicit dynamical behavior, we establish that the quantum
algorithm has no advantage over its classical counterpart: SRA succeeds not
only in every case where ARA does but even in a narrow range of parameters
where ARA fails.

</details>


### [2] [Closed-loop calculations of electronic structure on a quantum processor and a classical supercomputer at full scale](https://arxiv.org/abs/2511.00224)
*Tomonori Shirakawa,Javier Robledo-Moreno,Toshinari Itoko,Vinay Tripathi,Kento Ueda,Yukio Kawashima,Lukas Broers,William Kirby,Himadri Pathak,Hanhee Paik,Miwako Tsuji,Yuetsu Kodama,Mitsuhisa Sato,Constantinos Evangelinos,Seetharami Seelam,Robert Walkup,Seiji Yunoki,Mario Motta,Petar Jurcevic,Hiroshi Horii,Antonio Mezzacapo*

Main category: quant-ph

TL;DR: 该研究在Fugaku超级计算机上集成了Heron量子处理器，构建了量子-经典混合工作流，实现了超越精确对角化能力的最大规模电子结构计算。


<details>
  <summary>Details</summary>
Motivation: 理解量子与经典计算如何协同工作，以及如何表征混合量子-经典工作流的可扩展性和效率，实现量子优势的实际应用。

Method: 设计量子处理器与152,064个经典计算节点之间的闭环工作流，结合量子计算和经典高性能计算来近似电子结构。

Result: 成功计算了超出精确对角化范围的化学模型电子结构，精度与某些全经典近似方法相当，展示了当前最大规模的量子-经典高性能计算集成。

Conclusion: 该工作推动了量子与经典高性能计算集成的极限，展示了在当前经典超级计算机上最大规模的资源编排能力。

Abstract: Quantum computers must operate in concert with classical computers to deliver
on the promise of quantum advantage for practical problems. To achieve that, it
is important to understand how quantum and classical computing can interact
together, and how one can characterize the scalability and efficiency of hybrid
quantum-classical workflows. So far, early experiments with quantum-centric
supercomputing workflows have been limited in scale and complexity. Here, we
use a Heron quantum processor deployed on premises with the entire
supercomputer Fugaku to perform the largest computation of electronic structure
involving quantum and classical high-performance computing. We design a
closed-loop workflow between the quantum processors and 152,064 classical nodes
of Fugaku, to approximate the electronic structure of chemistry models beyond
the reach of exact diagonalization, with accuracy comparable to some
all-classical approximation methods. Our work pushes the limits of the
integration of quantum and classical high-performance computing, showcasing
computational resource orchestration at the largest scale possible for current
classical supercomputers.

</details>


### [3] [Quantum-dot single photon source performance with off-resonant pulse preparation schemes](https://arxiv.org/abs/2511.00243)
*Gavin Crowder,Lora Ramunno,Stephen Hughes*

Main category: quant-ph

TL;DR: 比较三种非共振量子点泵浦方案：对称失谐双色脉冲、陷波滤波绝热快速通道脉冲和量子发射器布居数摆动脉冲，评估它们在单光子源性能上的表现。


<details>
  <summary>Details</summary>
Motivation: 传统共振激发需要偏振滤波来去除泵浦信号，导致50%的效率损失。本研究旨在比较三种避免光谱重叠的泵浦方案，以绕过这一滤波需求。

Method: 使用三种泵浦方案进行对比：(i) 对称失谐双色脉冲，(ii) 陷波滤波绝热快速通道脉冲，(iii) 量子发射器布居数摆动脉冲，量化单光子源的关键性能指标。

Result: 双色脉冲因瞬时脉冲强度大而受声子诱导退相干影响，性能降低达50%。NARP和SUPER脉冲在不同程度上免受声子耦合影响，均保持优异的单光子源性能。SUPER脉冲对脉冲参数变化敏感，而NARP脉冲虽然实验实现较难，但对脉冲制备变化具有鲁棒性。

Conclusion: NARP和SUPER脉冲是避免传统共振激发滤波需求的有效替代方案，其中NARP脉冲在鲁棒性方面表现更佳，而SUPER脉冲对参数变化较为敏感。

Abstract: The preparation of photonic qubits in the excited state is an integral part
of the performance of an on-demand single photon source (SPS). Conventional
resonant excitation, an excellent approach to maximize the coherence and
indistinguishability of the SPS, often requires polarization filtering to
remove the pump signal and isolate the qubit emission, but this results in an
inherent 50\% hit to the efficiency. Recent excitation schemes strategically
try to exploit pulses that excite the qubit while avoiding spectral overlap to
bypass this required filtering. In this work, we compare three such pumping
schemes to quantify the important SPS figures-of-merit for off-resonant quantum
dot schemes, using: (i) a symmetrically detuned dichromatic pulse, (ii) a
notch-filtered adiabatic rapid passage (NARP) pulse, and (iii) a swing up of
the quantum emitter population (SUPER) pulse. Due to large instantaneous pulse
strengths, the dichromatic pulse suffers from phonon-induced dephasing which
can lower the SPS performance by up to 50\%. In contrast, the NARP and SUPER
pulses are shielded from phonon coupling to differing degrees but both maintain
excellent SPS performance. The SUPER pulse can lose significant efficiency if
there is variance in its constituent pulses' amplitude, pulse width, or
frequency, while the NARP pulse, though potentially more difficult to realize
in experiments, is robust against variance in the pulse preparation.

</details>


### [4] [Local perception operators and classicality: new tools for old tests](https://arxiv.org/abs/2511.00314)
*Rohit Kishan Ray*

Main category: quant-ph

TL;DR: 该论文提出了一种新的方法来判断量子态是否具有局域性，通过局部感知算子压缩全局可观测量为局部可访问的统计量，并推导出两种互补的见证方法。


<details>
  <summary>Details</summary>
Motivation: 传统上通过违反贝尔不等式来判断量子非局域性，但这是全局任务。作者希望从相反角度回答：何时一个给定态是局域的？

Method: 使用局部感知算子压缩全局可观测量，推导出两种见证方法：一种可由单方实现（带有经典边信息），另一种本质上是双边的。

Result: 开发了依赖于局部边缘分布和测量几何的状态感知约束，在传统贝尔违反可能不确定的情况下，提供了一种认证与局域隐变量解释兼容性的方法。

Conclusion: 该方法从新的操作角度重新审视熟悉的贝尔场景，提供了基于一阶矩和标准投影测量的判据，能够在传统贝尔测试不确定的情况下有效判断量子态的局域性。

Abstract: Quantum nonlocality is often judged by violations of Bell-type inequalities
for a given state. The computation of such violations is a global task,
requiring evaluation of global correlations and subsequent testing against a
Bell functional. We ask instead: when is a given state local (classical)? We
formalize this question via local perception operators (LPOs) that compress
global observables into locally accessible statistics, and we derive two
complementary witnesses -- one implementable by a single party with classical
side information, one intrinsically two-sided. These tools revisit familiar
Bell scenarios from a new operational angle. We show how the witness leads to
state-aware constraints that depend on local marginals and measurement
geometry, with natural specializations to canonical scenarios. The resulting
criteria are built from first moments and standard projective measurements and
provide a way to certify compatibility with local hidden variable explanations
for the LPO-processed data in regimes where conventional Bell violations may be
inconclusive.

</details>


### [5] [Optimal transfer of entanglement in oscillator chains in non-Markovian open systems](https://arxiv.org/abs/2511.00323)
*Da-Wei Luo,Edward Yu,Ting Yu*

Main category: quant-ph

TL;DR: 该论文研究了在非马尔可夫环境中通过最优控制实现连续变量纠缠态的高保真度传输，使用Krotov优化算法设计控制场，发现量子记忆效应有助于纠缠传输。


<details>
  <summary>Details</summary>
Motivation: 研究在一般环境中耦合振荡器链中连续变量纠缠态的传输问题，探索非马尔可夫动力学下的最优控制方法。

Method: 使用Krotov优化算法设计控制场，在线性链和X形链两种配置中实现状态传输，通过调节振荡器频率保持耦合强度恒定。

Result: 实现了高保真度的纠缠态传输，量子记忆效应比无记忆情况表现更好，且无需预先知道初始态参数即可针对多种纠缠态。

Conclusion: 在非马尔可夫环境中，通过最优控制可以高效传输纠缠态，量子记忆效应有助于提高传输性能，方法具有对初始态参数的鲁棒性。

Abstract: We considered the transfer of continuous-variable entangled states in coupled
oscillator chains embedded in a generic environment. We demonstrate
high-fidelity transfer via optimal control in two configurations - a linear
chain and an X-shaped chain. More specifically, we use the Krotov optimization
algorithm to design control fields that achieve the desired state transfer.
Under the environmental memory effects, the Krotov algorithm needs to be
modified, since the dissipative terms in non-Markovian dynamics are generally
governed by the time-dependent system Hamiltonian. Remarkably, we can achieve
high-fidelity transfer by simply tuning the frequencies of the oscillators
while keeping the coupling strength constant, even in the presence of
open-system effects. For the system under consideration, we find that quantum
memory effects can aid in the transfer of entanglement and show improvement
over the memoryless case. In addition, it is possible to target a range of
entangled states, making it unnecessary to know the parameters of the initial
state beforehand.

</details>


### [6] [Quantum Machine Unlearning: Foundations, Mechanisms, and Taxonomy](https://arxiv.org/abs/2511.00406)
*Thanveer Shaik,Xiaohui Tao,Haoran Xie,Robert Sang*

Main category: quant-ph

TL;DR: 本文为量子机器遗忘建立了一个统一物理约束、算法机制和伦理治理的形式化框架，将遗忘定义为CPTP动力学下前后模型可区分性的收缩，并提出了五轴分类法连接理论概念与可实施策略。


<details>
  <summary>Details</summary>
Motivation: 量子机器遗忘是量子信息理论、隐私保护计算和可信人工智能交叉领域的基础挑战，需要建立统一框架来连接物理可行性、算法可验证性和社会问责性。

Method: 定义了基于CPTP动力学的遗忘形式化框架，提出五轴分类法（范围、保证、机制、系统上下文、硬件实现），整合了影响函数、量子Fisher信息加权更新、参数重初始化等实用机制。

Result: 建立了可验证的量子遗忘范式，将量子机器遗忘从概念性概念提升为严格定义的学科，支持在NISQ设备上实现，并能扩展到联邦学习和隐私保护设置。

Conclusion: 该框架通过连接物理约束、算法机制和伦理治理，使量子机器遗忘成为在量子智能新兴时代具有物理可行性、算法可验证性和社会问责性的严谨学科。

Abstract: Quantum Machine Unlearning has emerged as a foundational challenge at the
intersection of quantum information theory privacypreserving computation and
trustworthy artificial intelligence This paper advances QMU by establishing a
formal framework that unifies physical constraints algorithmic mechanisms and
ethical governance within a verifiable paradigm We define forgetting as a
contraction of distinguishability between pre and postunlearning models under
completely positive trace-preserving dynamics grounding data removal in the
physics of quantum irreversibility Building on this foundation we present a
fiveaxis taxonomy spanning scope guarantees mechanisms system context and
hardware realization linking theoretical constructs to implementable strategies
Within this structure we incorporate influence and quantum Fisher information
weighted updates parameter reinitialization and kernel alignment as practical
mechanisms compatible with noisy intermediatescale quantum NISQ devices The
framework extends naturally to federated and privacyaware settings via quantum
differential privacy homomorphic encryption and verifiable delegation enabling
scalable auditable deletion across distributed quantum systems Beyond technical
design we outline a forwardlooking research roadmap emphasizing formal proofs
of forgetting scalable and secure architectures postunlearning interpretability
and ethically auditable governance Together these contributions elevate QMU
from a conceptual notion to a rigorously defined and ethically aligned
discipline bridging physical feasibility algorithmic verifiability and societal
accountability in the emerging era of quantum intelligence.

</details>


### [7] [Quantum Qomrades: Catalysts in Resource Theories and Memories in Dynamic Programming](https://arxiv.org/abs/2511.00454)
*Jeongrak Son*

Main category: quant-ph

TL;DR: 该论文系统研究了辅助系统在量子信息处理中的优势，识别了催化剂的三大优势，并提出了一种利用辅助状态作为记忆的量子算法策略，能在电路深度上实现指数级减少。


<details>
  <summary>Details</summary>
Motivation: 虽然辅助系统经常增强信息处理能力，但对其力量来源的系统性解释一直缺乏，本论文旨在填补这一空白。

Method: 第一部分研究催化剂（保持不变的辅助系统），识别其三大优势；第二部分提出利用辅助状态作为记忆来解决量子算法中的递归问题。

Result: 发现了催化剂的三个关键优势：记忆效应、状态微调能力和资源分配种子作用；提出的算法策略在电路深度上实现了指数级减少，但增加了电路宽度。

Conclusion: 本论文的研究发现将促进未来在资源转换等基础问题和最优量子电路合成等实际问题上的研究。

Abstract: Quantum information theory explores the limits of manipulating quantum
states. While auxiliary systems often enhance information processing, a
systematic explanation for their power has been lacking. This thesis addresses
this gap by investigating the underlying sources of strength in using auxiliary
systems. We then apply these insights to practical problems in quantum
computing and devise an algorithmic paradigm leveraging auxiliary systems. The
first part examines catalysts -- auxiliary systems that remain unaltered -- and
identifies three advantages: a memory effect, the ability to fine-tune catalyst
states, and their role as seed states for resource distribution. The second
part presents a strategy for solving recursive problems in quantum algorithms
by employing auxiliary states as memories, achieving an exponential reduction
in circuit depth at the cost of increased width. The findings in this thesis
would facilitate future research into fundamental problems like resource
interconversion and practical ones like optimal quantum circuit synthesis.

</details>


### [8] [Free-space multi-user quantum network with high key rate](https://arxiv.org/abs/2511.00466)
*Ayan Kumar Nai,G. K. Samanta*

Main category: quant-ph

TL;DR: 该论文展示了一种自由空间量子空分复用架构，使用单个纠缠源实现六用户全连接十二通道量子网络，克服了量子密钥分发中的点对点限制。


<details>
  <summary>Details</summary>
Motivation: 解决量子密钥分发中的点对点限制，实现多用户安全连接，特别是在自由空间环境中克服主动切换障碍。

Method: 采用自由空间量子空分复用架构，使用单个纠缠源构建全连接网络，实现完全被动操作。

Result: 网络实现了超过3×10^4 s^-1的符合计数率，任意节点间的筛选密钥率超过400 kbps，创下记录。

Conclusion: 该方法克服了自由空间量子复用的主动切换障碍，具有完全被动、易于扩展和与光纤集成兼容的特点，为可扩展和资源高效的量子网络开辟了新途径。

Abstract: Emergent quantum networks are the essential ingredient for securely
connecting multiple users worldwide, extensively deployed in both fibre and
free-space. An essential element is the multiplexing of entanglement to
multiple users, overcoming the peer-to-peer restriction of quantum key
distribution (QKD), so far successfully shown in fibre-based architectures.
Here, we demonstrate a free-space quantum space division multiplexing
architecture using just one entanglement source to realise a fully connected
twelve-channel quantum network for seamless QKD connections between six users.
The network achieves record coincidence rates exceeding $3 \times 10^{4}$
s$^{-1}$ between any pair of nodes on the network, for sifted key rate of over
400 kbps. Our approach overcomes the active switching hurdle that has hindered
the free-space deployment of quantum multiplexing, is fully passive, easily
scalable to more nodes and compatible with fibre-based integration, thus
opening a new path to scalable and resource-efficient quantum networks that
utilise free-space links.

</details>


### [9] [Lost and found charge in quantum batteries](https://arxiv.org/abs/2511.00483)
*Debanjan Dey Sarkar,Mallika Mondal,Preeti Parashar,Tamal Guha*

Main category: quant-ph

TL;DR: 该论文提出了一种利用热环境回收量子电池电荷的方法，通过两种不同的热环境辅助方式（主动参与和被动净化）来恢复量子电池的存储电荷。


<details>
  <summary>Details</summary>
Motivation: 量子电池在与热环境相互作用时会损失存储的电荷，研究如何利用热环境本身来回收这些电荷，使其可重复使用。

Method: 采用两种热环境辅助框架：一种是仅访问热粒子并主动参与相互作用；另一种是涉及热环境额外净化子系统的被动辅助。

Result: 研究发现两种辅助方式回收的电荷量之差表征了热操作在量子电池与热环境净化子系统之间产生的纠缠量。

Conclusion: 确认了利用热环境回收量子电池电荷的可行性，并揭示了回收电荷差与系统间纠缠的量化关系。

Abstract: Quantum batteries are prone to loosing their stored charge, when interacting
with a thermal environment. However, getting a limited assistance from the
thermal environment, is it possible to recover the charge back, in a reusable
form? Here we answer this question affirmatively, leveraging a non-trivial
usage of the seemingly useless thermal environment to recycle the quantum
batteries. The framework involves two different kind of assistance from thermal
environment - one by accessing only the thermal particle, actively
participating in the interaction; and the other, involving assistance from an
additional purifying subsystem for the thermal environment, bearing a passive
role to the interaction. Interestingly, we report that the difference between
the retrieved charge between these two degrees of assistance characterizes the
amount of entanglement generated by the thermal operation between the quantum
battery and the purifying subsystem for the thermal environment.

</details>


### [10] [Hierarchical Quantum Optimization for Large-Scale Vehicle Routing: A Multi-Angle QAOA Approach with Clustered Decomposition](https://arxiv.org/abs/2511.00506)
*Shreetam Dash,Shreya Banerjee,Prasanta K. Panigrahi*

Main category: quant-ph

TL;DR: 提出了一种结合标准QAOA和MA-QAOA的量子优化方法，通过聚类分解解决13个位置的车辆路径问题，将问题分解为三个平衡的4节点集群，分别处理集群内和集群间路由。


<details>
  <summary>Details</summary>
Motivation: 解决传统量子VRP实现只能处理4-6个位置的问题，通过聚类分解方法将问题规模扩展到13个位置，同时保持解的质量。

Method: 使用聚类方法将13个位置的VRP分解为三个平衡的4节点集群，对集群内路由使用标准QAOA解决开放回路旅行商问题，对集群间路由使用带SPSA优化器的MA-QAOA。

Result: 在10个不同数据集上的验证显示，标准QAOA在集群内路由中始终找到最优解，与经典Gurobi优化器结果完全匹配；MA-QAOA在集群间路由中表现出与经典优化方法竞争的性能，最终收敛到接近Gurobi优化器的解。

Conclusion: 聚类分解方法使量子优化能够处理比以往更大的问题规模，从4-6个位置扩展到13个位置，同时保持解的质量，为大规模量子优化提供了可行路径。

Abstract: We present a quantum optimization methodology for solving large-scale Vehicle
Routing Problem (VRP) using a combination of standard and Multi-Angle Quantum
Approximate Optimization Algorithms (MA-QAOA). The approach decomposes
13-locations based VRP problems through clustering into three balanced clusters
of 4 nodes each, then applies standard QAOA for intra-cluster Open Loop
Traveling Salesman Problem (OTSP) and MA-QAOA for inter-cluster VRP routing.
Validation across 10 distinct datasets demonstrates that standard QAOA
consistently identifies optimal solutions for intra-cluster routing, which is
matching classical Gurobi optimizer results exactly. More significantly,
MA-QAOA with Simultaneous Perturbation Stochastic Approximation(SPSA) optimizer
demonstrates competitive performance against classical optimization methods,
ultimately converging towards a solution that closely approximates the
classical Gurobi optimizer result.The clustered decomposition enables quantum
optimization of problem sizes generally larger than previous quantum VRP
implementations, advancing from 4-6 location limits to 13-location problems
while maintaining solution quality.

</details>


### [11] [Quantum Field Theory and the Measurement Problem in Quantum Mechanics](https://arxiv.org/abs/2511.00538)
*Avi Levy,Meir Hemmo*

Main category: quant-ph

TL;DR: 基于量子场论和Haag定理提出测量问题的新解决方案，认为在改变粒子内容的基本相互作用中，时间演化是非幺正的。


<details>
  <summary>Details</summary>
Motivation: 解决量子力学中的测量问题，为波函数坍缩提供物理机制解释。

Method: 利用量子场论框架和Haag定理，分析基本相互作用中粒子内容改变时的非幺正时间演化过程。

Result: 发现瞬时相互作用会导致对具有不同粒子内容但可以是动量态、自旋态等叠加的结果子空间的真正随机选择。

Conclusion: 测量问题可以通过基本相互作用中的非幺正演化得到解决，波函数坍缩是这些瞬时相互作用的结果。

Abstract: We propose a novel solution to the measurement problem based on quantum field
theory and Haag's theorem. According to our proposal in elementary interactions
where the particles content is changed, the temporal evolution is non unitary.
These interactions which are almost instantaneous lead to a genuine stochastic
selection of an outcome subspace that has a distinct particles content but can
be a superposition of momentum states, spin states, etc.

</details>


### [12] [Quantum dynamics in lattices in presence of bulk dephasing and a localized source](https://arxiv.org/abs/2511.00577)
*Tamoghna Ray,Katha Ganguly,Dario Poletti,Manas Kulkarni,Bijay Kumar Agarwalla*

Main category: quant-ph

TL;DR: 研究具有局域费米子源和体相退相干的量子系统动力学，分析非相互作用和相互作用两种一维晶格系统中的密度分布和粒子数增长。


<details>
  <summary>Details</summary>
Motivation: 探索哈密顿动力学与环境诱导机制（局域源和体相退相干）在开放量子系统中的相互作用，揭示丰富的动力学行为。

Method: 对非相互作用系统使用绝热近似分析，对相互作用系统使用TEBD算法进行数值计算，研究密度分布和增长指数。

Result: 发现普适动力学标度和异常行为，揭示了不同时间尺度上的丰富动力学特征。

Conclusion: 该研究为各种量子模拟平台提供了相关见解，展示了哈密顿动力学与环境机制之间有趣的相互作用。

Abstract: The aim of this work is to study the dynamics of quantum systems subjected to
a localized fermionic source in the presence of bulk dephasing. We consider two
classes of one-dimensional lattice systems: (i) a non-interacting lattice with
nearest-neighbor and beyond, i.e., long-ranged (power-law) hopping, and (ii) a
lattice that is interacting via short-range interactions modeled by a fermionic
quartic Hamiltonian. We study the evolution of the local density profile
$n_i(t)$ within the system and the growth of the total particle number $N(t)$
in it. For case (i), we provide analytical insights into the dynamics of the
nearest-neighbor model using an adiabatic approximation, which relies on
assuming faster relaxation of coherences of the single particle density matrix.
For case (ii), we perform numerical computations using the time-evolving block
decimation (TEBD) algorithm and analyze the density profile and the growth
exponent in $N(t)$. Our detailed study reveals an interesting interplay between
Hamiltonian dynamics and various environmentally induced mechanisms in open
quantum systems, such as local source and bulk dephasing. It brings out rich
dynamics, including universal dynamical scaling and anomalous behavior across
various time scales and is of relevance to various quantum simulation
platforms.

</details>


### [13] [Principle of Minimal Heating for Collapse and Hybrid Gravitational Models](https://arxiv.org/abs/2511.00644)
*Nicolò Piccione*

Main category: quant-ph

TL;DR: 该论文提出通过最小化加热率来选择塌缩模型中的平滑分布，以消除分布选择的任意性，并应用于GRW、CSL、DP等塌缩模型及Tilloy-Diósi经典-量子混合引力模型。


<details>
  <summary>Details</summary>
Motivation: 能量不守恒是塌缩和经典-量子混合引力模型的可测试预测，但相关的加热率在未平滑某些算子时会发散，而平滑分布的选择是任意的。作者希望通过最小化加热率来消除这种任意性。

Method: 提出一个简单原则：对于固定的平滑长度r_C，选择使加热率最小化的分布。这种方法旨在识别与标准量子力学的最小偏差，并为实验验证提供基础。

Result: 应用该方法发现高斯分布仅在GRW情况下是最优的。将该方法应用于Tilloy-Diósi牛顿引力混合模型，得到了该模型的最小偏差版本。

Conclusion: 该最小偏差版本的模型完全由单一自由参数（平滑长度r_C）确定，如果被实验证伪，将强烈不利于该模型的其他版本。

Abstract: Energy nonconservation is a prominent, testable prediction of collapse and
hybrid classical-quantum gravitational models. Without smearing of certain
operators, the associated heating (or energy increase) rate diverges, yet the
smearing distribution is arbitrary and, on scales much larger than the smearing
length $r_C$, much of the phenomenology is expected to be independent of this
choice. We propose to resolve this arbitrariness by a simple principle: for a
fixed $r_C$, select the distribution that minimizes the heating rate.
Conceptually, this should identify the minimal deviation from standard quantum
mechanics and provide models that, once experimentally refuted, would strongly
disfavor all variants with different distributions. We apply this approach to
the most investigated collapse models: GRW, CSL, and DP. Notably, the Gaussian
is optimal only for the GRW case. Finally, we apply it to the Tilloy-Di\'osi
hybrid classical-quantum model of Newtonian gravity, leading to the minimally
deviating variant of it. This version of the model is entirely determined by
only one free parameter (the smearing length $r_C$) and, if experimentally
refuted, would strongly disfavor any other version of it.

</details>


### [14] [Kostant relation in filtered randomized benchmarking for passive bosonic devices](https://arxiv.org/abs/2511.00842)
*David Amaro-Alcalá*

Main category: quant-ph

TL;DR: 提出了一种使用immanants的滤波器函数，降低了玻色子随机基准测试的成本，避免了Clebsch-Gordan系数的计算，并简化了数据收集和分析过程。


<details>
  <summary>Details</summary>
Motivation: 降低当前玻色子随机基准测试方案的成本，使其能够在更简单的平台上实现，并简化数据分析过程。

Method: 引入基于immanants的滤波器函数，提出仅需单一类型测量的数据收集过程，使用弱相干态和强度测量进行表征。

Result: 新方法避免了Clebsch-Gordan系数的计算需求，使用与原始方法相同的数据但简化了数据收集，能够使用更简单的测量设备。

Conclusion: 该工作使得更简单的平台能够进行表征，并简化了数据分析过程，为玻色子系统的基准测试提供了更实用的解决方案。

Abstract: We reduce the cost of the current bosonic randomized benchmarking proposal.
First, we introduce a filter function using immanants. With this filter, we
avoid the need to compute Clebsch-Gordan coefficients. Our filter uses the same
data as the original, although we propose a distinct data collection process
that requires a single type of measurement. Furthermore, we argue that weak
coherent states and intensity measurements are sufficient to proceed with the
characterization. Our work could then allow simpler platforms to be
characterized and simplify the data analysis process.

</details>


### [15] [No, classical gravity does not entangle quantized matter fields](https://arxiv.org/abs/2511.00852)
*Lajos Diósi*

Main category: quant-ph

TL;DR: 作者反驳了Aziz和Howl关于经典引力能产生量子纠缠的结论，通过量子场论重新计算表明不存在纠缠效应


<details>
  <summary>Details</summary>
Motivation: 质疑Aziz和Howl在Nature论文中声称经典引力能产生量子纠缠的结论，认为他们的计算存在问题

Method: 使用量子场论对作者示例进行重新计算和分析

Result: 重新计算表明不存在纠缠效应，与Aziz和Howl的结论相反

Conclusion: 经典引力不会产生量子纠缠，Aziz和Howl的结论是错误的

Abstract: In their recent work [Nature,646,813(2025)], Aziz and Howl claim that
classical (unquantized) gravity produces entanglement of quantized matter if
matter is treated within quantum field theory which is, no doubt, our ultimate
theory to use. However, an elementary quantum field re-calculation of the
authors' example shows that there is no entangling effect.

</details>


### [16] [Optimizing magnetic coupling in lumped element superconducting resonators for molecular spin qubits](https://arxiv.org/abs/2511.00857)
*Marcos Rubín-Osanz,Marina C. de Ory,Ignacio Gimeno,Wenzel Kersten,Marta Mas-Torrent,María C. Pallarés,Sebastián Roca-Jerat,David Rodriguez,Nerea González-Prato,J. Alejandro de Sousa,Lorenzo Tesi,Daniel Granados,Jaume Veciana,David Zueco,Anabel Lostao,Joerg Schmiedmayer,Inma Ratera,Joris van Slageren,Núria Crivillers,Alicia Gomez,Fernando Luis*

Main category: quant-ph

TL;DR: 开发了超导谐振器与分子自旋量子比特的强磁耦合，实现了100 kHz的单自旋耦合和10 MHz的集体耦合，研究了自旋弛豫和相干动力学，为集成分子自旋量子处理器提供了可扩展路径。


<details>
  <summary>Details</summary>
Motivation: 最大化分子自旋量子比特与超导电路的磁耦合，探索自旋-光子相互作用机制，为实现可扩展的分子自旋量子处理器奠定基础。

Method: 设计具有不同电感结构的超导谐振器，与PTMr有机自由基分子自旋系统相互作用，研究自旋弛豫、Purcell效应和相干Rabi振荡。

Result: 实现了创纪录的单自旋耦合100 kHz和集体耦合10 MHz，观察到Purcell效应和相干Rabi振荡，发现脉冲形状对激发机制的影响。

Conclusion: 该平台为集成分子自旋量子处理器提供了可扩展的路线，证明了超导谐振器与分子自旋系统强耦合的可行性。

Abstract: We engineer lumped-element superconducting resonators that maximize magnetic
coupling to molecular spin qubits, achieving record single-spin couplings up to
100 kHz and collective couplings exceeding 10 MHz. The resonators were made
interact with PTMr organic free radicals, model spin systems with $S=1/2$ and a
quasi-isotropic $g \simeq 2$, dispersed in polymer matrices. The highest
collective spin-photon coupling strengths are attained with resonators having
large inductors, which therefore interact with most spins in the molecular
ensemble. By contrast, the coupling of each individual spin $G_{1}$ is
maximized in resonators having a minimum size inductor, made of a single
microwire. The same platform has been used to study spin relaxation and spin
coherent dynamics in the dispersive regime, when spins are energetically
detuned from the resonator. We find evidences for the Purcell effect, i.e. the
photon induced relaxation of those spins that are most strongly coupled to the
circuit. The rate of this process has been used to infer the distribution of
single spin photon couplings in a given device. For resonators with a 50 nm
wide constriction fabricated at the center of its single maximum $G_{1}$ values
reach $\sim 100$ kHz. Pumping the spins with strong pulses fed through an
independent transmission line induces coherent Rabi oscillations. The spin
excitation then proceeds via either direct resonant processes induced by the
main pulse frequency or, in the case of square-shaped pulses, via the
excitation of the cavity by side frequency components. The latter process
measures the cavity mode hybridization with the spins and can be eliminated by
using Gaussian shaped pulses. These results establish a scalable route toward
integrated molecular-spin quantum processors.

</details>


### [17] [On entropy production of repeated quantum measurements III. Quantum detailed balance](https://arxiv.org/abs/2511.00910)
*Tristan Benoist,Noé Cuneo,Vojkan Jakšić,Claude-Alain Pillet*

Main category: quant-ph

TL;DR: 该论文基于动态系统方法研究量子测量中的熵产生，通过时间反演不变性和信息完备量子仪器的熵产生为零来表征量子通道的KMS量子细致平衡条件。


<details>
  <summary>Details</summary>
Motivation: 研究量子测量中的熵产生动态，并建立量子通道的细致平衡条件与熵产生之间的关系。

Method: 采用动态系统方法，结合时间反演不变性分析和信息完备量子仪器的熵产生特性。

Result: 成功表征了KMS量子细致平衡条件，表明当关联的信息完备量子仪器熵产生为零时，量子通道满足该平衡条件。

Conclusion: 量子通道的KMS细致平衡条件可以通过时间反演不变性和信息完备量子仪器的零熵产生来完全表征。

Abstract: In light of the dynamical-systems approach to entropy production in repeated
quantum measurements, proposed and illustrated in Commun. Math. Phys. 357,
77-123 (2018) [arXiv:1607.00162] and J. Stat. Phys. 182, 44 (2021)
[arXiv:2012.03885], we characterize the KMS quantum detailed balance condition
for quantum channels via time-reversal invariance and the vanishing of the
entropy production for the associated informationally complete quantum
instruments.

</details>


### [18] [Adiabatic theorem for non-Hermitian quantum systems with non-degenerate real eigenvalues: A proof following Kato's approach](https://arxiv.org/abs/2511.00968)
*Minyi Huang,Ray-Kuang Lee*

Main category: quant-ph

TL;DR: 本文通过遵循Kato的方法，严格证明了具有非简并实特征值的非厄米量子系统的绝热定理仍然成立。


<details>
  <summary>Details</summary>
Motivation: 验证绝热定理在非厄米量子系统中的有效性，因为之前该定理在非厄米系统中的适用性尚未揭示。

Method: 采用Kato的方法，但使用复Berry相位代替Kato工作中使用的正交投影。

Result: 成功证明了具有非简并实特征值的非厄米系统的绝热定理仍然有效。

Conclusion: 绝热定理可以扩展到非厄米量子系统，前提是系统具有非简并的实特征值。

Abstract: The adiabatic theorem is one of the most interesting and significant theorem
in quantum mechanics. In 1950, T. Kato gave an elegant proof of this result
[1]. However, the validation of adiabatic theorem for non-Hermitian quantum
systems is unrevealed. In this paper, by following Kato' approach, we prove
rigorously that the adiabatic theorem is still valid for non-Hermitian systems
with non-degenerate real eigenvalues. Moreover, our proof utilizes the complex
Berry phase, instead of the orthogonal projections used in Kato's work.

</details>


### [19] [Enhancing Kerr-Cat Qubit Coherence with Controlled Dissipation](https://arxiv.org/abs/2511.01027)
*Francesco Adinolfi,Daniel Z. Haxell,Alessandro Bruno,Laurent Michaud,Venus Hasanuzzaman Kamrul,Preeti Pandey,Alexander Grimm*

Main category: quant-ph

TL;DR: 该论文研究了Kerr-cat量子比特的比特翻转时间限制问题，发现泄漏出量子比特流形是主要限制因素，并通过工程耗散冷却泄漏态来延长比特翻转时间。


<details>
  <summary>Details</summary>
Motivation: Kerr-cat量子比特在量子计算中具有潜力，但其比特翻转时间需要进一步提高才能充分发挥其优势。

Method: 通过相干控制泄漏态并测量其占比，然后使用工程耗散将泄漏态冷却回量子比特流形，结合哈密顿约束和工程耗散两种稳定化方法。

Result: 实验显示泄漏态占比高达9%，是未驱动系统的12倍；通过工程耗散冷却后，比特翻转时间延长至3.6毫秒。

Conclusion: 结合哈密顿约束和工程耗散可以有效抑制比特翻转，为实现Kerr-cat量子比特在量子纠错中的潜力指明了路径。

Abstract: Quantum computing crucially relies on maintaining quantum coherence for the
duration of a calculation. Bosonic quantum error correction protects this
coherence by encoding qubits into superpositions of noise-resilient oscillator
states. In the case of the Kerr-cat qubit (KCQ), these states derive their
stability from being the quasi-degenerate ground states of an engineered
Hamiltonian in a driven nonlinear oscillator. KCQs are experimentally
compatible with on-chip architectures and high-fidelity operations, making them
promising candidates for a scalable bosonic quantum processor. However, their
bit-flip time must increase further to fully leverage these advantages. Here,
we present direct evidence that the bit-flip time in a KCQ is limited by
leakage out of the qubit manifold and experimentally mitigate this process. We
coherently control the leakage population and measure it to be > 9%, twelve
times higher than in the undriven system. We then cool this population back
into the KCQ manifold with engineered dissipation, identify conditions under
which this suppresses bit-flips, and demonstrate increased bit-flip times up to
3.6 milliseconds. By employing both Hamiltonian confinement and engineered
dissipation, our experiment combines two paradigms for Schr\"odinger-cat qubit
stabilization. Our results elucidate the interplay between these stabilization
processes and indicate a path towards fully realizing the potential of these
qubits for quantum error correction.

</details>


### [20] [Pseudo quantum advantages in perceptron storage capacity](https://arxiv.org/abs/2511.01028)
*Fabio Benatti,Masoud Gharahi,Giovanni Gramegna,Stefano Mancini,Vincenzo Parisi*

Main category: quant-ph

TL;DR: 该论文研究了具有振荡激活函数的广义量子感知器架构，发现随着频率增加，量子存储能力增强，但这种优势本质上是伪量子优势，可以在经典框架中模拟。


<details>
  <summary>Details</summary>
Motivation: 研究量子感知器架构的存储能力，探索量子计算在机器学习中的潜在优势，特别是通过激活函数形式带来的性能提升。

Method: 使用统计力学分析方法，研究具有可调频率振荡激活函数的广义量子感知器架构，分析其存储容量随频率变化的关系。

Result: 当频率为零时恢复经典结果，随着频率增加，架构表现出增强的量子存储能力，但这种增强仅源于激活函数的形式。

Conclusion: 量子感知器架构的存储能力提升是伪量子优势，因为这种改进可以在经典框架中模拟，不依赖于量子力学的本质特性。

Abstract: We investigate a generalized quantum perceptron architecture characterized by
an oscillating activation function with a tunable frequency ranging from zero
to infinity. Employing analytical techniques from statistical mechanics, we
derive the optimal storage capacity and demonstrate that the classical result
is recovered in the limit of vanishing frequency. As the frequency increases,
however, the architecture exhibits enhanced quantum storage capabilities.
Notably, this improvement stems solely from the specific form of the activation
function and, in principle, could be emulated within a classical framework.
Accordingly, we refer to this enhancement as a pseudo quantum advantage.

</details>


### [21] [ECCentric: An Empirical Analysis of Quantum Error Correction Codes](https://arxiv.org/abs/2511.01062)
*Aleksandra Świerkowska,Jannik Pflieger,Emmanouil Giortamis,Pramod Bhatotia*

Main category: quant-ph

TL;DR: ECCentric是一个端到端的量子纠错码基准测试框架，用于系统评估不同QEC代码在真实条件下的性能表现。研究发现：QPU内部执行优于分布式方法，量子比特连接性比增加代码距离更重要，编译器开销是主要错误源，离子阱架构最有前景。


<details>
  <summary>Details</summary>
Motivation: 量子纠错对于构建可扩展量子计算机至关重要，但缺乏系统性的端到端评估方法，难以在真实条件下评估不同QEC代码的性能。代码多样性大、实验搜索空间广阔且缺乏标准化框架阻碍了全面分析。

Method: 引入ECCentric框架，这是一个模块化、可扩展且通用的端到端基准测试框架，允许在不同硬件拓扑、噪声模型和编译策略下对QEC代码家族进行全面分析。

Result: 实证分析显示：QPU内部执行显著优于分布式方法；量子比特连接性比增加代码距离对减少逻辑错误更为关键；编译器开销仍然是主要错误源；离子阱架构结合量子比特穿梭是最有前景的近平台；在噪声设备上需要策略性地选择性应用QEC。

Conclusion: 这项研究为硬件设计者和实践者提供了关键且可操作的见解，指导容错量子系统的开发，强调需要战略性应用QEC以避免引入比纠正更多的错误。

Abstract: Quantum error correction (QEC) is essential for building scalable quantum
computers, but a lack of systematic, end-to-end evaluation methods makes it
difficult to assess how different QEC codes perform under realistic conditions.
The vast diversity of codes, an expansive experimental search space, and the
absence of a standardized framework prevent a thorough, holistic analysis. To
address this, we introduce ECCentric, an end-to-end benchmarking framework
designed to systematically evaluate QEC codes across the full quantum computing
stack. ECCentric is designed to be modular, extensible, and general, allowing
for a comprehensive analysis of QEC code families under varying hardware
topologies, noise models, and compilation strategies.
  Using ECCentric, we conduct the first systematic benchmarking of major QEC
code families against realistic, mid-term quantum device parameters. Our
empirical analysis reveals that intra-QPU execution significantly outperforms
distributed methods, that qubit connectivity is a far more critical factor for
reducing logical errors than increasing code distance, and that compiler
overhead remains a major source of error. Furthermore, our findings suggest
that trapped-ion architectures with qubit shuttling are the most promising
near-term platforms and that on noisy devices, a strategic and selective
application of QEC is necessary to avoid introducing more errors than are
corrected. This study provides crucial, actionable insights for both hardware
designers and practitioners, guiding the development of fault-tolerant quantum
systems.

</details>


### [22] [In situ calibration of unitary operations during quantum error correction](https://arxiv.org/abs/2511.01080)
*Jonathan Kunjummen,Jacob M. Taylor*

Main category: quant-ph

TL;DR: 该论文提出了一种基于贝叶斯更新和卡尔曼滤波的量子纠错方法，通过实时反馈解码器输出信息来学习特定比特的错误率，从而提升纠错性能。


<details>
  <summary>Details</summary>
Motivation: 传统量子纠错通常使用固定先验信息，无法适应特定噪声比特的情况。本文旨在利用先验信息和贝叶斯更新来改进量子纠错性能，特别是在存在特别噪声比特的场景下。

Method: 使用近似卡尔曼滤波器实时反馈解码器输出，更新先验信息，学习站点特定的错误率。通过模拟完整的闭环系统，从均匀先验开始，逐步学习特定比特的错误率。

Result: 该方法能够使解码器性能超过固定先验基线，改变逻辑错误率与物理错误率的幂律缩放关系。即使在低噪声比特的典型场景下，也能以中等开销实现门集层析操作的现场校准。

Conclusion: 贝叶斯更新和实时反馈机制为量子纠错提供了有效的自举方法，能够适应特定噪声环境，提升纠错性能并实现现场校准。

Abstract: Quantum error correction uses the measurement of syndromes and classical
decoding algorithms to estimate the location and type of errors while
protecting the encoded quantum bits. Here we consider how prior information and
Bayesian updates can play a critical role in improving the performance of QEC
in the scenario of a particularly noisy qubit. This allows for leveraging even
distance codes, which typically are less valued in QEC, to handle the noisy
qubit, changing the power-law scaling of the logical error rate with the
baseline physical error rate. A crucial component of this is updating the prior
by real time feeding of decoder outputs into a approximate Kalman filter. Thus
our approach provides a bootstrap to the actual error rates. We show this via
simulation of the full closed-loop system: starting from uniform priors, the
update procedure gradually learns site-specific error rates, enabling the
decoder to outperform a fixed-prior baseline. In turn, we show that this
enables in situ calibration of unitary operations via injection of gate set
tomography operations with only moderate overhead in the more typical scenario
of low noise qubits.

</details>


### [23] [Robust Quantum State Generation in Symmetric Spin Networks](https://arxiv.org/abs/2511.01085)
*Andre Luiz P. de Lima,Luke S. Baker,Anatoly Zlotnik,Andrew K. Harter,Michael J. Martin,Jr-Shin Li*

Main category: quant-ph

TL;DR: 提出了一种基于矩量化方法的鲁棒电磁幅度脉冲设计方法，用于参数化伊辛模型中的量子态控制，能够同时补偿参数变化并实现GHZ和W态等量子传感中的重要状态。


<details>
  <summary>Details</summary>
Motivation: 针对具有长程对称相互作用的参数化伊辛模型，考虑电磁场不确定性带来的参数变化，需要设计能够同时控制无限多个动力学系统的鲁棒控制脉冲。

Method: 采用离散化矩基量化技术，利用参数化系统与其有限维截断矩动力学之间的对偶性，设计控制脉冲来同时补偿参数变化。

Result: 仿真结果表明该方法能够有效实现量子传感中重要的GHZ态和W态等量子态。

Conclusion: 基于矩量化的鲁棒控制方法能够有效处理参数不确定性，在量子系统中实现精确的态控制。

Abstract: In this work, we consider a parameterized Ising model with long-range
symmetric pairwise interactions on a network of spin $\frac{1}{2}$ particles.
The system is designed with symmetric dynamics, allowing for the reduction of
the state space to a subspace defined by the set of Dicke states. We propose a
method for designing robust electromagnetic amplitude pulses based on a moment
quantization approach. The introduced parameter accounts for uncertainties in
the electromagnetic field, resulting in a family of distinct Hamiltonians. By
employing a discretized moment-based quantization technique, we design a
control pulse capable of simultaneously steering an infinite collection of
dynamical systems to compensate for parameter variations. This approach
benefits from the duality between the infinite-dimensional parameterized system
and its finite-dimensional trucnated moment dynamics. Simulation results
demonstrate the efficacy of this method in achieving states of significant
interest in quantum sensing, including the GHZ and W states.

</details>


### [24] [Attosecond quantum optical interferometry](https://arxiv.org/abs/2511.01097)
*Javier Rivera-Dean,Lidija Petrovic,Maciej Lewenstein,Philipp Stammer*

Main category: quant-ph

TL;DR: 本文提出阿秒量子干涉测量方案，将经典阿秒干涉测量扩展到量子光学领域，能够在阿秒时间尺度上测量量子光学特性，并用于量子态工程和场关联操控。


<details>
  <summary>Details</summary>
Motivation: 将全光学阿秒测量方案与量子光学连接起来，实现在阿秒时间尺度上测量量子光学可观测量，并操控谐波的量子态特性。

Method: 开发阿秒量子干涉测量方案，通过改变双色驱动场的相对相位来进行量子态工程，测量阿秒量子层析迹来获取谐波量子态的相空间分布特性。

Result: 实现了谐波发射的量子态工程，能够操控场关联及其纠缠特性，建立了用于原位阿秒测量量子光学可观测量的新型协议。

Conclusion: 该方案成功连接了全光学阿秒测量与量子光学，为实现丰富的观测提供了可能，为阿秒时间尺度的量子光学研究开辟了新途径。

Abstract: In this work, we explore the scheme of attosecond quantum interferometry
(AQI), the quantum optical version of classical attosecond interferometry,
which allows to measure quantum optical properties on the attosecond
time-scale. We develop how the scheme of AQI can be used for quantum state
engineering of the emitted harmonics, by varying the relative phase of a
two-color driving field, and further enables one to manipulate the field
correlations as well as their entanglement characteristics. In addition, this
scheme allows us to learn properties of the phase-space distribution of the
harmonic quantum state, by means of measuring an attosecond quantum tomography
trace. This serves as a new type of protocol for in situ attosecond
measurements of quantum optical observables. With this, we achieve to further
connect all-optical attosecond measurement schemes with quantum optics,
allowing for a rich manifold of observations.

</details>


### [25] [Characterizing QUBO Reformulations of the Max-k-Cut Problem for Quantum Computing](https://arxiv.org/abs/2511.01108)
*Adrian Harkness,Hamidreza Validi,Ramin Fakhimi,Illya V. Hicks,Tamás Terlaky,Luis F. Zuluaga*

Main category: quant-ph

TL;DR: 该论文提出了针对最大k割问题的两种QUBO重构方法，并给出了紧致惩罚系数的闭式表征，这些系数依赖于图的顶点加权度数。


<details>
  <summary>Details</summary>
Motivation: 量子计算在解决NP难组合优化问题方面具有巨大潜力，但需要将问题重构为QUBO形式。确定紧致的惩罚系数对于在现有和近期量子计算设备上求解至关重要。

Method: 提出了两种最大k割问题的QUBO重构方法，通过闭式表征确定了依赖于图顶点加权度数的紧致惩罚系数。

Result: 理论结果通过示例进行了验证，并在量子计算机模拟器上对提出的QUBO重构进行了基准测试。

Conclusion: 这些发现有助于推动量子计算成为解决大规模组合问题的可行工具。

Abstract: Quantum computing offers significant potential for solving NP-hard
combinatorial (optimization) problems that are beyond the reach of classical
computers. One way to tap into this potential is by reformulating combinatorial
problems as a quadratic unconstrained binary optimization (QUBO) problem. The
solution of the QUBO reformulation can then be addressed using adiabatic
quantum computing devices or appropriate quantum computing algorithms on
gate-based quantum computing devices. In general, QUBO reformulations of
combinatorial problems can be readily obtained by properly penalizing the
violation of the problem's constraints in the original problem's objective.
However, characterizing tight (i.e., minimal but sufficient) penalty
coefficients for this purpose is critical for enabling the solution of the
resulting QUBO in current and near-term quantum computing devices. Along these
lines, we here focus on the (weighted) max $k$-cut problem, a fundamental
combinatorial problem with wide-ranging applications that generalizes the
well-known max cut problem. We present closed-form characterizations of tight
penalty coefficients for two distinct QUBO reformulations of the max $k$-cut
problem whose values depend on the (weighted) degree of the vertices of the
graph defining the problem. These findings contribute to the ongoing effort to
make quantum computing a viable tool for solving combinatorial problems at
scale. We support our theoretical results with illustrative examples. Further,
we benchmark the proposed QUBO reformulations to solve the max $k$-cut problem
on a quantum computer simulator.

</details>


### [26] [Spatial Incompatibility Witnesses for Quantum Temporal Correlations](https://arxiv.org/abs/2511.01179)
*Xiangjing Liu,Harshit Verma,Yunlong Xiao,Oscar Dahlsten,Mile Gu*

Main category: quant-ph

TL;DR: 提出了基于伪密度矩阵的见证框架来认证量子时间关联，定义了空间不兼容性作为伪密度矩阵与有效密度矩阵的最小距离，并构建了实验可访问的SI见证。


<details>
  <summary>Details</summary>
Motivation: 现有Leggett-Garg框架在认证时间关联方面存在局限性，需要开发更通用的方法来利用测量扰动增强时间关联的认证能力。

Method: 使用伪密度矩阵形式体系，定义空间不兼容性为伪密度矩阵与有效密度矩阵的最小距离，对于迹范数距离，这简化为PDM的负性，从而构建SI见证。

Result: SI见证比Leggett-Garg测试能认证更广泛通道类别的时间关联，测量扰动增强了时间关联的认证能力，满足LG不等式的通道仍可检测到SI。

Conclusion: 基于伪密度矩阵的空间不兼容性框架提供了比Leggett-Garg测试更强大的时间关联认证工具，测量扰动在认证时间关联中发挥关键作用。

Abstract: We introduce a witness-based framework for certifying quantum temporal
correlations via the pseudo-density matrix (PDM) formalism, which is a
spatiotemporal generalization of the density matrix. We define spatial
incompatibility (SI) as the minimum distance between a PDM and valid density
matrices. For trace-norm distance, we show that this reduces to the PDM's
negativity, enabling the construction of experimentally accessible SI
witnesses. We derive a tight bound on SI for quantum channels and analyze the
respective roles of state and channel coherence in witnessing SI. Crucially, SI
witnesses certify temporal correlations across a strictly broader class of
channels than Leggett-Garg (LG) tests. Our approach, unlike the LG framework,
exploits incompatible measurements that generate coherence through state
disturbance. We further show that channels satisfying the LG inequality for
incoherent states can still exhibit detectable SI, demonstrating that
measurement disturbance enhances the certification of temporal correlations.

</details>


### [27] [GRAPE.jl: Gradient Ascent Pulse Engineering in Julia](https://arxiv.org/abs/2511.01217)
*Michael H. Goerz,Sebastián C. Carrasco,Alastair Marshall,Vladimir S. Malinovsky*

Main category: quant-ph

TL;DR: GRAPE.jl 是一个基于 Julia 语言的量子最优控制包，实现了梯度上升脉冲工程方法，用于寻找控制量子系统的优化脉冲。


<details>
  <summary>Details</summary>
Motivation: 为下一代量子技术（如量子计算和量子传感）提供高效灵活的量子控制解决方案，利用 Julia 语言的优势实现数值性能。

Method: 基于 QuantumControl.jl 框架，实现梯度上升脉冲工程（GRAPE）方法，通过优化控制脉冲来引导量子系统。

Result: 开发了一个兼具灵活性和数值性能的量子最优控制软件包，能够有效寻找控制量子系统的优化脉冲。

Conclusion: GRAPE.jl 成功实现了基于 Julia 的高效量子最优控制，为量子技术应用提供了重要工具。

Abstract: The GRAPE$.$jl package (https://github.com/JuliaQuantumControl/GRAPE.jl)
implements Gradient Ascent Pulse Engineering, a widely used method of quantum
optimal control. Its purpose is to find controls that steer a quantum system in
a particular way. This is a prerequisite for next-generation quantum
technology, such as quantum computing or quantum sensing. GRAPE$.$jl exploits
the unique strengths of the Julia programming language to achieve both
flexibility and numerical performance. It builds on the QuantumControl$.$jl
framework.

</details>


### [28] [Open-Source Highly Parallel Electromagnetic Simulations for Superconducting Circuits](https://arxiv.org/abs/2511.01220)
*David Sommers,Prasanna Pakkiam,Zach Degnan,Chun-Ching Chiu,Divita Gautam,Yi-Hsun Chen,Arkady Fedorov*

Main category: quant-ph

TL;DR: SQDMetal是一个基于Python的开源API，集成了Qiskit Metal、Gmsh、Palace和Paraview等工具，为超导量子电路提供高度并行的仿真工作流。


<details>
  <summary>Details</summary>
Motivation: 解决商业仿真平台限制和开源高性能计算替代方案不足的问题，为超导量子设备设计提供社区驱动、无商业约束的仿真工具。

Method: 通过集成多个开源工具构建统一框架，支持网格收敛研究、本征模和静电仿真、哈密顿量提取、动感效应建模和完整3D几何建模。

Result: 验证显示SQDMetal与COMSOL Multiphysics和Ansys在仿真结果上具有良好一致性，超导谐振器和transmon量子比特仿真与实验测量结果合理吻合。

Conclusion: SQDMetal通过统一开源工具降低了高性能仿真门槛，为超导量子设备设计和优化提供了可访问的解决方案。

Abstract: Electromagnetic simulations form an indispensable part of the design and
optimization process for superconducting quantum devices. Although several
commercial platforms exist, open-source alternatives optimized for
high-performance computing remain limited. To address this gap, we introduce
SQDMetal, a Python-based API that integrates Qiskit Metal (IBM), Gmsh, Palace
(AWS), and Paraview (Kitware) into an open-source, highly parallel simulation
workflow for superconducting quantum circuits. SQDMetal enables accurate,
efficient, and scalable simulations while remaining community-driven and free
from commercial constraints. In this work, we validate SQDMetal through mesh
convergence studies which benchmark SQDMetal against COMSOL Multiphysics and
Ansys, demonstrating excellent agreement for both eigenmode and electrostatic
(capacitance) simulations. Furthermore, we simulate superconducting resonators
and transmon qubits, showing reasonable agreement with experimental
measurements. SQDMetal also supports advanced capabilities, including
Hamiltonian extraction via the energy participation ratio (EPR) method,
incorporation of kinetic inductance effects, and full 3D modelling of device
geometry for improved predictive accuracy. By unifying open-source tools into a
single framework, SQDMetal lowers the barriers to entry for community members
seeking to access high-performance simulations to assist in the design and
optimization of their devices.

</details>


### [29] [Experimental Demonstration of Software-Orchestrated Quantum Network Applications over a Campus-Scale Testbed](https://arxiv.org/abs/2511.01247)
*Md. Shariful Islam,Joaquin Chung,Ely Marcus Eastman,Robert J. Hayek,Prem Kumar,Rajkumar Kettimuthu*

Main category: quant-ph

TL;DR: 提出了一个量子网络编排器原型，利用SDN设计原则在Argonne量子网络测试床上自动化量子通信实验，验证了可扩展架构并实现了12小时连续稳定的远程纠缠分发服务。


<details>
  <summary>Details</summary>
Motivation: 量子网络需要从孤立的测试床转变为可扩展的基础设施，以支持分布式量子应用。

Method: 采用软件定义网络(SDN)设计原则，在Argonne校园内通过部署的电信光纤连接建筑物，自动化量子通信实验。

Result: 验证了支持量子网络任务服务级抽象、分布式时间同步和远程节点纠缠验证的可扩展架构，实现了12小时连续稳定的远程纠缠分发服务。

Conclusion: 这为可扩展量子网络的发展定义了一条有前景的道路。

Abstract: To fulfill their promise, quantum networks must transform from isolated
testbeds into scalable infrastructures for distributed quantum applications. In
this paper, we present a prototype orchestrator for the Argonne Quantum Network
(ArQNet) testbed that leverages design principles of software-defined
networking (SDN) to automate typical quantum communication experiments across
buildings in the Argonne campus connected over deployed, telecom fiber. Our
implementation validates a scalable architecture supporting service-level
abstraction of quantum networking tasks, distributed time synchronization, and
entanglement verification across remote nodes. We present a prototype service
of continuous, stable entanglement distribution between remote sites that ran
for 12 hours, which defines a promising path towards scalable quantum networks.

</details>


### [30] [Quantum Deep Learning Still Needs a Quantum Leap](https://arxiv.org/abs/2511.01253)
*Hans Gundlach,Hrvoje Kukina,Jayson Lynch,Neil Thompson*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Quantum computing technology is advancing rapidly. Yet, even accounting for
these trends, a quantum leap would be needed for quantum computers to mean-
ingfully impact deep learning over the coming decade or two. We arrive at this
conclusion based on a first-of-its-kind survey of quantum algorithms and how
they match potential deep learning applications. This survey reveals three
important areas where quantum computing could potentially accelerate deep
learning, each of which faces a challenging roadblock to realizing its
potential. First, quantum algorithms for matrix multiplication and other
algorithms central to deep learning offer small theoretical improvements in the
number of operations needed, but this advantage is overwhelmed on practical
problem sizes by how slowly quantum computers do each operation. Second, some
promising quantum algorithms depend on practical Quantum Random Access Memory
(QRAM), which is underdeveloped. Finally, there are quantum algorithms that
offer large theoretical advantages, but which are only applicable to special
cases, limiting their practical benefits. In each of these areas, we support
our arguments using quantitative forecasts of quantum advantage that build on
the work by Choi et al. [2023] as well as new research on limitations and
quantum hardware trends. Our analysis outlines the current scope of quantum
deep learning and points to research directions that could lead to greater
practical advances in the field.

</details>


### [31] [High-fidelity all-microwave CZ gate with partial erasure-error detection via a transmon coupler](https://arxiv.org/abs/2511.01260)
*Shotaro Shirai,Shinichi Inoue,Shuhei Tamate,Rui Li,Yasunobu Nakamura,Atsushi Noguchi*

Main category: quant-ph

TL;DR: 提出并实验验证了一种全微波控制的CZ门，通过固定频率传输子耦合器和多路径耦合来抑制残余ZZ相互作用，同时实现高保真度。


<details>
  <summary>Details</summary>
Motivation: 在超导量子处理器中，实现全微波控制的量子门可以简化信号路由和控制装置，同时需要抑制残余ZZ相互作用以提高门保真度。

Method: 使用固定频率传输子耦合器和多路径耦合方案，通过驱动两个色散偏移共振频率中点处的跃迁来产生状态相关的几何相位，实现CZ门操作。

Result: 实验实现了高保真度的CZ门，同时有效抑制了残余ZZ相互作用，并能通过测量耦合器状态来检测退相干引起的故障。

Conclusion: 该方法能够在保持数据传输子间小净横向相互作用的同时，通过增加数据与耦合器传输子间的耦合来加速CZ门速度，为量子纠错提供了有利条件。

Abstract: Entangling gates between neighboring physical qubits are essential for
quantum error correction. Implementing them in an all-microwave manner
simplifies signal routing and control apparatus of superconducting quantum
processors. We propose and experimentally demonstrate an all-microwave
controlled-Z (CZ) gate that achieves high fidelity while suppressing residual
ZZ interactions. Our approach utilizes a fixed-frequency transmon coupler and
multi-path coupling, thereby sufficiently reducing the net transverse
interaction between data transmons to suppress residual ZZ interactions. The
controlled phase arises from the dispersive frequency shift of the $\fggetxt$
transition between the coupler and one of the data transmons conditioned on the
state of the other data transmon. Driving the transitions at the midpoint of
two dispersively shifted resonance frequencies induces state-dependent
geometric phases to achieve the CZ gate. Crucially, with this scheme, we can
maintain a small net transverse interaction between two data transmons while
increasing the coupling between the data and coupler transmons to accelerate
the CZ-gate speed. Additionally, we measure the coupler state after the gate to
detect a subset of decoherence-induced failures that occur during the gate
operation. These events constitute erasure errors with known locations,
enabling erasure-aware quantum error-correcting codes to improve future logical
qubit performance.

</details>


### [32] [Super-resolved reconstruction of single-photon emitter locations from $g^{(2)}(0)$ maps](https://arxiv.org/abs/2511.01279)
*Sonali Gupta,Amit Kumar,Vikas S Bhat,Sushil Mujumdar*

Main category: quant-ph

TL;DR: 提出一种结合光栅扫描g(2)(0)映射和反演重建算法的新方法，用于高效准确地识别金刚石中单个氮空位中心，突破衍射极限限制。


<details>
  <summary>Details</summary>
Motivation: 传统共聚焦显微镜受衍射极限限制，无法分辨焦点内的发射器分布，且强度扫描耗时。需要开发能可靠识别孤立NV中心的高效方法。

Method: 采用光栅扫描g(2)(0)映射技术，结合反演重建算法，通过直接测量局部光子反聚束来提取焦点内的有效发射器数量。

Result: 能够在小于共聚焦焦点的区域内重建发射器数量和空间分布，模拟证实了NV中心分布的稳健重建。

Conclusion: 该方法为定位单光子源提供了实用的诊断工具，相比传统强度扫描更高效准确，有助于NV基量子光子技术的精确集成。

Abstract: Single-photon sources are vital for emerging quantum technologies. In
particular, Nitrogen-vacancy (NV) centers in diamond are promising due to their
room-temperature stability, long spin coherence, and compatibility with
nanophotonic structures. A key challenge, however, is the reliable
identification of isolated NV centers, since conventional confocal microscopy
is diffraction-limited and cannot resolve emitter distributions within a focal
spot. Besides, the associated intensity scanning is a time-expensive procedure.
Here, we introduce a raster-scanned $g^{(2)}(0)$ mapping technique combined
with an inversion-based reconstruction algorithm. By directly measuring local
photon antibunching across the field of view, we extract the effective emitter
number within each focal spot and reconstruct occupancy maps on a
sub-focal-spot grid. This enables recovery of the number and spatial
distribution of emitters within regions smaller than the confocal focal spot,
thereby offering possibilities of going beyond the diffraction limit. Our
simulations confirm robust reconstruction of NV-center distributions. The
method provides a practical diagnostic tool for locating single-photon sources
in an efficient and accurate manner, at much lesser time and effort compared to
conventional intensity scanning. It offers valuable feedback for nanophotonic
device fabrication, supporting more precise and scalable integration of
NV-based quantum photonic technologies.

</details>


### [33] [Strongly coupled giant-atom waveguide quantum electrodynamics](https://arxiv.org/abs/2511.01300)
*Zong-Wei Wu,Jun-Hong An*

Main category: quant-ph

TL;DR: 本文研究了巨型原子与波导耦合系统中的非马尔可夫动力学，发现束缚态的存在能抑制退相干并产生稳定的拉比振荡。


<details>
  <summary>Details</summary>
Motivation: 巨型原子波导量子电动力学是实现量子互连的有前景平台，但传统Born-Markov和Wigner-Weisskopf近似无法准确描述其动力学行为。

Method: 超越传统近似方法，研究单个和两个巨型原子与耦合谐振器阵列波导相互作用的非马尔可夫动力学。

Result: 发现巨型原子的动力学行为由复合系统的能谱决定：当能谱中存在束缚态时，激发态概率趋于稳定有限值，并产生无损耗的拉比振荡。

Conclusion: 结果为抑制巨型原子退相干提供了重要指导，有助于开发基于巨型原子波导QED的量子互连器件。

Abstract: Describing systems of superconducting atoms coupled to a continuum of
photonic modes at multiple separated locations in a waveguide, waveguide
quantum electrodynamics (QED) with giant atoms has emerged as a promising
platform for realizing quantum interconnect. Such systems have been reported to
exhibit rich phenomena that differ from those of natural atoms. Going beyond
the widely used Born-Markov and Wigner-Weisskopf approximations, we investigate
the non-Markovian dynamics of one and two giant atoms interacting with a
waveguide formed by an array of coupled resonators. We discover that the
diverse dynamical behaviors of the giant atoms are intrinsically determined by
the energy spectrum of the composite system consisting of the giant atoms and
the photonic modes in the waveguide. As long as one and more bound states are
present in the energy spectrum, their excited-state probabilities,
respectively, tend to stable finite values and lossless Rabi-like oscillations
with frequencies proportional to the differences of the bound-state
eigenenergies. Our result provides an insightful guideline for suppressing the
decoherence of giant atoms and facilitates the development of quantum
interconnect devices using giant-atom waveguide QED.

</details>


### [34] [Multi-stage quantum walks for finding Ising ground states](https://arxiv.org/abs/2511.01312)
*Asa Hopkins,Viv Kendon*

Main category: quant-ph

TL;DR: 本文提出了一种用于多阶段量子行走(MSQW)的高效启发式参数选择方法，相比单阶段量子行走获得了改进的缩放性能。对于简单问题，该方法可实现多项式时间缩放；对于困难问题，缩放性能会下降。


<details>
  <summary>Details</summary>
Motivation: 多阶段量子行走(MSQW)已被证明在解决优化任务时优于QAOA，但需要有效的方法来选择自由参数以获得更好的缩放性能。

Method: 开发了一种高效的启发式算法来选择MSQW中的自由参数，并将其应用于获得比单阶段量子行走更好的缩放性能。

Result: 数值实验表明，对于具有大最小能隙的简单问题，启发式方法表现良好，实现了阶段数的多项式缩放，从而得到多项式时间复杂度的整体算法。对于困难问题，缩放性能会下降，导致指数时间复杂度。

Conclusion: 该方法具有通用性，可应用于任何优化问题以获得良好的退火调度方案。

Abstract: One way to approximate a quantum annealing schedule is to use multiple
quantum walks chained together, without intermediate measurements, to produce a
multi-stage quantum walk (MSQW). Previous work has shown that MSQW is better
than QAOA (quantum alternating operator ansatz) for solving optimization tasks
using multiple stages [Gerblich et al, arXiv:2407.06663]. In this work, we
develop an efficient heuristic for choosing the free parameters in MSQW, and
use it to obtain improved scaling compared to single stage quantum walks. We
show numerically that the heuristic works well for easy problems with a large
minimum energy gap, giving a scaling polynomial in the number of stages,
leading to an overall algorithm that scales polynomially in time. For harder
problems, the scaling breaks down such that adding more stages decreases the
success probability, leading to an overall scaling that is exponential in time,
as expected. Our methods are general and can be applied to any optimization
problem to obtain good annealing schedules.

</details>


### [35] [Light-induced Frequency Shift and Relaxation of Ground-State 3He via Metastability-Exchange Collisions](https://arxiv.org/abs/2511.01313)
*L. Y. Wu,H. Yan*

Main category: quant-ph

TL;DR: 本文发现并研究了氦-3核自旋中由光位移引起的新频率偏移和弛豫效应，该效应源于亚稳态交换碰撞介导的光与核子自旋相互作用，对低磁场下的核磁共振测量有重要影响。


<details>
  <summary>Details</summary>
Motivation: 亚稳态交换碰撞在氦-3核磁强计中既实现了光学泵浦和检测，又引入了额外的频率偏移和弛豫，限制了磁强计的灵敏度。本文旨在识别并理解这些效应的新来源。

Method: 建立了理论模型来描述光诱导效应，并通过实验验证了该效应，研究了其与磁场强度、光强和波长等参数的关系。

Result: 实验证实了光位移引起的频率偏移和弛豫效应，揭示了亚稳态交换碰撞介导的氦-3核自旋与光之间的耦合。

Conclusion: 该研究为理解氦-3自旋进动在亚稳态交换光学泵浦条件下的频率偏移和弛豫提供了新视角，并表明亚稳态交换碰撞可能用于量子水平的光学操控氦-3核自旋。

Abstract: Metastability-exchange collisions (MECs) lie at the heart of
metastability-exchange optical pumping (MEOP) in 3He, enabling the transfer of
polarization from the metastable state to the ground state, as well as the
optical detection of nuclear magnetic resonance. Leveraging MECs, optically
pumped 3He nuclear magnetometers have been developed since the earliest
demonstrations of MEOP. However, it also induces an additional frequency shift
and relaxation of the nuclear spin precession, thereby limiting the sensitivity
of the magnetometer. In this work, we identify a new source of frequency shift
and relaxation in the 3He nuclear spin, arising from the light shift. This
effect arises from an MEC-mediated interaction between light and the nucleon
spin. We develop a theoretical model to describe this light-induced effect and
highlight its significance in low magnetic fields. This effect is
experimentally demonstrated, and its dependence on various parameters --
including magnetic field strength, light intensity, and wavelength -- is
investigated. Our result provides a better understanding of the frequency shift
and relaxation of 3He spin precession under MEOP conditions. Moreover, our
experiment reveals an MEC-mediated coupling between the 3He nuclear spin and
light, which may indicate the feasibility of MEC-assisted optical manipulation
of 3He nuclear spins at the quantum level, as proposed in several theoretical
schemes.

</details>


### [36] [Distinct Critical Scaling of Quantum Fisher Information in a Quantum Rabi Triangle System](https://arxiv.org/abs/2511.01314)
*Yuyang Tang,Yu Yang,Min An,Fuli Li*

Main category: quant-ph

TL;DR: 该论文研究量子拉比三角系统中的临界增强传感，发现在相变边界附近通过调节耦合强度或跳跃相位可以实现增强的参数估计精度，量子Fisher信息在量子相变点附近呈现发散标度行为，且能达到海森堡极限。


<details>
  <summary>Details</summary>
Motivation: 量子系统的临界特性被认为是量子计量学的宝贵资源，研究量子拉比三角系统中的临界增强传感，探索在相变边界附近实现增强参数估计精度的可能性。

Method: 研究量子拉比三角系统，该系统呈现多个相。在相边界附近，通过调节标度耦合强度或由人工磁场控制的跳跃相位来实现增强的参数估计精度。分析量子Fisher信息在不同量子相变点附近的行为，并考虑资源消耗。

Result: 量子Fisher信息在不同量子相变点附近呈现发散标度行为，具有不同的临界指数。当考虑资源消耗时，发散的量子Fisher信息可以达到海森堡极限。提出了平均光子数的测量方案，量子Cramér-Rao界可以达到饱和。

Conclusion: 量子拉比三角系统在相变边界附近可以实现临界增强的传感性能，量子Fisher信息在相变点附近呈现发散行为且能达到海森堡极限，为量子计量学提供了有价值的资源。

Abstract: Critical properties of a quantum system are recognized as valuable resources
for quantum metrology. In this work, we investigate the criticality-enhanced
sensing in a quantum Rabi triangle system, which exhibits multiple phases.
Around the phase boundary, enhanced parameter estimation precision can be
achieved by tuning either the scaled coupling strength or the hopping phase
controlled by an artificial magnetic field. We observe that the quantum Fisher
information shows divergent scaling near different quantum phase transition
points, characterized by distinct critical exponents. When the resource
consumption is taken into account, we find that the divergent quantum Fisher
information can reach the Heisenberg limit. Furthermore, we propose a
measurement scheme of the average photon number and the quantum Cram\'er-Rao
bound can be saturated.

</details>


### [37] [Non-Markovian dynamics in nonstationary Gaussian baths](https://arxiv.org/abs/2511.01358)
*Vladisalv Sukharnikov,Stasis Chuchurka,Frank Schlawin*

Main category: quant-ph

TL;DR: 提出了适用于非平稳高斯浴的广义HOPS方法，通过扩展浴相关函数的时间相关分解，在非平衡浴中实现高效量子系统模拟


<details>
  <summary>Details</summary>
Motivation: 传统HOPS方法主要针对平稳浴，需要扩展到非平稳高斯浴以处理非平衡量子系统问题

Method: 扩展浴相关函数的指数分解为显式时间相关形式，建立广义HOPS框架，并与伪模表示结合提高效率

Result: 在均匀压缩和简并参量放大产生的非平稳压缩库上验证方法，相比层次主方程具有更优的截断效率

Conclusion: HOPS是模拟非平稳浴中开放量子系统的通用强大工具，适用于压缩光-物质相互作用、驱动量子材料和耗散相变等应用

Abstract: Building on the standard hierarchy of pure states (HOPS) approach, we
construct a generalized formulation suitable for open quantum systems
interacting with nonstationary Gaussian baths, potentially extending its
applicability to nonequilibrium baths. This is achieved by extending the
conventional exponential decomposition of bath correlation functions (BCF) to
allow explicitly time-dependent forms. We demonstrate the method's performance
on two examples of nonstationary squeezed reservoirs generated via uniform
squeezing and degenerate parametric amplification. Benchmarking against the
associated hierarchy of master equations shows that HOPS achieves superior
efficiency under hierarchy truncation. In cases where each contribution in the
BCF expansion can be associated with an independent physical bath, the
formalism can be simplified in a pseudomode representation which is more
efficient in a strongly non-Markovian regime. Our results highlight HOPS as a
versatile and powerful tool for simulating open quantum systems in
nonstationary baths, with potential applications ranging from squeezed
light-matter interactions to driven quantum materials and dissipative phase
transitions.

</details>


### [38] [Entanglement estimation of Werner states with a quantum extreme learning machine](https://arxiv.org/abs/2511.01387)
*Hajar Assil,Abderrahim El Allati,Gian Luca Giorgi*

Main category: quant-ph

TL;DR: 提出了一个基于量子极限学习机的协议，用于估计Werner态的纠缠量。该协议通过生成随机Werner态序列，与储层态结合并在Ising哈密顿量下演化，构建基于Bloch基的观测量来训练系统识别未知特征。


<details>
  <summary>Details</summary>
Motivation: 量子极限学习机已成为量子信息处理的有力工具，本文旨在开发一个专门用于估计Werner态纠缠量的QELM协议。

Method: 生成随机Werner态序列，与储层态结合并在Ising哈密顿量下演化，构建基于Bloch基的观测量来训练系统。还研究了噪声对输入态的影响以及Ising哈密顿量中磁场参数对估计精度的影响。

Result: 协议能够有效估计Werner态的纠缠量，并在存在噪声的情况下保持鲁棒性。磁场参数对估计精度有显著影响。

Conclusion: 该QELM协议为Werner态纠缠估计提供了一种有效且鲁棒的方法，Ising哈密顿量中的磁场参数是优化性能的关键因素。

Abstract: Quantum Extreme Learning Machines (QELMs) have emerged as a potent tool for
various quantum information processing tasks. We present a QELM protocol for
estimating the amount of entanglement in Werner states. The protocol requires
the generation of a sequence of random Werner states, which are then combined
with a reservoir state and evolved using an Ising Hamiltonian. A set of
observables based on the Bloch basis is constructed and employed to train the
system to recognize unseen features. To assess the protocol's robustness, noise
is introduced into the input states, and the system's performance under these
noisy conditions is analyzed. Additionally, the influence of the magnetic field
parameter within the Ising Hamiltonian on the estimation accuracy is
investigated.

</details>


### [39] [Fast and Robust Remote Two-Qubit Gates on Distributed Qubits](https://arxiv.org/abs/2511.01418)
*Yunan Li,Xi Zhang,Weixin Zhang,Ruonan Guo,Yu Zhang,Xinsheng Tan,Yang Yu*

Main category: quant-ph

TL;DR: 本文提出并实验验证了一种通过参数调制实现远程量子几何门的方案，该方案具有几何相位的内在鲁棒性，并利用深度学习优化算法设计控制波形来抑制布居数泄漏。


<details>
  <summary>Details</summary>
Motivation: 分布式量子计算可以解决超导芯片硬件布局和纠错算法的复杂性，分布式芯片间的高质量门操作能够简化现有纠错算法。

Method: 采用参数调制的远程量子几何门方案，结合基于梯度的优化算法（自适应矩估计）设计控制波形，抑制布居数泄漏。

Result: 实验实现了快速远程SWAP和√SWAP门，操作时间约30纳秒，排除能量弛豫影响后，SWAP门错误率为1.16%，√SWAP门错误率为0.91%。模拟表明该方案可在数米电缆连接的分布式芯片中实现。

Conclusion: 该方案为模块化量子处理器提供了有效途径，为实现容错量子计算提供了有前景的路径。

Abstract: Distributed quantum computing offers a potential solution to the complexity
of superconducting chip hardware layouts and error correction algorithms.
High-quality gates between distributed chips enable the simplification of
existing error correction algorithms. This article proposes and demonstrates a
remote quantum geometric gate scheme via parametric modulation. Our scheme
inherits the intrinsic robustness of geometric phases. Meanwhile, by employing
gradient-based optimization algorithms(Adaptive Moment Estimation) from deep
learning, we design control waveforms that significantly suppress population
leakage. We experimentally realize the rapid remote SWAP and
$\sqrt{\text{SWAP}}$ gates with high fidelity, completing operation in about 30
ns. The gate error of SWAP ($\sqrt{\text{SWAP}}$) is 1.16\% (0.91\%) after
excluding the effect of energy relaxation. The simulation demonstrate that this
scheme can be implemented in the distributed chips connected by cables
extending several meters. Our results highlight the effectiveness of the
proposed protocol in enabling modular quantum processors, offering a promising
path toward the realization of fault-tolerant quantum computation.

</details>


### [40] [Quantum Blackwell's Ordering and Differential Privacy](https://arxiv.org/abs/2511.01467)
*Ayanava Dasgupta,Naqueeb Ahmad Warsi,Masahito Hayashi*

Main category: quant-ph

TL;DR: 基于量子假设检验和Blackwell排序构建量子差分隐私框架，分析量子学习算法的稳定性、量子参数估计的隐私保护，以及量子信道的收缩界限。


<details>
  <summary>Details</summary>
Motivation: 将经典差分隐私概念扩展到量子领域，为量子算法和量子机器学习提供隐私保护理论基础。

Method: 使用量子假设检验和Blackwell排序理论，通过假设检验散度来表征量子差分隐私，并分析量子状态对在隐私约束下的信息量。

Result: 推广了经典学习算法的稳定性结果到δ>0的情况，推导了量子Fisher信息在QDP下的紧界，建立了量子信道在hockey-stick散度下的近最优收缩界限。

Conclusion: 该框架为量子差分隐私提供了理论基础，在量子学习、参数估计和信道分析等方面都有重要应用价值。

Abstract: We develop a framework for quantum differential privacy (QDP) based on
quantum hypothesis testing and Blackwell's ordering. This approach
characterizes $(\eps,\delta)$-QDP via hypothesis testing divergences and
identifies the most informative quantum state pairs under privacy constraints.
We apply this to analyze the stability of quantum learning algorithms,
generalizing classical results to the case $\delta>0$. Additionally, we study
privatized quantum parameter estimation, deriving tight bounds on the quantum
Fisher information under QDP. Finally, we establish near-optimal contraction
bounds for differentially private quantum channels with respect to the
hockey-stick divergence.

</details>


### [41] [Non-unitary Quantum Physical Unclonable Functions: Modelling, Simulation, and Evaluation under Open Quantum Dynamics](https://arxiv.org/abs/2511.01514)
*Mohammadreza Vali,Hossein Aghababa,Nasser Yazdani*

Main category: quant-ph

TL;DR: 该论文提出了基于开放量子系统动力学的非幺正量子物理不可克隆函数(QPUFs)，通过利用退相干和耗散等非幺正效应来增强硬件安全性。


<details>
  <summary>Details</summary>
Motivation: 传统PUFs面临机器学习和侧信道攻击的威胁，而现有QPUFs模型往往忽略实际量子设备中的非幺正效应，需要新的方法来确保不可伪造性。

Method: 提出了三种非幺正QPUF架构：D-QPUF利用振幅阻尼作为熵源；MF-QPUF采用中间电路测量和条件幺正操作；L-QPUF通过Lindblad主方程和Trotter-Suzuki分解模拟马尔可夫噪声。

Result: 仿真结果显示这些非幺正设计实现了强独特性、均匀性和不可伪造性，通过随机噪声实现了可控的可靠性权衡，其中L-QPUF在有限挑战-响应访问下表现出指数级建模抵抗能力。

Conclusion: 通过将环境噪声重构为建设性资源，该工作建立了噪声感知量子硬件认证框架，并证明非幺正演化是后量子安全的可行基础。

Abstract: Physical Unclonable Functions (PUFs) provide hardware-level security by
exploiting intrinsic randomness to produce device-unique responses. However,
machine learning and side-channel attacks increasingly undermine their
classical assumptions, calling for new approaches to ensure unforgeability.
Quantum mechanics naturally supports this goal through intrinsic randomness and
the no-cloning theorem, motivating the study of Quantum Physical Unclonable
Functions (QPUFs). Yet, existing QPUF models often assume ideal unitary
dynamics, neglecting non-unitary effects such as decoherence and dissipation
that arise in real quantum devices. This work introduces a new class of
non-unitary QPUFs that leverage open quantum system dynamics as a foundation
for security. Three architectures are proposed: the Dissipative QPUF (D-QPUF),
which uses amplitude damping as an entropy source; the Measurement-Feedback
QPUF (MF-QPUF), which employs mid-circuit measurements and conditional
unitaries; and the Lindbladian QPUF (L-QPUF), which models Markovian noise via
the Lindblad master equation and Trotter-Suzuki decomposition. Simulation
results show that these non-unitary designs achieve strong uniqueness,
uniformity, and unforgeability, with controllable reliability trade-offs from
stochastic noise. The L-QPUF, in particular, exhibits exponential modeling
resistance under limited challenge-response access. By reframing environmental
noise as a constructive resource, this work establishes a framework for
noise-aware quantum hardware authentication and highlights non-unitary
evolution as a viable foundation for post-quantum security.

</details>


### [42] [Quantum Energy Teleportation under Equilibrium and Nonequilibrium Environments](https://arxiv.org/abs/2511.01518)
*Xiaokun Yan,Kun Zhang,Jin Wang*

Main category: quant-ph

TL;DR: 该研究分析了量子能量隐形传态在混合态下的性能，发现在非平衡环境中某些参数范围内可以增强能量输出。


<details>
  <summary>Details</summary>
Motivation: 量子能量隐形传态虽然理论上和实验上已被广泛研究，但在系统与环境相互作用产生混合态时，如何提高能量输出仍是一个重要挑战。

Method: 研究双量子比特系统与平衡或非平衡热库耦合，推导了能量输出的解析表达式，并使用Redfield主方程系统分析量子比特失谐、非平衡温度差和化学势差对能量输出的影响。

Result: 发现混合态的能量输出通常遵循最高布居数本征态的行为，非平衡环境在特定参数范围内可以增强能量输出。

Conclusion: 非平衡环境为增强量子能量隐形传态的能量输出提供了新的可能性。

Abstract: Quantum energy teleportation (QET), implemented via local operations and
classical communication, enables carrier-free energy transfer by exploiting
quantum resources. While QET has been extensively studied theoretically and
validated experimentally in various quantum platforms, enhancing energy output
for mixed initial states, as the system inevitably interacts with environments,
remains a significant challenge. In this work, we study QET performance in a
two-qubit system coupled to equilibrium or nonequilibrium reservoirs. We derive
an analytical expression for the energy output in terms of the system
Hamiltonian eigenstates, enabling analysis of energy output for mixed states.
Using the Redfield master equation, we systematically examine the effects of
qubit detuning, nonequilibrium temperature difference, and nonequilibrium
chemical potential difference on the energy output. We find that the energy
output for mixed states often follows that of the eigenstate with the highest
population, and that nonequilibrium environments can enhance the energy output
in certain parameter regimes.

</details>


### [43] [Graph Structured Operator Inequalities and Tsirelson-Type Bounds](https://arxiv.org/abs/2511.01525)
*James Tian*

Main category: quant-ph

TL;DR: 该论文建立了自伴收缩算子的二分张量和的算子范数界，推广了Tsirelson和CHSH界限的解析结构，给出了通过交换子和反对易子范数表达的无维度估计。


<details>
  <summary>Details</summary>
Motivation: 将解析算子不等式与量子信息场景（如Bell关联和网络非局域性）联系起来，提供封闭形式的估计来补充半定和数值方法。

Method: 使用基于图的公式化方法，通过仅依赖于图连通性的常数来捕捉稀疏交互模式。

Result: 获得了维度无关的算子范数界，这些界限通过交换子和反对易子范数表达，并适用于稀疏交互模式。

Conclusion: 该研究建立了自伴收缩算子二分张量和的解析不等式，为量子信息理论提供了新的分析工具和封闭形式估计。

Abstract: We establish operator norm bounds for bipartite tensor sums of self-adjoint
contractions. The inequalities generalize the analytic structure underlying the
Tsirelson and CHSH bounds, giving dimension-free estimates expressed through
commutator and anticommutator norms. A graph based formulation captures sparse
interaction patterns via constants depending only on graph connectivity. The
results link analytic operator inequalities with quantum information settings
such as Bell correlations and network nonlocality, offering closed-form
estimates that complement semidefinite and numerical methods.

</details>


### [44] [Security in a prepare-and-measure quantum key distribution protocol when the receiver uses weak values to guess the sender's bits](https://arxiv.org/abs/2511.01559)
*Rajendra Singh Bhati*

Main category: quant-ph

TL;DR: 本文发现弱值形式主义在混合态量子通信协议中会产生不准确结果，挑战了弱值作为弱测量现实元素的观点。


<details>
  <summary>Details</summary>
Motivation: 虽然弱值形式主义在实验量子力学中取得了显著成就并被广泛接受，但作者想要探索其在混合态量子通信协议中的适用性，验证其是否真的完美无缺。

Method: 在量子通信协议中应用混合态的弱值形式主义，分析其产生的结果准确性。

Result: 发现弱值形式主义在混合态量子通信协议中会产生不准确的结果，这与弱值支持者的观点相矛盾。

Conclusion: 弱值可能不是弱测量的现实元素，这强化了作者之前关于弱值本质的结论，挑战了弱值形式主义的完美性假设。

Abstract: The weak values and weak measurement formalism were initially limited to pure
states, which were later extended to mixed states, leading to intriguing
applications in quantum information processing tasks. Weak values are
considered to be abstract properties of systems describing a complete picture
between successive measurements in the two-state vector formalism (TSVF). The
remarkable achievements of the weak value formalism in experimental quantum
mechanics have persuaded most quantum physicists that it is impeccable.
However, we explore a scenario where the formalism of weak values for mixed
states is employed in a quantum communication protocol, but discover that it
generates inaccurate outcomes. This reinforces our previous conclusion that the
weak values may not be elements of the reality of weak measurements, contrary
to what the proponents of weak values proposed.

</details>


### [45] [Operator-aware shadow importance sampling for accurate fidelity estimation](https://arxiv.org/abs/2511.01608)
*Hyunho Cha,Sangwoo Hong,Jungwoo Lee*

Main category: quant-ph

TL;DR: 提出了两种基于算子感知影子重要性采样的算法，使用信息过完备的正算子值测度来改进直接保真度估计，解决了现有方法在精度和可扩展性之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有的直接保真度估计方法面临显著权衡：基于分组的DFE在小系统上精度高但存在指数级扩展问题，且仅限于Pauli测量；而基于经典影子的DFE具有可扩展性但对结构化状态精度较低。

Method: 开发了两种基于算子感知影子重要性采样的算法，使用信息过完备的正算子值测度，在局部Pauli测量实例化下运行。

Result: 对于Haar随机状态，算法优于基于分组的方法；对于GHZ和W等结构化状态，消除了先前分组方法的内存指数需求。数值实验证实该方法在Haar随机、GHZ和W目标上达到最先进性能。

Conclusion: 提出的算子感知影子重要性采样算法有效解决了直接保真度估计中精度与可扩展性的权衡问题，在各种量子状态上实现了最优性能。

Abstract: Estimating the fidelity between an unknown quantum state and a fixed target
is a fundamental task in quantum information science. Direct fidelity
estimation (DFE) enables this without full tomography by sampling observables
according to a target-dependent distribution. However, existing approaches face
notable trade-offs. Grouping-based DFE achieves strong accuracy for small
systems but suffers from exponential scaling, and its applicability is
restricted to Pauli measurements. In contrast, classical-shadow-based DFE
offers scalability but yields lower accuracy on structured states. In this
work, we address these limitations by developing two classes of operator-aware
shadow importance sampling algorithms using informationally overcomplete
positive operator-valued measures. Instantiated with local Pauli measurements,
our algorithm improves upon the grouping-based algorithms for Haar-random
states. For structured states such as the GHZ and W states, our algorithm also
eliminates the exponential memory requirements of previous grouping-based
methods. Numerical experiments confirm that our methods achieve
state-of-the-art performance across Haar-random, GHZ, and W targets.

</details>


### [46] [Measurement Strategies and Estimation Precision in Quantum Network Tomography](https://arxiv.org/abs/2511.01657)
*Athira Kalavampara Raghunadhan,Matheus Guedes De Andrade,Don Towsley,Indrakshi Dey,Daniel Kilper,Nicola Marchetti*

Main category: quant-ph

TL;DR: 该研究比较了量子网络层析成像中三种测量策略：局域Z基测量(LZM)、联合贝尔态测量(JBM)和预共享纠缠辅助测量(PEM)。PEM方案达到最低量子Cramér-Rao界，精度最高；JBM在精度和实现复杂度间取得良好平衡；LZM实验简单但在高噪声单链路估计中优于JBM。


<details>
  <summary>Details</summary>
Motivation: 研究量子网络层析成像中不同测量策略对链路参数估计精度的影响，为实验量子网络选择测量策略提供实用基础，在现实噪声条件下实现更准确和可扩展的链路参数估计。

Method: 分析三种测量方案：LZM、JBM和PEM，推导测量结果的概率分布，研究分布式态中噪声对估计精度的影响，获得量子Fisher信息矩阵的闭式表达式，通过量子Cramér-Rao界评估估计精度，并在四节点星型网络上进行数值分析。

Result: PEM方案达到最低QCRB，估计精度最高；JBM在精度和实现复杂度间平衡良好；LZM实验简单但在高噪声单链路估计中优于JBM。在四节点网络中，双监控器时JBM-only策略在所有噪声条件下优于混合策略；三监控器时仅在低噪声异质链路中达到更低QCRB。

Conclusion: 研究结果为实验量子网络选择测量策略建立了实用基础，PEM提供最高精度，JBM在复杂度和精度间平衡良好，LZM适合简单实验设置，不同策略在不同网络配置和噪声条件下各有优势。

Abstract: This work investigates measurement strategies for link parameter estimation
in Quantum Network Tomography (QNT), where network links are modeled as
depolarizing quantum channels distributing Werner states. Three distinct
measurement schemes are analyzed: local Z-basis measurements (LZM), joint
Bell-state measurements (JBM), and pre-shared entanglement-assisted
measurements (PEM). For each scheme, we derive the probability distributions of
measurement outcomes and examine how noise in the distributed states influences
estimation precision. Closed-form expressions for the Quantum Fisher
Information Matrix (QFIM) are obtained, and the estimation precision is
evaluated through the Quantum Cramer-Rao Bound (QCRB). Numerical analysis
reveals that the PEM scheme achieves the lowest QCRB, offering the highest
estimation accuracy, while JBM provides a favorable balance between precision
and implementation complexity. The LZM method, although experimentally simpler,
exhibits higher estimation error relative to the other schemes; however, it
outperforms JBM in high-noise regimes for single-link estimation. We further
evaluate the estimation performance on a four-node star network by comparing a
JBM-only configuration with a hybrid configuration that combines JBM and LZM.
When two monitors are used, the JBM-only strategy outperforms the hybrid
approach across all noise regimes. However, with three monitors, it achieves a
lower QCRB only in low-noise regimes with heterogeneous links. The results
establish a practical basis for selecting measurement strategies in
experimental quantum networks, enabling more accurate and scalable link
parameter estimation under realistic noise conditions.

</details>


### [47] [Initial-State Typicality in Quantum Relaxation](https://arxiv.org/abs/2511.01709)
*Ruicheng Bao*

Main category: quant-ph

TL;DR: 该论文发现高维开放量子系统中存在典型性现象：在可验证条件下，随着系统尺寸增大，弛豫过程几乎与初始状态无关，特别是在热化过程中。


<details>
  <summary>Details</summary>
Motivation: 开放量子系统中的弛豫是量子科学与技术的基础，但初始状态对弛豫的影响仍是一个未解决的核心问题。

Method: 通过系统表征一般初始状态的弛豫行为，研究高维开放量子系统中的典型性现象，并证明在尺寸无关温度以上的热化过程中存在这种典型性。

Result: 发现弛豫过程随着系统尺寸增大变得几乎与初始状态无关，提出了'典型强姆彭巴效应'和'典型弛豫时间'两个新概念，对Liouvillian能隙和最大弛豫时间等常用量提出了重新审视的必要。

Conclusion: 研究结果不仅扩展了典型性到开放量子动力学，还为加速弛豫提供了可扩展路径，并为量子模拟和态制备提供了典型混合时间基准，补充了传统的worst-case度量标准。

Abstract: Relaxation in open quantum systems is fundamental to quantum science and
technologies. Yet, the influence of the initial state on relaxation remains a
central, largely unanswered question. Here, by systematically characterizing
the relaxation behavior of generic initial states, we uncover a typicality
phenomenon in high-dimensional open quantum systems: relaxation becomes nearly
initial-state-independent as system size increases under verifiable conditions.
Crucially, we prove this typicality for thermalization processes above a
size-independent temperature. Our findings extend the typicality to open
quantum dynamics, in turn identifying a class of systems where two widely used
quantities -- the Liouvillian gap and the maximal relaxation time -- merit
re-examination. We formalize this with two new concepts: the 'typical strong
Mpemba effect' and the 'typical relaxation time'. Beyond these conceptual
advances, our results provide practical implications: a scalable route to
accelerating relaxation and a typical mixing-time benchmark that complements
conventional worst-case metrics for quantum simulations and state preparation.

</details>


### [48] [Non-Gaussianity and security of entanglement-based QKD](https://arxiv.org/abs/2511.01761)
*Mariia Gumberidze,Vladyslav C. Usenko*

Main category: quant-ph

TL;DR: 该论文分析了非高斯性与基于纠缠的量子密钥分发协议安全性之间的关系，考虑了不完美检测和不同噪声统计，为QKD协议实现提供了预检方法。


<details>
  <summary>Details</summary>
Motivation: 研究非高斯性与量子密钥分发协议安全性之间的理论关系，为实际QKD实现提供理论指导。

Method: 理论分析设备无关和基于纠缠的BB84协议，考虑不完美检测（暗计数和有限效率）、不同噪声统计（热噪声和泊松噪声）以及不同类型探测器。

Result: 发现了安全性和非高斯性之间的交叉区域，能够判断给定信道是否适合密钥分发。

Conclusion: 研究结果可作为QKD协议实现的预检工具，有助于评估信道对量子密钥分发的适用性。

Abstract: We theoretically analyse the relation between non-Gaussianity and security of
entanglement-based quantum key distribution (QKD) protocols, namely
device-independent (DI) and entanglement-based BB$84$. A similar analysis has
already been made for prepare-and-measure (P\&M) protocols \cite{Lasota2017}.
In addition, we consider imperfect detection with dark counts and limited
efficiency. We assume a perfect source of entangled Bell states as produced by
quantum-dot type sources, depolarisation in the channel and different noise
statistics, namely thermal and Poissonian. We consider single-photon avalanche
photodiodes (SPAD) and photon number resolving detectors (PNRD) and use their
respective criteria for non-Gaussianity. The results show cross-regions for
both security and non-Gaussianity, hence, the possibility to conclude about the
suitability of a given channel for secret key distribution. Our results can be
useful as a pre-check for the implementation of QKD protocols.

</details>


### [49] [Multi-objective optimization by quantum annealing](https://arxiv.org/abs/2511.01762)
*Andrew D. King*

Main category: quant-ph

TL;DR: 本文比较了量子退火和QAOA在多目标优化问题上的性能，发现量子退火在生成帕累托前沿方面显著优于QAOA和所有其他经典与量子方法。


<details>
  <summary>Details</summary>
Motivation: 多目标优化中生成帕累托前沿的计算成本很高，这使其成为量子优化的自然目标。研究旨在比较量子退火和QAOA在相同问题上的性能表现。

Method: 使用与之前研究相同的方法论，在两个输入问题上比较量子退火和QAOA（在IBM门模型处理器上运行）的性能。

Result: 量子退火在性能上大幅超越QAOA以及之前研究中分析的所有经典和量子方法。在更难的问题上，量子退火改进了已知的最佳帕累托前沿。

Conclusion: 这项小型研究强化了量子退火在多目标优化中的潜力。

Abstract: An important task in multi-objective optimization is generating the Pareto
front -- the set of all Pareto-optimal compromises among multiple objective
functions applied to the same set of variables. Since this task can be
computationally intensive even for small problems, it is a natural target for
quantum optimization. Indeed, this problem was recently approached using the
quantum approximate optimization algorithm (QAOA) on an IBM gate-model
processor. Here we compare these QAOA results with quantum annealing on the
same two input problems, using the same methodology. We find that quantum
annealing vastly outperforms not just QAOA run on the IBM processor, but all
classical and quantum methods analyzed in the previous study. On the harder
problem, quantum annealing improves upon the best known Pareto front. This
small study reinforces the promise of quantum annealing in multi-objective
optimization.

</details>


### [50] [The Born Ultimatum: Conditions for Classical Surrogation of Quantum Generative Models with Correlators](https://arxiv.org/abs/2511.01845)
*Mario Herrero-Gonzalez,Brian Coyle,Kieran McDowall,Ross Grassie,Sjoerd Beentjes,Ava Khamseh,Elham Kashefi*

Main category: quant-ph

TL;DR: 本文分析了量子电路玻恩机(QCBM)作为量子傅里叶模型的特性，研究了训练-经典、部署-量子方法中的局限性，通过傅里叶分解量化了经典训练与量子部署之间的差异。


<details>
  <summary>Details</summary>
Motivation: 量子电路玻恩机在生成任务中具有潜在量子优势，但面临训练困难问题。最近提出的训练-经典、部署-量子方法试图通过训练经典代理模型来解决这些问题，但存在部署差异问题。

Method: 使用傅里叶分解分析玻恩规则，通过分布截断和经典代理来近似分解，使用张量网络和泡利传播基经典代理方法，研究了IQP电路、匹配电路、海森堡链电路和霍尔丹链电路。

Result: 推导了IQP电路中泡利传播的闭式表达式和霍尔丹链的动态李代数，数值演示了经典训练与量子部署之间的差异。

Conclusion: 训练-经典、部署-量子方法存在显著的部署差异问题，需要进一步研究如何减少这种差异以实现有效的量子生成模型部署。

Abstract: Quantum Circuit Born Machines (QCBMs) are powerful quantum generative models
that sample according to the Born rule, with complexity-theoretic evidence
suggesting potential quantum advantages for generative tasks. Here, we identify
QCBMs as a quantum Fourier model independently of the loss function. This
allows us to apply known dequantization conditions when the optimal quantum
distribution is available. However, realizing this distribution is hindered by
trainability issues such as vanishing gradients on quantum hardware. Recent
train-classical, deploy-quantum approaches propose training classical
surrogates of QCBMs and using quantum devices only for inference. We analyze
the limitations of these methods arising from deployment discrepancies between
classically trained and quantumly deployed parameters. Using the Fourier
decomposition of the Born rule in terms of correlators, we quantify this
discrepancy analytically. Approximating the decomposition via distribution
truncation and classical surrogation provides concrete examples of such
discrepancies, which we demonstrate numerically. We study this effect using
tensor-networks and Pauli-propagation-based classical surrogates. Our study
examines the use of IQP circuits, matchcircuits, Heisenberg-chain circuits, and
Haldane-chain circuits for the QCBM ansatz. In doing so, we derive closed-form
expressions for Pauli propagation in IQP circuits and the dynamical Lie algebra
of the Haldane chain, which may be of independent interest.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [51] [Biomechanical and Mechanobiological Modelling of Functionally Graded Scaffolds for Large Bone Defects](https://arxiv.org/abs/2511.00743)
*Ali Entezari,Vahid Badali,Sara Checa*

Main category: physics.comp-ph

TL;DR: 提出了一个结合有限元分析和基于代理建模的框架，用于评估功能梯度支架在骨缺损修复中的生物力学和再生潜力，发现轴向梯度促进骨长入，径向梯度增强结构稳定性。


<details>
  <summary>Details</summary>
Motivation: 临界尺寸骨缺损是临床主要挑战，需要兼具机械稳定性和再生能力的支架。功能梯度支架通过空间变化孔隙率来优化载荷传递和组织长入。

Method: 开发了集成有限元-基于代理建模框架，模拟圆柱形支架在生理相关条件下的生物力学和细胞行为。有限元模型包含多孔弹性组织力学和步态相关载荷，基于代理模型模拟细胞迁移、增殖、分化和凋亡。

Result: 150天模拟显示：轴向梯度（骨界面处较大孔隙）促进更多骨长入，径向梯度（较密外周支柱）显著降低峰值冯·米塞斯应力，揭示了再生性能与结构能力之间的设计权衡。

Conclusion: 耦合FEA-ABM框架为下一代功能梯度支架的理性设计建立了机制平台，为针对缺损位置和载荷环境的植入物临床前优化提供了途径。

Abstract: Critical sized bone defects remain a major clinical challenge, requiring
scaffolds that combine mechanical stability with regenerative capacity.
Functionally graded (FG) scaffolds, inspired by the graded architecture of
native bone, offer a promising solution by spatially varying porosity to
optimise both load transfer and tissue ingrowth. Here, we present an integrated
finite element agent based modelling (FEA ABM) framework to simultaneously
evaluate the biomechanics and regenerative potential of FG scaffolds under
physiologically relevant conditions. Cylindrical scaffolds with axial or radial
pore size gradients were compared with uniform controls. The finite element
model incorporated poroelastic tissue mechanics and gait related loading to
compute local shear strain and fluid velocity, which guided cellular behaviours
in the agent based model, including progenitor migration, proliferation,
differentiation, and apoptosis. Simulations over 150 days revealed that axial
gradients with larger pores at the host bone interface promoted greater bone
ingrowth, while radial gradients with denser peripheral struts substantially
reduced peak von Mises stresses. These findings highlight a fundamental design
trade off between maximising regenerative performance and enhancing structural
competence. The coupled FEA ABM framework establishes a mechanistic platform
for the rational design of next-generation FG scaffolds, offering a pathway
toward preclinical optimisation of implants tailored to defect location and
loading environment.

</details>


### [52] [Integrated photonic multigrid solver for partial differential equations](https://arxiv.org/abs/2511.01005)
*Timoteo Lee,Frank Brückerhoff-Plückelmann,Jelle Dijkstra,Jan M. Pawlowski,Wolfram Pernice*

Main category: physics.comp-ph

TL;DR: 提出了一种混合精度的光子多重网格求解器，将计算密集的平滑过程卸载到光学域，在光子加速器上求解偏微分方程，可减少80%以上的数字运算。


<details>
  <summary>Details</summary>
Motivation: 传统高性能计算机在求解偏微分方程时面临极限，需要构建高效、超快的光子硬件加速器来分析和预测复杂的大规模物理系统。

Method: 利用低延迟光子矩阵向量乘法器与多重网格偏微分方程求解器的协同作用，提出混合精度光子多重网格求解器，将平滑过程卸载到光学域。

Result: 在2 GSPS的集成光子加速器上测试了泊松方程和薛定谔方程求解，通过将平滑操作卸载到光子系统，可减少80%以上的数字运算。在晶格量子色动力学计算中，光子多重网格求解器可能减少高达97%的数字运算。

Conclusion: 光子多重网格求解器实现了计算速度和效率的数量级提升，为大规模物理系统分析提供了高效的光子计算解决方案。

Abstract: Solving partial differential equations is crucial to analysing and predicting
complex, large-scale physical systems but pushes conventional high-performance
computers to their limits. Application specific photonic processors are an
exciting computing paradigm for building efficient, ultrafast hardware
accelerators. Here, we investigate the synergy between multigrid based partial
differential equations solvers and low latency photonic matrix vector
multipliers. We propose a mixed-precision photonic multigrid solver, that
offloads the computationally demanding smoothening procedure to the optical
domain. We test our approach on an integrated photonic accelerator operating at
2 GSPS solving a Poisson and Schr\"odinger equation. By offloading the
smoothening operation to the photonic system, we can reduce the digital
operation by more than 80%. Finally, we show that the photonic multigrid solver
potentially reduces digital operations by up to 97 % in lattice quantum
chromodynamics (LQCD) calculations, enabling an order-of-magnitude gain in
computational speed and efficiency.

</details>


### [53] [BzScope: an absolute cross section calculator for neutron-phonon scattering](https://arxiv.org/abs/2511.01178)
*Ming Tang,Zi-Yi Pan,Ni Yang,Xiao-Xiao Cai*

Main category: physics.comp-ph

TL;DR: BzScope是一个用于高效计算晶体粉末中中子-声子非弹性散射绝对截面的Python软件包，改进了传统直方图方法在再现尖锐结构和确保收敛方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统的中子散射模拟方法在处理大相空间时难以准确再现尖锐结构并确保收敛，需要更高效和精确的计算工具。

Method: 采用改进的积分方法，支持理想晶体粉末中单声子和双声子散射函数的计算，数值稳定性可达100 Å^-1动量转移。高阶散射通过NCrystal包中的非相干近似计算。

Result: 验证显示与NCrystal在立方系统Ni的非相干散射中具有良好一致性，对低对称性材料NiP2避免了各向同性原子位移近似，提高了准确性。LiH和Be的实验数据基准测试证实了可靠性。

Conclusion: BzScope通过NCrystal插件与蒙特卡罗软件包集成，提高了中子散射模拟的效率和准确性，推动了凝聚态动力学研究。

Abstract: BzScope is a Python package designed for efficiently calculating absolute
cross sections of neutron-phonon inelastic scattering for crystalline powders
in large phase spaces, addressing the limitations of traditional histogramming
techniques in reproducing sharp structures and ensuring convergence. The
package employs an adapted integral method and supports calculations of single-
and two-phonon scattering functions in ideal crystalline powders, with
numerical robustness up to a momentum transfer of 100 Ang^-1. Higher order
scatterings up to several hundred orders are calculated by incoherent
approximation in a well-established thermal neutron scattering physics package,
NCrystal. In addition, a NCrystal plugin is made available for NCrystal-enabled
Monte Carlo packages, facilitating direct comparison between the new physics
and experimental data.
  Validation against NCrystal demonstrates good agreement in incoherent
scattering for cubic systems Ni. In addition, it shows improved accuracy for
low-symmetry materials $NiP_2$ by avoiding the isotropic atomic displacement
approximations in NCrystal. Benchmarks the experimental differential cross
section of LiH and total cross section of Be confirm its reliability.
  BzScope integrates with NCrystal via a plugin and therefore can be directly
used in any NCrystal-enabled Monte Carlo package. This tool enhances the
efficiency and accuracy of neutron scattering simulations, advancing the study
of condensed matter dynamics.

</details>


### [54] [A fast and rigorous numerical tool to measure length-scale artifacts in molecular simulations](https://arxiv.org/abs/2511.01442)
*Benedikt M. Reible,Nils Liebreich,Carsten Hartmann,Luigi Delle Site*

Main category: physics.comp-ph

TL;DR: 该论文提出了一个数值算法来计算质量因子，用于量化给定模拟盒子尺寸下统计力学的一致性程度，并验证了其与文献中不同盒子尺寸模拟结果的一致性。


<details>
  <summary>Details</summary>
Motivation: 基于双面Bogoliubov不等式定理，该研究旨在定义一个质量因子来直接量化给定模拟盒子尺寸下统计力学的一致性程度，从而为多体系统的自由能成本提供严格界限。

Method: 开发了一个数值算法来计算质量因子，该算法对于具有二体相互作用和已知径向分布函数的系统，只需计算两个六维积分即可。

Result: 算法计算结果与文献中不同盒子尺寸模拟结果一致，验证了该方法的可靠性。

Conclusion: 提出的数值算法能够有效计算质量因子，为评估模拟盒子尺寸的统计力学一致性提供了实用工具，并在实际应用中得到了验证。

Abstract: The two-sided Bogoliubov inequality for classical and quantum many-body
systems is a theorem that provides rigorous bounds on the free-energy cost of
partitioning a given system into two or more independent subsystems. This
theorem motivates the definition of a quality factor which directly quantifies
the degree of statistical-mechanical consistency achieved by a given simulation
box size. A major technical merit of the theorem is that, for systems with
two-body interactions and a known radial distribution function, the quality
factor can be computed by evaluating just two six-dimensional integrals. In
this work, we present a numerical algorithm for computing the quality factor
and demonstrate its consistency with respect to results in the literature
obtained from simulations performed at different box sizes.

</details>


### [55] [Simulation of Self-Assembled Monolayers of Polyalanine $α$-Helix Using an Effective Potential](https://arxiv.org/abs/2511.01596)
*Hadis Ghodrati Saeini,Kevin Preis,Thi Ngoc Ha,Christoph Tegenkamp,Sibylle Gemming,Jeffrey Kelling,Florian Günther*

Main category: physics.comp-ph

TL;DR: 本文研究了α-聚丙氨酸螺旋自组装单层膜的结构相及其对手性诱导自旋选择性的影响，发现对映纯体系形成六方晶格，外消旋混合物形成矩形相，并通过理论模型揭示了结构差异的机制。


<details>
  <summary>Details</summary>
Motivation: 研究手性组成如何控制超分子组织，建立肽基自旋电子材料的基本结构-性质关系。

Method: 结合扫描隧道显微镜和SCC-DFTB理论建模，分析不同手性组成的自组装结构。

Result: 对映纯体系形成六方晶格，外消旋混合物形成更紧密的矩形相；STM对比度源于相反手性螺旋的反平行排列而非物理高度变化。

Conclusion: 建立了肽基自旋电子材料设计的基本结构-性质关系，揭示了手性组成对超分子组织的调控机制。

Abstract: Self-assembled monolayers of $\alpha$-polyalanine helices exhibit distinct
structural phases with implications for chiral-induced spin selectivity. We
combine scanning tunneling microscopy and theoretical modeling to reveal how
chiral composition governs supramolecular organization. Enantiopure systems
form hexagonal lattices, while racemic mixtures organize into rectangular
phases with stripe-like features. Our SCC-DFTB derived interaction potentials
show that opposite-handed helix pairs exhibit stronger binding and closer
packing, explaining the denser racemic structures. Crucially, we demonstrate
that the observed STM contrast arises from anti-parallel alignment of
opposite-handed helices rather than physical height variations. These findings
establish fundamental structure-property relationships for designing
peptide-based spintronic materials.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [56] [VRScout: Towards Real-Time, Autonomous Testing of Virtual Reality Games](https://arxiv.org/abs/2511.00002)
*Yurun Wu,Yousong Sun,Burkhard Wunsche,Jia Wang,Elliott Wen*

Main category: cs.LG

TL;DR: VRScout是一个基于深度学习的自主VR测试代理，能够以人类方式实时导航VR环境并交互，用于自动化VR游戏测试。


<details>
  <summary>Details</summary>
Motivation: 传统人工VR内容质量保证劳动密集且无法适应行业快速增长，而现有自动化测试方法难以应对VR的高维感官输入和实时性能要求。

Method: 使用增强型Action Chunking Transformer从人类演示中学习，预测多步动作序列，并引入动态可调滑动视界来平衡响应性和精度。

Result: 在商业VR游戏中达到专家级性能，仅需有限训练数据，并在消费级硬件上保持60 FPS的实时推理。

Conclusion: VRScout为自动化VR游戏测试提供了一个实用且可扩展的框架，可直接应用于质量保证和安全审计。

Abstract: Virtual Reality (VR) has rapidly become a mainstream platform for gaming and
interactive experiences, yet ensuring the quality, safety, and appropriateness
of VR content remains a pressing challenge. Traditional human-based quality
assurance is labor-intensive and cannot scale with the industry's rapid growth.
While automated testing has been applied to traditional 2D and 3D games,
extending it to VR introduces unique difficulties due to high-dimensional
sensory inputs and strict real-time performance requirements. We present
VRScout, a deep learning-based agent capable of autonomously navigating VR
environments and interacting with virtual objects in a human-like and real-time
manner. VRScout learns from human demonstrations using an enhanced Action
Chunking Transformer that predicts multi-step action sequences. This enables
our agent to capture higher-level strategies and generalize across diverse
environments. To balance responsiveness and precision, we introduce a
dynamically adjustable sliding horizon that adapts the agent's temporal context
at runtime. We evaluate VRScout on commercial VR titles and show that it
achieves expert-level performance with only limited training data, while
maintaining real-time inference at 60 FPS on consumer-grade hardware. These
results position VRScout as a practical and scalable framework for automated VR
game testing, with direct applications in both quality assurance and safety
auditing.

</details>


### [57] [Feature-Guided SAE Steering for Refusal-Rate Control using Contrasting Prompts](https://arxiv.org/abs/2511.00029)
*Samaksh Bhargav,Zining Zhu*

Main category: cs.LG

TL;DR: 使用稀疏自编码器(SAE)特征选择和对比提示方法，在Llama-3 8B模型上实现了安全性和实用性的双重提升，打破了传统的安全-效用权衡。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全部署方法需要调整模型权重且成本高昂，而现有的SAE方法缺乏系统性的特征选择和原则性的安全-效用权衡评估。

Method: 使用稀疏自编码器提取可解释特征，通过创新的对比提示方法从AI-Generated Prompts Dataset和Air Bench eu-dataset中高效选择最佳特征进行引导。

Result: 在Llama-3 8B模型上，安全性性能提升18.9%，同时实用性增加11.1%。

Conclusion: 通过原则性特征选择方法识别最优特征，定向SAE引导可以克服传统的安全-效用权衡。

Abstract: Large Language Model (LLM) deployment requires guiding the LLM to recognize
and not answer unsafe prompts while complying with safe prompts. Previous
methods for achieving this require adjusting model weights along with other
expensive procedures. While recent advances in Sparse Autoencoders (SAEs) have
enabled interpretable feature extraction from LLMs, existing approaches lack
systematic feature selection methods and principled evaluation of
safety-utility tradeoffs. We explored using different steering features and
steering strengths using Sparse Auto Encoders (SAEs) to provide a solution.
Using an accurate and innovative contrasting prompt method with the
AI-Generated Prompts Dataset from teknium/OpenHermes-2p5-Mistral-7B and Air
Bench eu-dataset to efficiently choose the best features in the model to steer,
we tested this method on Llama-3 8B. We conclude that using this method, our
approach achieves an 18.9% improvement in safety performance while
simultaneously increasing utility by 11.1%, demonstrating that targeted SAE
steering can overcome traditional safety-utility tradeoffs when optimal
features are identified through principled selection methods.

</details>


### [58] [Probing Knowledge Holes in Unlearned LLMs](https://arxiv.org/abs/2511.00030)
*Myeongseob Ko,Hoang Anh Just,Charles Fleming,Ming Jin,Ruoxi Jia*

Main category: cs.LG

TL;DR: 论文发现机器遗忘技术虽然能有效移除不良知识，但会意外造成"知识空洞"——良性知识的非预期损失，标准基准测试无法检测到这些隐藏成本。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘技术虽然能有效移除不良内容且不影响标准基准性能，但可能存在未被发现的"知识空洞"问题，需要更全面的评估方法。

Method: 提出测试用例生成框架，探索遗忘内容邻近区域和更广泛的潜在失效区域，评估遗忘模型的知识保留情况。

Result: 评估显示遗忘存在显著隐藏成本：高达98.7%的测试用例在遗忘模型中产生无关或荒谬回答，而这些在预训练模型中是可回答的。

Conclusion: 需要重新思考评估机器遗忘中知识保留的传统方法，超越标准静态基准测试，以发现和解决知识空洞问题。

Abstract: Machine unlearning has emerged as a prevalent technical solution for
selectively removing unwanted knowledge absorbed during pre-training, without
requiring full retraining. While recent unlearning techniques can effectively
remove undesirable content without severely compromising performance on
standard benchmarks, we find that they may inadvertently create ``knowledge
holes'' -- unintended losses of benign knowledge that standard benchmarks fail
to capture. To probe where unlearned models reveal knowledge holes, we propose
a test case generation framework that explores both immediate neighbors of
unlearned content and broader areas of potential failures. Our evaluation
demonstrates significant hidden costs of unlearning: up to 98.7\% of the test
cases yield irrelevant or nonsensical responses from unlearned models, despite
being answerable by the pretrained model. These findings necessitate rethinking
the conventional approach to evaluating knowledge preservation in unlearning,
moving beyond standard, static benchmarks.

</details>


### [59] [From Uniform to Adaptive: General Skip-Block Mechanisms for Efficient PDE Neural Operators](https://arxiv.org/abs/2511.00032)
*Lei Liu,Zhongyi Yu,Hong Wang,Huanshuo Dong,Haiyang Xin,Hongwei Zhao,Bin Li*

Main category: cs.LG

TL;DR: 提出Skip-Block Routing (SBR)框架，通过路由机制学习token复杂度并动态分配计算资源，显著降低Transformer神经算子的计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前神经算子在处理大规模工程问题时存在显著计算开销，且统一计算成本与物理场复杂度差异不匹配，导致效率低下。

Method: SBR框架集成到Transformer神经算子中，通过学习token复杂度和排序，在推理时根据排名决定后续层传递的token数量，将更多计算资源分配给复杂区域。

Result: SBR可集成到多种神经算子中，减少约50%的FLOPs计算成本，推理速度提升达2倍，且不牺牲精度。

Conclusion: SBR框架有效解决了神经算子在处理复杂物理场时的计算效率问题，为大规模工程应用提供了高效解决方案。

Abstract: In recent years, Neural Operators(NO) have gradually emerged as a popular
approach for solving Partial Differential Equations (PDEs). However, their
application to large-scale engineering tasks suffers from significant
computational overhead. And the fact that current models impose a uniform
computational cost while physical fields exhibit vastly different complexities
constitutes a fundamental mismatch, which is the root of this inefficiency. For
instance, in turbulence flows, intricate vortex regions require deeper network
processing compared to stable flows. To address this, we introduce a framework:
Skip-Block Routing (SBR), a general framework designed for Transformer-based
neural operators, capable of being integrated into their multi-layer
architectures. First, SBR uses a routing mechanism to learn the complexity and
ranking of tokens, which is then applied during inference. Then, in later
layers, it decides how many tokens are passed forward based on this ranking.
This way, the model focuses more processing capacity on the tokens that are
more complex. Experiments demonstrate that SBR is a general framework that
seamlessly integrates into various neural operators. Our method reduces
computational cost by approximately 50% in terms of Floating Point Operations
(FLOPs), while still delivering up to 2x faster inference without sacrificing
accuracy.

</details>


### [60] [Neural Architecture Search for global multi-step Forecasting of Energy Production Time Series](https://arxiv.org/abs/2511.00035)
*Georg Velev,Stefan Lessmann*

Main category: cs.LG

TL;DR: 提出基于神经架构搜索(NAS)的自动化框架，用于发现平衡计算效率、预测性能和泛化能力的时序模型，用于能源生产的全局多步短期预测。


<details>
  <summary>Details</summary>
Motivation: 能源领域需要预测准确性和运行时效率，但手动配置复杂方法耗时且易出错，同时需要考虑时序动态特性和对未见数据的泛化能力。

Method: 设计包含高效组件的搜索空间，提出考虑时序上下文性能泛化和高维搜索空间探索的新目标函数，通过NAS自动发现轻量级架构。

Result: 在能源生产时序数据上，NAS发现的轻量级架构集成在效率和准确性方面均优于Transformer等最先进技术及预训练预测模型。

Conclusion: NAS框架能够自动发现高效且准确的预测模型，解决了能源预测中的计算效率、预测性能和泛化能力平衡问题。

Abstract: The dynamic energy sector requires both predictive accuracy and runtime
efficiency for short-term forecasting of energy generation under operational
constraints, where timely and precise predictions are crucial. The manual
configuration of complex methods, which can generate accurate global multi-step
predictions without suffering from a computational bottleneck, represents a
procedure with significant time requirements and high risk for human-made
errors. A further intricacy arises from the temporal dynamics present in
energy-related data. Additionally, the generalization to unseen data is
imperative for continuously deploying forecasting techniques over time. To
overcome these challenges, in this research, we design a neural architecture
search (NAS)-based framework for the automated discovery of time series models
that strike a balance between computational efficiency, predictive performance,
and generalization power for the global, multi-step short-term forecasting of
energy production time series. In particular, we introduce a search space
consisting only of efficient components, which can capture distinctive patterns
of energy time series. Furthermore, we formulate a novel objective function
that accounts for performance generalization in temporal context and the
maximal exploration of different regions of our high-dimensional search space.
The results obtained on energy production time series show that an ensemble of
lightweight architectures discovered with NAS outperforms state-of-the-art
techniques, such as Transformers, as well as pre-trained forecasting models, in
terms of both efficiency and accuracy.

</details>


### [61] [Semi-Supervised Preference Optimization with Limited Feedback](https://arxiv.org/abs/2511.00040)
*Seonggyun Lee,Sungjun Lim,Seojin Park,Soeun Cheon,Kyungwoo Song*

Main category: cs.LG

TL;DR: 提出半监督偏好优化(SSPO)方法，利用少量配对偏好标签和大量未配对样本同时学习，显著降低数据获取成本


<details>
  <summary>Details</summary>
Motivation: 现有偏好优化方法严重依赖大量配对反馈数据，导致资源消耗巨大，需要更高效的数据利用方法

Method: 通过理论证明存在最优奖励阈值能高概率区分胜负响应，基于此对未配对数据进行原则性伪标注，利用伪标签从大规模未配对数据中提取潜在偏好

Result: 在多个数据集上的实验验证了显著的数据效率，例如使用Llama3-8B-Instruct在仅1% UltraFeedback数据上训练的SSPO持续超越在10%数据上训练的强基线

Conclusion: SSPO能有效维持人类对齐同时大幅降低获取成本，为偏好优化提供了高效的数据利用方案

Abstract: The field of preference optimization has made outstanding contributions to
the alignment of language models with human preferences. Despite these
advancements, recent methods still rely heavily on substantial paired (labeled)
feedback data, leading to substantial resource expenditures. To address these
challenges, we study the problem of Semi-Supervised Preference Optimization
(SSPO) in which the idea is to learn from both a small number of pairwise
preference labels and a large pool of unpaired samples simultaneously. Our key
theoretical contribution proves the existence of an optimal reward threshold
capable of separating winning and losing responses with high probability, which
enables a principled pseudo-labeling of unpaired data. By leveraging these
pseudo-labels, SSPO effectively distills latent preferences from large-scale
unpaired data, thus maintaining human alignment while drastically reducing
acquisition costs. Extensive experiments across datasets validate this
remarkable data efficiency; for instance, SSPO trained with Llama3-8B-Instruct
on just 1% of UltraFeedback consistently surpasses strong baselines trained on
10% of UltraFeedback.

</details>


### [62] [Physics-Informed Neural Network Frameworks for the Analysis of Engineering and Biological Dynamical Systems Governed by Ordinary Differential Equations](https://arxiv.org/abs/2511.00043)
*Tyrus Whitman,Andrew Particka,Christopher Diers,Ian Griffin,Charuka Wickramasinghe,Pradeep Ranaweera*

Main category: cs.LG

TL;DR: PINNs方法在解决工程和生物动力学系统ODE问题时展现出优越性能，通过嵌入物理定律和精心调整超参数，能够有效处理传统数值方法难以收敛的复杂问题。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法在处理高刚度、冲击、不规则域、奇异扰动、高维或边界不连续等复杂ODE问题时往往难以收敛，需要寻找更强大的替代方法。

Method: 使用PINNs框架，通过将物理定律直接嵌入学习过程，并系统调整网络深度、层宽、激活函数、学习率、优化算法、权重初始化方案和配点采样等超参数。

Result: PINNs在复杂问题中能够收敛到正确解，但需要适当平衡数据损失、初始条件损失和残差损失，并通过硬约束和先验知识增强预测能力。

Conclusion: PINNs虽然不是万能解决方案，但通过精心设计的损失函数平衡和超参数调优，能够为传统数值方法难以处理的复杂ODE问题提供有效的替代方案。

Abstract: In this study, we present and validate the predictive capability of the
Physics-Informed Neural Networks (PINNs) methodology for solving a variety of
engineering and biological dynamical systems governed by ordinary differential
equations (ODEs). While traditional numerical methods a re effective for many
ODEs, they often struggle to achieve convergence in problems involving high
stiffness, shocks, irregular domains, singular perturbations, high dimensions,
or boundary discontinuities. Alternatively, PINNs offer a powerful approach for
handling challenging numerical scenarios. In this study, classical ODE problems
are employed as controlled testbeds to systematically evaluate the accuracy,
training efficiency, and generalization capability under controlled conditions
of the PINNs framework. Although not a universal solution, PINNs can achieve
superior results by embedding physical laws directly into the learning process.
We first analyze the existence and uniqueness properties of several benchmark
problems and subsequently validate the PINNs methodology on these model
systems. Our results demonstrate that for complex problems to converge to
correct solutions, the loss function components data loss, initial condition
loss, and residual loss must be appropriately balanced through careful
weighting. We further establish that systematic tuning of hyperparameters,
including network depth, layer width, activation functions, learning rate,
optimization algorithms, w eight initialization schemes, and collocation point
sampling, plays a crucial role in achieving accurate solutions. Additionally,
embedding prior knowledge and imposing hard constraints on the network
architecture, without loss the generality of the ODE system, significantly
enhances the predictive capability of PINNs.

</details>


### [63] [ReLaX-Net: Reusing Layers for Parameter-Efficient Physical Neural Networks](https://arxiv.org/abs/2511.00044)
*Kohei Tsuchiyama,Andre Roehm,Takatomo Mihana,Ryoichi Horisaki*

Main category: cs.LG

TL;DR: 提出了ReLaX-Net架构，通过时间复用层来扩展物理神经网络的有效深度，提高参数利用效率


<details>
  <summary>Details</summary>
Motivation: 物理神经网络在规模上落后数字神经网络几个数量级，需要参数高效的重用方法，类似于早期数字神经网络中卷积网络的发展

Method: 采用层间时间复用方案，在现有PNN基础上仅需添加快速开关，通过重用层来扩展网络深度

Result: 在图像分类和自然语言处理任务上的数值实验显示，ReLaX-Net在相同参数数量下性能优于传统RNN或DNN

Conclusion: ReLaX-Net通过简单修改显著提升PNN计算性能，展现出有利的缩放特性

Abstract: Physical Neural Networks (PNN) are promising platforms for next-generation
computing systems. However, recent advances in digital neural network
performance are largely driven by the rapid growth in the number of trainable
parameters and, so far, demonstrated PNNs are lagging behind by several orders
of magnitude in terms of scale. This mirrors size and performance constraints
found in early digital neural networks. In that period, efficient reuse of
parameters contributed to the development of parameter-efficient architectures
such as convolutional neural networks.
  In this work, we numerically investigate hardware-friendly weight-tying for
PNNs. Crucially, with many PNN systems, there is a time-scale separation
between the fast dynamic active elements of the forward pass and the only
slowly trainable elements implementing weights and biases. With this in mind,we
propose the Reuse of Layers for eXpanding a Neural Network (ReLaX-Net)
architecture, which employs a simple layer-by-layer time-multiplexing scheme to
increase the effective network depth and efficiently use the number of
parameters. We only require the addition of fast switches for existing PNNs. We
validate ReLaX-Nets via numerical experiments on image classification and
natural language processing tasks. Our results show that ReLaX-Net improves
computational performance with only minor modifications to a conventional PNN.
We observe a favorable scaling, where ReLaX-Nets exceed the performance of
equivalent traditional RNNs or DNNs with the same number of parameters.

</details>


### [64] [DynBERG: Dynamic BERT-based Graph neural network for financial fraud detection](https://arxiv.org/abs/2511.00047)
*Omkar Kulkarni,Rohitash Chandra*

Main category: cs.LG

TL;DR: 提出DynBERG模型，将Graph-BERT与GRU结合，用于动态金融交易网络的欺诈检测，在比特币交易数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有Graph-BERT模型主要针对静态无向图设计，而金融交易网络具有动态演化和有向边特性，需要专门解决方案。

Method: 结合Graph-BERT与GRU层捕捉时序演化，修改算法支持有向边，适用于动态金融交易分析。

Result: 在Elliptic比特币交易数据集上，DynBERG在市场关闭事件前后均优于EvolveGCN和GCN，GRU组件对时序建模至关重要。

Conclusion: DynBERG能有效适应金融市场重大变化，为动态金融欺诈检测提供了有效解决方案。

Abstract: Financial fraud detection is critical for maintaining the integrity of
financial systems, particularly in decentralised environments such as
cryptocurrency networks. Although Graph Convolutional Networks (GCNs) are
widely used for financial fraud detection, graph Transformer models such as
Graph-BERT are gaining prominence due to their Transformer-based architecture,
which mitigates issues such as over-smoothing. Graph-BERT is designed for
static graphs and primarily evaluated on citation networks with undirected
edges. However, financial transaction networks are inherently dynamic, with
evolving structures and directed edges representing the flow of money. To
address these challenges, we introduce DynBERG, a novel architecture that
integrates Graph-BERT with a Gated Recurrent Unit (GRU) layer to capture
temporal evolution over multiple time steps. Additionally, we modify the
underlying algorithm to support directed edges, making DynBERG well-suited for
dynamic financial transaction analysis. We evaluate our model on the Elliptic
dataset, which includes Bitcoin transactions, including all transactions during
a major cryptocurrency market event, the Dark Market Shutdown. By assessing
DynBERG's resilience before and after this event, we analyse its ability to
adapt to significant market shifts that impact transaction behaviours. Our
model is benchmarked against state-of-the-art dynamic graph classification
approaches, such as EvolveGCN and GCN, demonstrating superior performance,
outperforming EvolveGCN before the market shutdown and surpassing GCN after the
event. Additionally, an ablation study highlights the critical role of
incorporating a time-series deep learning component, showcasing the
effectiveness of GRU in modelling the temporal dynamics of financial
transactions.

</details>


### [65] [Adaptive Spatio-Temporal Graphs with Self-Supervised Pretraining for Multi-Horizon Weather Forecasting](https://arxiv.org/abs/2511.00049)
*Yao Liu*

Main category: cs.LG

TL;DR: 提出了一种基于自监督学习的多变量天气预测框架，通过图神经网络和时空适应机制提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 由于大气系统固有的时空复杂性，准确稳健的天气预报仍然是一个基本挑战。

Method: 整合图神经网络进行空间推理，采用自监督预训练方案进行表示学习，并利用时空适应机制增强不同预测时长的泛化能力。

Result: 在ERA5和MERRA-2再分析数据集上的广泛实验表明，该方法相比传统数值天气预报模型和近期深度学习方法具有更优性能。在北京和上海的定量评估和视觉分析证实了模型捕捉细粒度气象模式的能力。

Conclusion: 该框架为未来数据驱动的天气预报系统提供了一个可扩展且标签高效的解决方案。

Abstract: Accurate and robust weather forecasting remains a fundamental challenge due
to the inherent spatio-temporal complexity of atmospheric systems. In this
paper, we propose a novel self-supervised learning framework that leverages
spatio-temporal structures to improve multi-variable weather prediction. The
model integrates a graph neural network (GNN) for spatial reasoning, a
self-supervised pretraining scheme for representation learning, and a
spatio-temporal adaptation mechanism to enhance generalization across varying
forecasting horizons. Extensive experiments on both ERA5 and MERRA-2 reanalysis
datasets demonstrate that our approach achieves superior performance compared
to traditional numerical weather prediction (NWP) models and recent deep
learning methods. Quantitative evaluations and visual analyses in Beijing and
Shanghai confirm the model's capability to capture fine-grained meteorological
patterns. The proposed framework provides a scalable and label-efficient
solution for future data-driven weather forecasting systems.

</details>


### [66] [FLoRA: Fused forward-backward adapters for parameter efficient fine-tuning and reducing inference-time latencies of LLMs](https://arxiv.org/abs/2511.00050)
*Dhananjaya Gowda,Seoha Song,Junhyun Lee,Harshith Goka*

Main category: cs.LG

TL;DR: 提出了FLoRA方法，一种融合前向-后向适配器的参数高效微调技术，结合了LoRA和并行适配器的优势，在保持相似参数预算的同时显著提升了准确性和降低了延迟。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模日益增长，高效训练和微调变得至关重要。虽然参数高效微调方法已有广泛研究，但仍有大量自由度未被探索。

Method: FLoRA融合了流行的LoRA和并行适配器的思想，通过将前向和后向适配器融合到基础模型的现有投影层中来最小化延迟。

Result: 实验结果表明，在相似参数预算下，FLoRA在准确性和延迟方面都显著优于常用的LoRA方法。

Conclusion: FLoRA为LLM的下游任务微调提供了一种高效的参数高效微调方案，在性能和效率上都表现出色。

Abstract: As the large language models (LLMs) grow in size each day, efficient training
and fine-tuning has never been as important as nowadays. This resulted in the
great interest in parameter efficient fine-tuning (PEFT), and effective methods
including low-rank adapters (LoRA) has emerged. Although the various PEFT
methods have been studied extensively in the recent years, the greater part of
the subject remains unexplored with the huge degree of freedom. In this paper,
we propose FLoRA, a family of fused forward-backward adapters (FFBA) for
parameter-efficient fine-tuning of LLMs on downstream tasks. The FFBA combine
ideas from the popular LoRA and parallel adapters to improve the overall
fine-tuning accuracies. At the same time, latencies are minimized by fusing the
forward and backward adapters into existing projection layers of the base
model. Experimental results show that the proposed FFB adapters perform
significantly better than the popularly used LoRA in both accuracy and latency
for a similar parameter budget.

</details>


### [67] [Calibrating and Rotating: A Unified Framework for Weight Conditioning in PEFT](https://arxiv.org/abs/2511.00051)
*Da Chang,Peng Xue,Yu Li,Yongxiang Liu,Pengxiang Xu,Shixun Zhang*

Main category: cs.LG

TL;DR: 本文分析了DoRA方法的成功机制，发现其通过增加权重更新矩阵的奇异值熵来提升性能。作者将DoRA重新表述为更高效的矩阵形式，提出了统一的PEFT框架，并引入了Pre-Diag和SORA两种新方法，在性能和效率上均优于LoRA和DoRA。


<details>
  <summary>Details</summary>
Motivation: DoRA方法虽然性能优越但机制不明确且计算开销大，需要揭示其成功原理并开发更高效的参数高效微调方法。

Method: 1) 分析DoRA机制，发现其通过增加奇异值熵实现更均匀的更新分布；2) 将DoRA重新表述为可学习的权重条件化方法；3) 提出统一PEFT框架，探索架构放置和变换类型两个维度；4) 引入Pre-Diag和SORA两种新方法。

Result: 在自然语言理解和生成任务上的广泛实验表明，提出的方法在性能和效率上都优于LoRA和DoRA。

Conclusion: 通过揭示DoRA的机制并建立统一框架，成功开发出更高效且性能优越的PEFT方法，为参数高效微调提供了新的设计思路。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods are crucial for adapting large
pre-trained models. Among these, LoRA is considered a foundational approach.
Building on this, the influential DoRA method enhances performance by
decomposing weight updates into magnitude and direction. However, its
underlying mechanism remains unclear, and it introduces significant
computational overhead. In this work, we first identify that DoRA's success
stems from its capacity to increase the singular value entropy of the weight
update matrix, which promotes a more uniform update distribution akin to full
fine-tuning. We then reformulate DoRA into a mathematically equivalent and more
efficient matrix form, revealing it as a learnable weight conditioning method.
Based on this insight, we propose a unified framework for designing advanced
PEFT methods by exploring two orthogonal dimensions: the architectural
placement and the transformation type of the conditioning matrix. Within this
framework, we introduce two novel methods: (1) \textbf{Pre-Diag}, which applies
a diagonal conditioning matrix before the LoRA update to efficiently calibrate
the pre-trained weights, thereby enhancing performance while reducing training
time; and (2) \textbf{S}kewed \textbf{O}rthogonal \textbf{R}otation
\textbf{A}daptation (\textbf{SORA}), which employs a parameter-efficient
orthogonal rotation to perform a more powerful, norm-preserving transformation
of the feature space. Extensive experiments on natural language understanding
and generation tasks demonstrate that our proposed methods achieve superior
performance and efficiency compared to both LoRA and DoRA. The code is
available at https://github.com/MaeChd/SORA.

</details>


### [68] [Feature-Guided Analysis of Neural Networks: A Replication Study](https://arxiv.org/abs/2511.00052)
*Federico Formica,Stefano Gregis,Aurora Francesca Zanenga,Andrea Rota,Mark Lawford,Claudio Menghi*

Main category: cs.LG

TL;DR: 评估特征引导分析（FGA）在MNIST和LSC数据集基准上的适用性，结果显示FGA在精度上优于文献结果，网络架构和特征选择对召回率有显著影响但对精度影响可忽略


<details>
  <summary>Details</summary>
Motivation: 理解神经网络决策原因对其在安全关键应用中使用至关重要，现有特征引导方法需要更多工业环境下的实证证据

Method: 在MNIST和LSC数据集基准上评估FGA的有效性，分析神经网络架构、训练和特征选择对FGA效果的影响

Result: FGA在基准测试中的精度高于文献结果，网络架构和特征选择对FGA召回率有显著影响但对精度影响很小

Conclusion: FGA在解释神经网络行为方面具有实用性，但需要仔细选择网络架构和特征以获得更好的召回率

Abstract: Understanding why neural networks make certain decisions is pivotal for their
use in safety-critical applications. Feature-Guided Analysis (FGA) extracts
slices of neural networks relevant to their tasks. Existing feature-guided
approaches typically monitor the activation of the neural network neurons to
extract the relevant rules. Preliminary results are encouraging and demonstrate
the feasibility of this solution by assessing the precision and recall of
Feature-Guided Analysis on two pilot case studies. However, the applicability
in industrial contexts needs additional empirical evidence.
  To mitigate this need, this paper assesses the applicability of FGA on a
benchmark made by the MNIST and LSC datasets. We assessed the effectiveness of
FGA in computing rules that explain the behavior of the neural network. Our
results show that FGA has a higher precision on our benchmark than the results
from the literature. We also evaluated how the selection of the neural network
architecture, training, and feature selection affect the effectiveness of FGA.
Our results show that the selection significantly affects the recall of FGA,
while it has a negligible impact on its precision.

</details>


### [69] [Quadratic Direct Forecast for Training Multi-Step Time-Series Forecast Models](https://arxiv.org/abs/2511.00053)
*Hao Wang,Licheng Pan,Yuan Lu,Zhichao Chen,Tianqiao Liu,Shuting He,Zhixuan Chu,Qingsong Wen,Haoxuan Li,Zhouchen Lin*

Main category: cs.LG

TL;DR: 提出了一种新的二次形式加权训练目标，通过考虑标签自相关效应和设置异质任务权重，解决了现有时间序列预测训练目标的问题。


<details>
  <summary>Details</summary>
Motivation: 现有训练目标（如均方误差）将每个未来步骤视为独立、等权重的任务，这导致两个问题：(1) 忽略未来步骤间的标签自相关效应，造成训练目标偏差；(2) 无法为不同预测步骤设置异质任务权重，限制了预测性能。

Method: 提出二次直接预测(QDF)学习算法，使用自适应更新的二次形式加权矩阵进行训练。加权矩阵的非对角线元素考虑标签自相关效应，非均匀对角线匹配不同预测步骤的最优权重。

Result: 实验表明QDF有效提升了各种预测模型的性能，达到了最先进的结果。

Conclusion: 所提出的二次形式加权训练目标能够同时解决标签自相关效应和异质任务权重问题，显著提高时间序列预测性能。

Abstract: The design of training objective is central to training time-series
forecasting models. Existing training objectives such as mean squared error
mostly treat each future step as an independent, equally weighted task, which
we found leading to the following two issues: (1) overlook the label
autocorrelation effect among future steps, leading to biased training
objective; (2) fail to set heterogeneous task weights for different forecasting
tasks corresponding to varying future steps, limiting the forecasting
performance. To fill this gap, we propose a novel quadratic-form weighted
training objective, addressing both of the issues simultaneously. Specifically,
the off-diagonal elements of the weighting matrix account for the label
autocorrelation effect, whereas the non-uniform diagonals are expected to match
the most preferable weights of the forecasting tasks with varying future steps.
To achieve this, we propose a Quadratic Direct Forecast (QDF) learning
algorithm, which trains the forecast model using the adaptively updated
quadratic-form weighting matrix. Experiments show that our QDF effectively
improves performance of various forecast models, achieving state-of-the-art
results. Code is available at https://anonymous.4open.science/r/QDF-8937.

</details>


### [70] [SpatialTraceGen: High-Fidelity Traces for Efficient VLM Spatial Reasoning Distillation](https://arxiv.org/abs/2511.00054)
*Gio Huh,Dhruv Sheth,Rayhan Zirvi,Frank Xiao*

Main category: cs.LG

TL;DR: 提出了SpatialTraceGen框架，通过蒸馏大型教师模型的推理过程来生成高质量的多步骤、多工具推理轨迹数据集，解决视觉语言模型在复杂空间推理任务中缺乏高质量训练数据的问题。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在复杂空间推理方面表现不佳，需要问题分解和策略性工具使用。微调小型可部署模型是高效路径，但缺乏高质量的分步推理数据成为主要瓶颈。

Method: 开发SpatialTraceGen框架，从大型教师模型蒸馏推理过程生成多跳、多工具推理轨迹。关键创新是自动化验证器，可扩展地确保每个推理步骤的保真度，替代昂贵的人工标注。

Result: 在CLEVR-Humans基准测试中，验证器引导的过程使轨迹平均质量得分提高17%，质量方差降低超过40%。

Conclusion: SpatialTraceGen提供了专家轨迹数据集，为有效微调和样本高效的离线强化学习提供了结构化的分步工具使用示例。

Abstract: While Vision-Language Models (VLMs) excel in many areas, they struggle with
complex spatial reasoning, which requires problem decomposition and strategic
tool use. Fine-tuning smaller, more deployable models offers an efficient path
to strong performance, but this is hampered by a major bottleneck: the absence
of high-quality, step-by-step reasoning data. To address this data-efficiency
gap, we introduce SpatialTraceGen, a framework to distill the reasoning
processes of a large teacher model into a high-quality dataset of multi-hop,
multi-tool reasoning traces. A key innovation is our automated Verifier, which
scalably ensures the fidelity of each reasoning step, providing a
cost-effective alternative to manual human annotation. On the CLEVR-Humans
benchmark, this verifier-guided process improves the average quality score of
traces by 17\% while reducing quality variance by over 40\%. SpatialTraceGen
delivers a dataset of expert traces, providing the structured, step-by-step
examples of tool use necessary for effective fine-tuning and sample-efficient
offline reinforcement learning.

</details>


### [71] [Exploring Federated Learning for Thermal Urban Feature Segmentation -- A Comparison of Centralized and Decentralized Approaches](https://arxiv.org/abs/2511.00055)
*Leonhard Duda,Khadijeh Alibabaei,Elena Vollmer,Leon Klug,Valentin Kozlov,Lisana Berberi,Mishal Benz,Rebekka Volk,Juan Pedro Gutiérrez Hermosillo Muriedas,Markus Götz,Judith Sáínz-Pardo Díaz,Álvaro López García,Frank Schultmann,Achim Streit*

Main category: cs.LG

TL;DR: 本文研究了联邦学习在无人机热成像分割任务中的实际应用效果，比较了多种FL方法与集中式学习的性能差异。


<details>
  <summary>Details</summary>
Motivation: 由于隐私和技术限制，分布式数据无法集中存储和共享，联邦学习能够绕过传统集中式机器学习的限制，让参与者在本地训练模型而无需共享数据。

Method: 在真实部署场景中评估联邦学习算法，使用来自两个德国城市的无人机热成像数据，比较多种FL方法与集中式学习基线，评估模型准确性、训练时间、通信开销和能耗等指标。

Result: 研究发现联邦学习在无人机热成像分割任务中具有实际应用价值，但面临数据非独立同分布和特征特性差异的挑战。

Conclusion: 这项工作为理解联邦学习方法在无人机成像分割任务中的实际应用和局限性提供了有价值的参考。

Abstract: Federated Learning (FL) is an approach for training a shared Machine Learning
(ML) model with distributed training data and multiple participants. FL allows
bypassing limitations of the traditional Centralized Machine Learning CL if
data cannot be shared or stored centrally due to privacy or technical
restrictions -- the participants train the model locally with their training
data and do not need to share it among the other participants. This paper
investigates the practical implementation and effectiveness of FL in a
real-world scenario, specifically focusing on unmanned aerial vehicle
(UAV)-based thermal images for common thermal feature detection in urban
environments. The distributed nature of the data arises naturally and makes it
suitable for FL applications, as images captured in two German cities are
available. This application presents unique challenges due to non-identical
distribution and feature characteristics of data captured at both locations.
The study makes several key contributions by evaluating FL algorithms in real
deployment scenarios rather than simulation. We compare several FL approaches
with a centralized learning baseline across key performance metrics such as
model accuracy, training time, communication overhead, and energy usage. This
paper also explores various FL workflows, comparing client-controlled workflows
and server-controlled workflows. The findings of this work serve as a valuable
reference for understanding the practical application and limitations of the FL
methods in segmentation tasks in UAV-based imaging.

</details>


### [72] [MISA: Memory-Efficient LLMs Optimization with Module-wise Importance Sampling](https://arxiv.org/abs/2511.00056)
*Yuxi Liu,Renjia Deng,Yutong He,Xue Wang,Tao Yao,Kun Yuan*

Main category: cs.LG

TL;DR: 提出了模块重要性采样方法MISA，通过将每个层划分为更小的模块并分配重要性分数，使用加权随机采样机制激活模块，相比层间采样能有效减少梯度方差，在内存效率和性能上优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型预训练和微调的内存需求巨大，需要内存高效的优化算法。现有的层间优化方法虽然有效，但忽略了层内模块的重要性差异，且内存节省有限。

Method: 将每个层划分为更小的模块，为每个模块分配重要性分数，使用加权随机采样机制选择激活的模块，在非凸和随机条件下建立了收敛率理论保证。

Result: 实验验证了MISA在各种学习任务上的有效性，提供了详细的内存分析，展示了其相对于现有基线方法的优越性。

Conclusion: MISA通过模块级的重要性采样，在保证收敛性的同时显著提高了内存效率，为大型语言模型的高效优化提供了新的解决方案。

Abstract: The substantial memory demands of pre-training and fine-tuning large language
models (LLMs) require memory-efficient optimization algorithms. One promising
approach is layer-wise optimization, which treats each transformer block as a
single layer and optimizes it sequentially, while freezing the other layers to
save optimizer states and activations. Although effective, these methods ignore
the varying importance of the modules within each layer, leading to suboptimal
performance. Moreover, layer-wise sampling provides only limited memory
savings, as at least one full layer must remain active during optimization. To
overcome these limitations, we propose Module-wise Importance SAmpling (MISA),
a novel method that divides each layer into smaller modules and assigns
importance scores to each module. MISA uses a weighted random sampling
mechanism to activate modules, provably reducing gradient variance compared to
layer-wise sampling. Additionally, we establish an \(\mathcal{O}(1/\sqrt{K})\)
convergence rate under non-convex and stochastic conditions, where $K$ is the
total number of block updates, and provide a detailed memory analysis
showcasing MISA's superiority over existing baseline methods. Experiments on
diverse learning tasks validate the effectiveness of MISA. Source code is
available at https://github.com/pkumelon/MISA.

</details>


### [73] [Automatically Finding Rule-Based Neurons in OthelloGPT](https://arxiv.org/abs/2511.00059)
*Aditya Singh,Zihang Wen,Srujananjali Medicherla,Adam Karvonen,Can Rager*

Main category: cs.LG

TL;DR: 本文提出了一种基于决策树的自动化方法，用于识别和解释OthelloGPT模型中编码游戏规则的MLP神经元，发现约一半的神经元可以用紧凑的规则描述，并通过干预实验验证了这些模式的重要性。


<details>
  <summary>Details</summary>
Motivation: OthelloGPT模型为可解释性研究提供了理想测试平台——既足够复杂展现丰富计算模式，又基于规则游戏逻辑便于逆向工程。

Method: 使用回归决策树将棋盘状态映射到神经元激活，提取高激活决策路径并转换为人类可读的逻辑形式，同时通过针对性干预验证因果关系。

Result: 在第五层2048个神经元中，913个（约一半）可以用紧凑的规则决策树准确描述（R² > 0.7），其余可能参与分布式或非规则计算。干预实验显示特定模式对应的神经元被消融后，模型预测合法移动的能力下降5-10倍。

Conclusion: 该方法成功识别了模型中的规则编码神经元，验证了其因果相关性，并提供了工具支持未来研究，表明神经网络确实学习到了可解释的游戏规则表示。

Abstract: OthelloGPT, a transformer trained to predict valid moves in Othello, provides
an ideal testbed for interpretability research. The model is complex enough to
exhibit rich computational patterns, yet grounded in rule-based game logic that
enables meaningful reverse-engineering. We present an automated approach based
on decision trees to identify and interpret MLP neurons that encode rule-based
game logic. Our method trains regression decision trees to map board states to
neuron activations, then extracts decision paths where neurons are highly
active to convert them into human-readable logical forms. These descriptions
reveal highly interpretable patterns; for instance, neurons that specifically
detect when diagonal moves become legal. Our findings suggest that roughly half
of the neurons in layer 5 can be accurately described by compact, rule-based
decision trees ($R^2 > 0.7$ for 913 of 2,048 neurons), while the remainder
likely participate in more distributed or non-rule-based computations. We
verify the causal relevance of patterns identified by our decision trees
through targeted interventions. For a specific square, for specific game
patterns, we ablate neurons corresponding to those patterns and find an
approximately 5-10 fold stronger degradation in the model's ability to predict
legal moves along those patterns compared to control patterns. To facilitate
future work, we provide a Python tool that maps rule-based game behaviors to
their implementing neurons, serving as a resource for researchers to test
whether their interpretability methods recover meaningful computational
structures.

</details>


### [74] [EVINGCA: Adaptive Graph Clustering with Evolving Neighborhood Statistics](https://arxiv.org/abs/2511.00064)
*Randolph Wiredu-Aidoo*

Main category: cs.LG

TL;DR: EVINGCA是一种基于密度-方差的聚类算法，将聚类形成视为最近邻图上的自适应演化过程，通过局部统计反馈替代固定密度阈值，具有对数线性复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有聚类算法存在限制性假设：K-Means和高斯混合假设凸的类高斯簇，DBSCAN和HDBSCAN能捕获非凸性但对参数高度敏感。

Method: 在最近邻图上通过广度优先搜索扩展根图，使用持续更新的局部距离和形状统计作为指导，用局部统计反馈替代固定密度阈值。

Result: 通过空间索引，EVINGCA在平均情况下具有对数线性复杂度，在各种合成、真实世界、低维和高维数据集上展现出与基线方法竞争的性能。

Conclusion: EVINGCA通过将聚类视为自适应演化过程，提供了一种更灵活、对参数不敏感的聚类方法，在多个数据集上表现优异。

Abstract: Clustering algorithms often rely on restrictive assumptions: K-Means and
Gaussian Mixtures presuppose convex, Gaussian-like clusters, while DBSCAN and
HDBSCAN capture non-convexity but can be highly sensitive. I introduce EVINGCA
(Evolving Variance-Informed Nonparametric Graph Construction Algorithm), a
density-variance based clustering algorithm that treats cluster formation as an
adaptive, evolving process on a nearest-neighbor graph. EVINGCA expands rooted
graphs via breadth-first search, guided by continuously updated local distance
and shape statistics, replacing fixed density thresholds with local statistical
feedback. With spatial indexing, EVINGCA features log-linear complexity in the
average case and exhibits competitive performance against baselines across a
variety of synthetic, real-world, low-d, and high-d datasets.

</details>


### [75] [Aligning Brain Signals with Multimodal Speech and Vision Embeddings](https://arxiv.org/abs/2511.00065)
*Kateryna Shapovalenko,Quentin Auster*

Main category: cs.LG

TL;DR: 该研究探索了预训练模型的不同层如何反映大脑对语言的分层处理过程，通过比较wav2vec2和CLIP模型的嵌入与脑电图信号的对应关系。


<details>
  <summary>Details</summary>
Motivation: 受大脑从原始声学到丰富多模态关联的分层意义构建过程启发，研究旨在确定哪些预训练模型层最能反映大脑的这种分层处理机制。

Method: 使用自然语音感知期间记录的EEG数据，通过岭回归和对比解码评估wav2vec2和CLIP模型嵌入与大脑活动的对齐程度，测试了三种策略：单层、渐进拼接和渐进求和。

Result: 研究发现结合多模态、分层感知的表征可能更接近解码大脑如何理解语言——不仅是声音，更是体验。

Conclusion: 多模态、分层感知的表征组合为理解大脑语言处理机制提供了新视角，超越了单纯的声音处理层面。

Abstract: When we hear the word "house", we don't just process sound, we imagine walls,
doors, memories. The brain builds meaning through layers, moving from raw
acoustics to rich, multimodal associations. Inspired by this, we build on
recent work from Meta that aligned EEG signals with averaged wav2vec2 speech
embeddings, and ask a deeper question: which layers of pre-trained models best
reflect this layered processing in the brain? We compare embeddings from two
models: wav2vec2, which encodes sound into language, and CLIP, which maps words
to images. Using EEG recorded during natural speech perception, we evaluate how
these embeddings align with brain activity using ridge regression and
contrastive decoding. We test three strategies: individual layers, progressive
concatenation, and progressive summation. The findings suggest that combining
multimodal, layer-aware representations may bring us closer to decoding how the
brain understands language, not just as sound, but as experience.

</details>


### [76] [Token-Regulated Group Relative Policy Optimization for Stable Reinforcement Learning in Large Language Models](https://arxiv.org/abs/2511.00066)
*Tue Le,Nghi D. Q. Bui,Linh Ngo Van,Trung Le*

Main category: cs.LG

TL;DR: TR-GRPO是一种改进的强化学习方法，通过基于token概率的权重调节来解决GRPO中低概率token梯度过度放大的问题，从而提升LLM推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO方法存在关键问题：低概率token由于其固有的梯度幅度较大，会不成比例地主导梯度更新，导致训练不稳定并抑制高概率token的学习贡献。

Method: 提出Token-Regulated Group Relative Policy Optimization (TR-GRPO)，通过分配与模型预测概率正相关的token级权重，降低低概率token的权重并强调高概率token，从而缓解梯度过度放大同时保留信息学习信号。

Result: 广泛实验表明TR-GRPO在RLVR任务（包括逻辑、数学和智能体推理）中持续优于GRPO。

Conclusion: 研究强调了在RL训练期间调节token贡献的重要性，并将TR-GRPO确立为增强LLM推理的稳健框架。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has emerged as a
powerful approach for strengthening the reasoning capabilities of large
language models (LLMs). Among existing algorithms, Group Relative Policy
Optimization (GRPO) has demonstrated strong performance, yet it suffers from a
critical issue: low-probability tokens disproportionately dominate gradient
updates due to their inherently large gradient magnitudes. This imbalance leads
to unstable training and suppresses the contribution of high-probability tokens
that are more reliable for learning. In this work, we introduce Token-Regulated
Group Relative Policy Optimization (TR-GRPO), a simple yet effective extension
of GRPO that assigns token-level weights positively correlated with the model's
predicted probability. By downweighting low-probability tokens and emphasizing
high-probability ones, TR-GRPO mitigates gradient over-amplification while
preserving informative learning signals. Extensive experiments demonstrate that
TR-GRPO consistently outperforms GRPO across RLVR tasks, including logic, math,
and agentic reasoning, highlighting the importance of regulating token
contributions during RL training and establishing TR-GRPO as a robust framework
for enhancing LLM reasoning.

</details>


### [77] [Latent Domain Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2511.00067)
*Zhixing Li,Arsham Gholamzadeh Khoee,Yinan Yu*

Main category: cs.LG

TL;DR: 该论文提出了一种无需显式域标签的领域泛化方法，通过自动发现训练数据中的潜在域，并将未见目标域表示为这些潜在域的组合，从而提升视觉语言模型在域偏移下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有领域泛化方法大多依赖域标签，但这些标签在实际应用中可能不可得或模糊不清。本文研究无需显式域标签的DG设置，旨在提升模型在真实场景中的部署能力。

Method: 在图像特征上执行潜在域聚类，并根据输入图像与每个潜在域的相似度融合域特定的文本特征，实现跨域知识的自适应迁移。

Result: 在四个基准测试上的实验表明，该方法相比基于VLM的基线模型取得了稳定的性能提升。

Conclusion: 该方法为在域偏移下提高模型鲁棒性提供了新的思路，证明了通过自动发现潜在域来实现领域泛化的有效性。

Abstract: The objective of domain generalization (DG) is to enable models to be robust
against domain shift. DG is crucial for deploying vision-language models (VLMs)
in real-world applications, yet most existing methods rely on domain labels
that may not be available and often ambiguous. We instead study the DG setting
where models must generalize well without access to explicit domain labels. Our
key idea is to represent an unseen target domain as a combination of latent
domains automatically discovered from training data, enabling the model to
adaptively transfer knowledge across domains. To realize this, we perform
latent domain clustering on image features and fuse domain-specific text
features based on the similarity between the input image and each latent
domain. Experiments on four benchmarks show that this strategy yields
consistent gains over VLM-based baselines and provides new insights into
improving robustness under domain shift.

</details>


### [78] [Benchmarking Generative AI Against Bayesian Optimization for Constrained Multi-Objective Inverse Design](https://arxiv.org/abs/2511.00070)
*Muhammad Bilal Awan,Abdul Razzaq,Abdul Shahid*

Main category: cs.LG

TL;DR: 该论文研究大型语言模型作为生成优化器在约束多目标回归任务中的表现，特别是在逆向设计领域。通过与贝叶斯优化框架的对比，发现微调的LLM在计算速度方面表现出潜力，但专用BO框架在收敛保证方面仍领先。


<details>
  <summary>Details</summary>
Motivation: 研究LLM在约束、连续、高维数值空间任务中的效用，这些任务并非LLM的原始设计目标，但在材料信息学中的逆向设计问题具有重要意义。

Method: 进行了贝叶斯优化框架与微调LLM/BERT模型的比较研究。BO方面测试了BoTorch Ax和qEHVI，生成方法采用参数高效微调，将问题构建为带自定义输出头的回归问题。

Result: BoTorch qEHVI实现了完美收敛(GD=0.0)，而表现最佳的LLM(WizardMath-7B)达到GD=1.21，显著优于传统BoTorch Ax基线(GD=15.03)。

Conclusion: 专用BO框架在保证收敛方面仍是性能领先者，但微调LLM被验证为有前景、计算快速的替代方案，为AI驱动优化领域提供了重要比较指标。

Abstract: This paper investigates the performance of Large Language Models (LLMs) as
generative optimizers for solving constrained multi-objective regression tasks,
specifically within the challenging domain of inverse design
(property-to-structure mapping). This problem, critical to materials
informatics, demands finding complex, feasible input vectors that lie on the
Pareto optimal front. While LLMs have demonstrated universal effectiveness
across generative and reasoning tasks, their utility in constrained,
continuous, high-dimensional numerical spaces tasks they weren't explicitly
architected for remains an open research question. We conducted a rigorous
comparative study between established Bayesian Optimization (BO) frameworks and
a suite of fine-tuned LLMs and BERT models. For BO, we benchmarked the
foundational BoTorch Ax implementation against the state-of-the-art q-Expected
Hypervolume Improvement (qEHVI, BoTorchM). The generative approach involved
fine-tuning models via Parameter-Efficient Fine-Tuning (PEFT), framing the
challenge as a regression problem with a custom output head. Our results show
that BoTorch qEHVI achieved perfect convergence (GD=0.0), setting the
performance ceiling. Crucially, the best-performing LLM (WizardMath-7B)
achieved a Generational Distance (GD) of 1.21, significantly outperforming the
traditional BoTorch Ax baseline (GD=15.03). We conclude that specialized BO
frameworks remain the performance leader for guaranteed convergence, but
fine-tuned LLMs are validated as a promising, computationally fast alternative,
contributing essential comparative metrics to the field of AI-driven
optimization. The findings have direct industrial applications in optimizing
formulation design for resins, polymers, and paints, where multi-objective
trade-offs between mechanical, rheological, and chemical properties are
critical to innovation and production efficiency.

</details>


### [79] [Wavelet-Based Feature Extraction and Unsupervised Clustering for Parity Detection: A Feature Engineering Perspective](https://arxiv.org/abs/2511.00071)
*Ertugrul Mutlu*

Main category: cs.LG

TL;DR: 使用小波变换和k-means聚类检测数字奇偶性的过度工程方法，无需监督标签达到69.67%准确率


<details>
  <summary>Details</summary>
Motivation: 探索经典信号处理技术如何揭示离散符号域中的潜在结构，桥接符号推理和基于特征的学习

Method: 将整数转换为小波域表示，提取多尺度统计特征，使用k-means算法进行无监督聚类

Result: 在特征空间中揭示了奇偶数之间的结构性差异，分类准确率达到69.67%

Conclusion: 信号处理技术可以应用于离散符号域，特征工程和聚类可重新用于非常规机器学习问题

Abstract: This paper explores a deliberately over-engineered approach to the classical
problem of parity detection -- determining whether a number is odd or even --
by combining wavelet-based feature extraction with unsupervised clustering.
Instead of relying on modular arithmetic, integers are transformed into
wavelet-domain representations, from which multi-scale statistical features are
extracted and clustered using the k-means algorithm. The resulting feature
space reveals meaningful structural differences between odd and even numbers,
achieving a classification accuracy of approximately 69.67% without any label
supervision. These results suggest that classical signal-processing techniques,
originally designed for continuous data, can uncover latent structure even in
purely discrete symbolic domains. Beyond parity detection, the study provides
an illustrative perspective on how feature engineering and clustering may be
repurposed for unconventional machine learning problems, potentially bridging
symbolic reasoning and feature-based learning.

</details>


### [80] [Bridging Vision, Language, and Mathematics: Pictographic Character Reconstruction with Bézier Curves](https://arxiv.org/abs/2511.00076)
*Zihao Wan,Pau Tong Lin Xu,Fuwen Luo,Ziyue Wang,Peng Li,Yang Liu*

Main category: cs.LG

TL;DR: 该论文研究了视觉语言模型在理解视觉几何结构方面的能力，通过将象形文字识别构建为程序合成任务，训练VLM将栅格图像反编译为由贝塞尔曲线组成的几何程序。


<details>
  <summary>Details</summary>
Motivation: 探索视觉语言模型解释视觉信息底层几何结构的能力，象形文字结合了视觉形式和符号结构，为此能力提供了理想的测试案例。

Method: 将视觉识别挑战构建为程序合成任务，训练VLM将栅格图像反编译为由贝塞尔曲线组成的几何程序，模型作为"视觉反编译器"工作。

Result: 模型表现优于强大的零样本基线（包括GPT-4o），最显著的发现是仅在现代汉字上训练的模型能够在零样本情况下重建古代甲骨文。

Conclusion: 这种泛化能力提供了强有力的证据，表明模型获得了抽象且可转移的几何语法，超越了像素级模式识别，达到了更结构化的视觉理解形式。

Abstract: While Vision-language Models (VLMs) have demonstrated strong semantic
capabilities, their ability to interpret the underlying geometric structure of
visual information is less explored. Pictographic characters, which combine
visual form with symbolic structure, provide an ideal test case for this
capability. We formulate this visual recognition challenge in the mathematical
domain, where each character is represented by an executable program of
geometric primitives. This is framed as a program synthesis task, training a
VLM to decompile raster images into programs composed of B\'ezier curves. Our
model, acting as a "visual decompiler", demonstrates performance superior to
strong zero-shot baselines, including GPT-4o. The most significant finding is
that when trained solely on modern Chinese characters, the model is able to
reconstruct ancient Oracle Bone Script in a zero-shot context. This
generalization provides strong evidence that the model acquires an abstract and
transferable geometric grammar, moving beyond pixel-level pattern recognition
to a more structured form of visual understanding.

</details>


### [81] [flowengineR: A Modular and Extensible Framework for Fair and Reproducible Workflow Design in R](https://arxiv.org/abs/2511.00079)
*Maximilian Willer,Peter Ruckdeschel*

Main category: cs.LG

TL;DR: flowengineR是一个R包，为构建可复现的通用机器学习算法工作流提供模块化和可扩展框架，特别关注算法公平性领域。


<details>
  <summary>Details</summary>
Motivation: 算法公平性领域快速发展，新指标、缓解策略和机器学习方法不断涌现，现有工具包要么专注于单一干预，要么将可复现性和可扩展性作为次要考虑而非核心设计原则。

Method: 引入统一的标准化引擎架构，包括数据分割、执行、预处理、训练、处理中、后处理、评估和报告等模块。每个引擎封装一个方法任务，通过轻量级接口通信，确保工作流透明、可审计且易于扩展。

Result: 通过将公平性方法构建为可互换的引擎，研究人员可以在建模流程中集成、比较和评估干预措施。该架构还可推广到可解释性、鲁棒性和合规性指标，无需核心修改。

Conclusion: 虽然受公平性研究启发，但flowengineR最终为任何需要可复现性、透明度和可扩展性的工作流场景提供通用基础设施。

Abstract: flowengineR is an R package designed to provide a modular and extensible
framework for building reproducible algorithmic workflows for general-purpose
machine learning pipelines. It is motivated by the rapidly evolving field of
algorithmic fairness where new metrics, mitigation strategies, and machine
learning methods continuously emerge. A central challenge in fairness, but also
far beyond, is that existing toolkits either focus narrowly on single
interventions or treat reproducibility and extensibility as secondary
considerations rather than core design principles. flowengineR addresses this
by introducing a unified architecture of standardized engines for data
splitting, execution, preprocessing, training, inprocessing, postprocessing,
evaluation, and reporting. Each engine encapsulates one methodological task yet
communicates via a lightweight interface, ensuring workflows remain
transparent, auditable, and easily extensible. Although implemented in R,
flowengineR builds on ideas from workflow languages (CWL, YAWL), graph-oriented
visual programming languages (KNIME), and R frameworks (BatchJobs, batchtools).
Its emphasis, however, is less on orchestrating engines for resilient parallel
execution but rather on the straightforward setup and management of distinct
engines as data structures. This orthogonalization enables distributed
responsibilities, independent development, and streamlined integration. In
fairness context, by structuring fairness methods as interchangeable engines,
flowengineR lets researchers integrate, compare, and evaluate interventions
across the modeling pipeline. At the same time, the architecture generalizes to
explainability, robustness, and compliance metrics without core modifications.
While motivated by fairness, it ultimately provides a general infrastructure
for any workflow context where reproducibility, transparency, and extensibility
are essential.

</details>


### [82] [Fixed-point graph convolutional networks against adversarial attacks](https://arxiv.org/abs/2511.00083)
*Shakib Khan,A. Ben Hamza,Amr Youssef*

Main category: cs.LG

TL;DR: 提出Fix-GCN模型，通过固定点迭代图卷积网络实现对抗攻击鲁棒性，利用谱调制滤波器选择性衰减高频成分，保护图结构信息。


<details>
  <summary>Details</summary>
Motivation: 对抗攻击对图神经网络构成严重威胁，特别是在图结构和节点特征易受操纵的任务中，需要开发鲁棒的防御机制。

Method: 引入通用谱调制滤波器，通过固定点迭代推导特征传播规则，提供灵活通带滤波方法，选择性衰减高频成分同时保留低频结构信息。

Result: 在多个基准图数据集上的实验表明，该模型能有效抵抗对抗攻击，展现出良好的鲁棒性。

Conclusion: Fix-GCN提供了一种灵活高效的框架，在保护基本图信息的同时减轻对抗操纵的影响，无需额外内存或计算复杂度。

Abstract: Adversarial attacks present a significant risk to the integrity and
performance of graph neural networks, particularly in tasks where graph
structure and node features are vulnerable to manipulation. In this paper, we
present a novel model, called fixed-point iterative graph convolutional network
(Fix-GCN), which achieves robustness against adversarial perturbations by
effectively capturing higher-order node neighborhood information in the graph
without additional memory or computational complexity. Specifically, we
introduce a versatile spectral modulation filter and derive the feature
propagation rule of our model using fixed-point iteration. Unlike traditional
defense mechanisms that rely on additional design elements to counteract
attacks, the proposed graph filter provides a flexible-pass filtering approach,
allowing it to selectively attenuate high-frequency components while preserving
low-frequency structural information in the graph signal. By iteratively
updating node representations, our model offers a flexible and efficient
framework for preserving essential graph information while mitigating the
impact of adversarial manipulation. We demonstrate the effectiveness of the
proposed model through extensive experiments on various benchmark graph
datasets, showcasing its resilience against adversarial attacks.

</details>


### [83] [Application of predictive machine learning in pen & paper RPG game design](https://arxiv.org/abs/2511.00084)
*Jolanta Śliwa*

Main category: cs.LG

TL;DR: 这篇论文研究了使用序数回归技术来自动预测纸笔RPG中怪物的挑战等级，以替代当前耗时的手动评估方法。


<details>
  <summary>Details</summary>
Motivation: 随着纸笔RPG市场的增长，出版商需要更高效的方法来设计新对手并评估其挑战等级。目前仅有的手动测试和专家评估方法虽然准确但耗时耗力。

Method: 构建了专门的等级估计数据集，开发了人类启发式模型作为基准，并设计了基于领域知识的专门评估程序来比较机器学习算法与传统方法。

Result: 论文对最先进的序数回归方法进行了概述和评估，并建立了比较机器学习和人类方法的基准框架。

Conclusion: 研究为纸笔RPG出版商提供了自动化的怪物等级预测解决方案，有望显著提高设计效率并降低资源消耗。

Abstract: In recent years, the pen and paper RPG market has experienced significant
growth. As a result, companies are increasingly exploring the integration of AI
technologies to enhance player experience and gain a competitive edge.
  One of the key challenges faced by publishers is designing new opponents and
estimating their challenge level. Currently, there are no automated methods for
determining a monster's level; the only approaches used are based on manual
testing and expert evaluation. Although these manual methods can provide
reasonably accurate estimates, they are time-consuming and resource-intensive.
  Level prediction can be approached using ordinal regression techniques. This
thesis presents an overview and evaluation of state-of-the-art methods for this
task. It also details the construction of a dedicated dataset for level
estimation. Furthermore, a human-inspired model was developed to serve as a
benchmark, allowing comparison between machine learning algorithms and the
approach typically employed by pen and paper RPG publishers. In addition, a
specialized evaluation procedure, grounded in domain knowledge, was designed to
assess model performance and facilitate meaningful comparisons.

</details>


### [84] [MaGNet: A Mamba Dual-Hypergraph Network for Stock Prediction via Temporal-Causal and Global Relational Learning](https://arxiv.org/abs/2511.00085)
*Peilin Tan,Chuanqi Shi,Dian Tu,Liang Xie*

Main category: cs.LG

TL;DR: 提出了MaGNet模型，一种基于Mamba双超图网络的股票预测方法，通过三个关键创新：MAGE块、2D时空注意力模块和双超图框架，有效捕捉时间依赖性和股票间动态关系，在六个主要股票指数上表现出优越的预测性能和投资回报。


<details>
  <summary>Details</summary>
Motivation: 股票趋势预测对交易策略和投资组合管理至关重要，但由于市场波动性、复杂的时间动态和多方面的股票间关系而具有挑战性。现有方法难以有效捕捉时间依赖性和动态股票间交互，往往忽略横截面市场影响。

Method: MaGNet模型包含三个核心组件：(1) MAGE块，使用双向Mamba和自适应门控机制进行上下文时间建模，集成稀疏专家混合层以适应不同市场条件；(2) 特征和股票维度的2D时空注意力模块，精确融合多变量特征和跨股票依赖关系；(3) 双超图框架，包括时间因果超图捕捉细粒度因果依赖，和全局概率超图建模市场范围模式。

Result: 在六个主要股票指数上的广泛实验表明，MaGNet在预测性能和投资回报方面均优于最先进的方法，并具有稳健的风险管理能力。

Conclusion: MaGNet通过创新的Mamba双超图网络架构，成功解决了股票预测中的时间依赖性和动态关系建模问题，为股票趋势预测提供了有效的解决方案。

Abstract: Stock trend prediction is crucial for profitable trading strategies and
portfolio management yet remains challenging due to market volatility, complex
temporal dynamics and multifaceted inter-stock relationships. Existing methods
struggle to effectively capture temporal dependencies and dynamic inter-stock
interactions, often neglecting cross-sectional market influences, relying on
static correlations, employing uniform treatments of nodes and edges, and
conflating diverse relationships. This work introduces MaGNet, a novel Mamba
dual-hyperGraph Network for stock prediction, integrating three key
innovations: (1) a MAGE block, which leverages bidirectional Mamba with
adaptive gating mechanisms for contextual temporal modeling and integrates a
sparse Mixture-of-Experts layer to enable dynamic adaptation to diverse market
conditions, alongside multi-head attention for capturing global dependencies;
(2) Feature-wise and Stock-wise 2D Spatiotemporal Attention modules enable
precise fusion of multivariate features and cross-stock dependencies,
effectively enhancing informativeness while preserving intrinsic data
structures, bridging temporal modeling with relational reasoning; and (3) a
dual hypergraph framework consisting of the Temporal-Causal Hypergraph (TCH)
that captures fine-grained causal dependencies with temporal constraints, and
Global Probabilistic Hypergraph (GPH) that models market-wide patterns through
soft hyperedge assignments and Jensen-Shannon Divergence weighting mechanism,
jointly disentangling localized temporal influences from instantaneous global
structures for multi-scale relational learning. Extensive experiments on six
major stock indices demonstrate MaGNet outperforms state-of-the-art methods in
both superior predictive performance and exceptional investment returns with
robust risk management capabilities. Codes available at:
https://github.com/PeilinTime/MaGNet.

</details>


### [85] [Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph](https://arxiv.org/abs/2511.00086)
*Fali Wang,Jihai Chen,Shuhua Yang,Runxue Bao,Tianxiang Zhao,Zhiwei Zhang,Xianfeng Tang,Hui Liu,Qi He,Suhang Wang*

Main category: cs.LG

TL;DR: 本文提出Agent-REINFORCE框架，用于在固定计算预算下搜索测试时扩展(TTS)中的最优多LLM协作图和模型组合，解决了传统方法忽视任务特定架构需求的问题。


<details>
  <summary>Details</summary>
Motivation: 现有测试时扩展方法通常假设固定的协作架构和单一模型使用，忽略了不同任务需要不同最优架构和模型组合的事实。

Method: 将问题形式化为概率图优化，提出Agent-REINFORCE框架，通过LLM代理增强的REINFORCE流程，将采样-梯度-更新映射为采样-反馈-更新，使用文本反馈作为梯度来更新概率图。

Result: 实验表明Agent-REINFORCE在样本效率和搜索性能上优于传统和基于LLM的基线方法，能有效识别在准确性和推理延迟联合目标下的最优图。

Conclusion: Agent-REINFORCE框架成功解决了TTS中多LLM协作图搜索的挑战，为不同任务找到计算最优的模型组合和架构。

Abstract: Test-Time Scaling (TTS) improves large language models (LLMs) by allocating
additional computation during inference, typically through parallel,
sequential, or hybrid scaling. However, prior studies often assume fixed
collaboration architectures (e.g., topologies) and single-model usage,
overlooking that optimal architectures and model combinations can vary across
tasks. Therefore, we study the novel problem of searching for compute-optimal
model combinations and architectures in TTS under a fixed budget. We formalize
it as a multi-LLM collaboration graph, where nodes encode roles and LLM model
assignments, and edges capture information flow. This problem is challenging
because (i) the combinatorial search space is prohibitively large, and (ii)
task-specific requirements demand tailored designs. To address these, we
reformulate the problem as probabilistic graph optimization and, through pilot
experiments, derive three empirical insights into TTS collaboration graphs.
Guided by these insights, we propose Agent-REINFORCE, an LLM-agent-augmented
framework that mirrors the REINFORCE pipeline by mapping
sampling-gradient-update to sampling-feedback-update, where feedback serves as
a textual gradient to update the probabilistic graph and efficiently search for
optimal multi-LLM collaboration graphs. Experiments show that Agent-REINFORCE
outperforms both traditional and LLM-based baselines in sample efficiency and
search performance, and effectively identifies optimal graphs under joint
objectives of accuracy and inference latency.

</details>


### [86] [GraphKeeper: Graph Domain-Incremental Learning via Knowledge Disentanglement and Preservation](https://arxiv.org/abs/2511.00097)
*Zihao Guo,Qingyun Sun,Ziwei Zhang,Haonan Yuan,Huiping Zhuang,Xingcheng Fu,Jianxin Li*

Main category: cs.LG

TL;DR: 提出了GraphKeeper方法来解决图域增量学习中的灾难性遗忘问题，通过知识解缠和保持来应对嵌入漂移和决策边界偏差。


<details>
  <summary>Details</summary>
Motivation: 现有的图增量学习方法主要关注单域内的任务增量和类别增量场景，而图域增量学习（跨多个图域更新模型）在图形基础模型发展中变得至关重要，但在文献中尚未被探索。

Method: GraphKeeper方法包括：1）域特定参数高效微调及域内和域间解缠目标，防止跨增量图域的嵌入漂移和混淆；2）无偏差知识保持以维持稳定决策边界；3）对于不可观测域的图，执行域感知分布判别以获得精确嵌入。

Result: 大量实验表明，GraphKeeper取得了最先进的结果，相比第二名有6.5%~16.6%的提升，且遗忘可忽略不计。该方法可与各种代表性图形基础模型无缝集成。

Conclusion: GraphKeeper有效解决了图域增量学习中的关键挑战，展示了广泛的适用潜力，为图形基础模型在多域环境中的持续学习提供了可行方案。

Abstract: Graph incremental learning (GIL), which continuously updates graph models by
sequential knowledge acquisition, has garnered significant interest recently.
However, existing GIL approaches focus on task-incremental and
class-incremental scenarios within a single domain. Graph domain-incremental
learning (Domain-IL), aiming at updating models across multiple graph domains,
has become critical with the development of graph foundation models (GFMs), but
remains unexplored in the literature. In this paper, we propose Graph
Domain-Incremental Learning via Knowledge Dientanglement and Preservation
(GraphKeeper), to address catastrophic forgetting in Domain-IL scenario from
the perspectives of embedding shifts and decision boundary deviations.
Specifically, to prevent embedding shifts and confusion across incremental
graph domains, we first propose the domain-specific parameter-efficient
fine-tuning together with intra- and inter-domain disentanglement objectives.
Consequently, to maintain a stable decision boundary, we introduce
deviation-free knowledge preservation to continuously fit incremental domains.
Additionally, for graphs with unobservable domains, we perform domain-aware
distribution discrimination to obtain precise embeddings. Extensive experiments
demonstrate the proposed GraphKeeper achieves state-of-the-art results with
6.5%~16.6% improvement over the runner-up with negligible forgetting. Moreover,
we show GraphKeeper can be seamlessly integrated with various representative
GFMs, highlighting its broad applicative potential.

</details>


### [87] [A generative adversarial network optimization method for damage detection and digital twinning by deep AI fault learning: Z24 Bridge structural health monitoring benchmark validation](https://arxiv.org/abs/2511.00099)
*Marios Impraimakis,Evangelia Nektaria Palkanoglou*

Main category: cs.LG

TL;DR: 提出了一种基于条件标签生成对抗网络的无监督损伤检测和数字孪生方法，无需系统健康状态的先验信息，在Z24桥梁基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前基于AI的数字孪生方法在测量数据少、物理知识缺失或损伤状态未知时预测效果不佳，需要开发不依赖先验信息的无监督框架。

Method: 使用条件标签生成对抗网络，将不同损伤级别的测量数据作为输入，强制模型收敛到两个不同的损伤状态，通过比较收敛得分来识别不同损伤状态。

Result: 该方法能够准确捕捉健康测量数据中的损伤，为基于振动的系统级监测和可扩展基础设施韧性提供了强大工具。

Conclusion: 提出的无监督框架在损伤检测和数字孪生方面优于现有方法，特别适用于实际应用中系统健康状态未知的情况。

Abstract: The optimization-based damage detection and damage state digital twinning
capabilities are examined here of a novel conditional-labeled generative
adversarial network methodology. The framework outperforms current approaches
for fault anomaly detection as no prior information is required for the health
state of the system: a topic of high significance for real-world applications.
Specifically, current artificial intelligence-based digital twinning approaches
suffer from the uncertainty related to obtaining poor predictions when a low
number of measurements is available, physics knowledge is missing, or when the
damage state is unknown. To this end, an unsupervised framework is examined and
validated rigorously on the benchmark structural health monitoring measurements
of Z24 Bridge: a post-tensioned concrete highway bridge in Switzerland. In
implementing the approach, firstly, different same damage-level measurements
are used as inputs, while the model is forced to converge conditionally to two
different damage states. Secondly, the process is repeated for a different
group of measurements. Finally, the convergence scores are compared to identify
which one belongs to a different damage state. The process for both
healthy-to-healthy and damage-to-healthy input data creates, simultaneously,
measurements for digital twinning purposes at different damage states, capable
of pattern recognition and machine learning data generation. Further to this
process, a support vector machine classifier and a principal component analysis
procedure is developed to assess the generated and real measurements of each
damage category, serving as a secondary new dynamics learning indicator in
damage scenarios. Importantly, the approach is shown to capture accurately
damage over healthy measurements, providing a powerful tool for vibration-based
system-level monitoring and scalable infrastructure resilience.

</details>


### [88] [Deep recurrent-convolutional neural network learning and physics Kalman filtering comparison in dynamic load identification](https://arxiv.org/abs/2511.00100)
*Marios Impraimakis*

Main category: cs.LG

TL;DR: 比较门控循环单元、长短期记忆网络和卷积神经网络在动态结构载荷识别中的性能，并与基于物理的残差卡尔曼滤波器进行对比，分析在不同载荷场景下的表现差异。


<details>
  <summary>Details</summary>
Motivation: 解决土木工程应用中由于测试数据有限或结构模型不可识别导致的动态载荷识别不确定性问题，探索深度学习网络在小数据集条件下的适用性。

Method: 使用三种深度学习网络（GRU、LSTM、CNN）和残差卡尔曼滤波器（RKF），通过三个案例研究：模拟结构在顶层激振器激励下的响应、加州建筑在地震基础激励下的响应、IASC-ASCE结构健康监测基准问题的冲击和瞬时载荷条件。

Result: 不同方法在不同载荷场景下表现各异，RKF在物理参数可识别情况下优于神经网络，而神经网络在其他场景下可能表现更好。

Conclusion: 各种方法在不同载荷识别场景中各有优势，RKF在物理参数可识别情况下表现最佳，但神经网络在小数据集条件下仍具有应用价值。

Abstract: The dynamic structural load identification capabilities of the gated
recurrent unit, long short-term memory, and convolutional neural networks are
examined herein. The examination is on realistic small dataset training
conditions and on a comparative view to the physics-based residual Kalman
filter (RKF). The dynamic load identification suffers from the uncertainty
related to obtaining poor predictions when in civil engineering applications
only a low number of tests are performed or are available, or when the
structural model is unidentifiable. In considering the methods, first, a
simulated structure is investigated under a shaker excitation at the top floor.
Second, a building in California is investigated under seismic base excitation,
which results in loading for all degrees of freedom. Finally, the International
Association for Structural Control-American Society of Civil Engineers
(IASC-ASCE) structural health monitoring benchmark problem is examined for
impact and instant loading conditions. Importantly, the methods are shown to
outperform each other on different loading scenarios, while the RKF is shown to
outperform the networks in physically parametrized identifiable cases.

</details>


### [89] [Loquetier: A Virtualized Multi-LoRA Framework for Unified LLM Fine-tuning and Serving](https://arxiv.org/abs/2511.00101)
*Yuchen Zhang,Hanyue Du,Chun Cao,Jingwei Xu*

Main category: cs.LG

TL;DR: Loquetier是一个虚拟化多LoRA框架，统一了LoRA微调和服务，通过虚拟化模块和优化计算流实现高效批处理和最小化内核调用开销。


<details>
  <summary>Details</summary>
Motivation: 现有方法在统一LoRA微调和服务方面存在差距，需要一种能同时支持多适配器微调和推理的框架。

Method: 提出虚拟化模块隔离PEFT修改并支持共享基础模型上的多适配器，以及优化的计算流设计，在正向传播中合并微调和推理路径。

Result: 在三个任务设置上的实验表明，Loquetier在性能和灵活性上均优于现有基线，推理任务吞吐量达到最先进共服务系统的3.0倍，统一微调和推理任务的SLO达成率比PEFT高46.4倍。

Conclusion: Loquetier成功填补了LoRA微调和服务之间的空白，提供了高效统一的多LoRA框架。

Abstract: Low-Rank Adaptation (LoRA) has become a widely adopted parameter-efficient
fine-tuning (PEFT) technique for adapting large language models (LLMs) to
downstream tasks. While prior work has explored strategies for integrating LLM
training and serving, there still remains a gap in unifying fine-tuning and
inference for LoRA-based models. We present Loquetier, a virtualized multi-LoRA
framework that seamlessly integrates LoRA fine-tuning and serving within a
single runtime. Loquetier introduces two key components: (1) a Virtualized
Module that isolates PEFT-based modifications and supports multiple adapters on
a shared base model, and (2) an optimized computation flow with a kernel design
that merges fine-tuning and inference paths in forward propagation, enabling
efficient batching and minimizing kernel invocation overhead. Extensive
experiments across three task settings show that Loquetier consistently
outperforms existing baselines in both performance and flexibility, achieving
up to $3.0\times$ the throughput of the state-of-the-art co-serving system on
inference-only tasks and $46.4\times$ higher SLO attainment than PEFT on
unified fine-tuning and inference tasks. The implementation of Loquetier is
publicly available at https://github.com/NJUDeepEngine/Loquetier.

</details>


### [90] [Automated Discovery of Conservation Laws via Hybrid Neural ODE-Transformers](https://arxiv.org/abs/2511.00102)
*Vivan Doshi*

Main category: cs.LG

TL;DR: 提出了一种混合框架，从噪声轨迹数据中自动发现守恒量，结合神经ODE、Transformer和符号-数值验证器，显著优于直接在轨迹数据上操作的基线方法。


<details>
  <summary>Details</summary>
Motivation: 从观测数据中识别守恒定律是科学进步的关键，但目前从观测数据中发现这些不变量仍然是一个重大挑战。

Method: 集成三个组件：(1) 神经ODE学习系统动力学的连续模型，(2) Transformer基于学习到的向量场生成符号候选不变量，(3) 符号-数值验证器为候选量的有效性提供强数值证书。

Result: 在典型物理系统上测试，显著优于直接在轨迹数据上操作的基线方法。

Conclusion: 证明了分离式"先学习后搜索"方法从非完美数据中发现数学原理的鲁棒性。

Abstract: The discovery of conservation laws is a cornerstone of scientific progress.
However, identifying these invariants from observational data remains a
significant challenge. We propose a hybrid framework to automate the discovery
of conserved quantities from noisy trajectory data. Our approach integrates
three components: (1) a Neural Ordinary Differential Equation (Neural ODE) that
learns a continuous model of the system's dynamics, (2) a Transformer that
generates symbolic candidate invariants conditioned on the learned vector
field, and (3) a symbolic-numeric verifier that provides a strong numerical
certificate for the validity of these candidates. We test our framework on
canonical physical systems and show that it significantly outperforms baselines
that operate directly on trajectory data. This work demonstrates the robustness
of a decoupled learn-then-search approach for discovering mathematical
principles from imperfect data.

</details>


### [91] [Pelican-VL 1.0: A Foundation Brain Model for Embodied Intelligence](https://arxiv.org/abs/2511.00108)
*Yi Zhang,Che Liu,Xiancong Ren,Hanchu Ni,Shuai Zhang,Zeyuan Ding,Jiayu Hu,Hanzhe Shan,Zhenwei Niu,Zhaoyang Liu,Yue Zhao,Junbo Qi,Qinfan Zhang,Dengjie Li,Yidong Wang,Jiachen Luo,Yong Dai,Jian Tang,Xiaozhu Ju*

Main category: cs.LG

TL;DR: Pelican-VL 1.0是当前最大规模的开源具身多模态大脑模型，参数量从70亿到720亿，在1000+ A800 GPU集群上训练，性能比基础模型提升20.3%，超越1000亿级开源模型10.6%。


<details>
  <summary>Details</summary>
Motivation: 将强大智能嵌入到各种具身系统中，推动具身智能的发展。

Method: 采用DPPO（刻意练习策略优化）框架，通过metaloop机制实现RL-精炼-诊断-SFT循环训练。

Result: 在知名具身基准测试中达到与领先专有系统相当的水平，metaloop从40亿+token原始数据中蒸馏出高质量数据集。

Conclusion: Pelican-VL 1.0通过数据力量与智能自适应学习机制的深度融合，建立了新的具身智能模型标准。

Abstract: This report presents Pelican-VL 1.0, a new family of open-source embodied
brain models with parameter scales ranging from 7 billion to 72 billion. Our
explicit mission is clearly stated as: To embed powerful intelligence into
various embodiments. Pelican-VL 1.0 is currently the largest-scale open-source
embodied multimodal brain model. Its core advantage lies in the in-depth
integration of data power and intelligent adaptive learning mechanisms.
Specifically, metaloop distilled a high-quality dataset from a raw dataset
containing 4+ billion tokens. Pelican-VL 1.0 is trained on a large-scale
cluster of 1000+ A800 GPUs, consuming over 50k+ A800 GPU-hours per checkpoint.
This translates to a 20.3% performance uplift from its base model and
outperforms 100B-level open-source counterparts by 10.6%, placing it on par
with leading proprietary systems on well-known embodied benchmarks. We
establish a novel framework, DPPO (Deliberate Practice Policy Optimization),
inspired by human metacognition to train Pelican-VL 1.0. We operationalize this
as a metaloop that teaches the AI to practice deliberately, which is a
RL-Refine-Diagnose-SFT loop.

</details>


### [92] [MeixnerNet: Adaptive and Robust Spectral Graph Neural Networks with Discrete Orthogonal Polynomials](https://arxiv.org/abs/2511.00113)
*Huseyin Goksu*

Main category: cs.LG

TL;DR: MeixnerNet是一种新颖的谱图神经网络，使用离散正交多项式（Meixner多项式）替代传统的连续正交多项式，解决了图结构与连续域滤波器之间的理论不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 传统谱GNN使用连续正交多项式（如Chebyshev）作为图卷积滤波器，这与离散的图结构存在理论不匹配，可能导致性能不佳和对超参数设置的脆弱性。

Method: 提出MeixnerNet架构，使用离散Meixner多项式作为滤波器，并使其两个形状参数β和c可学习，同时引入结合拉普拉斯缩放和逐基LayerNorm的稳定化技术来解决数值不稳定性问题。

Result: 在最优K=2设置下，MeixnerNet在3个基准测试中的2个上表现优于ChebyNet基线，更重要的是，MeixnerNet对多项式阶数K的变化表现出极强的鲁棒性，而ChebyNet对此高度敏感且性能会崩溃。

Conclusion: MeixnerNet通过使用离散正交多项式解决了谱GNN中的理论不匹配问题，不仅实现了竞争性性能，更重要的是提供了对关键超参数的显著鲁棒性。

Abstract: Spectral Graph Neural Networks (GNNs) have achieved state-of-the-art results
by defining graph convolutions in the spectral domain. A common approach,
popularized by ChebyNet, is to use polynomial filters based on continuous
orthogonal polynomials (e.g., Chebyshev). This creates a theoretical
disconnect, as these continuous-domain filters are applied to inherently
discrete graph structures. We hypothesize this mismatch can lead to suboptimal
performance and fragility to hyperparameter settings.
  In this paper, we introduce MeixnerNet, a novel spectral GNN architecture
that employs discrete orthogonal polynomials -- specifically, the Meixner
polynomials $M_k(x; \beta, c)$. Our model makes the two key shape parameters of
the polynomial, beta and c, learnable, allowing the filter to adapt its
polynomial basis to the specific spectral properties of a given graph. We
overcome the significant numerical instability of these polynomials by
introducing a novel stabilization technique that combines Laplacian scaling
with per-basis LayerNorm.
  We demonstrate experimentally that MeixnerNet achieves
competitive-to-superior performance against the strong ChebyNet baseline at the
optimal K = 2 setting (winning on 2 out of 3 benchmarks). More critically, we
show that MeixnerNet is exceptionally robust to variations in the polynomial
degree K, a hyperparameter to which ChebyNet proves to be highly fragile,
collapsing in performance where MeixnerNet remains stable.

</details>


### [93] [LC-Opt: Benchmarking Reinforcement Learning and Agentic AI for End-to-End Liquid Cooling Optimization in Data Centers](https://arxiv.org/abs/2511.00116)
*Avisek Naug,Antonio Guillen,Vineet Kumar,Scott Greenwood,Wesley Brewer,Sahand Ghorbanpour,Ashwin Ramesh Babu,Vineet Gundecha,Ricardo Luna Gutierrez,Soumyendu Sarkar*

Main category: cs.LG

TL;DR: LC-Opt是一个可持续液体冷却基准环境，为高性能计算系统的节能液体冷却提供强化学习控制策略，基于橡树岭国家实验室Frontier超级计算机冷却系统的高保真数字孪生模型。


<details>
  <summary>Details</summary>
Motivation: 随着AI工作负载的增加，高密度数据中心的液体冷却变得至关重要。基于机器学习的控制器对于释放更高的能源效率和可靠性、促进可持续性至关重要。

Method: 基于Modelica的端到端模型，从站点级冷却塔到数据中心机柜和服务器刀片组。强化学习代理通过Gymnasium接口优化关键热控制参数，如液体供应温度、流量和阀门调节，以及冷却塔设定点。

Result: 创建了一个多目标实时优化挑战，平衡局部热调节和全局能源效率。支持集中式和分散式多智能体强化学习方法，演示了策略蒸馏到决策树和回归树以实现可解释控制，并探索了基于LLM的方法通过自然语言解释控制动作。

Conclusion: LC-Opt为ML社区、运营商和供应商提供了详细、可定制的液体冷却模型，使他们能够开发可持续的数据中心液体冷却控制解决方案，促进用户信任和简化系统管理。

Abstract: Liquid cooling is critical for thermal management in high-density data
centers with the rising AI workloads. However, machine learning-based
controllers are essential to unlock greater energy efficiency and reliability,
promoting sustainability. We present LC-Opt, a Sustainable Liquid Cooling (LC)
benchmark environment, for reinforcement learning (RL) control strategies in
energy-efficient liquid cooling of high-performance computing (HPC) systems.
Built on the baseline of a high-fidelity digital twin of Oak Ridge National
Lab's Frontier Supercomputer cooling system, LC-Opt provides detailed
Modelica-based end-to-end models spanning site-level cooling towers to data
center cabinets and server blade groups. RL agents optimize critical thermal
controls like liquid supply temperature, flow rate, and granular valve
actuation at the IT cabinet level, as well as cooling tower (CT) setpoints
through a Gymnasium interface, with dynamic changes in workloads. This
environment creates a multi-objective real-time optimization challenge
balancing local thermal regulation and global energy efficiency, and also
supports additional components like a heat recovery unit (HRU). We benchmark
centralized and decentralized multi-agent RL approaches, demonstrate policy
distillation into decision and regression trees for interpretable control, and
explore LLM-based methods that explain control actions in natural language
through an agentic mesh architecture designed to foster user trust and simplify
system management. LC-Opt democratizes access to detailed, customizable liquid
cooling models, enabling the ML community, operators, and vendors to develop
sustainable data center liquid cooling control solutions.

</details>


### [94] [DCcluster-Opt: Benchmarking Dynamic Multi-Objective Optimization for Geo-Distributed Data Center Workloads](https://arxiv.org/abs/2511.00117)
*Antonio Guillen-Perez,Avisek Naug,Vineet Gundecha,Sahand Ghorbanpour,Ricardo Luna Gutierrez,Ashwin Ramesh Babu,Munther Salim,Shubhanker Banerjee,Eoin H. Oude Essink,Damien Fay,Soumyendu Sarkar*

Main category: cs.LG

TL;DR: DCcluster-Opt是一个开源的高保真模拟基准，用于可持续的全球分布式数据中心任务调度，结合了真实世界数据和物理模型，支持多目标优化研究。


<details>
  <summary>Details</summary>
Motivation: 大规模AI的能源需求和碳足迹不断增加，需要智能的全球分布式数据中心工作负载管理，但缺乏能真实捕捉环境因素、数据中心物理和网络动态相互作用的基准。

Method: 结合精选的真实世界数据集（AI工作负载轨迹、电网碳强度、电力市场、天气、云传输成本等）与基于物理的数据中心运营模型，提供模块化奖励系统和Gymnasium API。

Result: 提出了一个具有挑战性的调度问题，支持动态重新分配或延迟任务，优化碳排放、能源成本、服务等级协议和用水等多个目标，并包含热回收等高级组件建模。

Conclusion: DCcluster-Opt通过提供真实、可配置且可访问的测试平台，加速了面向全球分布式数据中心的下一代可持续计算解决方案的开发和验证。

Abstract: The increasing energy demands and carbon footprint of large-scale AI require
intelligent workload management in globally distributed data centers. Yet
progress is limited by the absence of benchmarks that realistically capture the
interplay of time-varying environmental factors (grid carbon intensity,
electricity prices, weather), detailed data center physics (CPUs, GPUs, memory,
HVAC energy), and geo-distributed network dynamics (latency and transmission
costs). To bridge this gap, we present DCcluster-Opt: an open-source,
high-fidelity simulation benchmark for sustainable, geo-temporal task
scheduling. DCcluster-Opt combines curated real-world datasets, including AI
workload traces, grid carbon intensity, electricity markets, weather across 20
global regions, cloud transmission costs, and empirical network delay
parameters with physics-informed models of data center operations, enabling
rigorous and reproducible research in sustainable computing. It presents a
challenging scheduling problem where a top-level coordinating agent must
dynamically reassign or defer tasks that arrive with resource and service-level
agreement requirements across a configurable cluster of data centers to
optimize multiple objectives. The environment also models advanced components
such as heat recovery. A modular reward system enables an explicit study of
trade-offs among carbon emissions, energy costs, service level agreements, and
water use. It provides a Gymnasium API with baseline controllers, including
reinforcement learning and rule-based strategies, to support reproducible ML
research and a fair comparison of diverse algorithms. By offering a realistic,
configurable, and accessible testbed, DCcluster-Opt accelerates the development
and validation of next-generation sustainable computing solutions for
geo-distributed data centers.

</details>


### [95] [Analysis of Line Break prediction models for detecting defensive breakthrough in football](https://arxiv.org/abs/2511.00121)
*Shoma Yagi,Jun Ichikawa,Genki Ichinose*

Main category: cs.LG

TL;DR: 开发了一个机器学习模型来预测足球中的防线突破（Line Break），使用XGBoost分类器分析球员位置、速度和空间配置等特征，模型预测准确率高，揭示了防线突破与射门机会创造的密切关系。


<details>
  <summary>Details</summary>
Motivation: 足球中攻击方突破对方防线的能力是进攻有效性和战术表现的关键指标，但以往研究主要关注射门或进球机会，而忽略了球队如何突破防线的过程。

Method: 使用2023年J1联赛赛季的事件和追踪数据，构建包含189个特征的机器学习模型，包括球员位置、速度和空间配置等变量，采用XGBoost分类器来预测防线突破的概率。

Result: 模型预测准确率很高，AUC达到0.982，Brier分数为0.015。SHAP分析显示进攻球员速度、防线空隙和进攻球员空间分布是影响防线突破的关键因素。球队层面的防线突破概率与被射门次数和传中次数呈中度正相关。

Conclusion: 防线突破与得分机会创造密切相关，为理解足球战术动态提供了量化框架，有助于更全面地评估球队的进攻表现和战术效果。

Abstract: In football, attacking teams attempt to break through the opponent's
defensive line to create scoring opportunities. This action, known as a Line
Break, is a critical indicator of offensive effectiveness and tactical
performance, yet previous studies have mainly focused on shots or goal
opportunities rather than on how teams break the defensive line. In this study,
we develop a machine learning model to predict Line Breaks using event and
tracking data from the 2023 J1 League season. The model incorporates 189
features, including player positions, velocities, and spatial configurations,
and employs an XGBoost classifier to estimate the probability of Line Breaks.
The proposed model achieved high predictive accuracy, with an AUC of 0.982 and
a Brier score of 0.015. Furthermore, SHAP analysis revealed that factors such
as offensive player speed, gaps in the defensive line, and offensive players'
spatial distributions significantly contribute to the occurrence of Line
Breaks. Finally, we found a moderate positive correlation between the predicted
probability of being Line-Broken and the number of shots and crosses conceded
at the team level. These results suggest that Line Breaks are closely linked to
the creation of scoring opportunities and provide a quantitative framework for
understanding tactical dynamics in football.

</details>


### [96] [Cross-fluctuation phase transitions reveal sampling dynamics in diffusion models](https://arxiv.org/abs/2511.00124)
*Sai Niranjan Ramachandran,Manish Krishan Lal,Suvrit Sra*

Main category: cs.LG

TL;DR: 使用统计物理中的交叉涨落分析扩散模型采样动态，发现样本经历尖锐的离散转变，形成目标分布。这些转变可被检测并用于提升采样效率、加速条件生成和零样本任务。


<details>
  <summary>Details</summary>
Motivation: 理解扩散模型中采样动态的演变过程，探索如何通过检测离散转变来提升生成效率和性能。

Method: 使用交叉涨落统计量分析扩散模型的采样动态，推导方差保持SDE的交叉涨落闭式解，检测采样过程中的离散转变。

Result: 检测这些转变可直接提升采样效率，加速类别条件和稀有类别生成，改进图像分类和风格迁移等零样本任务，无需昂贵的网格搜索或重新训练。

Conclusion: 该框架统一了离散马尔可夫链理论与连续动力学，连接了经典耦合混合与现代生成建模，为扩散模型提供了新的理论视角。

Abstract: We analyse how the sampling dynamics of distributions evolve in score-based
diffusion models using cross-fluctuations, a centered-moment statistic from
statistical physics. Specifically, we show that starting from an unbiased
isotropic normal distribution, samples undergo sharp, discrete transitions,
eventually forming distinct events of a desired distribution while
progressively revealing finer structure. As this process is reversible, these
transitions also occur in reverse, where intermediate states progressively
merge, tracing a path back to the initial distribution. We demonstrate that
these transitions can be detected as discontinuities in $n^{\text{th}}$-order
cross-fluctuations. For variance-preserving SDEs, we derive a closed-form for
these cross-fluctuations that is efficiently computable for the reverse
trajectory. We find that detecting these transitions directly boosts sampling
efficiency, accelerates class-conditional and rare-class generation, and
improves two zero-shot tasks--image classification and style transfer--without
expensive grid search or retraining. We also show that this viewpoint unifies
classical coupling and mixing from finite Markov chains with continuous
dynamics while extending to stochastic SDEs and non Markovian samplers. Our
framework therefore bridges discrete Markov chain theory, phase analysis, and
modern generative modeling.

</details>


### [97] [Dynamic Model Selection for Trajectory Prediction via Pairwise Ranking and Meta-Features](https://arxiv.org/abs/2511.00126)
*Lu Bowen*

Main category: cs.LG

TL;DR: 提出动态多专家门控框架，通过内部模型信号自适应选择最佳轨迹预测器，在nuPlan-mini数据集上实现9.5%的FDE提升。


<details>
  <summary>Details</summary>
Motivation: 现有深度轨迹预测器在复杂长尾驾驶场景中不可靠，单一模型范式存在局限性，需要更可靠的预测方法。

Method: 使用动态多专家门控框架，基于稳定性、不确定性等内部模型信号，在物理信息LSTM、Transformer和微调GameFormer之间自适应选择。

Result: 在nuPlan-mini数据集上FDE达到2.567米，比GameFormer降低9.5%，达到oracle性能的57.8%。左转场景FDE降低约10%。

Conclusion: 自适应混合系统能提升自动驾驶轨迹预测的可靠性，为超越静态单一模型范式提供了实用路径。

Abstract: Recent deep trajectory predictors (e.g., Jiang et al., 2023; Zhou et al.,
2022) have achieved strong average accuracy but remain unreliable in complex
long-tail driving scenarios. These limitations reveal the weakness of the
prevailing "one-model-fits-all" paradigm, particularly in safety-critical urban
contexts where simpler physics-based models can occasionally outperform
advanced networks (Kalman, 1960). To bridge this gap, we propose a dynamic
multi-expert gating framework that adaptively selects the most reliable
trajectory predictor among a physics-informed LSTM, a Transformer, and a
fine-tuned GameFormer on a per-sample basis.
  Our method leverages internal model signals (meta-features) such as stability
and uncertainty (Gal and Ghahramani, 2016), which we demonstrate to be
substantially more informative than geometric scene descriptors. To the best of
our knowledge, this is the first work to formulate trajectory expert selection
as a pairwise-ranking problem over internal model signals (Burges et al.,
2005), directly optimizing decision quality without requiring post-hoc
calibration.
  Evaluated on the nuPlan-mini dataset (Caesar et al., 2021) with 1,287
samples, our LLM-enhanced tri-expert gate achieves a Final Displacement Error
(FDE) of 2.567 m, representing a 9.5 percent reduction over GameFormer (2.835
m), and realizes 57.8 percent of the oracle performance bound. In open-loop
simulations, after trajectory horizon alignment, the same configuration reduces
FDE on left-turn scenarios by approximately 10 percent, demonstrating
consistent improvements across both offline validation and open-loop
evaluation. These results indicate that adaptive hybrid systems enhance
trajectory reliability in safety-critical autonomous driving, providing a
practical pathway beyond static single-model paradigms.

</details>


### [98] [Casing Collar Identification using AlexNet-based Neural Networks for Depth Measurement in Oil and Gas Wells](https://arxiv.org/abs/2511.00129)
*Siyu Xiao,Xindi Zhao,Tianhao Mao,Yiwei Wang,Yuqiao Chen,Hongyun Zhang,Jian Wang,Junjie Wang,Shuang Liu,Tupei Chen,Yang Liu*

Main category: cs.LG

TL;DR: 本文提出了一种集成到井下工具中的CCL信号采集系统，用于构建数据集，并评估了数据增强预处理方法在基于AlexNet的神经网络模型中的有效性。


<details>
  <summary>Details</summary>
Motivation: 在油气井作业中，精确的井下深度测量至关重要，而基于神经网络的CCL信号识别在套管接箍识别方面取得了进展，但预处理方法和真实井数据有限的问题仍未解决。

Method: 提出了集成到井下工具中的CCL信号采集系统，用于数据集构建，并评估了标准化、标签分布平滑、随机裁剪、标签平滑正则化、时间缩放和多重采样等数据增强方法。

Result: 实验结果表明，标准化、标签分布平滑和随机裁剪是模型训练的基本要求，而标签平滑正则化、时间缩放和多重采样显著提高了模型的泛化能力。两个基准模型的F1分数分别从0.937和0.952提高到1.0。

Conclusion: 这项工作解决了在CCL数据有限环境中训练套管接箍识别模型的数据增强方法空白，验证了所提方法的有效性和实际适用性。

Abstract: Accurate downhole depth measurement is essential for oil and gas well
operations, directly influencing reservoir contact, production efficiency, and
operational safety. Collar correlation using a casing collar locator (CCL) is
fundamental for precise depth calibration. While neural network-based CCL
signal recognition has achieved significant progress in collar identification,
preprocessing methods for such applications remain underdeveloped. Moreover,
the limited availability of real well data poses substantial challenges for
training neural network models that require extensive datasets. This paper
presents a system integrated into downhole tools for CCL signal acquisition to
facilitate dataset construction. We propose comprehensive preprocessing methods
for data augmentation and evaluate their effectiveness using our AlexNet-based
neural network models. Through systematic experimentation across various
configuration combinations, we analyze the contribution of each augmentation
method. Results demonstrate that standardization, label distribution smoothing
(LDS), and random cropping are fundamental requirements for model training,
while label smoothing regularization (LSR), time scaling, and multiple sampling
significantly enhance model generalization capability. The F1 scores of our two
benchmark models trained with the proposed augmentation methods maximumly
improve from 0.937 and 0.952 to 1.0 and 1.0, respectively. Performance
validation on real CCL waveforms confirms the effectiveness and practical
applicability of our approach. This work addresses the gaps in data
augmentation methodologies for training casing collar recognition models in CCL
data-limited environments.

</details>


### [99] [A Comparative Analysis of LLM Adaptation: SFT, LoRA, and ICL in Data-Scarce Scenarios](https://arxiv.org/abs/2511.00130)
*Bernd Bohnet,Rumen Dangovski,Kevin Swersky,Sherry Moore,Arslan Chaudhry,Kathleen Kenealy,Noah Fiedel*

Main category: cs.LG

TL;DR: 本文比较了三种LLM适应方法（SFT、LoRA、ICL）在数据稀缺场景下的表现，发现LoRA在技能获取与通用知识保持之间提供最佳平衡。


<details>
  <summary>Details</summary>
Motivation: LLMs需要针对特定应用进行定制，但完全微调计算成本高且可能导致灾难性遗忘，需要寻找更有效的适应策略。

Method: 在数据稀缺场景下对监督微调(SFT)、低秩适应(LoRA)和上下文学习(ICL)进行对比分析。

Result: LoRA在技能获取与通用知识保持之间提供最佳平衡；SFT擅长技能获取但易发生灾难性遗忘；ICL适合事实知识整合但难以处理复杂技能。

Conclusion: 研究为选择LLM适应策略提供了实用框架，明确了技能获取与知识整合的关键区别，以及任务特定性能与通用能力保持之间的权衡关系。

Abstract: The remarkable capabilities of Large Language Models (LLMs) often need to be
tailored for specific applications, requiring the integration of new knowledge
or the acquisition of new skills. While full fine-tuning is a powerful
adaptation method, it is computationally expensive and can lead to a
degradation of general reasoning abilities, a phenomenon known as catastrophic
forgetting. A range of alternative techniques exists, each with its own
trade-offs. In-Context Learning (ICL) is fast but limited by context length,
while Parameter-Efficient Fine-Tuning (PEFT) methods like Low-Rank Adaptation
(LoRA) offer a middle ground by minimizing parameter changes. However, the
challenge of catastrophic forgetting persists, raising questions about the best
adaptation strategy for a given task. This paper presents a comparative
analysis of Supervised Finetuning (SFT), LoRA, and ICL in data-scarce
scenarios. We find that LoRA provides the most effective balance, successfully
instilling new skills with minimal impact on the base model's general
knowledge. In contrast, while SFT excels at skill acquisition, it is highly
susceptible to catastrophic forgetting. ICL is effective for incorporating
factual knowledge but struggles with complex skills. Our findings offer a
practical framework for selecting an LLM adaptation strategy. We highlight the
critical distinction between skill acquisition and knowledge integration,
clarify the trade-offs between task-specific performance and the preservation
of general capabilities.

</details>


### [100] [Feature Importance Guided Random Forest Learning with Simulated Annealing Based Hyperparameter Tuning](https://arxiv.org/abs/2511.00133)
*Kowshik Balasubramanian,Andre Williams,Ismail Butun*

Main category: cs.LG

TL;DR: 提出了一种结合概率特征采样和模拟退火超参数调优的随机森林增强框架，在多个领域显著提升了预测准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统随机森林在处理复杂分类问题时存在局限性，需要更好地捕捉数据中的相关信号并实现自适应超参数配置。

Method: 采用概率特征采样技术，强调对分类更有意义的特征，并通过模拟退火算法进行动态超参数调优。

Result: 实验结果显示在准确率方面获得持续提升，并提供了有意义的特征相关性分析。

Conclusion: 结合重要性感知采样和元启发式优化的方法在提升随机森林性能方面具有显著效果。

Abstract: This paper introduces a novel framework for enhancing Random Forest
classifiers by integrating probabilistic feature sampling and hyperparameter
tuning via Simulated Annealing. The proposed framework exhibits substantial
advancements in predictive accuracy and generalization, adeptly tackling the
multifaceted challenges of robust classification across diverse domains,
including credit risk evaluation, anomaly detection in IoT ecosystems,
early-stage medical diagnostics, and high-dimensional biological data analysis.
To overcome the limitations of conventional Random Forests, we present an
approach that places stronger emphasis on capturing the most relevant signals
from data while enabling adaptive hyperparameter configuration. The model is
guided towards features that contribute more meaningfully to classification and
optimizing this with dynamic parameter tuning. The results demonstrate
consistent accuracy improvements and meaningful insights into feature
relevance, showcasing the efficacy of combining importance aware sampling and
metaheuristic optimization.

</details>


### [101] [Physiologically Active Vegetation Reverses Its Cooling Effect in Humid Urban Climates](https://arxiv.org/abs/2511.00134)
*Angana Borah,Adrija Datta,Ashish S. Kumar,Raviraj Dave,Udit Bhatia*

Main category: cs.LG

TL;DR: 研究量化了植被结构与功能对热指数的影响，发现植被在特定条件下会逆转冷却效应并加剧热应激，为城市绿化策略提供了气候特异性阈值。


<details>
  <summary>Details</summary>
Motivation: 城市绿化冷却效果不均，因为植被在冷却表面的同时可能加剧空气热感。现有研究对植被如何调节冷却与湿度积累之间的权衡理解不足，导致缓解政策和设计缺乏指导。

Method: 使用极端感知的1公里热指数重建和可解释机器学习框架（SHAP和ALE），分析138个印度城市中植被-气候相互作用，涵盖不同气候带和城市密度区域。

Result: 当EVI≥0.4、LAI≥0.05时冷却效果增强，但当EVI≥0.5、LAI≥0.2、fPAR≥0.5时开始逆转为增温，在潮湿密集核心区fPAR≥0.25时更早出现逆转。高生理活性植被会快速提升近地表湿度，逆转冷却效应并放大感知热应激。

Conclusion: 确定了植被驱动冷却的气候限制，为促进公平和热韧性城市的气候特异性绿化策略提供了量化阈值。

Abstract: Efforts to green cities for cooling are succeeding unevenly because the same
vegetation that cools surfaces can also intensify how hot the air feels.
Previous studies have identified humid heat as a growing urban hazard, yet how
physiologically active vegetation governs this trade-off between cooling and
moisture accumulation remains poorly understood, leaving mitigation policy and
design largely unguided. Here we quantify how vegetation structure and function
influence the Heat Index (HI), a combined measure of temperature and humidity
in 138 Indian cities spanning tropical savanna, semi-arid steppe, and humid
subtropical climates, and across dense urban cores and semi-urban rings. Using
an extreme-aware, one kilometre reconstruction of HI and an interpretable
machine-learning framework that integrates SHapley Additive Explanations (SHAP)
and Accumulated Local Effects (ALE), we isolate vegetation-climate
interactions. Cooling generally strengthens for EVI >= 0.4 and LAI >= 0.05, but
joint-high regimes begin to reverse toward warming when EVI >= 0.5, LAI >= 0.2,
and fPAR >= 0.5,with an earlier onset for fPAR >= 0.25 in humid, dense cores.
In such environments, highly physiologically active vegetation elevates
near-surface humidity faster than it removes heat, reversing its cooling effect
and amplifying perceived heat stress. These findings establish the climatic
limits of vegetation-driven cooling and provide quantitative thresholds for
climate-specific greening strategies that promote equitable and heat-resilient
cities.

</details>


### [102] [A Dual Large Language Models Architecture with Herald Guided Prompts for Parallel Fine Grained Traffic Signal Control](https://arxiv.org/abs/2511.00136)
*Qing Guo,Xinhang Li,Junyu Chen,Zheng Guo,Xiaocong Li,Lin Zhang,Lei Li*

Main category: cs.LG

TL;DR: HeraldLight提出了一种双LLM架构，通过Herald引导提示增强交通信号控制，解决了现有LLM方法固定信号时长和幻觉错误的问题，以及RL方法缺乏鲁棒性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法在交通信号控制中存在固定时间信号时长和幻觉错误的限制，而传统RL方法在信号时序决策中缺乏鲁棒性且泛化能力差。

Method: 采用双LLM架构：Herald模块提取上下文信息并预测各交通相位队列长度；LLM-Agent进行细粒度交通信号控制；LLM-Critic修正LLM-Agent的输出错误和幻觉，并通过基于分数的微调提高准确性和鲁棒性。

Result: 在CityFlow模拟实验中，使用涵盖济南(12)、杭州(16)和纽约(196)共224个交叉口的真实数据集，HeraldLight在平均旅行时间上比现有最佳基线减少20.03%，在济南和杭州场景中平均队列长度减少10.74%。

Conclusion: HeraldLight通过双LLM架构和Herald引导提示，显著提升了交通信号控制的性能，在多个真实场景中表现出优越性。

Abstract: Leveraging large language models (LLMs) in traffic signal control (TSC)
improves optimization efficiency and interpretability compared to traditional
reinforcement learning (RL) methods. However, existing LLM-based approaches are
limited by fixed time signal durations and are prone to hallucination errors,
while RL methods lack robustness in signal timing decisions and suffer from
poor generalization. To address these challenges, this paper proposes
HeraldLight, a dual LLMs architecture enhanced by Herald guided prompts. The
Herald Module extracts contextual information and forecasts queue lengths for
each traffic phase based on real-time conditions. The first LLM, LLM-Agent,
uses these forecasts to make fine grained traffic signal control, while the
second LLM, LLM-Critic, refines LLM-Agent's outputs, correcting errors and
hallucinations. These refined outputs are used for score-based fine-tuning to
improve accuracy and robustness. Simulation experiments using CityFlow on real
world datasets covering 224 intersections in Jinan (12), Hangzhou (16), and New
York (196) demonstrate that HeraldLight outperforms state of the art baselines,
achieving a 20.03% reduction in average travel time across all scenarios and a
10.74% reduction in average queue length on the Jinan and Hangzhou scenarios.
The source code is available on GitHub:
https://github.com/BUPT-ANTlab/HeraldLight.

</details>


### [103] [Study on Supply Chain Finance Decision-Making Model and Enterprise Economic Performance Prediction Based on Deep Reinforcement Learning](https://arxiv.org/abs/2511.00166)
*Shiman Zhang,Jinghan Zhou,Zhoufan Yu,Ningai Leng*

Main category: cs.LG

TL;DR: 提出了一种结合深度学习与智能粒子群优化的决策模型，用于提升后端集中式冗余供应链的决策效率和规划能力。


<details>
  <summary>Details</summary>
Motivation: 为了提高后端集中式冗余供应链的决策制定和规划效率，需要开发更智能的优化方法。

Method: 构建分布式节点部署模型和最优规划路径，使用卷积神经网络从历史数据中提取特征，线性规划捕获高阶统计特征，采用模糊关联规则调度和深度强化学习优化模型，神经网络拟合动态变化，建立"深度学习特征提取-智能粒子群优化"混合机制。

Result: 仿真结果显示资源消耗减少，空间规划增强，在动态环境中改善了实时决策调整、配送路径优化和鲁棒智能控制。

Conclusion: 该集成模型有效提升了供应链决策效率和规划能力，在动态环境下表现出良好的适应性和优化效果。

Abstract: To improve decision-making and planning efficiency in back-end centralized
redundant supply chains, this paper proposes a decision model integrating deep
learning with intelligent particle swarm optimization. A distributed node
deployment model and optimal planning path are constructed for the supply chain
network. Deep learning such as convolutional neural networks extracts features
from historical data, and linear programming captures high-order statistical
features. The model is optimized using fuzzy association rule scheduling and
deep reinforcement learning, while neural networks fit dynamic changes. A
hybrid mechanism of "deep learning feature extraction - intelligent particle
swarm optimization" guides global optimization and selects optimal decisions
for adaptive control. Simulations show reduced resource consumption, enhanced
spatial planning, and in dynamic environments improved real-time decision
adjustment, distribution path optimization, and robust intelligent control.

</details>


### [104] [Can SAEs reveal and mitigate racial biases of LLMs in healthcare?](https://arxiv.org/abs/2511.00177)
*Hiba Ahsan,Byron C. Wallace*

Main category: cs.LG

TL;DR: 使用稀疏自编码器(SAEs)分析Gemma-2模型中与种族相关的潜在特征，发现这些特征能识别模型对黑人患者的偏见关联，但通过SAE调控来减轻偏见在复杂临床任务中效果有限。


<details>
  <summary>Details</summary>
Motivation: LLMs在医疗领域的应用存在加剧现有偏见的风险，需要开发方法来检测模型是否错误地依赖患者种族信息进行预测。

Method: 在Gemma-2模型中识别与黑人个体相关的SAE潜在特征，分析这些特征的激活模式，并通过潜在特征调控来测试模型输出的变化。

Result: 发现与黑人相关的潜在特征不仅对合理输入(如"非裔美国人")激活，也对问题词汇(如"监禁")激活；激活该特征会增加模型将患者预测为"好斗"的风险；SAE调控在简单场景中能改善偏见，但在复杂临床任务中效果不佳。

Conclusion: SAEs可作为识别LLMs在临床应用中问题性依赖人口统计特征的有用工具，但通过SAE调控来减轻偏见在现实任务中的实用性有限。

Abstract: LLMs are increasingly being used in healthcare. This promises to free
physicians from drudgery, enabling better care to be delivered at scale. But
the use of LLMs in this space also brings risks; for example, such models may
worsen existing biases. How can we spot when LLMs are (spuriously) relying on
patient race to inform predictions? In this work we assess the degree to which
Sparse Autoencoders (SAEs) can reveal (and control) associations the model has
made between race and stigmatizing concepts. We first identify SAE latents in
Gemma-2 models which appear to correlate with Black individuals. We find that
this latent activates on reasonable input sequences (e.g., "African American")
but also problematic words like "incarceration". We then show that we can use
this latent to steer models to generate outputs about Black patients, and
further that this can induce problematic associations in model outputs as a
result. For example, activating the Black latent increases the risk assigned to
the probability that a patient will become "belligerent". We evaluate the
degree to which such steering via latents might be useful for mitigating bias.
We find that this offers improvements in simple settings, but is less
successful for more realistic and complex clinical tasks. Overall, our results
suggest that: SAEs may offer a useful tool in clinical applications of LLMs to
identify problematic reliance on demographics but mitigating bias via SAE
steering appears to be of marginal utility for realistic tasks.

</details>


### [105] [PDE-SHARP: PDE Solver Hybrids Through Analysis & Refinement Passes](https://arxiv.org/abs/2511.00183)
*Shaghayegh Fazliani,Madeleine Udell*

Main category: cs.LG

TL;DR: PDE-SHARP是一个通过用更便宜的LLM推理替代昂贵的科学计算来降低计算成本的框架，能够在减少60-75%计算评估的情况下实现更优的求解器精度。


<details>
  <summary>Details</summary>
Motivation: 当前使用测试时计算的LLM驱动方法需要执行大量求解器样本来识别高精度求解器，对于需要大量计算资源进行数值评估的复杂PDE来说成本尤其高昂。

Method: 采用三阶段框架：(1)分析：包括PDE分类、解类型检测和稳定性分析的数学思维链分析；(2)生成：基于前一阶段数学洞察的求解器生成；(3)合成：LLM评委通过灵活性能反馈迭代优化实现的协作选择-混合锦标赛。

Result: 平均需要少于13次求解器评估（基线方法需要30+次），在测试的PDE上平均精度提高4倍，并在从通用到专用推理模型的各种LLM架构上表现出稳健性能。

Conclusion: PDE-SHARP通过将昂贵科学计算替换为更便宜的LLM推理，显著降低了计算成本，同时实现了更优的求解器精度和稳健性能。

Abstract: Current LLM-driven approaches using test-time computing to generate PDE
solvers execute a large number of solver samples to identify high-accuracy
solvers. These paradigms are especially costly for complex PDEs requiring
substantial computational resources for numerical evaluation. We introduce
PDE-SHARP, a framework to reduce computational costs by replacing expensive
scientific computation by cheaper LLM inference that achieves superior solver
accuracy with 60-75% fewer computational evaluations. PDE-SHARP employs three
stages: (1) Analysis: mathematical chain-of-thought analysis including PDE
classification, solution type detection, and stability analysis; (2) Genesis:
solver generation based on mathematical insights from the previous stage; and
(3) Synthesis: collaborative selection-hybridization tournaments in which LLM
judges iteratively refine implementations through flexible performance
feedback. To generate high-quality solvers, PDE-SHARP requires fewer than 13
solver evaluations on average compared to 30+ for baseline methods, improving
accuracy uniformly across tested PDEs by $4\times$ on average, and demonstrates
robust performance across LLM architectures, from general-purpose to
specialized reasoning models.

</details>


### [106] [EL-MIA: Quantifying Membership Inference Risks of Sensitive Entities in LLMs](https://arxiv.org/abs/2511.00192)
*Ali Satvaty,Suzan Verberne,Fatih Turkmen*

Main category: cs.LG

TL;DR: 提出了针对LLM隐私的新任务——实体级成员推断风险发现，专注于敏感信息。现有MIA方法只能检测整个提示或文档，无法捕捉更细粒度的风险。作者提出了EL-MIA框架来审计LLM中的实体级成员风险。


<details>
  <summary>Details</summary>
Motivation: 现有成员推断攻击方法在检测LLM训练数据中的敏感信息时存在局限性，只能识别整个提示或文档的成员资格，无法在实体级别上发现风险。

Method: 提出了EL-MIA框架用于审计LLM中的实体级成员风险，构建了评估基准数据集，系统比较了现有MIA技术和两种新方法，分析了实体级MIA易感性与模型规模、训练轮次等因素的关系。

Result: 研究发现现有MIA方法在敏感属性的实体级成员推断方面存在局限性，但这种易感性可以通过相对简单的方法来识别，表明需要更强的对抗方法来充分测试威胁模型。

Conclusion: 实体级成员推断风险是LLM隐私保护中的重要问题，现有方法对此类细粒度攻击的检测能力有限，需要开发更强大的对抗方法来评估和改进模型隐私保护能力。

Abstract: Membership inference attacks (MIA) aim to infer whether a particular data
point is part of the training dataset of a model. In this paper, we propose a
new task in the context of LLM privacy: entity-level discovery of membership
risk focused on sensitive information (PII, credit card numbers, etc). Existing
methods for MIA can detect the presence of entire prompts or documents in the
LLM training data, but they fail to capture risks at a finer granularity. We
propose the ``EL-MIA'' framework for auditing entity-level membership risks in
LLMs. We construct a benchmark dataset for the evaluation of MIA methods on
this task. Using this benchmark, we conduct a systematic comparison of existing
MIA techniques as well as two newly proposed methods. We provide a
comprehensive analysis of the results, trying to explain the relation of the
entity level MIA susceptability with the model scale, training epochs, and
other surface level factors. Our findings reveal that existing MIA methods are
limited when it comes to entity-level membership inference of the sensitive
attributes, while this susceptibility can be outlined with relatively
straightforward methods, highlighting the need for stronger adversaries to
stress test the provided threat model.

</details>


### [107] [Diffusion LLMs are Natural Adversaries for any LLM](https://arxiv.org/abs/2511.00203)
*David Lüdke,Tom Wollschläger,Paul Ungermann,Stephan Günnemann,Leo Schwinn*

Main category: cs.LG

TL;DR: 提出一个将对抗性提示优化转化为高效摊销推理任务的新框架，利用预训练的非自回归生成LLM直接条件生成提示，替代昂贵的离散优化


<details>
  <summary>Details</summary>
Motivation: 解决传统对抗性提示优化资源密集的问题，通过摊销推理提高效率

Method: 使用预训练的非自回归生成LLM（如Diffusion LLMs）作为提示搜索的代理模型，直接条件生成提示，将离散优化转化为少量并行样本生成

Result: 生成的提示具有低困惑度、多样性，能有效攻击多种黑盒目标模型，包括鲁棒训练和专有LLM

Conclusion: 该框架为红队测试、自动提示优化和基于Flow/Diffusion的LLM应用开辟了新方向

Abstract: We introduce a novel framework that transforms the resource-intensive
(adversarial) prompt optimization problem into an \emph{efficient, amortized
inference task}. Our core insight is that pretrained, non-autoregressive
generative LLMs, such as Diffusion LLMs, which model the joint distribution
over prompt-response pairs, can serve as powerful surrogates for prompt search.
This approach enables the direct conditional generation of prompts, effectively
replacing costly, per-instance discrete optimization with a small number of
parallelizable samples. We provide a probabilistic analysis demonstrating that
under mild fidelity assumptions, only a few conditional samples are required to
recover high-reward (harmful) prompts. Empirically, we find that the generated
prompts are low-perplexity, diverse jailbreaks that exhibit strong
transferability to a wide range of black-box target models, including robustly
trained and proprietary LLMs. Beyond adversarial prompting, our framework opens
new directions for red teaming, automated prompt optimization, and leveraging
emerging Flow- and Diffusion-based LLMs.

</details>


### [108] [HyperNQ: A Hypergraph Neural Network Decoder for Quantum LDPC Codes](https://arxiv.org/abs/2511.01741)
*Ameya S. Bhave,Navnil Choudhury,Kanad Basu*

Main category: cs.LG

TL;DR: 提出了HyperNQ，首个基于超图神经网络的QLDPC解码器，通过利用超边捕捉高阶稳定子约束，在伪阈值区域下比BP和GNN方法显著提升逻辑错误率。


<details>
  <summary>Details</summary>
Motivation: 传统BP解码在短环存在时收敛性差，而GNN方法受限于Tanner图的成对交互，无法捕捉高阶相关性，需要更有效的解码策略。

Method: 使用超图神经网络，通过两阶段消息传递方案，利用超边捕捉高阶稳定子约束，实现高度表达性和紧凑的解码。

Result: 在伪阈值标记以下，HyperNQ比BP提升逻辑错误率高达84%，比GNN策略提升50%，性能优于现有最先进解码器。

Conclusion: HyperNQ通过超图神经网络有效捕捉高阶相关性，显著提升了QLDPC码的解码性能，为量子纠错提供了有前景的解决方案。

Abstract: Quantum computing requires effective error correction strategies to mitigate
noise and decoherence. Quantum Low-Density Parity-Check (QLDPC) codes have
emerged as a promising solution for scalable Quantum Error Correction (QEC)
applications by supporting constant-rate encoding and a sparse parity-check
structure. However, decoding QLDPC codes via traditional approaches such as
Belief Propagation (BP) suffers from poor convergence in the presence of short
cycles. Machine learning techniques like Graph Neural Networks (GNNs) utilize
learned message passing over their node features; however, they are restricted
to pairwise interactions on Tanner graphs, which limits their ability to
capture higher-order correlations. In this work, we propose HyperNQ, the first
Hypergraph Neural Network (HGNN)- based QLDPC decoder that captures
higher-order stabilizer constraints by utilizing hyperedges-thus enabling
highly expressive and compact decoding. We use a two-stage message passing
scheme and evaluate the decoder over the pseudo-threshold region. Below the
pseudo-threshold mark, HyperNQ improves the Logical Error Rate (LER) up to 84%
over BP and 50% over GNN-based strategies, demonstrating enhanced performance
over the existing state-of-the-art decoders.

</details>


### [109] [Diffusion Models at the Drug Discovery Frontier: A Review on Generating Small Molecules versus Therapeutic Peptides](https://arxiv.org/abs/2511.00209)
*Yiquan Wang,Yahui Ma,Yuhan Chang,Jiayao Yan,Jialin Zhang,Minnuo Cai,Kai Wei*

Main category: cs.LG

TL;DR: 这篇综述系统比较了扩散模型在小分子和肽类治疗药物设计中的应用，分析了统一去噪框架如何适应不同分子表征、化学空间和设计目标，并讨论了各自面临的挑战和未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成建模中表现出巨大潜力，有望加速和改变传统缓慢且昂贵的药物发现过程。本文旨在系统比较该框架在两种主要治疗模式（小分子和肽类）设计中的应用。

Method: 采用系统比较分析方法，分析扩散模型的迭代去噪统一框架如何适应不同分子表征、化学空间和设计目标，重点关注结构基础设计、功能序列生成和从头结构设计等应用。

Result: 对于小分子，扩散模型在结构基础设计方面表现出色，能生成新颖的、适合口袋的配体并具备所需理化性质，但面临确保化学可合成性的关键障碍。对于肽类，重点转向生成功能序列和设计从头结构，主要挑战是实现对蛋白水解的生物稳定性、确保正确折叠和最小化免疫原性。

Conclusion: 扩散模型的全部潜力将通过弥合这些模式特定差距并将其整合到自动化的闭环设计-构建-测试-学习平台中来释放，从而将范式从化学探索转向靶向创造新型治疗药物。

Abstract: Diffusion models have emerged as a leading framework in generative modeling,
showing significant potential to accelerate and transform the traditionally
slow and costly process of drug discovery. This review provides a systematic
comparison of their application in designing two principal therapeutic
modalities: small molecules and therapeutic peptides. We analyze how a unified
framework of iterative denoising is adapted to the distinct molecular
representations, chemical spaces, and design objectives of each modality. For
small molecules, these models excel at structure-based design, generating
novel, pocket-fitting ligands with desired physicochemical properties, yet face
the critical hurdle of ensuring chemical synthesizability. Conversely, for
therapeutic peptides, the focus shifts to generating functional sequences and
designing de novo structures, where the primary challenges are achieving
biological stability against proteolysis, ensuring proper folding, and
minimizing immunogenicity. Despite these distinct challenges, both domains face
shared hurdles: the need for more accurate scoring functions, the scarcity of
high-quality experimental data, and the crucial requirement for experimental
validation. We conclude that the full potential of diffusion models will be
unlocked by bridging these modality-specific gaps and integrating them into
automated, closed-loop Design-Build-Test-Learn (DBTL) platforms, thereby
shifting the paradigm from chemical exploration to the targeted creation of
novel therapeutics.

</details>


### [110] [Iterative Foundation Model Fine-Tuning on Multiple Rewards](https://arxiv.org/abs/2511.00220)
*Pouya M. Ghari,Simone Sciabola,Ye Wang*

Main category: cs.LG

TL;DR: 提出了一种基于强化学习的多奖励信号微调基础模型的新方法，通过迭代微调策略提升模型性能


<details>
  <summary>Details</summary>
Motivation: 在许多应用如文本生成和药物发现中，单一奖励信号优化可能不够理想，需要多个评估标准

Method: 采用基于强化学习的多奖励信号微调方法，使用迭代微调策略处理多个奖励

Result: 在文本、生物序列和小分子生成等多个领域的实验结果表明，该方法优于现有最先进基线

Conclusion: 多奖励强化学习微调方法有效提升了基础模型在复杂任务中的性能，并提供了理论分析支持

Abstract: Fine-tuning foundation models has emerged as a powerful approach for
generating objects with specific desired properties. Reinforcement learning
(RL) provides an effective framework for this purpose, enabling models to
generate outputs that maximize a given reward function. However, in many
applications such as text generation and drug discovery, it can be suboptimal
to optimize using a single reward signal, as multiple evaluation criteria are
often necessary. This paper proposes a novel reinforcement learning-based
method for fine-tuning foundation models using multiple reward signals. By
employing an iterative fine-tuning strategy across these rewards, our approach
generalizes state-of-the-art RL-based methods. We further provide a theoretical
analysis that offers insights into the performance of multi-reward RL
fine-tuning. Experimental results across diverse domains including text,
biological sequence, and small molecule generation, demonstrate the
effectiveness of the proposed algorithm compared to state-of-the-art baselines.

</details>


### [111] [Melanoma Classification Through Deep Ensemble Learning and Explainable AI](https://arxiv.org/abs/2511.00246)
*Wadduwage Shanika Perera,ABM Islam,Van Vung Pham,Min Kyung An*

Main category: cs.LG

TL;DR: 该论文提出了一种结合集成学习和可解释人工智能(XAI)的机器学习模型，用于提高黑色素瘤早期检测的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在医疗诊断中缺乏可解释性的问题，提高黑色素瘤检测系统的可靠性和可信度。

Method: 使用三种最先进的深度迁移学习网络进行集成学习，并应用XAI技术来解释预测依据。

Result: 开发了一个能够高精度检测黑色素瘤且具有可解释性的系统。

Conclusion: XAI技术能够有效解决深度学习模型在医疗诊断中的黑盒问题，提高预测结果的可靠性和信任度。

Abstract: Melanoma is one of the most aggressive and deadliest skin cancers, leading to
mortality if not detected and treated in the early stages. Artificial
intelligence techniques have recently been developed to help dermatologists in
the early detection of melanoma, and systems based on deep learning (DL) have
been able to detect these lesions with high accuracy. However, the entire
community must overcome the explainability limit to get the maximum benefit
from DL for diagnostics in the healthcare domain. Because of the black box
operation's shortcomings in DL models' decisions, there is a lack of
reliability and trust in the outcomes. However, Explainable Artificial
Intelligence (XAI) can solve this problem by interpreting the predictions of AI
systems. This paper proposes a machine learning model using ensemble learning
of three state-of-the-art deep transfer Learning networks, along with an
approach to ensure the reliability of the predictions by utilizing XAI
techniques to explain the basis of the predictions.

</details>


### [112] [A Tight Lower Bound for Non-stochastic Multi-armed Bandits with Expert Advice](https://arxiv.org/abs/2511.00257)
*Zachary Chase,Shinji Ito,Idan Mehalel*

Main category: cs.LG

TL;DR: 本文确定了非随机多臂老虎机专家建议问题的极小极大最优期望遗憾，通过证明与Kale(2014)上界匹配的下界，得出最优遗憾为Θ(√(TKlog(N/K)))。


<details>
  <summary>Details</summary>
Motivation: 解决非随机多臂老虎机专家建议问题的极小极大最优期望遗憾的精确确定问题，填补现有理论空白。

Method: 通过证明与已有上界匹配的下界来确定极小极大最优期望遗憾，其中K为臂数，N为专家数，T为时间范围。

Result: 确定了极小极大最优期望遗憾为Θ(√(TKlog(N/K)))，其中下界与Kale(2014)的上界完全匹配。

Conclusion: 该工作完整刻画了非随机多臂老虎机专家建议问题的理论极限，为相关算法设计提供了理论依据。

Abstract: We determine the minimax optimal expected regret in the classic
non-stochastic multi-armed bandit with expert advice problem, by proving a
lower bound that matches the upper bound of Kale (2014). The two bounds
determine the minimax optimal expected regret to be $\Theta\left( \sqrt{T K
\log (N/K) } \right)$, where $K$ is the number of arms, $N$ is the number of
experts, and $T$ is the time horizon.

</details>


### [113] [X-TRACK: Physics-Aware xLSTM for Realistic Vehicle Trajectory Prediction](https://arxiv.org/abs/2511.00266)
*Aanchal Rajesh Chugh,Marion Neumeier,Sebastian Dorn*

Main category: cs.LG

TL;DR: 本文提出了基于xLSTM的车辆轨迹预测框架X-TRAJ及其物理感知变体X-TRACK，通过整合车辆运动学约束来生成更真实可行的轨迹。


<details>
  <summary>Details</summary>
Motivation: 虽然xLSTM在时间序列预测中表现出色，但在车辆轨迹预测领域尚未得到充分探索。传统LSTM存在局限性，而xLSTM通过指数门控和增强内存结构能够更好地建模长期时间依赖关系。

Method: 开发了X-TRAJ框架及其物理感知变体X-TRACK，后者在模型学习过程中显式整合车辆运动学约束，确保生成的轨迹符合物理规律。

Result: 在highD和NGSIM数据集上的综合评估表明，X-TRACK优于现有的最先进基线方法。

Conclusion: xLSTM架构结合物理约束能够有效提升车辆轨迹预测的准确性和可行性，为自动驾驶等应用提供了更可靠的轨迹预测解决方案。

Abstract: Recent advancements in Recurrent Neural Network (RNN) architectures,
particularly the Extended Long Short Term Memory (xLSTM), have addressed the
limitations of traditional Long Short Term Memory (LSTM) networks by
introducing exponential gating and enhanced memory structures. These
improvements make xLSTM suitable for time-series prediction tasks as they
exhibit the ability to model long-term temporal dependencies better than LSTMs.
Despite their potential, these xLSTM-based models remain largely unexplored in
the context of vehicle trajectory prediction. Therefore, this paper introduces
a novel xLSTM-based vehicle trajectory prediction framework, X-TRAJ, and its
physics-aware variant, X-TRACK (eXtended LSTM for TRAjectory prediction
Constraint by Kinematics), which explicitly integrates vehicle motion
kinematics into the model learning process. By introducing physical
constraints, the proposed model generates realistic and feasible trajectories.
A comprehensive evaluation on the highD and NGSIM datasets demonstrates that
X-TRACK outperforms state-of-the-art baselines.

</details>


### [114] [Improving the Robustness of Control of Chaotic Convective Flows with Domain-Informed Reinforcement Learning](https://arxiv.org/abs/2511.00272)
*Michiel Straat,Thorben Markmann,Sebastian Peitz,Barbara Hammer*

Main category: cs.LG

TL;DR: 本文提出了一种基于领域知识的强化学习方法，用于控制混沌对流流动，在Rayleigh-Bénard对流系统中实现了热传输的显著减少，并展示了跨流态的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 混沌对流流动在微流体设备和化学反应器等实际系统中普遍存在，但传统控制方法在混沌状态下往往失效。虽然强化学习在层流控制中表现出潜力，但其在混沌和湍流动力学下的泛化能力和鲁棒性尚未得到充分探索。

Method: 引入领域知识增强的强化学习代理，使用近端策略优化算法在不同初始条件和流态下进行训练。在奖励函数中融入领域知识，通过鼓励Bénard单元合并来引导学习过程。

Result: 在层流状态下，领域知识增强的强化学习代理将热传输减少了33%；在混沌流态下仍实现了10%的减少，显著优于传统控制器。与无领域知识的代理相比，该方法产生了稳定流动、训练收敛更快，且无需重新训练即可跨流态泛化。

Conclusion: 优雅的领域知识先验可以显著增强强化学习控制混沌流动的鲁棒性，为实现实际部署迈出了重要一步。

Abstract: Chaotic convective flows arise in many real-world systems, such as
microfluidic devices and chemical reactors. Stabilizing these flows is highly
desirable but remains challenging, particularly in chaotic regimes where
conventional control methods often fail. Reinforcement Learning (RL) has shown
promise for control in laminar flow settings, but its ability to generalize and
remain robust under chaotic and turbulent dynamics is not well explored,
despite being critical for real-world deployment. In this work, we improve the
practical feasibility of RL-based control of such flows focusing on
Rayleigh-B\'enard Convection (RBC), a canonical model for convective heat
transport. To enhance generalization and sample efficiency, we introduce
domain-informed RL agents that are trained using Proximal Policy Optimization
across diverse initial conditions and flow regimes. We incorporate domain
knowledge in the reward function via a term that encourages B\'enard cell
merging, as an example of a desirable macroscopic property. In laminar flow
regimes, the domain-informed RL agents reduce convective heat transport by up
to 33%, and in chaotic flow regimes, they still achieve a 10% reduction, which
is significantly better than the conventional controllers used in practice. We
compare the domain-informed to uninformed agents: Our results show that the
domain-informed reward design results in steady flows, faster convergence
during training, and generalization across flow regimes without retraining. Our
work demonstrates that elegant domain-informed priors can greatly enhance the
robustness of RL-based control of chaotic flows, bringing real-world deployment
closer.

</details>


### [115] [Calibration Across Layers: Understanding Calibration Evolution in LLMs](https://arxiv.org/abs/2511.00280)
*Abhinav Joshi,Areeb Ahmad,Ashutosh Modi*

Main category: cs.LG

TL;DR: 研究发现LLMs的校准能力在网络深度中演化，上层存在置信度校正阶段，并识别出残差流中的低维校准方向，扰动该方向可显著改善校准指标而不损害准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管先前研究发现深度神经网络通常过度自信，但LLMs表现出内在的校准能力。本研究旨在从网络深度演化的角度补充理解LLMs的校准机制。

Method: 分析多个开源权重模型在MMLU基准上的表现，研究校准如何随网络深度演化，识别上层层的置信度校正阶段和残差流中的低维校准方向。

Result: 发现上层存在主动重新校准模型置信度的阶段，识别出残差流中的低维校准方向，扰动该方向可显著改善ECE和MCE等校准指标而不影响准确性。

Conclusion: 校准是分布式现象，在整个网络前向传播过程中形成，而不仅限于最终投影层，这为理解LLMs内部的置信度调节机制提供了新视角。

Abstract: Large Language Models (LLMs) have demonstrated inherent calibration
capabilities, where predicted probabilities align well with correctness,
despite prior findings that deep neural networks are often overconfident.
Recent studies have linked this behavior to specific components in the final
layer, such as entropy neurons and the unembedding matrix null space. In this
work, we provide a complementary perspective by investigating how calibration
evolves throughout the network depth. Analyzing multiple open-weight models on
the MMLU benchmark, we uncover a distinct confidence correction phase in the
upper/later layers, where model confidence is actively recalibrated after
decision certainty has been reached. Furthermore, we identify a low-dimensional
calibration direction in the residual stream whose perturbation significantly
improves calibration metrics (ECE and MCE) without harming accuracy. Our
findings suggest that calibration is a distributed phenomenon, shaped
throughout the network forward pass, not just in its final projection,
providing new insights into how confidence-regulating mechanisms operate within
LLMs.

</details>


### [116] [A systematic evaluation of uncertainty quantification techniques in deep learning: a case study in photoplethysmography signal analysis](https://arxiv.org/abs/2511.00301)
*Ciaran Bench,Oskar Pfeffer,Vivek Desai,Mohammad Moulaeifard,Loïc Coquelin,Peter H. Charlton,Nils Strodthoff,Nando Hegemann,Philip J. Aston,Andrew Thompson*

Main category: cs.LG

TL;DR: 本文比较了8种不确定性量化技术在医疗时间序列数据（特别是PPG传感器数据）上的表现，重点关注心房颤动检测和血压回归任务，强调需要根据实际应用场景选择合适的不确定性评估方法。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在医疗时间序列数据上的应用存在实际部署时性能不佳的风险，可靠的不确定性量化可以为临床医生提供模型输出可信度的指导，因此需要比较不同不确定性量化方法的有效性。

Method: 实现了8种不确定性量化技术，应用于两个临床相关预测任务：心房颤动检测（分类）和两种血压回归变体，并制定了全面的评估程序来严格比较这些方法。

Result: 不同技术的不确定性可靠性呈现复杂图景，最优方法取决于选择的不确定性表达方式、评估指标和可靠性评估尺度。局部校准和适应性评估提供了实际相关的模型行为洞察。

Conclusion: 评估不确定性量化技术的标准应适应模型的实际使用场景，在每位患者数据量较少的情况下，应优先实现小尺度可靠性，同时尽可能保持预测性能。

Abstract: In principle, deep learning models trained on medical time-series, including
wearable photoplethysmography (PPG) sensor data, can provide a means to
continuously monitor physiological parameters outside of clinical settings.
However, there is considerable risk of poor performance when deployed in
practical measurement scenarios leading to negative patient outcomes. Reliable
uncertainties accompanying predictions can provide guidance to clinicians in
their interpretation of the trustworthiness of model outputs. It is therefore
of interest to compare the effectiveness of different approaches. Here we
implement an unprecedented set of eight uncertainty quantification (UQ)
techniques to models trained on two clinically relevant prediction tasks:
Atrial Fibrillation (AF) detection (classification), and two variants of blood
pressure regression. We formulate a comprehensive evaluation procedure to
enable a rigorous comparison of these approaches. We observe a complex picture
of uncertainty reliability across the different techniques, where the most
optimal for a given task depends on the chosen expression of uncertainty,
evaluation metric, and scale of reliability assessed. We find that assessing
local calibration and adaptivity provides practically relevant insights about
model behaviour that otherwise cannot be acquired using more commonly
implemented global reliability metrics. We emphasise that criteria for
evaluating UQ techniques should cater to the model's practical use case, where
the use of a small number of measurements per patient places a premium on
achieving small-scale reliability for the chosen expression of uncertainty,
while preserving as much predictive performance as possible.

</details>


### [117] [A Technical Exploration of Causal Inference with Hybrid LLM Synthetic Data](https://arxiv.org/abs/2511.00318)
*Dana Kim,Yichen Xu,Tiffany Lin*

Main category: cs.LG

TL;DR: 本文提出了一种结合模型协变量合成和因果结构保持的混合框架，用于生成保留因果效应的合成表格数据，解决了现有方法在估计平均处理效应(ATE)时的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型(LLM)和GAN方法在生成合成表格数据时，虽然能达到较高的预测保真度，但往往无法保持关键的因果参数如平均处理效应(ATE)，限制了其在因果分析中的应用。

Method: 提出混合生成框架：结合基于模型的协变量合成（通过距离到最近记录过滤监控）、分别学习的倾向评分和结果模型，确保(W, A, Y)三元组保留底层因果结构；引入合成配对策略缓解正性违例；利用无限合成样本建立复杂协变量分布下传统估计器(IPTW, AIPW, substitution)的评估协议。

Result: 该框架能够生成保留因果结构的合成数据，为支持稳健因果分析的LLM驱动数据管道奠定基础。

Conclusion: 这项工作为基于LLM的数据管道支持稳健因果分析奠定了基础，提供了保留因果效应的合成数据生成方法。

Abstract: Large Language Models (LLMs) offer a flexible means to generate synthetic
tabular data, yet existing approaches often fail to preserve key causal
parameters such as the average treatment effect (ATE). In this technical
exploration, we first demonstrate that state-of-the-art synthetic data
generators, both GAN- and LLM-based, can achieve high predictive fidelity while
substantially misestimating causal effects. To address this gap, we propose a
hybrid generation framework that combines model-based covariate synthesis
(monitored via distance-to-closest-record filtering) with separately learned
propensity and outcome models, thereby ensuring that (W, A, Y) triplets retain
their underlying causal structure. We further introduce a synthetic pairing
strategy to mitigate positivity violations and a realistic evaluation protocol
that leverages unlimited synthetic samples to benchmark traditional estimators
(IPTW, AIPW, substitution) under complex covariate distributions. This work
lays the groundwork for LLM-powered data pipelines that support robust causal
analysis. Our code is available at
https://github.com/Xyc-arch/llm-synthetic-for-causal-inference.git.

</details>


### [118] [Reject Only Critical Tokens: Pivot-Aware Speculative Decoding](https://arxiv.org/abs/2511.00351)
*Amir Ziashahabi,Yavuz Faruk Bakman,Duygu Nur Yaldiz,Mostafa El-Khamy,Sai Praneeth Karimireddy,Salman Avestimehr*

Main category: cs.LG

TL;DR: 提出了一种新的解码策略——Pivot-Aware Speculative Decoding，通过只拒绝会导致最终输出效用下降的关键token（pivot tokens）来提升接受率，在保持任务性能的同时实现高达2.5倍的加速。


<details>
  <summary>Details</summary>
Motivation: 传统的Speculative Decoding要求输出分布与目标模型完全匹配，这导致接受率过低，限制了加速潜力。作者认为在实际应用中，任务特定的性能（如代码正确性、事实准确性）比采样分布更重要。

Method: 提出Pivot-Aware Speculative Decoding策略，识别关键token（pivot tokens），只拒绝那些会导致最终输出效用下降的token。训练轻量级分类器来检测这些关键token。

Result: 在多个数据集上的评估显示，该方法能够实现高达2.5倍的加速，同时保持与目标模型相当的任务性能。

Conclusion: 通过放宽分布匹配要求，专注于任务效用匹配，可以显著提高解码效率，为实际LLM应用提供更好的加速方案。

Abstract: Speculative Decoding (SD) ensures that the output matches the target model's
distribution exactly. However, we argue that this distribution matching
requirement is too stringent and results in unnecessarily low acceptance rates,
limiting potential speedups. Instead, we advocate a reformulation of the
decoding objective: the proposed decoding strategy should match the expected
utility, i.e., the task-specific performance, of the target model. This
perspective also aligns better with real-world use cases of LLMs, where utility
(e.g., code correctness, factual accuracy) is often more important than
sampling distribution. Based on this reformulation, we propose a novel decoding
strategy: Pivot-Aware Speculative Decoding, which rejects only those tokens
that would lead to a utility drop in the final output. We refer to these
critical tokens as pivot tokens. We propose a method for labeling tokens as
pivotal or non-pivotal and train a lightweight classifier to detect them. This
method can be viewed as a relaxed version of standard SD, which offers much
higher acceptance while preserving utility. We evaluate our method across
various datasets, demonstrating that we can achieve up to $2.5\times$ speedup
with comparable utility. Source code is available at
https://github.com/amir-zsh/PAD.

</details>


### [119] [Toward Unifying Group Fairness Evaluation from a Sparsity Perspective](https://arxiv.org/abs/2511.00359)
*Zhecheng Sheng,Jiawei Zhang,Enmao Diao*

Main category: cs.LG

TL;DR: 提出了一个基于稀疏性的统一框架来评估算法公平性，该框架与现有公平性标准一致，适用于多种机器学习任务。


<details>
  <summary>Details</summary>
Motivation: 算法公平性是机器学习中的重要挑战，现有公平性标准缺乏跨不同机器学习问题的泛化性。

Method: 研究各种稀疏性度量在促进公平性方面的联系和差异，提出基于稀疏性的统一评估框架。

Result: 通过在多种数据集和偏差缓解方法上的广泛实验，证明了所提框架作为评估指标的有效性。

Conclusion: 通过稀疏性和社会公平的视角为算法公平性提供了新视角，对公平性研究和应用具有广泛影响潜力。

Abstract: Ensuring algorithmic fairness remains a significant challenge in machine
learning, particularly as models are increasingly applied across diverse
domains. While numerous fairness criteria exist, they often lack
generalizability across different machine learning problems. This paper
examines the connections and differences among various sparsity measures in
promoting fairness and proposes a unified sparsity-based framework for
evaluating algorithmic fairness. The framework aligns with existing fairness
criteria and demonstrates broad applicability to a wide range of machine
learning tasks. We demonstrate the effectiveness of the proposed framework as
an evaluation metric through extensive experiments on a variety of datasets and
bias mitigation methods. This work provides a novel perspective to algorithmic
fairness by framing it through the lens of sparsity and social equity, offering
potential for broader impact on fairness research and applications.

</details>


### [120] [Balancing Interpretability and Performance in Motor Imagery EEG Classification: A Comparative Study of ANFIS-FBCSP-PSO and EEGNet](https://arxiv.org/abs/2511.00369)
*Farjana Aktar,Mohd Ruhul Ameen,Akif Islam,Md Ekramul Hamid*

Main category: cs.LG

TL;DR: 该论文比较了模糊推理方法(ANFIS-FBCSP-PSO)与深度学习基准(EEGNet)在运动想象EEG分类中的表现，发现模糊方法在个体内测试中表现更好，而深度学习方法在跨个体测试中泛化能力更强。


<details>
  <summary>Details</summary>
Motivation: 解决运动想象EEG分类中准确性和可解释性难以兼顾的关键挑战，为BCI系统设计提供实用指导。

Method: 使用BCI Competition IV-2a数据集，比较ANFIS-FBCSP-PSO（结合滤波器组共同空间模式特征提取和粒子群优化的模糊IF-THEN规则）与EEGNet（直接从原始EEG数据学习层次时空表示）。

Result: 个体内实验：模糊神经网络模型表现更好（准确率68.58%±13.76%，kappa=58.04%±18.43）；跨个体测试：深度模型泛化能力更强（准确率68.20%±12.13%，kappa=57.33%±16.22）。

Conclusion: 研究为根据设计目标（可解释性或跨用户鲁棒性）选择MI-BCI系统提供实用指导，未来基于Transformer和混合神经符号框架的研究有望推进透明EEG解码。

Abstract: Achieving both accurate and interpretable classification of motor imagery EEG
remains a key challenge in brain computer interface (BCI) research. This paper
compares a transparent fuzzy reasoning approach (ANFIS-FBCSP-PSO) with a deep
learning benchmark (EEGNet) using the BCI Competition IV-2a dataset. The ANFIS
pipeline combines filter bank common spatial pattern feature extraction with
fuzzy IF-THEN rules optimized via particle swarm optimization, while EEGNet
learns hierarchical spatial temporal representations directly from raw EEG
data. In within-subject experiments, the fuzzy neural model performed better
(68.58 percent +/- 13.76 percent accuracy, kappa = 58.04 percent +/- 18.43),
while in cross-subject (LOSO) tests, the deep model exhibited stronger
generalization (68.20 percent +/- 12.13 percent accuracy, kappa = 57.33 percent
+/- 16.22). The study provides practical guidance for selecting MI-BCI systems
according to design goals: interpretability or robustness across users. Future
investigations into transformer based and hybrid neuro symbolic frameworks are
expected to advance transparent EEG decoding.

</details>


### [121] [PolyRecommender: A Multimodal Recommendation System for Polymer Discovery](https://arxiv.org/abs/2511.00375)
*Xin Wang,Yunhao Xiao,Rui Qiao*

Main category: cs.LG

TL;DR: PolyRecommender是一个多模态聚合物发现框架，结合了PolyBERT的化学语言表示和图编码器的分子图表示，通过多模态嵌入进行候选聚合物的检索和排序。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够整合不同模态信息的多模态框架，以提升聚合物发现过程中的检索效率和排序鲁棒性。

Method: 首先使用基于语言的相似性检索候选聚合物，然后使用融合的多模态嵌入根据多个目标属性对候选聚合物进行排序。

Result: 通过利用两种模态中编码的互补知识，PolyRecommender实现了跨相关聚合物属性的高效检索和鲁棒排序。

Conclusion: 该工作建立了一个可推广的多模态范式，推动了AI引导的下一代聚合物发现设计。

Abstract: We introduce PolyRecommender, a multimodal discovery framework that
integrates chemical language representations from PolyBERT with molecular
graph-based representations from a graph encoder. The system first retrieves
candidate polymers using language-based similarity and then ranks them using
fused multimodal embeddings according to multiple target properties. By
leveraging the complementary knowledge encoded in both modalities,
PolyRecommender enables efficient retrieval and robust ranking across related
polymer properties. Our work establishes a generalizable multimodal paradigm,
advancing AI-guided design for the discovery of next-generation polymers.

</details>


### [122] [UME-R1: Exploring Reasoning-Driven Generative Multimodal Embeddings](https://arxiv.org/abs/2511.00405)
*Zhibin Lan,Liqiang Niu,Fandong Meng,Jie Zhou,Jinsong Su*

Main category: cs.LG

TL;DR: 提出UME-R1生成式多模态嵌入框架，通过两阶段训练策略统一嵌入任务于生成范式，显著超越传统判别式嵌入模型性能


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs的多模态嵌入模型本质上是判别式的，限制了其从推理驱动的生成范式中获益的能力

Method: 两阶段训练策略：冷启动监督微调赋予模型推理能力，使其能生成判别式和生成式嵌入；后续强化学习增强推理并优化生成式嵌入质量

Result: 在MMEB-V2基准的78个任务上显著优于传统判别式嵌入模型，生成式嵌入通过利用MLLMs的强大生成推理能力实现性能大幅提升

Conclusion: 生成式嵌入为更可解释、推理驱动的生成式多模态嵌入奠定了基础，展示了推理时扩展潜力

Abstract: The remarkable success of multimodal large language models (MLLMs) has driven
advances in multimodal embeddings, yet existing models remain inherently
discriminative, limiting their ability to benefit from reasoning-driven
generation paradigm. In this work, we pioneer the exploration of generative
embeddings, unifying embedding tasks within a generative paradigm. We propose
UME-R1, a universal multimodal embedding framework consisting of a two-stage
training strategy: a cold-start supervised fine-tuning equips the model with
reasoning capabilities and enables it to generate both discriminative and
generative embeddings; a subsequent reinforcement learning enhances reasoning
and further optimizes generative embedding quality. This pioneering work
reveals four key insights: 1) generative embeddings unlock substantial
performance gains over conventional discriminative embeddings by leveraging the
powerful generative reasoning capabilities of MLLMs; 2) discriminative and
generative embeddings are complementary, whose combined oracle performance far
exceeding that of either alone; 3) RL can effectively enhance generative
embeddings, establishing a scalable optimization paradigm.; 4) repeated
sampling at inference boosts downstream task coverage (pass@k), highlighting
the inference-time scalability potential of generative embeddings. Evaluated on
the MMEB-V2 benchmark across 78 tasks spanning video, image, and visual
documents, UME-R1 significantly outperforms conventional discriminative
embedding models and offers a foundation for more interpretable,
reasoning-driven generative multimodal embeddings. Our code, models, and
datasets will be publicly available at https://github.com/XMUDeepLIT/UME-R1.

</details>


### [123] [Enhancing Adversarial Transferability by Balancing Exploration and Exploitation with Gradient-Guided Sampling](https://arxiv.org/abs/2511.00411)
*Zenghao Niu,Weicheng Xie,Siyang Song,Zitong Yu,Feng Liu,Linlin Shen*

Main category: cs.LG

TL;DR: 提出Gradient-Guided Sampling (GGS)方法，通过梯度引导采样平衡对抗攻击的利用性（攻击强度）和探索性（跨模型泛化能力），解决传统方法在这两个目标间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 对抗攻击在跨模型架构的迁移场景中面临利用性（最大化攻击强度）和探索性（增强跨模型泛化）之间的根本困境。传统动量方法过度优先利用性，而近期内迭代采样方法过度优先探索性。

Method: 基于MI-FGSM，GGS引入内迭代随机采样，并使用前一次内迭代的梯度来引导采样方向（采样幅度由随机分布决定），使对抗样本位于既平坦又具有较高局部最大值的平衡区域。

Result: 在多个DNN架构和多模态大语言模型上的综合实验表明，该方法在迁移攻击方面优于现有最先进方法。

Conclusion: GGS方法通过梯度引导采样有效平衡了对抗攻击的利用性和探索性，在保持强攻击效力的同时提升了跨模型泛化能力。

Abstract: Adversarial attacks present a critical challenge to deep neural networks'
robustness, particularly in transfer scenarios across different model
architectures. However, the transferability of adversarial attacks faces a
fundamental dilemma between Exploitation (maximizing attack potency) and
Exploration (enhancing cross-model generalization). Traditional momentum-based
methods over-prioritize Exploitation, i.e., higher loss maxima for attack
potency but weakened generalization (narrow loss surface). Conversely, recent
methods with inner-iteration sampling over-prioritize Exploration, i.e.,
flatter loss surfaces for cross-model generalization but weakened attack
potency (suboptimal local maxima). To resolve this dilemma, we propose a simple
yet effective Gradient-Guided Sampling (GGS), which harmonizes both objectives
through guiding sampling along the gradient ascent direction to improve both
sampling efficiency and stability. Specifically, based on MI-FGSM, GGS
introduces inner-iteration random sampling and guides the sampling direction
using the gradient from the previous inner-iteration (the sampling's magnitude
is determined by a random distribution). This mechanism encourages adversarial
examples to reside in balanced regions with both flatness for cross-model
generalization and higher local maxima for strong attack potency. Comprehensive
experiments across multiple DNN architectures and multimodal large language
models (MLLMs) demonstrate the superiority of our method over state-of-the-art
transfer attacks. Code is made available at https://github.com/anuin-cat/GGS.

</details>


### [124] [Tree Training: Accelerating Agentic LLMs Training via Shared Prefix Reuse](https://arxiv.org/abs/2511.00413)
*Shaojie Wang,Jinghui Wang,Yinghan Cui,Xuxing Chen,Chao Wang,Liang Huang,Xiaojiang Zhang,Junyi Peng,Li Wan,Haotian Zhang,Bin Chen*

Main category: cs.LG

TL;DR: 提出Tree Training方法，通过树打包和梯度恢复技术，在智能体LLM训练中重用共享前缀计算，显著提升训练效率


<details>
  <summary>Details</summary>
Motivation: 当前训练流程将树状轨迹分解为独立线性序列，导致共享前缀被重复计算，造成计算效率低下

Method: 使用树打包技术重用轨迹间的共享计算，配合梯度恢复确保重用前缀的正确梯度传播

Result: 在多个开源模型上实验显示，总训练时间最多减少3.9倍

Conclusion: Tree Training方法能够显著提升智能体LLM SFT和RL训练的效率

Abstract: In agentic LLM scenarios, an agent's interaction process during a single
rollout often exhibits branching behaviors. Due to memory retrieval and
concurrent tool executions at certain decision points, the token trajectory of
one task evolves into a tree-like structure rather than a linear sequence.
However, current training pipelines decompose such tree-structured trajectories
into separate linear segments, treating each branch as an independent sequence.
As a result, shared prefixes across these branches are repeatedly recomputed
during both forward and backward passes. To address this inefficiency, we
propose Tree Training, a paradigm that computes each shared prefix only once
and reuses its intermediate results across related branches during both forward
and backward passes, substantially improving computation efficiency in
large-scale agentic training. This is achieved via (i) Tree Packing, which
efficiently reuses shared computations across trajectories, and (ii) Gradient
Restoration, which ensures correct gradient propagation across reused prefixes.
Experiments on multiple open-source models demonstrate up to 3.9x reduction in
total training time, enabling more efficient agentic LLM SFT and RL training.

</details>


### [125] [Structure-Preserving Physics-Informed Neural Network for the Korteweg--de Vries (KdV) Equation](https://arxiv.org/abs/2511.00418)
*Victory Obieke,Emmanuel Oguadimma*

Main category: cs.LG

TL;DR: 提出了一种结构保持的PINN框架，通过嵌入质量守恒和哈密顿能量守恒到损失函数中，结合正弦激活函数，解决了KdV方程长期积分中物理不变量保持的问题。


<details>
  <summary>Details</summary>
Motivation: 传统PINNs在求解非线性偏微分方程时，往往无法在长期积分过程中保持关键的物理不变量，导致物理不一致和能量不稳定的演化。

Method: 将质量守恒和哈密顿能量守恒直接嵌入损失函数，使用正弦激活函数增强频谱表达能力，准确捕捉KdV孤子的振荡和色散特性。

Result: 在单孤子传播、双孤子相互作用和余弦脉冲初始化等代表性案例中，成功重现了KdV动力学的标志性行为，同时保持了守恒不变量。消融研究表明，结合不变约束优化和正弦特征映射可以加速收敛、提高长期稳定性并减轻漂移。

Conclusion: 计算高效的不变感知正则化与正弦表示相结合，可以为哈密顿偏微分方程（如KdV方程）产生稳健、能量一致的PINNs。

Abstract: Physics-Informed Neural Networks (PINNs) offer a flexible framework for
solving nonlinear partial differential equations (PDEs), yet conventional
implementations often fail to preserve key physical invariants during long-term
integration. This paper introduces a \emph{structure-preserving PINN} framework
for the nonlinear Korteweg--de Vries (KdV) equation, a prototypical model for
nonlinear and dispersive wave propagation. The proposed method embeds the
conservation of mass and Hamiltonian energy directly into the loss function,
ensuring physically consistent and energy-stable evolution throughout training
and prediction. Unlike standard \texttt{tanh}-based
PINNs~\cite{raissi2019pinn,wang2022modifiedpinn}, our approach employs
sinusoidal activation functions that enhance spectral expressiveness and
accurately capture the oscillatory and dispersive nature of KdV solitons.
Through representative case studies -- including single-soliton propagation
(shape-preserving translation), two-soliton interaction (elastic collision with
phase shift), and cosine-pulse initialization (nonlinear dispersive breakup) --
the model successfully reproduces hallmark behaviors of KdV dynamics while
maintaining conserved invariants. Ablation studies demonstrate that combining
invariant-constrained optimization with sinusoidal feature mappings accelerates
convergence, improves long-term stability, and mitigates drift without
multi-stage pretraining. These results highlight that computationally
efficient, invariant-aware regularization coupled with sinusoidal
representations yields robust, energy-consistent PINNs for Hamiltonian partial
differential equations such as the KdV equation.

</details>


### [126] [Bootstrap Off-policy with World Model](https://arxiv.org/abs/2511.00423)
*Guojian Zhan,Likun Wang,Xiangteng Zhang,Jiaxin Gao,Masayoshi Tomizuka,Shengbo Eben Li*

Main category: cs.LG

TL;DR: BOOM框架通过引导循环将规划与离线学习紧密结合，使用联合学习的世界模型来提升强化学习的样本效率和最终性能。


<details>
  <summary>Details</summary>
Motivation: 在线规划虽然能提高强化学习的样本效率和性能，但会导致收集数据与实际策略行为之间的差异，从而影响模型学习和策略改进。

Method: 提出BOOM框架，通过引导循环将策略初始化规划器，规划器通过行为对齐引导策略。使用联合学习的世界模型进行轨迹模拟和价值目标提供，核心是使用规划器的非参数动作分布引导策略的无似然对齐损失。

Result: 在DeepMind Control Suite和Humanoid-Bench等高维环境上的实验表明，BOOM在训练稳定性和最终性能方面达到了最先进的结果。

Conclusion: BOOM通过紧密集成规划和离线学习，有效解决了规划带来的数据-策略行为差异问题，显著提升了强化学习的性能。

Abstract: Online planning has proven effective in reinforcement learning (RL) for
improving sample efficiency and final performance. However, using planning for
environment interaction inevitably introduces a divergence between the
collected data and the policy's actual behaviors, degrading both model learning
and policy improvement. To address this, we propose BOOM (Bootstrap Off-policy
with WOrld Model), a framework that tightly integrates planning and off-policy
learning through a bootstrap loop: the policy initializes the planner, and the
planner refines actions to bootstrap the policy through behavior alignment.
This loop is supported by a jointly learned world model, which enables the
planner to simulate future trajectories and provides value targets to
facilitate policy improvement. The core of BOOM is a likelihood-free alignment
loss that bootstraps the policy using the planner's non-parametric action
distribution, combined with a soft value-weighted mechanism that prioritizes
high-return behaviors and mitigates variability in the planner's action quality
within the replay buffer. Experiments on the high-dimensional DeepMind Control
Suite and Humanoid-Bench show that BOOM achieves state-of-the-art results in
both training stability and final performance. The code is accessible at
https://github.com/molumitu/BOOM_MBRL.

</details>


### [127] [Region-Aware Reconstruction Strategy for Pre-training fMRI Foundation Model](https://arxiv.org/abs/2511.00443)
*Ruthwik Reddy Doodipala,Pankaj Pandey,Carolina Torres Rojas,Manob Jyoti Saikia,Ranganatha Sitaram*

Main category: cs.LG

TL;DR: 该研究提出了一种基于ROI引导掩码策略的fMRI基础模型预训练方法，相比传统随机掩码在ADHD分类任务上提升了4.23%的准确率。


<details>
  <summary>Details</summary>
Motivation: 推动神经影像基础模型发展，利用大规模异质性脑成像数据集，探索超越随机区域掩码的区域感知重建策略。

Method: 使用AAL3图谱的ROI引导掩码策略，在4D fMRI全脑体积上选择性掩码语义连贯的脑区，进行自监督预训练。

Result: 在ADHD-200数据集上，该方法比传统随机掩码在ADHD分类准确率上提升4.23%，边缘系统和脑小脑区域对重建保真度和模型表示贡献最大。

Conclusion: 在模型预训练中掩码解剖区域不仅能增强可解释性，还能产生更鲁棒和可区分的表示，未来将扩展到更多神经影像数据集和开发新的损失函数。

Abstract: The emergence of foundation models in neuroimaging is driven by the
increasing availability of large-scale and heterogeneous brain imaging
datasets. Recent advances in self-supervised learning, particularly
reconstruction-based objectives, have demonstrated strong potential for
pretraining models that generalize effectively across diverse downstream
functional MRI (fMRI) tasks. In this study, we explore region-aware
reconstruction strategies for a foundation model in resting-state fMRI, moving
beyond approaches that rely on random region masking. Specifically, we
introduce an ROI-guided masking strategy using the Automated Anatomical
Labelling Atlas (AAL3), applied directly to full 4D fMRI volumes to selectively
mask semantically coherent brain regions during self-supervised pretraining.
Using the ADHD-200 dataset comprising 973 subjects with resting-state fMRI
scans, we show that our method achieves a 4.23% improvement in classification
accuracy for distinguishing healthy controls from individuals diagnosed with
ADHD, compared to conventional random masking. Region-level attribution
analysis reveals that brain volumes within the limbic region and cerebellum
contribute most significantly to reconstruction fidelity and model
representation. Our results demonstrate that masking anatomical regions during
model pretraining not only enhances interpretability but also yields more
robust and discriminative representations. In future work, we plan to extend
this approach by evaluating it on additional neuroimaging datasets, and
developing new loss functions explicitly derived from region-aware
reconstruction objectives. These directions aim to further improve the
robustness and interpretability of foundation models for functional
neuroimaging.

</details>


### [128] [Deep Learning Approach to Anomaly Detection in Enterprise ETL Processes with Autoencoders](https://arxiv.org/abs/2511.00462)
*Xin Chen,Saili Uday Gadgil,Kangning Gao,Yi Hu,Cong Nie*

Main category: cs.LG

TL;DR: 提出基于深度自编码器的异常检测方法，用于检测企业级ETL数据流中的多种异常类型，通过编码器-解码器结构和正则化约束实现高效异常识别。


<details>
  <summary>Details</summary>
Motivation: 解决企业级ETL数据流中经常出现的异常问题，包括延迟、缺失值、重复加载和突发异常变化等，需要稳定可靠的异常检测机制来保障企业数据处理质量。

Method: 采用深度自编码器结构，将高维输入压缩为潜在表示并重构，利用重构误差衡量异常程度；在潜在空间引入正则化约束增强特征稀疏性和分布学习，提升复杂数据流中的鲁棒性。

Result: 在不同超参数设置、环境变化和数据特征下的系统分析表明，该方法在AUC、ACC、Precision和Recall等指标上表现优异，能够有效捕获ETL数据流的潜在分布模式。

Conclusion: 基于深度自编码器的检测机制能够准确识别企业级ETL数据流中的多种异常，为企业数据处理和智能分析提供可靠支持。

Abstract: An anomaly detection method based on deep autoencoders is proposed to address
anomalies that often occur in enterprise-level ETL data streams. The study
first analyzes multiple types of anomalies in ETL processes, including delays,
missing values, duplicate loading, and sudden abnormal changes, and applies
data standardization and feature modeling to ensure stable and usable inputs.
In the method design, the encoder-decoder structure compresses high-dimensional
inputs into latent representations and reconstructs them, while reconstruction
error is used to measure anomaly levels. Regularization constraints are
introduced in the latent space to enhance feature sparsity and distribution
learning, thereby improving robustness in complex data streams. Systematic
analyses under different hyperparameter settings, environmental changes, and
data characteristics show that the proposed method achieves superior
performance in AUC, ACC, Precision, and Recall. The results demonstrate that
the deep autoencoder-based detection mechanism can effectively capture latent
distribution patterns in enterprise-level ETL data streams and accurately
identify diverse anomalies, providing reliable support for enterprise data
processing and intelligent analysis.

</details>


### [129] [Why Federated Optimization Fails to Achieve Perfect Fitting? A Theoretical Perspective on Client-Side Optima](https://arxiv.org/abs/2511.00469)
*Zhongxiang Lei,Qi Yang,Ping Qiu,Gang Zhang,Yuanchi Ma,Jinyan Liu*

Main category: cs.LG

TL;DR: 本文从理论角度解释了联邦学习中数据异质性导致性能下降的原因，指出异质数据会产生不同的局部最优解，这既提高了全局目标的下界，又导致模型在训练后期在区域内振荡而非收敛到单一最优解。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习算法虽然在理论和实践中都能保证收敛，但在数据异质性下性能下降的原因尚不清楚。本文旨在填补这一理论空白，解释为什么非独立同分布数据会导致性能退化。

Method: 引入假设：异质客户端数据会产生不同的局部最优解，并基于此假设推导出两个关键后果：1) 客户端局部最优解之间的距离提高了全局目标的下界；2) 在训练后期，全局模型在区域内振荡而非收敛到单一最优解。

Result: 理论分析表明，数据异质性使得完美拟合所有客户端数据变得不可能，且模型在最终训练阶段无法完全收敛，限制了其数据拟合能力。这些发现在多个任务和神经网络架构的实验中得到了验证。

Conclusion: 本文提供了一个原则性的理论框架来解释联邦学习中非独立同分布数据导致的性能下降问题，为理解这一现象提供了新的理论视角。

Abstract: Federated optimization is a constrained form of distributed optimization that
enables training a global model without directly sharing client data. Although
existing algorithms can guarantee convergence in theory and often achieve
stable training in practice, the reasons behind performance degradation under
data heterogeneity remain unclear. To address this gap, the main contribution
of this paper is to provide a theoretical perspective that explains why such
degradation occurs. We introduce the assumption that heterogeneous client data
lead to distinct local optima, and show that this assumption implies two key
consequences: 1) the distance among clients' local optima raises the lower
bound of the global objective, making perfect fitting of all client data
impossible; and 2) in the final training stage, the global model oscillates
within a region instead of converging to a single optimum, limiting its ability
to fully fit the data. These results provide a principled explanation for
performance degradation in non-iid settings, which we further validate through
experiments across multiple tasks and neural network architectures. The
framework used in this paper is open-sourced at:
https://github.com/NPCLEI/fedtorch.

</details>


### [130] [Variational Autoencoder for Calibration: A New Approach](https://arxiv.org/abs/2511.00475)
*Travis Barrett,Amit Kumar Mishra,Joyce Mwangama*

Main category: cs.LG

TL;DR: 提出了一种基于变分自编码器（VAE）的传感器校准新方法，通过将潜在空间作为校准输出来校准传感器数据，并在多传感器气体数据集上进行了概念验证。


<details>
  <summary>Details</summary>
Motivation: 探索使用VAE进行传感器校准的可能性，利用其潜在空间特性来实现同时进行数据重建和校准的双重功能。

Method: 使用变分自编码器框架，将潜在空间训练为校准输出，并在现有的多传感器气体数据集上进行训练和验证。

Result: 提出的校准VAE能够同时作为校准模型和自编码器工作，从校准输出和重建输出都能产生与真实数据统计相似的输出。

Conclusion: 该方法展示了VAE在传感器校准中的潜力，为未来测试和扩展工作奠定了基础。

Abstract: In this paper we present a new implementation of a Variational Autoencoder
(VAE) for the calibration of sensors. We propose that the VAE can be used to
calibrate sensor data by training the latent space as a calibration output. We
discuss this new approach and show a proof-of-concept using an existing
multi-sensor gas dataset. We show the performance of the proposed calibration
VAE and found that it was capable of performing as calibration model while
performing as an autoencoder simultaneously. Additionally, these models have
shown that they are capable of creating statistically similar outputs from both
the calibration output as well as the reconstruction output to their respective
truth data. We then discuss the methods of future testing and planned expansion
of this work.

</details>


### [131] [Reasoning Planning for Language Models](https://arxiv.org/abs/2511.00521)
*Bao Nguyen,Hieu Trung Nguyen,Ruifeng She,Xiaojin Fu,Viet Anh Nguyen*

Main category: cs.LG

TL;DR: EPIC框架通过对比学习构建共享表示空间，选择最优推理方法，在提高准确率的同时减少计算开销


<details>
  <summary>Details</summary>
Motivation: 现有方法通常生成多个候选响应并使用聚合策略选择答案，但假设更多候选答案能带来更高准确率。本文重新审视这一假设，通过理论分析发现固定生成分布和候选数量下的准确率边界

Method: 提出EPIC框架，使用对比学习构建共享表示空间来捕捉模型推理能力和查询-方法兼容性，将概率边界作为正则化器纳入效用驱动的优化中，平衡准确率和计算成本

Result: 在多样化数学推理任务上的实验表明，EPIC能持续选择最优推理方法，提高准确率的同时减少计算开销

Conclusion: EPIC框架通过理论指导的方法选择，在推理任务中实现了准确率和计算效率的平衡

Abstract: Selecting an appropriate reasoning method for a given query remains a key
challenge in language model generation. Existing approaches typically generate
multiple candidate responses and use an aggregation strategy to select the
output answer, often assuming that more candidate answers yield higher
accuracy. We revisit this assumption through a rigorous theoretical analysis,
deriving accuracy bounds for standard aggregation methods under fixed
generation distributions and candidate sizes. Building on these insights, we
introduce EPIC, an Ensemble Planning with Contrastive learning framework to
learn a shared representation space that captures both model reasoning
abilities and query-method compatibility. EPIC incorporates our probability
bounds as a regularizer in a utility-driven optimization that balances accuracy
and computational cost. Experiments on diverse mathematical reasoning tasks
show that EPIC consistently selects optimal reasoning methods, improving
accuracy while reducing computational overhead. Our code can be found at
https://github.com/nguyenngocbaocmt02/EPIC.

</details>


### [132] [Air Pollution Forecasting in Bucharest](https://arxiv.org/abs/2511.00532)
*Dragoş-Andrei Şerban,Răzvan-Alexandru Smădu,Dumitru-Clementin Cercel*

Main category: cs.LG

TL;DR: 本文旨在设计和评估多种机器学习模型来预测PM2.5浓度，包括线性回归、集成方法、深度学习模型和大型语言模型。


<details>
  <summary>Details</summary>
Motivation: PM2.5空气污染对健康造成严重影响，预测未来PM2.5水平可以提供早期预警并帮助预防疾病。

Method: 设计、微调、测试和评估多种机器学习模型，包括线性回归算法、集成方法、循环神经网络、变换器和大型语言模型。

Result: 评估和比较了多种模型在PM2.5预测任务上的性能表现。

Conclusion: 通过比较不同模型的性能，为PM2.5预测任务提供了有效的模型选择参考。

Abstract: Air pollution, especially the particulate matter 2.5 (PM2.5), has become a
growing concern in recent years, primarily in urban areas. Being exposed to air
pollution is linked to developing numerous health problems, like the
aggravation of respiratory diseases, cardiovascular disorders, lung function
impairment, and even cancer or early death. Forecasting future levels of PM2.5
has become increasingly important over the past few years, as it can provide
early warnings and help prevent diseases. This paper aims to design, fine-tune,
test, and evaluate machine learning models for predicting future levels of
PM2.5 over various time horizons. Our primary objective is to assess and
compare the performance of multiple models, ranging from linear regression
algorithms and ensemble-based methods to deep learning models, such as advanced
recurrent neural networks and transformers, as well as large language models,
on this forecasting task.

</details>


### [133] [Learning an Efficient Optimizer via Hybrid-Policy Sub-Trajectory Balance](https://arxiv.org/abs/2511.00543)
*Yunchuan Guan,Yu Liu,Ke Zhou,Hui Li,Sen Jia,Zhiqi Shen,Ziyang Wang,Xinglin Zhang,Tao Chen,Jenq-Neng Hwang,Lei Li*

Main category: cs.LG

TL;DR: 提出Lo-Hp框架，通过解耦的两阶段权重生成方法解决现有生成式权重优化中的过耦合和长视野问题，采用混合策略子轨迹平衡目标来学习局部优化策略。


<details>
  <summary>Details</summary>
Motivation: 当前基于生成式建模的权重生成方法存在过耦合和长视野问题，前者限制了优化器的灵活性，后者导致推理效率低下和精度不足。

Method: 采用解耦的两阶段权重生成框架，结合混合策略子轨迹平衡目标，整合在线和离线学习来捕捉局部优化策略。

Result: 理论证明仅学习局部优化策略即可解决长视野问题并提升全局最优权重生成，在迁移学习、少样本学习等领域展现出优越的准确性和推理效率。

Conclusion: Lo-Hp框架通过解耦设计和局部策略学习有效解决了现有方法的局限性，在需要频繁权重更新的任务中表现优异。

Abstract: Recent advances in generative modeling enable neural networks to generate
weights without relying on gradient-based optimization. However, current
methods are limited by issues of over-coupling and long-horizon. The former
tightly binds weight generation with task-specific objectives, thereby limiting
the flexibility of the learned optimizer. The latter leads to inefficiency and
low accuracy during inference, caused by the lack of local constraints. In this
paper, we propose Lo-Hp, a decoupled two-stage weight generation framework that
enhances flexibility through learning various optimization policies. It adopts
a hybrid-policy sub-trajectory balance objective, which integrates on-policy
and off-policy learning to capture local optimization policies. Theoretically,
we demonstrate that learning solely local optimization policies can address the
long-horizon issue while enhancing the generation of global optimal weights. In
addition, we validate Lo-Hp's superior accuracy and inference efficiency in
tasks that require frequent weight updates, such as transfer learning, few-shot
learning, domain generalization, and large language model adaptation.

</details>


### [134] [Robust Single-Agent Reinforcement Learning for Regional Traffic Signal Control Under Demand Fluctuations](https://arxiv.org/abs/2511.00549)
*Qiang Li,Jin Niu,Lina Yu*

Main category: cs.LG

TL;DR: 提出基于单智能体强化学习的区域自适应交通信号控制框架，通过集中决策避免多智能体协调复杂性，利用DreamerV3世界模型高效学习控制策略，在SUMO仿真中验证了良好的抗波动能力和队列长度减少效果。


<details>
  <summary>Details</summary>
Motivation: 交通拥堵主要由交叉口排队引起，传统优化模型难以捕捉真实交通复杂性。需要开发能有效缓解拥堵的智能交通信号控制系统。

Method: 使用单智能体强化学习框架，通过邻接矩阵统一编码路网拓扑、实时队列状态和信号配时参数，利用DreamerV3世界模型学习控制策略，动作序列选择交叉口并调整信号相位配时。

Result: 在SUMO仿真实验中，面对多级别(10%、20%、30%)OD需求波动，该框架展现出强大的抗波动能力，并显著减少了队列长度。

Conclusion: 建立了一个与探测车辆技术兼容的智能交通控制新范式，未来研究将关注训练中纳入随机OD需求波动和区域优化应急机制。

Abstract: Traffic congestion, primarily driven by intersection queuing, significantly
impacts urban living standards, safety, environmental quality, and economic
efficiency. While Traffic Signal Control (TSC) systems hold potential for
congestion mitigation, traditional optimization models often fail to capture
real-world traffic complexity and dynamics. This study introduces a novel
single-agent reinforcement learning (RL) framework for regional adaptive TSC,
circumventing the coordination complexities inherent in multi-agent systems
through a centralized decision-making paradigm. The model employs an adjacency
matrix to unify the encoding of road network topology, real-time queue states
derived from probe vehicle data, and current signal timing parameters.
Leveraging the efficient learning capabilities of the DreamerV3 world model,
the agent learns control policies where actions sequentially select
intersections and adjust their signal phase splits to regulate traffic
inflow/outflow, analogous to a feedback control system. Reward design
prioritizes queue dissipation, directly linking congestion metrics (queue
length) to control actions. Simulation experiments conducted in SUMO
demonstrate the model's effectiveness: under inference scenarios with
multi-level (10%, 20%, 30%) Origin-Destination (OD) demand fluctuations, the
framework exhibits robust anti-fluctuation capability and significantly reduces
queue lengths. This work establishes a new paradigm for intelligent traffic
control compatible with probe vehicle technology. Future research will focus on
enhancing practical applicability by incorporating stochastic OD demand
fluctuations during training and exploring regional optimization mechanisms for
contingency events.

</details>


### [135] [Temporal Fusion Transformer for Multi-Horizon Probabilistic Forecasting of Weekly Retail Sales](https://arxiv.org/abs/2511.00552)
*Santhi Bharath Punati,Sandeep Kanta,Udaya Bhasker Cheerala,Madhusudan G Lanjewar,Praveen Damacharla*

Main category: cs.LG

TL;DR: 使用时间融合变换器(TFT)对沃尔玛周销售额进行多期预测，融合静态商店标识符和动态外部信号，在1-5周预测范围内优于基线模型，具有校准预测区间和可解释性。


<details>
  <summary>Details</summary>
Motivation: 准确的零售多期预测对库存管理和促销策略至关重要，需要能够处理静态和动态特征并保持模型透明度的解决方案。

Method: 采用时间融合变换器(TFT)模型，融合静态商店标识符与时间变化的外部信号(节假日、CPI、燃料价格、温度)，通过分位数损失产生概率预测，利用变量选择网络、静态丰富化和时间注意力机制实现可解释性。

Result: 在2012年固定测试集上，TFT达到每店周RMSE 57.9k美元和R² 0.9875；在5折时序交叉验证中，平均RMSE 64.6k美元和R² 0.9844，优于XGB、CNN、LSTM和CNN-LSTM基线模型。

Conclusion: TFT模型在零售预测中表现出实用价值，特别适用于库存规划和节假日优化，同时保持模型透明度。

Abstract: Accurate multi-horizon retail forecasts are critical for inventory and
promotions. We present a novel study of weekly Walmart sales (45 stores,
2010--2012) using a Temporal Fusion Transformer (TFT) that fuses static store
identifiers with time-varying exogenous signals (holidays, CPI, fuel price,
temperature). The pipeline produces 1--5-week-ahead probabilistic forecasts via
Quantile Loss, yielding calibrated 90\% prediction intervals and
interpretability through variable-selection networks, static enrichment, and
temporal attention. On a fixed 2012 hold-out dataset, TFT achieves an RMSE of
\$57.9k USD per store-week and an $R^2$ of 0.9875. Across a 5-fold
chronological cross-validation, the averages are RMSE = \$64.6k USD and $R^2$ =
0.9844, outperforming the XGB, CNN, LSTM, and CNN-LSTM baseline models. These
results demonstrate practical value for inventory planning and holiday-period
optimization, while maintaining model transparency.

</details>


### [136] [Red-teaming Activation Probes using Prompted LLMs](https://arxiv.org/abs/2511.00554)
*Phil Blandfort,Robert Graham*

Main category: cs.LG

TL;DR: 提出了一种轻量级黑盒红队测试方法，使用现成LLM通过迭代反馈和上下文学习来发现激活探针的脆弱性模式，无需微调或架构访问。


<details>
  <summary>Details</summary>
Motivation: 激活探针作为AI系统监控器成本低、延迟小，但其在真实世界黑盒对抗压力下的鲁棒性尚未充分探索，需要发现其失效模式。

Method: 采用轻量级黑盒红队测试流程，将现成LLM包装在迭代反馈和上下文学习框架中，无需微调、梯度或架构访问。

Result: 在高风险交互案例研究中，发现了可解释的脆弱性模式（如法律术语导致的误报、平淡程序性语调导致的漏报），以及在场景约束攻击下减少但仍持续的漏洞。

Conclusion: 简单的提示式红队测试框架可以在部署前预测失效模式，并为未来探针的强化提供有前景、可操作的见解。

Abstract: Activation probes are attractive monitors for AI systems due to low cost and
latency, but their real-world robustness remains underexplored. We ask: What
failure modes arise under realistic, black-box adversarial pressure, and how
can we surface them with minimal effort? We present a lightweight black-box
red-teaming procedure that wraps an off-the-shelf LLM with iterative feedback
and in-context learning (ICL), and requires no fine-tuning, gradients, or
architectural access. Running a case study with probes for high-stakes
interactions, we show that our approach can help discover valuable insights
about a SOTA probe. Our analysis uncovers interpretable brittleness patterns
(e.g., legalese-induced FPs; bland procedural tone FNs) and reduced but
persistent vulnerabilities under scenario-constraint attacks. These results
suggest that simple prompted red-teaming scaffolding can anticipate failure
patterns before deployment and might yield promising, actionable insights to
harden future probes.

</details>


### [137] [FTT-GRU: A Hybrid Fast Temporal Transformer with GRU for Remaining Useful Life Prediction](https://arxiv.org/abs/2511.00564)
*Varun Teja Chirukiri,Udaya Bhasker Cheerala,Sandeep Kanta,Abdul Karim,Praveen Damacharla*

Main category: cs.LG

TL;DR: 提出了FTT-GRU混合模型，结合快速时序Transformer和GRU，用于工业机械剩余使用寿命预测，在CMAPSS数据集上实现了准确高效的预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法如LSTM和CNN难以同时建模全局时间依赖性和细粒度退化趋势，需要一种能同时捕获全局和局部退化模式的紧凑架构。

Method: 使用快速时序Transformer（基于FFT线性化注意力的轻量级Transformer变体）与GRU层结合的混合模型，首次将FTT与GRU应用于RUL预测。

Result: 在CMAPSS FD001上获得RMSE 30.76、MAE 18.97、R²=0.45，CPU延迟1.12ms，相比最佳深度基线TCN-Attention，RMSE提升1.16%，MAE提升4.00%。

Conclusion: 紧凑的Transformer-RNN混合模型能够在CMAPSS上提供准确高效的RUL预测，适用于实时工业预测性维护。

Abstract: Accurate prediction of the remaining useful life (RUL) of industrial
machinery is essential for reducing downtime and optimizing maintenance
schedules. Existing approaches, such as long short-term memory (LSTM) networks
and convolutional neural networks (CNNs), often struggle to model both global
temporal dependencies and fine-grained degradation trends in multivariate
sensor data. We propose a hybrid model, FTT-GRU, which combines a Fast Temporal
Transformer (FTT) -- a lightweight Transformer variant using linearized
attention via fast Fourier transform (FFT) -- with a gated recurrent unit (GRU)
layer for sequential modeling. To the best of our knowledge, this is the first
application of an FTT with a GRU for RUL prediction on NASA CMAPSS, enabling
simultaneous capture of global and local degradation patterns in a compact
architecture. On CMAPSS FD001, FTT-GRU attains RMSE 30.76, MAE 18.97, and
$R^2=0.45$, with 1.12 ms CPU latency at batch=1. Relative to the best published
deep baseline (TCN--Attention), it improves RMSE by 1.16\% and MAE by 4.00\%.
Training curves averaged over $k=3$ runs show smooth convergence with narrow
95\% confidence bands, and ablations (GRU-only, FTT-only) support the
contribution of both components. These results demonstrate that a compact
Transformer-RNN hybrid delivers accurate and efficient RUL predictions on
CMAPSS, making it suitable for real-time industrial prognostics.

</details>


### [138] [Bayesian Network Structure Discovery Using Large Language Models](https://arxiv.org/abs/2511.00574)
*Yinghuan Zhang,Yufei Zhang,Parisa Kordjamshidi,Zijun Cui*

Main category: cs.LG

TL;DR: 提出了一个统一的贝叶斯网络结构发现框架，将大语言模型置于核心位置，支持无数据和有数据两种场景，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统结构学习方法需要大量观测数据且计算成本高，现有LLM方法仅将其作为辅助工具，未充分发挥其核心作用。

Method: 提出PromptBN（无数据场景）和ReActBN（有数据场景）两个方法，前者通过元数据查询LLM发现概率关系，后者结合ReAct推理范式与结构评分进行迭代优化。

Result: 实验表明该方法在低数据或无数据场景下显著优于现有LLM方法和传统数据驱动算法。

Conclusion: 将LLM置于结构学习核心位置的有效性得到验证，为概率关系发现提供了新的解决方案。

Abstract: Understanding probabilistic relationships among variables is crucial for
analyzing complex systems. Traditional structure learning methods often require
extensive observational data and incur high computational costs. Recent studies
have explored using large language models (LLMs) for structure learning, but
most treat LLMs as auxiliary tools for pre-processing or post-processing,
leaving the core learning process data-driven. In this work, we propose a
unified framework for Bayesian network structure discovery that places LLMs at
the center, supporting both data-free and data-aware settings. In the data-free
case, we introduce \textbf{PromptBN} to query LLMs with metadata and
efficiently uncover valid probabilistic relationships. When observational data
are available, we introduce \textbf{ReActBN}, which integrates the ReAct
reasoning paradigm with structure scores such as the Bayesian Information
Criterion (BIC) for iterative refinement. Unlike prior methods that offload
refinement to external algorithms, our framework maintains the LLM actively in
the loop throughout the discovery process. Experiments demonstrate that our
method significantly outperforms both existing LLM-based approaches and
traditional data-driven algorithms, particularly in the low- or no-data
scenario. Code is publicly available at
{\texttt{\textcolor{magenta}{https://github.com/sherryzyh/prompt2bn}}}.

</details>


### [139] [Sparse and nonparametric estimation of equations governing dynamical systems with applications to biology](https://arxiv.org/abs/2511.00579)
*G. Pillonetto,A. Giaretta,A. Aravkin,M. Bisiacco,T. Elston*

Main category: cs.LG

TL;DR: 提出了一种结合稀疏参数估计和非参数技术的新框架，用于从数据中发现复杂系统的数学模型，特别适用于系统生物学中难以通过自下而上方法建模的情况。


<details>
  <summary>Details</summary>
Motivation: 传统参数模型在准确表示复杂系统的某些非线性特性方面存在不足，而Sindy等稀疏估计方法虽然能有效捕捉系统动态，但需要预先知道非线性函数形式或扩展函数库。

Method: 将稀疏参数估计与非参数技术相结合，无需预先了解非线性函数形式或扩展函数库，就能捕捉Sindy无法描述的非线性特性。

Result: 在多个与复杂生物现象估计相关的示例中验证了该方法的有效性。

Conclusion: 该框架能够更准确地表示复杂系统中的非线性特性，为系统生物学等领域的数据驱动建模提供了更强大的工具。

Abstract: Data-driven discovery of model equations is a powerful approach for
understanding the behavior of dynamical systems in many scientific fields. In
particular, the ability to learn mathematical models from data would benefit
systems biology, where the complex nature of these systems often makes a bottom
up approach to modeling unfeasible. In recent years, sparse estimation
techniques have gained prominence in system identification, primarily using
parametric paradigms to efficiently capture system dynamics with minimal model
complexity. In particular, the Sindy algorithm has successfully used sparsity
to estimate nonlinear systems by extracting from a library of functions only a
few key terms needed to capture the dynamics of these systems. However,
parametric models often fall short in accurately representing certain
nonlinearities inherent in complex systems. To address this limitation, we
introduce a novel framework that integrates sparse parametric estimation with
nonparametric techniques. It captures nonlinearities that Sindy cannot describe
without requiring a priori information about their functional form. That is,
without expanding the library of functions to include the one that is trying to
be discovered. We illustrate our approach on several examples related to
estimation of complex biological phenomena.

</details>


### [140] [Diagnosing Hallucination Risk in AI Surgical Decision-Support: A Sequential Framework for Sequential Validation](https://arxiv.org/abs/2511.00588)
*Dong Chen,Yanzhe Wei,Zonglin He,Guan-Ming Kuang,Canhua Ye,Meiru An,Huili Peng,Yong Hu,Huiren Tao,Kenneth MC Cheung*

Main category: cs.LG

TL;DR: 本研究评估了6个领先大语言模型在脊柱外科临床决策中的幻觉风险，发现DeepSeek-R1表现最佳，而增强推理模型并不总是优于标准版本，强调需要将可解释性机制整合到临床工作流程中。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在脊柱外科临床决策支持中具有变革潜力，但存在幻觉风险，可能危及患者安全，需要量化评估这些风险。

Method: 采用临床医生中心框架，评估诊断精度、推荐质量、推理稳健性、输出一致性和知识对齐，在30个专家验证的脊柱病例上测试6个领先LLM。

Result: DeepSeek-R1总体表现最佳（总分：86.03±2.08），在创伤和感染等高风险领域表现突出。增强推理模型变体并未一致优于标准版本，多维压力测试暴露了模型特定脆弱性。

Conclusion: 需要将可解释性机制整合到临床工作流程中，并建立安全感知的验证框架用于外科LLM部署。

Abstract: Large language models (LLMs) offer transformative potential for clinical
decision support in spine surgery but pose significant risks through
hallucinations, which are factually inconsistent or contextually misaligned
outputs that may compromise patient safety. This study introduces a
clinician-centered framework to quantify hallucination risks by evaluating
diagnostic precision, recommendation quality, reasoning robustness, output
coherence, and knowledge alignment. We assessed six leading LLMs across 30
expert-validated spinal cases. DeepSeek-R1 demonstrated superior overall
performance (total score: 86.03 $\pm$ 2.08), particularly in high-stakes
domains such as trauma and infection. A critical finding reveals that
reasoning-enhanced model variants did not uniformly outperform standard
counterparts: Claude-3.7-Sonnet's extended thinking mode underperformed
relative to its standard version (80.79 $\pm$ 1.83 vs. 81.56 $\pm$ 1.92),
indicating extended chain-of-thought reasoning alone is insufficient for
clinical reliability. Multidimensional stress-testing exposed model-specific
vulnerabilities, with recommendation quality degrading by 7.4% under amplified
complexity. This decline contrasted with marginal improvements in rationality
(+2.0%), readability (+1.7%) and diagnosis (+4.7%), highlighting a concerning
divergence between perceived coherence and actionable guidance. Our findings
advocate integrating interpretability mechanisms (e.g., reasoning chain
visualization) into clinical workflows and establish a safety-aware validation
framework for surgical LLM deployment.

</details>


### [141] [Gaining Momentum: Uncovering Hidden Scoring Dynamics in Hockey through Deep Neural Sequencing and Causal Modeling](https://arxiv.org/abs/2511.00615)
*Daniel Griffiths,Piper Moskow*

Main category: cs.LG

TL;DR: 提出一个统一的数据驱动框架，通过五个阶段量化并增强冰球比赛中的进攻势头和得分可能性，发现结构化序列和紧凑阵型能显著提升进攻表现。


<details>
  <summary>Details</summary>
Motivation: 开发一个端到端的分析框架，为教练和分析师提供实时、可操作的战术优化见解，推动冰球分析向基于因果关系的原则性战术优化发展。

Method: 五阶段流程：1) 通过逻辑回归进行可解释的动量加权；2) 使用梯度提升决策树进行非线性xG估计；3) LSTM网络进行时间序列建模；4) PCA和K-Means聚类发现空间阵型；5) X-Learner因果推断估计器量化最优序列和阵型的平均处理效应。

Result: 观察到ATE为0.12（95% CI: 0.05-0.17, p < 1e-50），相当于得分潜力相对提升15%，证明结构化序列和紧凑阵型能因果性地提升进攻表现。

Conclusion: 该框架为教练和分析师提供实时、可操作的见解，将冰球分析推向基于因果关系的原则性战术优化，证明了战略结构化的比赛序列和阵型对提升进攻效果的重要性。

Abstract: We present a unified, data-driven framework for quantifying and enhancing
offensive momentum and scoring likelihood (expected goals, xG) in professional
hockey. Leveraging a Sportlogiq dataset of 541,000 NHL event records, our
end-to-end pipeline comprises five stages: (1) interpretable momentum weighting
of micro-events via logistic regression; (2) nonlinear xG estimation using
gradient-boosted decision trees; (3) temporal sequence modeling with Long
Short-Term Memory (LSTM) networks; (4) spatial formation discovery through
principal component analysis (PCA) followed by K-Means clustering on
standardized player coordinates; and (5) use of an X-Learner causal inference
estimator to quantify the average treatment effect (ATE) of adopting the
identified "optimal" event sequences and formations. We observe an ATE of 0.12
(95% CI: 0.05-0.17, p < 1e-50), corresponding to a 15% relative gain in scoring
potential. These results demonstrate that strategically structured sequences
and compact formations causally elevate offensive performance. Our framework
delivers real-time, actionable insights for coaches and analysts, advancing
hockey analytics toward principled, causally grounded tactical optimization.

</details>


### [142] [Belief Dynamics Reveal the Dual Nature of In-Context Learning and Activation Steering](https://arxiv.org/abs/2511.00617)
*Eric Bigelow,Daniel Wurgaft,YingQiao Wang,Noah Goodman,Tomer Ullman,Hidenori Tanaka,Ekdeep Singh Lubana*

Main category: cs.LG

TL;DR: 本文提出了一个统一的贝叶斯框架来解释大语言模型的两种控制方法：提示学习和激活引导，认为它们都是通过改变模型对潜在概念的信念来实现控制的。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM控制方法（提示学习和激活引导）虽然看似不同，但都旨在控制模型行为。作者希望建立一个统一的理论框架来解释这些方法的共同机制。

Method: 从贝叶斯视角出发，将提示学习视为证据积累过程，将激活引导视为改变概念先验，建立了一个闭式贝叶斯模型来预测LLM行为。

Result: 该模型能够准确预测LLM在多种干预下的行为，包括解释S型学习曲线等已知现象，并预测了新的现象如对数信念空间中的干预可加性。

Conclusion: 这项工作为基于提示和基于激活的LLM行为控制提供了统一的理论解释，并提供了预测这些干预效果的经验方法。

Abstract: Large language models (LLMs) can be controlled at inference time through
prompts (in-context learning) and internal activations (activation steering).
Different accounts have been proposed to explain these methods, yet their
common goal of controlling model behavior raises the question of whether these
seemingly disparate methodologies can be seen as specific instances of a
broader framework. Motivated by this, we develop a unifying, predictive account
of LLM control from a Bayesian perspective. Specifically, we posit that both
context- and activation-based interventions impact model behavior by altering
its belief in latent concepts: steering operates by changing concept priors,
while in-context learning leads to an accumulation of evidence. This results in
a closed-form Bayesian model that is highly predictive of LLM behavior across
context- and activation-based interventions in a set of domains inspired by
prior work on many-shot in-context learning. This model helps us explain prior
empirical phenomena - e.g., sigmoidal learning curves as in-context evidence
accumulates - while predicting novel ones - e.g., additivity of both
interventions in log-belief space, which results in distinct phases such that
sudden and dramatic behavioral shifts can be induced by slightly changing
intervention controls. Taken together, this work offers a unified account of
prompt-based and activation-based control of LLM behavior, and a methodology
for empirically predicting the effects of these interventions.

</details>


### [143] [Stochastic Shortest Path with Sparse Adversarial Costs](https://arxiv.org/abs/2511.00637)
*Emmeran Johnson,Alberto Rumi,Ciara Pike-Burke,Patrick Rebeschini*

Main category: cs.LG

TL;DR: 本文研究了具有稀疏成本的对抗性随机最短路径问题，提出了ℓ_r-范数正则化器来适应稀疏性，在已知转移设置下实现了与√log M成比例的遗憾，而不是√log SA。


<details>
  <summary>Details</summary>
Motivation: 现有基于负熵正则化的在线镜像下降方法在已知转移设置下的遗憾边界与√log SA成比例，这无法充分利用稀疏性带来的好处，当只有少量状态-动作对产生成本时表现不佳。

Method: 提出了一族ℓ_r-范数正则化器（r∈(1,2)），这些正则化器能够适应问题的稀疏性，在已知转移设置下使用这些正则化器进行优化。

Result: 在已知转移设置下，新方法实现了与√log M成比例的遗憾，而不是√log SA，其中M是产生成本的状态-动作对数量，这通过匹配下界证明是最优的。在未知转移设置下，稀疏性的好处有限，任何学习者的极小极大遗憾都与SA成多项式关系。

Conclusion: ℓ_r-范数正则化器能够有效适应稀疏成本问题，M而非SA捕获了问题的有效维度，但在未知转移设置下稀疏性的好处受到限制。

Abstract: We study the adversarial Stochastic Shortest Path (SSP) problem with sparse
costs under full-information feedback. In the known transition setting,
existing bounds based on Online Mirror Descent (OMD) with negative-entropy
regularization scale with $\sqrt{\log S A}$, where $SA$ is the size of the
state-action space. While we show that this is optimal in the worst-case, this
bound fails to capture the benefits of sparsity when only a small number $M \ll
SA$ of state-action pairs incur cost. In fact, we also show that the
negative-entropy is inherently non-adaptive to sparsity: it provably incurs
regret scaling with $\sqrt{\log S}$ on sparse problems. Instead, we propose a
family of $\ell_r$-norm regularizers ($r \in (1,2)$) that adapts to the
sparsity and achieves regret scaling with $\sqrt{\log M}$ instead of
$\sqrt{\log SA}$. We show this is optimal via a matching lower bound,
highlighting that $M$ captures the effective dimension of the problem instead
of $SA$. Finally, in the unknown transition setting the benefits of sparsity
are limited: we prove that even on sparse problems, the minimax regret for any
learner scales polynomially with $SA$.

</details>


### [144] [Diluting Restricted Boltzmann Machines](https://arxiv.org/abs/2511.00648)
*C. Díaz-Faloh,R. Mulet*

Main category: cs.LG

TL;DR: 研究表明RBMs在训练前剪枝80%仍能保持良好生成性能，但训练后剪枝会导致性能无法完全恢复，且重训练网络表现不如从头训练的稀疏网络。


<details>
  <summary>Details</summary>
Motivation: 研究大型神经网络的计算和环境成本问题，探索更简单稀疏的网络是否能保持强性能，受彩票假设启发研究RBMs在极端剪枝条件下的表现。

Method: 在受限玻尔兹曼机(RBMs)上进行极端剪枝实验，研究训练前剪枝和训练后剪枝的效果差异，分析关键连接被破坏时的性能变化。

Result: RBMs在训练前剪枝80%仍能保持高质量生成性能，但训练后剪枝会导致性能急剧下降且无法通过重训练完全恢复，重训练网络表现不如从头训练的同等稀疏网络。

Conclusion: 稀疏网络要有效工作，剪枝应在训练早期实施而非训练后尝试，初始条件对网络能力有持久影响。

Abstract: Recent advances in artificial intelligence have relied heavily on
increasingly large neural networks, raising concerns about their computational
and environmental costs. This paper investigates whether simpler, sparser
networks can maintain strong performance by studying Restricted Boltzmann
Machines (RBMs) under extreme pruning conditions. Inspired by the Lottery
Ticket Hypothesis, we demonstrate that RBMs can achieve high-quality generative
performance even when up to 80% of the connections are pruned before training,
confirming that they contain viable sub-networks. However, our experiments
reveal crucial limitations: trained networks cannot fully recover lost
performance through retraining once additional pruning is applied. We identify
a sharp transition above which the generative quality degrades abruptly when
pruning disrupts a minimal core of essential connections. Moreover, re-trained
networks remain constrained by the parameters originally learned performing
worse than networks trained from scratch at equivalent sparsity levels. These
results suggest that for sparse networks to work effectively, pruning should be
implemented early in training rather than attempted afterwards. Our findings
provide practical insights for the development of efficient neural
architectures and highlight the persistent influence of initial conditions on
network capabilities.

</details>


### [145] [Reviving Stale Updates: Data-Free Knowledge Distillation for Asynchronous Federated Learning](https://arxiv.org/abs/2511.00655)
*Baris Askin,Holger R. Roth,Zhenyu Sun,Carlee Joe-Wong,Gauri Joshi,Ziyue Xu*

Main category: cs.LG

TL;DR: FedRevive是一个异步联邦学习框架，通过无数据知识蒸馏来缓解陈旧更新问题，在保持AFL可扩展性的同时显著提升训练速度和最终精度。


<details>
  <summary>Details</summary>
Motivation: 异步联邦学习(AFL)虽然通过独立通信提高了大规模异构环境中的训练效率，但异步性引入了陈旧更新（基于过时全局模型的客户端更新），这会破坏优化稳定性并阻碍收敛。

Method: FedRevive结合参数空间聚合与轻量级服务器端无数据知识蒸馏(DFKD)，通过元学习生成器合成伪样本实现多教师蒸馏，采用混合聚合方案结合原始更新和DFKD更新。

Result: 在各种视觉和文本基准测试中，FedRevive相比异步基线方法，训练速度提升高达32.1%，最终精度提升高达21.5%。

Conclusion: FedRevive通过无数据知识蒸馏有效缓解了异步联邦学习中的陈旧更新问题，在保持可扩展性的同时显著提升了训练效率和模型性能。

Abstract: Federated Learning (FL) enables collaborative model training across
distributed clients without sharing raw data, yet its scalability is limited by
synchronization overhead. Asynchronous Federated Learning (AFL) alleviates this
issue by allowing clients to communicate independently, thereby improving
wall-clock efficiency in large-scale, heterogeneous environments. However, this
asynchrony introduces stale updates (client updates computed on outdated global
models) that can destabilize optimization and hinder convergence. We propose
FedRevive, an asynchronous FL framework that revives stale updates through
data-free knowledge distillation (DFKD). FedRevive integrates parameter-space
aggregation with a lightweight, server-side DFKD process that transfers
knowledge from stale client models to the current global model without access
to real or public data. A meta-learned generator synthesizes pseudo-samples,
which enables multi-teacher distillation. A hybrid aggregation scheme that
combines raw updates with DFKD updates effectively mitigates staleness while
retaining the scalability of AFL. Experiments on various vision and text
benchmarks show that FedRevive achieves faster training up to 32.1% and higher
final accuracy up to 21.5% compared to asynchronous baselines.

</details>


### [146] [Sensitivity Analysis for Climate Science with Generative Flow Models](https://arxiv.org/abs/2511.00663)
*Alex Dobra,Jakiw Pidstrigach,Tim Reichelt,Paolo Fraccaro,Johannes Jakubik,Anne Jones,Christian Schroeder de Witt,Philip Stier,Philip Torr*

Main category: cs.LG

TL;DR: 该论文提出使用伴随状态方法计算生成流模型（特别是扩散模型）的梯度，用于气候敏感性分析，将计算成本从数周大幅降低到数小时。


<details>
  <summary>Details</summary>
Motivation: 传统物理模型计算气候敏感性成本过高，而AI生成模型虽然评估速度快，但计算敏感性仍是瓶颈。

Method: 应用伴随状态方法计算生成流模型的梯度，以扩散模型为特例，在cBottle生成模型上进行海表温度敏感性分析，并提出梯度自一致性验证方法。

Result: 该方法能产生可靠的梯度，将敏感性分析的计算成本从超级计算机上的数周降低到GPU上的数小时。

Conclusion: 该方法简化了气候科学中的关键工作流程，为可靠的气候敏感性分析提供了高效解决方案。

Abstract: Sensitivity analysis is a cornerstone of climate science, essential for
understanding phenomena ranging from storm intensity to long-term climate
feedbacks. However, computing these sensitivities using traditional physical
models is often prohibitively expensive in terms of both computation and
development time. While modern AI-based generative models are orders of
magnitude faster to evaluate, computing sensitivities with them remains a
significant bottleneck. This work addresses this challenge by applying the
adjoint state method for calculating gradients in generative flow models, with
diffusion models as a special case. We apply this method to the cBottle
generative model, an emulator of ERA5 data, to perform sensitivity analysis
with respect to sea surface temperatures. Furthermore, we propose a novel
gradient self-consistency check to quantitatively validate the computed
sensitivities against the model's own outputs. Our results provide initial
evidence that this approach can produce reliable gradients, reducing the
computational cost of sensitivity analysis from weeks on a supercomputer with a
physical model to hours on a GPU, thereby simplifying a critical workflow in
climate science.

</details>


### [147] [Inference-Time Chain-of-Thought Pruning with Latent Informativeness Signals](https://arxiv.org/abs/2511.00699)
*Sophie Li,Nicholas Huang,Nayan Saxena,Nina Luo,Vincent Lin,Kevin Zhu,Sunishchal Dev*

Main category: cs.LG

TL;DR: KAPPA是一种推理时方法，通过结合KL散度、置信度和熵的评分函数来指导渐进式剪枝，显著减少内存和token使用，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法如Best-of-N计算成本高，Self-Truncation Best-of-N依赖一致性启发式方法，不能直接评估分支质量。

Method: 使用KL散度、置信度和熵构建评分函数，在探索阶段促进多样性，并选择性消除低分分支。

Result: 在GSM8K和MATH500上的实验显示，KAPPA在小模型中稳定性能，相比BoN减少约60%峰值内存和90%总token生成，对准确性影响最小。

Conclusion: KAPPA通过原则性评分函数有效平衡计算效率和推理准确性。

Abstract: Large language models (LLMs) improve reasoning accuracy when generating
multiple candidate solutions at test time, but standard methods like Best-of-N
(BoN) incur high computational cost by fully generating all branches.
Self-Truncation Best-of-N (ST-BoN) mitigates this by truncating unpromising
paths early, but its reliance on consistency-based heuristics is a limitation
as it does not directly evaluate branch quality. We present KL-Adjusted Pruned
Path Algorithm (KAPPA), an inference-time method that combines Kullback-Leibler
divergence, confidence, and entropy into a principled scoring function to guide
progressive pruning. By promoting diversity during exploration and selectively
eliminating low-scoring branches, KAPPA maintains accuracy while substantially
reducing memory and token usage. Experiments on GSM8K and MATH500 with
DeepSeek-R1-Distill-Qwen-1.5B and Qwen2.5-7B-Instruct demonstrate that KAPPA
stabilizes performance in smaller models and achieves up to ~60% reduction in
peak memory and ~90% reduction in total token generation relative to BoN, with
minimal impact on accuracy.

</details>


### [148] [Privacy-Aware Time Series Synthesis via Public Knowledge Distillation](https://arxiv.org/abs/2511.00700)
*Penghang Liu,Haibei Zhu,Eleonora Kreacic,Svitlana Vyetrenko*

Main category: cs.LG

TL;DR: Pub2Priv是一个利用异构公共知识生成私有时间序列数据的框架，通过自注意力机制编码公共数据作为扩散模型的输入，在金融、能源和商品交易领域实现了更好的隐私-效用平衡。


<details>
  <summary>Details</summary>
Motivation: 在金融、医疗和能源消费等领域，敏感时间序列数据的共享常因隐私问题受限。现有隐私感知数据生成方法往往忽略了与公开上下文元数据的相关性，导致隐私-效用权衡不理想。

Method: 使用自注意力机制将公共数据编码为时间和特征嵌入，作为扩散模型的条件输入来生成合成私有序列，并引入评估合成数据可识别性的隐私度量方法。

Result: 实验结果表明，Pub2Priv在金融、能源和商品交易领域始终优于现有基准方法，显著改善了隐私-效用权衡。

Conclusion: Pub2Priv通过有效利用异构公共知识，为隐私敏感的时间序列数据生成提供了更优的解决方案，在多个实际应用领域都表现出色。

Abstract: Sharing sensitive time series data in domains such as finance, healthcare,
and energy consumption, such as patient records or investment accounts, is
often restricted due to privacy concerns. Privacy-aware synthetic time series
generation addresses this challenge by enforcing noise during training,
inherently introducing a trade-off between privacy and utility. In many cases,
sensitive sequences is correlated with publicly available, non-sensitive
contextual metadata (e.g., household electricity consumption may be influenced
by weather conditions and electricity prices). However, existing privacy-aware
data generation methods often overlook this opportunity, resulting in
suboptimal privacy-utility trade-offs. In this paper, we present Pub2Priv, a
novel framework for generating private time series data by leveraging
heterogeneous public knowledge. Our model employs a self-attention mechanism to
encode public data into temporal and feature embeddings, which serve as
conditional inputs for a diffusion model to generate synthetic private
sequences. Additionally, we introduce a practical metric to assess privacy by
evaluating the identifiability of the synthetic data. Experimental results show
that Pub2Priv consistently outperforms state-of-the-art benchmarks in improving
the privacy-utility trade-off across finance, energy, and commodity trading
domains.

</details>


### [149] [Investigating the Robustness of Knowledge Tracing Models in the Presence of Student Concept Drift](https://arxiv.org/abs/2511.00704)
*Morgan Lee,Artem Frenk,Eamon Worden,Karish Gupta,Thinh Pham,Ethan Croteau,Neil Heffernan*

Main category: cs.LG

TL;DR: 该论文研究了知识追踪模型在在线学习平台中面临的概念漂移问题，发现所有模型都会出现性能下降，其中贝叶斯知识追踪模型最稳定，而复杂的注意力模型性能下降最快。


<details>
  <summary>Details</summary>
Motivation: 传统知识追踪模型假设学习过程是静态的，但现实中在线学习平台的学生群体和行为会随时间变化，需要研究概念漂移对模型性能的影响。

Method: 使用四种经典知识追踪模型（包括贝叶斯知识追踪和注意力模型），在五年学术数据上进行测试，评估模型在单学年内和跨学年的性能变化。

Result: 所有知识追踪模型都会出现性能下降，贝叶斯知识追踪模型在新数据上表现最稳定，而复杂注意力模型的预测能力下降速度显著更快。

Conclusion: 知识追踪模型容易受到概念漂移影响，需要更多纵向评估研究，贝叶斯知识追踪模型在变化环境中相对更稳健。

Abstract: Knowledge Tracing (KT) has been an established problem in the educational
data mining field for decades, and it is commonly assumed that the underlying
learning process be- ing modeled remains static. Given the ever-changing land-
scape of online learning platforms (OLPs), we investigate how concept drift and
changing student populations can im- pact student behavior within an OLP
through testing model performance both within a single academic year and across
multiple academic years. Four well-studied KT models were applied to five
academic years of data to assess how suscep- tible KT models are to concept
drift. Through our analysis, we find that all four families of KT models can
exhibit de- graded performance, Bayesian Knowledge Tracing (BKT) remains the
most stable KT model when applied to newer data, while more complex, attention
based models lose pre- dictive power significantly faster. To foster more
longitu- dinal evaluations of KT models, the data used to conduct our analysis
is available at https://osf.io/hvfn9/?view_
only=b936c63dfdae4b0b987a2f0d4038f72a

</details>


### [150] [TRISKELION-1: Unified Descriptive-Predictive-Generative AI](https://arxiv.org/abs/2511.00711)
*Nardeep Kumar,Arun Kanwar*

Main category: cs.LG

TL;DR: TRISKELION-1是一个统一的描述-预测-生成架构，在单一编码器-解码器框架中整合了统计、机制和生成推理。


<details>
  <summary>Details</summary>
Motivation: 构建一个能够连接可解释性、准确性和创造性的通用智能架构蓝图。

Method: 使用变分目标联合优化描述性表示学习、预测推理和生成合成，在MNIST数据集上进行实验验证。

Result: 实验证明描述性重建、预测分类和生成采样可以在一个模型中稳定共存。

Conclusion: 该框架为实现连接可解释性、准确性和创造性的通用智能架构提供了蓝图。

Abstract: TRISKELION-1 is a unified descriptive-predictive-generative architecture that
integrates statistical, mechanistic, and generative reasoning within a single
encoder-decoder framework. The model demonstrates how descriptive
representation learning, predictive inference, and generative synthesis can be
jointly optimized using variational objectives. Experiments on MNIST validate
that descriptive reconstruction, predictive classification, and generative
sampling can coexist stably within one model. The framework provides a
blueprint toward universal intelligence architectures that connect
interpretability, accuracy, and creativity.

</details>


### [151] [Enhancing Heavy Rain Nowcasting with Multimodal Data: Integrating Radar and Satellite Observations](https://arxiv.org/abs/2511.00716)
*Rama Kassoumeh,David Rügamer,Henning Oppel*

Main category: cs.LG

TL;DR: 提出融合卫星和雷达数据的多模态临近预报模型，用于5-30分钟降水预测，显著优于仅使用雷达的方法，特别是在强降水预测方面表现更好。


<details>
  <summary>Details</summary>
Motivation: 传统地面传感器难以监测城市局部强降雨事件，德国2001-2018年仅有17.3%的小时强降雨事件被雨量计记录，雷达单独预报强降雨仍具挑战性。

Method: 开发多模态临近预报模型，结合雷达和卫星图像进行降水预测，预测时间范围为5、15和30分钟。

Result: 多模态策略显著优于雷达单独方法，卫星数据集成提高了预测准确性，特别是对强降水。在5分钟预测时间，强降雨CSI提高4%，暴雨CSI提高3%。在2021年德国北莱茵-威斯特法伦州洪水事件中表现更优。

Conclusion: 多模态模型能提供更详细准确的强降雨区域预报，实现及时可靠的救生预警。

Abstract: The increasing frequency of heavy rainfall events, which are a major cause of
urban flooding, underscores the urgent need for accurate precipitation
forecasting - particularly in urban areas where localized events often go
undetected by ground-based sensors. In Germany, only 17.3% of hourly heavy rain
events between 2001 and 2018 were recorded by rain gauges, highlighting the
limitations of traditional monitoring systems. Radar data are another source
that effectively tracks ongoing precipitation; however, forecasting the
development of heavy rain using radar alone remains challenging due to the
brief and unpredictable nature of such events. Our focus is on evaluating the
effectiveness of fusing satellite and radar data for nowcasting. We develop a
multimodal nowcasting model that combines both radar and satellite imagery for
predicting precipitation at lead times of 5, 15, and 30 minutes. We demonstrate
that this multimodal strategy significantly outperforms radar-only approaches.
Experimental results show that integrating satellite data improves prediction
accuracy, particularly for intense precipitation. The proposed model increases
the Critical Success Index for heavy rain by 4% and for violent rain by 3% at a
5-minute lead time. Moreover, it maintains higher predictive skill at longer
lead times, where radar-only performance declines. A qualitative analysis of
the severe flooding event in the state of North Rhine-Westphalia, Germany in
2021 further illustrates the superior performance of the multimodal model.
Unlike the radar-only model, which captures general precipitation patterns, the
multimodal model yields more detailed and accurate forecasts for regions
affected by heavy rain. This improved precision enables timely, reliable,
life-saving warnings. Implementation available at
https://github.com/RamaKassoumeh/Multimodal_heavy_rain

</details>


### [152] [Effective Series Decomposition and Components Learning for Time Series Generation](https://arxiv.org/abs/2511.00747)
*Zixuan Ma,Chenfeng Huang*

Main category: cs.LG

TL;DR: STDiffusion是一个新颖的多变量时间序列生成框架，结合扩散概率模型和可学习的序列分解技术，通过分别学习趋势和季节性成分来提升生成过程的可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列生成方法缺乏可解释的分解技术，难以合成有意义的趋势和季节性模式，限制了生成数据的质量。

Method: 使用扩散概率模型，将趋势和季节性学习分离到不同模块：MLP结构捕获趋势，自适应小波蒸馏实现季节性成分的多分辨率学习，并设计了综合校正机制确保生成成分的一致性。

Result: 在8个真实世界数据集上的实验表明，STDiffusion在时间序列生成任务中达到了最先进的性能，并在多窗口长序列生成中表现出鲁棒性和多功能性。

Conclusion: STDiffusion通过可解释的分解方法有效提升了时间序列生成的质量和可解释性，展示了在复杂时序数据生成任务中的强大能力。

Abstract: Time series generation focuses on modeling the underlying data distribution
and resampling to produce authentic time series data. Key components, such as
trend and seasonality, drive temporal fluctuations, yet many existing
approaches fail to employ interpretative decomposition methods, limiting their
ability to synthesize meaningful trend and seasonal patterns. To address this
gap, we introduce Seasonal-Trend Diffusion (STDiffusion), a novel framework for
multivariate time series generation that integrates diffusion probabilistic
models with advanced learnable series decomposition techniques, enhancing the
interpretability of the generation process. Our approach separates the trend
and seasonal learning into distinct blocks: a Multi-Layer Perceptron (MLP)
structure captures the trend, while adaptive wavelet distillation facilitates
effective multi-resolution learning of seasonal components. This decomposition
improves the interpretability of the model on multiple scales. In addition, we
designed a comprehensive correction mechanism aimed at ensuring that the
generated components exhibit a high degree of internal consistency and preserve
meaningful interrelationships with one another. Our empirical studies on eight
real-world datasets demonstrate that STDiffusion achieves state-of-the-art
performance in time series generation tasks. Furthermore, we extend the model's
application to multi-window long-sequence time series generation, which
delivered reliable results and highlighted its robustness and versatility.

</details>


### [153] [Fast PINN Eigensolvers via Biconvex Reformulation](https://arxiv.org/abs/2511.00792)
*Akshay Sai Banderwaar,Abhishek Gupta*

Main category: cs.LG

TL;DR: 提出了一种改进的PINN方法，将特征值问题重构为双凸优化问题，使用交替凸搜索实现快速收敛，比传统梯度训练快500倍


<details>
  <summary>Details</summary>
Motivation: 传统PINN方法求解特征值问题速度较慢，比经典数值方法慢几个数量级，需要更高效的优化策略

Method: 将特征值问题重构为双凸优化问题，采用交替凸搜索算法，对特征值和特征函数分别进行最优更新

Result: PINN-ACS方法实现了高精度，收敛速度比基于梯度的PINN训练快500倍

Conclusion: 该方法为特征值问题提供了快速、精确的网格无关解决方案，显著提升了PINN在特征值问题上的计算效率

Abstract: Eigenvalue problems have a distinctive forward-inverse structure and are
fundamental to characterizing a system's thermal response, stability, and
natural modes. Physics-Informed Neural Networks (PINNs) offer a mesh-free
alternative for solving such problems but are often orders of magnitude slower
than classical numerical schemes. In this paper, we introduce a reformulated
PINN approach that casts the search for eigenpairs as a biconvex optimization
problem, enabling fast and provably convergent alternating convex search (ACS)
over eigenvalues and eigenfunctions using analytically optimal updates.
Numerical experiments show that PINN-ACS attains high accuracy with convergence
speeds up to 500$\times$ faster than gradient-based PINN training. We release
our codes at https://github.com/NeurIPS-ML4PS-2025/PINN_ACS_CODES.

</details>


### [154] [Efficient Reinforcement Learning for Large Language Models with Intrinsic Exploration](https://arxiv.org/abs/2511.00794)
*Yan Sun,Jia Guo,Stanley Kok,Zihao Wang,Zujie Wen,Zhiqiang Zhang*

Main category: cs.LG

TL;DR: 提出了PREPO方法，通过利用内在数据特性提高RLVR的数据效率，包含两个互补组件：基于提示困惑度的适应性学习和基于相对熵差异的探索优先级排序，能在减少3倍rollout的情况下保持竞争性性能。


<details>
  <summary>Details</summary>
Motivation: RLVR虽然提升了大型语言模型的推理能力，但训练成本高昂，因为许多rollout对优化的贡献很小。本研究探索如何利用几乎免费的内在数据特性来提高RLVR的数据效率。

Method: PREPO方法包含两个组件：1）使用提示困惑度作为模型适应性指标，使模型从易到难学习；2）通过区分相对熵来放大rollout间的差异，优先选择探索程度更高的序列。

Result: 在Qwen和Llama模型上，PREPO在数学推理基准测试中取得了有效结果，相比基线方法减少了高达3倍的rollout需求，同时保持了竞争性性能。

Conclusion: PREPO通过利用内在数据特性显著提高了RLVR的数据效率，不仅获得了实证收益，还提供了理论和深入分析来解释方法的基本原理。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has improved the
reasoning ability of large language models, yet training remains costly because
many rollouts contribute little to optimization, considering the amount of
computation required. This study investigates how simply leveraging intrinsic
data properties, almost free benefit during training, can improve data
efficiency for RLVR. We propose PREPO with two complementary components. First,
we adopt prompt perplexity as an indicator of model adaptability in learning,
enabling the model to progress from well-understood contexts to more
challenging ones. Second, we amplify the discrepancy among the rollouts by
differentiating their relative entropy, and prioritize sequences that exhibit a
higher degree of exploration. Together, these mechanisms reduce rollout demand
while preserving competitive performance. On the Qwen and Llama models, PREPO
achieves effective results on mathematical reasoning benchmarks with up to 3
times fewer rollouts than the baselines. Beyond empirical gains, we provide
theoretical and in-depth analyses explaining the underlying rationale of our
method to improve the data efficiency of RLVR.

</details>


### [155] [Attention Saturation and Gradient Suppression at Inflection Layers: Diagnosing and Mitigating Bottlenecks in Transformer Adaptation](https://arxiv.org/abs/2511.00797)
*Wang Zixian*

Main category: cs.LG

TL;DR: 该论文分析了预训练Transformer在微调时出现的输出饱和和梯度抑制问题，提出了诊断指标来识别拐点层，并设计了在拐点层选择性注入LoRA适配器的微调策略。


<details>
  <summary>Details</summary>
Motivation: 预训练Transformer在微调时往往对源域模式过度自信，难以形成新的目标域模式，这源于输出饱和导致的梯度抑制问题。

Method: 通过交叉熵和softmax分析形式化输出饱和机制，提出层间诊断指标（注意力熵、梯度范数、Delta-CKA），在拐点层选择性注入LoRA适配器恢复被抑制的梯度信号。

Result: 实验表明，在过训练初始化下，拐点层LoRA注入能提升性能；在欠训练初始化下则会导致性能下降。当基础特征强时，解除拐点层阻塞有助于高层组合适应；当基础特征弱时，需要全路径解除阻塞进行低层重建。

Conclusion: 提出了诊断优先、轻量注入的微调策略，通过识别和解除拐点层梯度抑制，以最小参数开销实现有效的领域适应。

Abstract: Pre-trained Transformers often exhibit over-confidence in source patterns and
difficulty in forming new target-domain patterns during fine-tuning. We
formalize the mechanism of output saturation leading to gradient suppression
through standard cross-entropy and softmax analysis, showing that gradient
suppression at inflection layers confines adaptation to high-level
recombination of existing features while preventing low-level reconstruction.
We introduce a set of layer-wise diagnostic metrics -- attention entropy
(saturation proxy), activation gradient norm, parameter gradient norm, and
Delta-CKA under a shared PCA basis -- to identify inflection layers
characterized by both low attention entropy and steep gradient decay. Building
on these findings, we propose a diagnose-first, inject-light fine-tuning
strategy: selectively inserting LoRA adapters at inflection layers to restore
suppressed backward signals with minimal parameter overhead. Experiments on
BERT-base transfer from SST-2 to Rotten Tomatoes under under-trained and
over-trained source regimes reveal that over-trained initialization benefits
from inflection-layer LoRA injection, while under-trained initialization
suffers performance degradation. When base features are strong, unblocking
inflection layers facilitates high-level compositional adaptation; when base
features are weak, full-pathway unblocking is required for low-level
reconstruction, as supported by joint analysis of layer-wise activation
gradients and Delta-CKA dynamics.

</details>


### [156] [EraseFlow: Learning Concept Erasure Policies via GFlowNet-Driven Alignment](https://arxiv.org/abs/2511.00804)
*Abhiram Kusumba,Maitreya Patel,Kyle Min,Changhoon Kim,Chitta Baral,Yezhou Yang*

Main category: cs.LG

TL;DR: EraseFlow是一个基于GFlowNets的概念擦除框架，通过探索去噪路径空间来引导生成过程远离目标概念，同时保持模型先验知识，无需精心设计的奖励模型。


<details>
  <summary>Details</summary>
Motivation: 当前的概念擦除技术存在图像质量下降、依赖脆弱的对抗损失或需要大量重新训练的问题，需要一种更有效的方法来从文本到图像生成器中移除有害或专有概念。

Method: 将概念遗忘重新定义为在去噪路径空间中的探索问题，使用配备轨迹平衡目标的GFlowNets来优化，通过采样整个轨迹而非单个最终状态来学习随机策略。

Result: EraseFlow在广泛实证结果中优于现有基线，在性能和先验保持之间实现了最佳权衡，能够有效泛化到未见概念并避免可被攻击的奖励。

Conclusion: EraseFlow通过重新思考去噪轨迹的探索方式，提供了一种无需精心设计奖励模型的概念擦除解决方案，在保持生成质量的同时有效移除目标概念。

Abstract: Erasing harmful or proprietary concepts from powerful text to image
generators is an emerging safety requirement, yet current "concept erasure"
techniques either collapse image quality, rely on brittle adversarial losses,
or demand prohibitive retraining cycles. We trace these limitations to a myopic
view of the denoising trajectories that govern diffusion based generation. We
introduce EraseFlow, the first framework that casts concept unlearning as
exploration in the space of denoising paths and optimizes it with GFlowNets
equipped with the trajectory balance objective. By sampling entire trajectories
rather than single end states, EraseFlow learns a stochastic policy that steers
generation away from target concepts while preserving the model's prior.
EraseFlow eliminates the need for carefully crafted reward models and by doing
this, it generalizes effectively to unseen concepts and avoids hackable rewards
while improving the performance. Extensive empirical results demonstrate that
EraseFlow outperforms existing baselines and achieves an optimal trade off
between performance and prior preservation.

</details>


### [157] [Logic-informed reinforcement learning for cross-domain optimization of large-scale cyber-physical systems](https://arxiv.org/abs/2511.00806)
*Guangxi Wan,Peng Zeng,Xiaoting Dong,Chunhe Song,Shijie Cui,Dong Li,Qingwei Dong,Yiyang Liu,Hongfei Bai*

Main category: cs.LG

TL;DR: 提出逻辑信息强化学习(LIRL)，通过投影机制将潜在动作映射到由一阶逻辑定义的可行混合流形上，保证每个探索步骤的可行性，无需惩罚调优。


<details>
  <summary>Details</summary>
Motivation: 现有方法在混合动作空间中难以保证约束满足，分层方法牺牲全局最优性，而强化学习方法依赖脆弱的惩罚机制。

Method: 为标准策略梯度算法配备投影机制，将低维潜在动作映射到由一阶逻辑动态定义的可行混合流形上。

Result: 在工业制造、电动汽车充电站和交通信号控制等多个场景中均优于现有方法，在机器人减速器装配系统中使完工时间-能耗目标降低36.47%至44.33%，始终保持零约束违反。

Conclusion: 该框架可无缝迁移到其他领域，为大规模信息物理系统实现安全实时优化铺平道路。

Abstract: Cyber-physical systems (CPS) require the joint optimization of discrete cyber
actions and continuous physical parameters under stringent safety logic
constraints. However, existing hierarchical approaches often compromise global
optimality, whereas reinforcement learning (RL) in hybrid action spaces often
relies on brittle reward penalties, masking, or shielding and struggles to
guarantee constraint satisfaction. We present logic-informed reinforcement
learning (LIRL), which equips standard policy-gradient algorithms with
projection that maps a low-dimensional latent action onto the admissible hybrid
manifold defined on-the-fly by first-order logic. This guarantees feasibility
of every exploratory step without penalty tuning. Experimental evaluations have
been conducted across multiple scenarios, including industrial manufacturing,
electric vehicle charging stations, and traffic signal control, in all of which
the proposed method outperforms existing hierarchical optimization approaches.
Taking a robotic reducer assembly system in industrial manufacturing as an
example, LIRL achieves a 36.47\% to 44.33\% reduction at most in the combined
makespan-energy objective compared to conventional industrial hierarchical
scheduling methods. Meanwhile, it consistently maintains zero constraint
violations and significantly surpasses state-of-the-art hybrid-action
reinforcement learning baselines. Thanks to its declarative logic-based
constraint formulation, the framework can be seamlessly transferred to other
domains such as smart transportation and smart grid, thereby paving the way for
safe and real-time optimization in large-scale CPS.

</details>


### [158] [Equilibrium Policy Generalization: A Reinforcement Learning Framework for Cross-Graph Zero-Shot Generalization in Pursuit-Evasion Games](https://arxiv.org/abs/2511.00811)
*Runyu Lu,Peng Zhang,Ruochuan Shi,Yuanheng Zhu,Dongbin Zhao,Yang Liu,Dong Wang,Cesare Alippi*

Main category: cs.LG

TL;DR: 提出了一个均衡策略泛化（EPG）框架，用于在对抗性游戏中学习具有跨图零样本性能的泛化策略，特别针对追逃游戏（PEG）。


<details>
  <summary>Details</summary>
Motivation: 解决追逃游戏中图结构变化时需要重新计算或微调的问题，提高实时应用性。

Method: 使用动态规划算法生成纯策略纳什均衡，通过分组机制和序列模型进行策略分解，利用均衡引导和距离特征进行跨图训练。

Result: 在未见过的真实世界图中实现了良好的零样本性能，在带出口的图中，泛化追捕策略甚至能与微调后的最先进方法相媲美。

Conclusion: EPG框架有效解决了追逃游戏中的泛化问题，为对抗性游戏中的均衡学习提供了新思路。

Abstract: Equilibrium learning in adversarial games is an important topic widely
examined in the fields of game theory and reinforcement learning (RL).
Pursuit-evasion game (PEG), as an important class of real-world games from the
fields of robotics and security, requires exponential time to be accurately
solved. When the underlying graph structure varies, even the state-of-the-art
RL methods require recomputation or at least fine-tuning, which can be
time-consuming and impair real-time applicability. This paper proposes an
Equilibrium Policy Generalization (EPG) framework to effectively learn a
generalized policy with robust cross-graph zero-shot performance. In the
context of PEGs, our framework is generally applicable to both pursuer and
evader sides in both no-exit and multi-exit scenarios. These two
generalizability properties, to our knowledge, are the first to appear in this
domain. The core idea of the EPG framework is to train an RL policy across
different graph structures against the equilibrium policy for each single
graph. To construct an equilibrium oracle for single-graph policies, we present
a dynamic programming (DP) algorithm that provably generates pure-strategy Nash
equilibrium with near-optimal time complexity. To guarantee scalability with
respect to pursuer number, we further extend DP and RL by designing a grouping
mechanism and a sequence model for joint policy decomposition, respectively.
Experimental results show that, using equilibrium guidance and a distance
feature proposed for cross-graph PEG training, the EPG framework guarantees
desirable zero-shot performance in various unseen real-world graphs. Besides,
when trained under an equilibrium heuristic proposed for the graphs with exits,
our generalized pursuer policy can even match the performance of the fine-tuned
policies from the state-of-the-art PEG methods.

</details>


### [159] [LL-ViT: Edge Deployable Vision Transformers with Look Up Table Neurons](https://arxiv.org/abs/2511.00812)
*Shashank Nag,Alan T. L. Bacellar,Zachary Susskind,Anshul Jha,Logan Liberty,Aishwarya Sivakumar,Eugene B. John,Krishnan Kailas,Priscila M. V. Lima,Neeraja J. Yadwadkar,Felipe M. G. Franca,Lizy K. John*

Main category: cs.LG

TL;DR: LL-ViT是一种针对边缘FPGA优化的视觉Transformer设计，通过集成LUT神经元层来减少模型大小和计算需求，在保持精度的同时显著提升能效。


<details>
  <summary>Details</summary>
Motivation: 解决视觉Transformer在边缘FPGA上部署时面临的计算、内存和能耗挑战，同时克服现有LUT网络在视觉任务上性能不佳的问题。

Method: 设计基于LUT的通道混合器替代传统MLP层，采用神经学习方法原生学习LUT函数，并开发相应的FPGA加速器。

Result: 在CIFAR-10、CIFAR-100和Tiny-ImageNet上分别达到95.5%、78.8%和60.9%的精度，消除60%权重和50%乘法操作，能效提升1.9倍，延迟降低1.3倍。

Conclusion: LL-ViT为边缘设备上的视觉Transformer部署提供了高效解决方案，在保持性能的同时显著减少了资源消耗。

Abstract: Vision Transformers have been tremendously successful in computer vision
tasks. However, their large computational, memory, and energy demands are a
challenge for edge inference on FPGAs -- a field that has seen a recent surge
in demand. We recognize the benefits of recent works on logic and Look Up Table
(LUT) based networks, such as LogicNets, NeuraLUT, DWN, among others, in
offering models that simultaneously reduce both the memory and compute
footprints. However, these models natively do not perform well on common vision
tasks, such as CIFAR-10/100. In this work, we propose LL-ViT, a novel edge
optimized vision transformer design that integrates layers of LUT neurons
within the transformer architecture. Based on our characterization that reveals
that a majority of model weights and computations are from the channel mixer
(MLP layer), we design an alternate LUT-based channel mixer, and simultaneously
develop an FPGA-based accelerator for LL-ViT. Contrary to some attempts to
replace each multiplication with a table lookup, our architecture utilizes a
neural learning approach which natively learns the LUT functions. This approach
allows for reduced model sizes, and a computational and energy-efficient
inference solution for vision transformer models. Evaluating on edge-suitable
workloads, we achieve accuracies of 95.5% on CIFAR-10, 78.8% on CIFAR-100, and
60.9% on Tiny-ImageNet datasets, comparable to the baseline transformer. LL-ViT
eliminates over 60% of the model weights and 50% of the multiplications in the
model, and achieves 1.9x energy efficiency and 1.3x lower latency over an
integer quantized ViT accelerator, while also offering superior throughput
against prior works at a 10.9W power budget.

</details>


### [160] [Identifying Slug Formation in Oil Well Pipelines: A Use Case from Industrial Analytics](https://arxiv.org/abs/2511.00851)
*Abhishek Patange,Sharat Chidambaran,Prabhat Shankar,Manjunath G. B.,Anindya Chatterjee*

Main category: cs.LG

TL;DR: 开发了一个用于油气管道段塞流检测的交互式应用，集成了数据探索、模型训练、可视化分析和实时推理功能，通过用户友好界面实现端到端的检测流程。


<details>
  <summary>Details</summary>
Motivation: 现有段塞流检测方法多为离线、需要专业知识且缺乏实时可解释性，难以满足工业实时监测需求。

Method: 构建交互式应用，包含数据标注、可配置模型训练、时间序列可视化分类结果和基于持久性的实时推理告警模块。

Result: 系统支持从CSV数据上传到实时推理的无缝工作流，具有轻量、便携、易部署的特点，结合了领域分析和创新的UI/UX功能。

Conclusion: 该工具展示了交互式人机协同ML系统如何弥合数据科学方法与关键过程工业中实际决策之间的差距，在时间序列故障诊断任务中具有广泛应用前景。

Abstract: Slug formation in oil and gas pipelines poses significant challenges to
operational safety and efficiency, yet existing detection approaches are often
offline, require domain expertise, and lack real-time interpretability. We
present an interactive application that enables end-to-end data-driven slug
detection through a compact and user-friendly interface. The system integrates
data exploration and labeling, configurable model training and evaluation with
multiple classifiers, visualization of classification results with time-series
overlays, and a real-time inference module that generates persistence-based
alerts when slug events are detected. The demo supports seamless workflows from
labeled CSV uploads to live inference on unseen datasets, making it
lightweight, portable, and easily deployable. By combining domain-relevant
analytics with novel UI/UX features such as snapshot persistence, visual
labeling, and real-time alerting, our tool adds significant dissemination value
as both a research prototype and a practical industrial application. The demo
showcases how interactive human-in-the-loop ML systems can bridge the gap
between data science methods and real-world decision-making in critical process
industries, with broader applicability to time-series fault diagnosis tasks
beyond oil and gas.

</details>


### [161] [FlexiCache: Leveraging Temporal Stability of Attention Heads for Efficient KV Cache Management](https://arxiv.org/abs/2511.00868)
*Nazmul Takbir,Hamidreza Alikhani,Nikil Dutt,Sangeetha Abdu Jyothi*

Main category: cs.LG

TL;DR: FlexiCache是一个分层KV缓存管理系统，利用KV头的时间稳定性来减少GPU内存使用和计算开销，同时保持模型精度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型服务受KV缓存增长的约束，现有系统难以在不降低精度的情况下有效利用注意力机制中关键令牌的稀疏性，特别是在长生成场景中。

Method: FlexiCache将KV头分类为稳定和不稳定：不稳定头的所有KV缓存页面保留在GPU内存中，而稳定头仅保留前K个页面在GPU上，其余卸载到主机内存，并利用时间稳定性进行定期重新排名以获取新提升的顶部页面。

Result: 在vLLM上实现，FlexiCache将长上下文请求的GPU内存占用减少高达70%，离线服务吞吐量提高1.38-1.55倍，在线令牌延迟降低1.6-2.1倍。

Conclusion: FlexiCache通过利用KV头的时间稳定性，在保持长上下文、长生成场景精度的同时，显著提升了服务效率和资源利用率。

Abstract: Large Language Model (LLM) serving is increasingly constrained by the growing
size of the key-value (KV) cache, which scales with both context length and
generation length. Prior work shows that attention is dominated by a small
subset of critical tokens, yet existing systems struggle to exploit this
efficiently without degrading accuracy, especially in long generation. We make
a key observation: the temporal stability of these critical tokens varies
significantly across KV heads: some heads consistently focus on the same
tokens, while others shift frequently. Building on this insight, we introduce
FlexiCache, a hierarchical KV-cache management system that leverages the
temporal stability of KV heads to reduce GPU memory usage and computation
overhead, while preserving model accuracy. FlexiCache classifies KV heads as
stable or unstable: it retains all KV-cache pages from unstable heads in GPU
memory, whereas for stable heads, it keeps only the top-K pages on the GPU and
offloads the rest to host memory. By exploiting temporal stability, FlexiCache
performs periodic reranking for stable heads to fetch newly promoted top pages.
Implemented atop vLLM, FlexiCache reduces GPU memory footprint for long-context
requests by up to 70%, improves offline serving throughput by 1.38-1.55x, and
lowers online token latency by 1.6-2.1x, all while maintaining accuracy in
long-context, long-generation scenarios.

</details>


### [162] [Training with Fewer Bits: Unlocking Edge LLMs Training with Stochastic Rounding](https://arxiv.org/abs/2511.00874)
*Taowen Liu,Marta Andronic,Deniz Gündüz,George A. Constantinides*

Main category: cs.LG

TL;DR: 研究表明增加批量大小可以补偿量化训练中的精度损失，量化权重和激活对梯度方差有不同影响


<details>
  <summary>Details</summary>
Motivation: 量化训练虽然提高计算和内存效率，但引入量化噪声会阻碍收敛和降低模型精度，随机舍入与批量大小等训练因素的相互作用尚未充分探索

Method: 对带有随机舍入的小批量随机梯度下降进行理论和实证研究，分析量化权重和激活对梯度方差的不同影响

Result: 实验验证了理论见解，表明增加批量大小可以补偿反向传播期间降低的精度

Conclusion: 随机舍入与批量大小的协同作用可以有效缓解量化训练中的精度损失问题

Abstract: LLM training is resource-intensive. Quantized training improves computational
and memory efficiency but introduces quantization noise, which can hinder
convergence and degrade model accuracy. Stochastic Rounding (SR) has emerged as
a theoretically attractive alternative to deterministic rounding, offering
unbiased gradient estimates. However, its interaction with other training
factors -- especially batch size -- remains under explored. In this paper, we
present a theoretical and empirical study of mini-batch stochastic gradient
descent (SGD) with SR, showing that increased batch sizes can compensate for
reduced precision during back-propagation. Furthermore, we show that quantizing
weights and activations impacts gradient variance in distinct ways. Our
experiments validate these theoretical insights.

</details>


### [163] [KFCPO: Kronecker-Factored Approximated Constrained Policy Optimization](https://arxiv.org/abs/2511.00880)
*Joonyoung Lim,Younghwan Yoo*

Main category: cs.LG

TL;DR: KFCPO是一种结合Kronecker分解近似曲率二阶策略优化与安全感知梯度操作的安全强化学习算法，通过Fisher信息矩阵近似实现高效自然梯度更新，并使用边界感知梯度操作机制平衡奖励最大化与约束满足。


<details>
  <summary>Details</summary>
Motivation: 解决安全强化学习中奖励最大化与安全约束之间的权衡问题，避免传统方法中固定阈值导致的性能不稳定和安全性不足。

Method: 使用K-FAC近似Fisher信息矩阵进行高效二阶策略优化，引入边界感知梯度操作机制自适应调整奖励和成本梯度影响，采用小批量KL回滚策略确保信任区域合规。

Result: 在Safety Gymnasium基准测试中，KFCPO相比最佳基线方法实现了10.3%至50.2%的平均回报提升，同时满足安全约束。

Conclusion: KFCPO在安全强化学习中实现了安全性与性能的优越平衡，通过二阶优化和自适应梯度操作有效解决了奖励与约束的权衡问题。

Abstract: We propose KFCPO, a novel Safe Reinforcement Learning (Safe RL) algorithm
that combines scalable Kronecker-Factored Approximate Curvature (K-FAC) based
second-order policy optimization with safety-aware gradient manipulation. KFCPO
leverages K-FAC to perform efficient and stable natural gradient updates by
approximating the Fisher Information Matrix (FIM) in a layerwise, closed form
manner, avoiding iterative approximation overheads. To address the tradeoff
between reward maximization and constraint satisfaction, we introduce a margin
aware gradient manipulation mechanism that adaptively adjusts the influence of
reward and cost gradients based on the agent's proximity to safety boundaries.
This method blends gradients using a direction sensitive projection,
eliminating harmful interference and avoiding abrupt changes caused by fixed
hard thresholds. Additionally, a minibatch level KL rollback strategy is
adopted to ensure trust region compliance and to prevent destabilizing policy
shifts. Experiments on Safety Gymnasium using OmniSafe show that KFCPO achieves
10.3% to 50.2% higher average return across environments compared to the best
baseline that respected the safety constraint, demonstrating superior balance
of safety and performance.

</details>


### [164] [SpEx: A Spectral Approach to Explainable Clustering](https://arxiv.org/abs/2511.00885)
*Tal Argov,Tal Wagner*

Main category: cs.LG

TL;DR: 提出了一种基于谱图划分的可解释聚类新方法，能够将解释树适配到任何给定的非可解释聚类或数据集本身，并通过实验验证了方法的优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有可解释聚类方法主要关注最小化特定聚类目标的可解释性代价，缺乏通用的方法来将解释树适配到任意给定的聚类，而不受限制。

Method: 基于谱图划分设计可解释聚类算法，通过Trevisan(2013)的广义框架将先前的算法也解释为图划分，其中在两个图中同时优化切割。

Result: 实验表明，该方法在一系列数据集上相比基线方法表现出更优的性能。

Conclusion: 提出的基于谱图划分的通用可解释聚类方法能够有效适配解释树到任意聚类，且性能优于现有方法。

Abstract: Explainable clustering by axis-aligned decision trees was introduced by
Moshkovitz et al. (2020) and has gained considerable interest. Prior work has
focused on minimizing the price of explainability for specific clustering
objectives, lacking a general method to fit an explanation tree to any given
clustering, without restrictions. In this work, we propose a new and generic
approach to explainable clustering, based on spectral graph partitioning. With
it, we design an explainable clustering algorithm that can fit an explanation
tree to any given non-explainable clustering, or directly to the dataset
itself. Moreover, we show that prior algorithms can also be interpreted as
graph partitioning, through a generalized framework due to Trevisan (2013)
wherein cuts are optimized in two graphs simultaneously. Our experiments show
the favorable performance of our method compared to baselines on a range of
datasets.

</details>


### [165] [Learning with Category-Equivariant Representations for Human Activity Recognition](https://arxiv.org/abs/2511.00900)
*Yoshihiro Maruyama*

Main category: cs.LG

TL;DR: 提出了一种基于范畴对称性感知的学习框架，通过将时间、尺度和传感器层次结构的变化因素融入特征表示结构，使模型在现实扭曲下保持稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决人类活动识别中传感器信号随上下文、运动和环境变化而漂移的问题，需要模型在周围世界变化时保持稳定。

Method: 使用范畴对称性感知学习框架，将信号在时间、尺度和传感器层次结构上的变化因素构建到特征表示结构中，实现类别等变表示理论。

Result: 在UCI人类活动识别基准测试中，该设计将分布外准确率提高了约46个百分点（约3.6倍于基线）。

Conclusion: 抽象对称性原理可以通过范畴等变表示理论转化为日常感知任务中的具体性能提升。

Abstract: Human activity recognition is challenging because sensor signals shift with
context, motion, and environment; effective models must therefore remain stable
as the world around them changes. We introduce a categorical symmetry-aware
learning framework that captures how signals vary over time, scale, and sensor
hierarchy. We build these factors into the structure of feature
representations, yielding models that automatically preserve the relationships
between sensors and remain stable under realistic distortions such as time
shifts, amplitude drift, and device orientation changes. On the UCI Human
Activity Recognition benchmark, this categorical symmetry-driven design
improves out-of-distribution accuracy by approx. 46 percentage points (approx.
3.6x over the baseline), demonstrating that abstract symmetry principles can
translate into concrete performance gains in everyday sensing tasks via
category-equivariant representation theory.

</details>


### [166] [Random Spiking Neural Networks are Stable and Spectrally Simple](https://arxiv.org/abs/2511.00904)
*Ernesto Araya,Massimiliano Datres,Gitta Kutyniok*

Main category: cs.LG

TL;DR: 该论文通过布尔函数分析研究泄漏积分发放( LIF )SNN的稳定性和鲁棒性，发现宽LIF-SNN分类器在平均意义上稳定，其傅里叶频谱集中在低频分量。提出了谱简单性概念，证明随机LIF-SNN偏向简单函数。


<details>
  <summary>Details</summary>
Motivation: SNN作为节能计算的有前景范式，其理论基础特别是关于稳定性和鲁棒性的研究相比人工神经网络仍然有限。

Method: 通过布尔函数分析的视角研究离散时间LIF-SNN，重点关注分类任务中的噪声敏感性和稳定性，量化输入扰动对输出的影响。

Result: 主要结果表明宽LIF-SNN分类器在平均意义上稳定，这种性质由其傅里叶频谱在低频分量的集中所解释。实验证实这些稳定性特性在实践中持续存在。

Conclusion: 这些结果为SNN的稳定性和鲁棒性特性提供了新的见解，通过谱简单性框架将分析与深度网络中观察到的简单性偏差联系起来。

Abstract: Spiking neural networks (SNNs) are a promising paradigm for energy-efficient
computation, yet their theoretical foundations-especially regarding stability
and robustness-remain limited compared to artificial neural networks. In this
work, we study discrete-time leaky integrate-and-fire (LIF) SNNs through the
lens of Boolean function analysis. We focus on noise sensitivity and stability
in classification tasks, quantifying how input perturbations affect outputs.
Our main result shows that wide LIF-SNN classifiers are stable on average, a
property explained by the concentration of their Fourier spectrum on
low-frequency components. Motivated by this, we introduce the notion of
spectral simplicity, which formalizes simplicity in terms of Fourier spectrum
concentration and connects our analysis to the simplicity bias observed in deep
networks. Within this framework, we show that random LIF-SNNs are biased toward
simple functions. Experiments on trained networks confirm that these stability
properties persist in practice. Together, these results provide new insights
into the stability and robustness properties of SNNs.

</details>


### [167] [Transformers as Intrinsic Optimizers: Forward Inference through the Energy Principle](https://arxiv.org/abs/2511.00907)
*Ruifeng Ren,Sheng Ouyang,Huayi Tang,Yong Liu*

Main category: cs.LG

TL;DR: 本文提出了一个基于能量的统一框架来理解Transformer注意力机制，将标准softmax注意力视为最小化亥姆霍兹自由能量的特例，并基于经典梯度下降算法提出了新的注意力结构变体。


<details>
  <summary>Details</summary>
Motivation: Transformer模型虽然应用广泛，但其内在机制仍需深入探索。能量视角长期以来为理解神经计算提供了有价值的原则，本文旨在通过能量视角重新理解基于注意力的Transformer模型。

Method: 提出了统一的能量框架，包含全局能量、能量函数和梯度下降形式三个关键组件。将标准softmax注意力建模为最小化亥姆霍兹自由能量的特例，并将线性注意力自然纳入该框架。基于经典梯度下降算法（动量GD、NAG、牛顿法）扩展了注意力结构。

Result: 实验初步支持了基于能量框架设计注意力机制的潜力。

Conclusion: 能量视角为理解Transformer注意力机制提供了统一框架，能够自然地解释现有注意力变体，并为设计新的注意力结构提供了理论基础。

Abstract: Transformers have demonstrated strong adaptability across a wide range of
tasks and have become the backbone of modern Large Language Models (LLMs).
However, their underlying mechanisms remain open for further exploration. The
energy-based perspective has long provided a valuable principle for
understanding neural computation. In this paper, we revisit the principle of
energy as a lens to understand attention-based Transformer models. We present a
unified energy-based framework which is composed of three key components: the
global energy $F^*$, the energy function $E_i$ and the employed gradient
descent (GD) form. Within this framework, standard softmax attention can be
viewed as a special case of minimizing the Helmholtz free energy as $F^*$ using
standard GD when $E_i$ takes the form of elastic potential energy, with
residual connections ensuring that this optimization proceeds in an incremental
manner. In addition, linear attentions can also be naturally incorporated into
this framework by adjusting the corresponding energy forms. We also extend the
above analysis to the multi-head setting, where the energy is defined across
multiple low-dimensional subspaces. Building on this framework, we propose
energy-based modifications of attention structures. Inspired by classical GD
algorithms, we extend the original attention formulation based on standard GD
to the momentum-based GD, Nesterov Accelerated Gradient (NAG), and Newton's
method variants, each inducing a corresponding new attention structure. Our
experiments provide preliminary support for the potential of the energy-based
framework for designing attention mechanisms.

</details>


### [168] [Motion-Robust Multimodal Fusion of PPG and Accelerometer Signals for Three-Class Heart Rhythm Classification](https://arxiv.org/abs/2511.00949)
*Yangyang Zhao,Matti Kaisti,Olli Lahdenoja,Tero Koivisto*

Main category: cs.LG

TL;DR: RhythmiNet是一种结合PPG和加速度计信号的多模态神经网络，通过时间与通道注意力机制改进心房颤动检测，在嘈杂的真实临床数据中表现优于单模态方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于单通道PPG的心房颤动检测方法易受运动伪影和生理噪声影响，且仅限于二元分类，无法捕捉临床中更广泛的心律失常类型。

Method: 提出RhythmiNet残差神经网络，集成时间和通道注意力模块，联合利用PPG和加速度计信号进行三类心律分类（AF、窦性心律、其他）。测试数据按运动强度分层评估鲁棒性。

Result: RhythmiNet相比仅使用PPG的基线方法在macro-AUC上提升4.3%，比基于手工HRV特征逻辑回归模型性能提升12%。

Conclusion: 多模态融合和注意力学习在嘈杂真实临床数据中具有显著优势，能够提高心律分类的准确性和鲁棒性。

Abstract: Atrial fibrillation (AF) is a leading cause of stroke and mortality,
particularly in elderly patients. Wrist-worn photoplethysmography (PPG) enables
non-invasive, continuous rhythm monitoring, yet suffers from significant
vulnerability to motion artifacts and physiological noise. Many existing
approaches rely solely on single-channel PPG and are limited to binary AF
detection, often failing to capture the broader range of arrhythmias
encountered in clinical settings. We introduce RhythmiNet, a residual neural
network enhanced with temporal and channel attention modules that jointly
leverage PPG and accelerometer (ACC) signals. The model performs three-class
rhythm classification: AF, sinus rhythm (SR), and Other. To assess robustness
across varying movement conditions, test data are stratified by
accelerometer-based motion intensity percentiles without excluding any
segments. RhythmiNet achieved a 4.3% improvement in macro-AUC over the PPG-only
baseline. In addition, performance surpassed a logistic regression model based
on handcrafted HRV features by 12%, highlighting the benefit of multimodal
fusion and attention-based learning in noisy, real-world clinical data.

</details>


### [169] [The Hidden Power of Normalization: Exponential Capacity Control in Deep Neural Networks](https://arxiv.org/abs/2511.00958)
*Khoat Than*

Main category: cs.LG

TL;DR: 该论文提出了一个理论框架，从容量控制的角度解释了归一化方法在深度神经网络中的作用。研究表明归一化层能指数级降低网络的Lipschitz常数，从而平滑损失景观并约束有效容量，改善优化稳定性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 归一化方法在现代深度神经网络中是基本组件，经验上已知能稳定优化动态并改善泛化，但其理论机制，特别是在使用多个归一化层时的作用原理，仍然缺乏解释。

Method: 开发了一个理论框架，通过容量控制的视角分析归一化作用。证明了未归一化DNN可能具有指数级大的Lipschitz常数，而插入归一化层能以指数速率降低Lipschitz常数。

Result: 归一化层能指数级降低Lipschitz常数，这导致两个基本后果：(1) 以指数速率平滑损失景观，促进更快更稳定的优化；(2) 约束网络的有效容量，从而增强在未见数据上的泛化保证。

Conclusion: 该研究为归一化方法在深度学习中的经验成功提供了原则性解释，从理论上阐明了归一化如何通过控制容量来同时改善优化和泛化性能。

Abstract: Normalization methods are fundamental components of modern deep neural
networks (DNNs). Empirically, they are known to stabilize optimization dynamics
and improve generalization. However, the underlying theoretical mechanism by
which normalization contributes to both optimization and generalization remains
largely unexplained, especially when using many normalization layers in a DNN
architecture.
  In this work, we develop a theoretical framework that elucidates the role of
normalization through the lens of capacity control. We prove that an
unnormalized DNN can exhibit exponentially large Lipschitz constants with
respect to either its parameters or inputs, implying excessive functional
capacity and potential overfitting. Such bad DNNs are uncountably many. In
contrast, the insertion of normalization layers provably can reduce the
Lipschitz constant at an exponential rate in the number of normalization
operations. This exponential reduction yields two fundamental consequences: (1)
it smooths the loss landscape at an exponential rate, facilitating faster and
more stable optimization; and (2) it constrains the effective capacity of the
network, thereby enhancing generalization guarantees on unseen data. Our
results thus offer a principled explanation for the empirical success of
normalization methods in deep learning.

</details>


### [170] [Using Synthetic Data to estimate the True Error is theoretically and practically doable](https://arxiv.org/abs/2511.00964)
*Hai Hoang Thanh,Duy-Tung Nguyen,Hung The Tran,Khoat Than*

Main category: cs.LG

TL;DR: 本文提出了一种使用生成模型合成数据来估计模型测试误差的方法，特别适用于标记数据有限的情况。通过理论推导和实验验证，该方法比现有基线更准确可靠。


<details>
  <summary>Details</summary>
Motivation: 在现实应用中，获取大量标记测试数据成本高昂且耗时，而传统评估方法需要足够大的标记测试集。因此需要研究如何在有限标记数据条件下准确评估模型性能。

Method: 开发了考虑合成数据的泛化边界理论，提出了基于理论指导的优化合成数据生成方法，用于模型评估。

Result: 在模拟和表格数据集上的实验结果表明，相比现有基线方法，该方法能够获得更准确可靠的测试误差估计。

Conclusion: 生成模型合成的优化数据可以有效解决有限标记数据下的模型评估问题，生成器质量对评估准确性具有重要影响。

Abstract: Accurately evaluating model performance is crucial for deploying machine
learning systems in real-world applications. Traditional methods often require
a sufficiently large labeled test set to ensure a reliable evaluation. However,
in many contexts, a large labeled dataset is costly and labor-intensive.
Therefore, we sometimes have to do evaluation by a few labeled samples, which
is theoretically challenging. Recent advances in generative models offer a
promising alternative by enabling the synthesis of high-quality data. In this
work, we make a systematic investigation about the use of synthetic data to
estimate the test error of a trained model under limited labeled data
conditions. To this end, we develop novel generalization bounds that take
synthetic data into account. Those bounds suggest novel ways to optimize
synthetic samples for evaluation and theoretically reveal the significant role
of the generator's quality. Inspired by those bounds, we propose a
theoretically grounded method to generate optimized synthetic data for model
evaluation. Experimental results on simulation and tabular datasets demonstrate
that, compared to existing baselines, our method achieves accurate and more
reliable estimates of the test error.

</details>


### [171] [Modeling Microenvironment Trajectories on Spatial Transcriptomics with NicheFlow](https://arxiv.org/abs/2511.00977)
*Kristiyan Sakalyan,Alessandro Palma,Filippo Guerranti,Fabian J. Theis,Stephan Günnemann*

Main category: cs.LG

TL;DR: NicheFlow是一种基于流的生成模型，用于推断连续空间切片中细胞微环境的时序轨迹，通过将局部细胞邻域表示为点云，结合最优传输和变分流匹配来联合建模细胞状态和空间坐标的演化。


<details>
  <summary>Details</summary>
Motivation: 理解细胞微环境在时空数据中的演化对于解析组织发育和疾病进展至关重要。现有的单细胞水平建模方法忽略了组织中细胞状态的协调发育。

Method: 将局部细胞邻域表示为点云，使用最优传输和变分流匹配联合建模细胞状态和空间坐标的演化。

Result: 该方法在多种时空数据集（从胚胎发育到大脑发育）中成功恢复了全局空间结构和局部微环境组成。

Conclusion: NicheFlow能够有效推断细胞微环境的时空演化轨迹，为理解组织发育和疾病进展提供了新工具。

Abstract: Understanding the evolution of cellular microenvironments in spatiotemporal
data is essential for deciphering tissue development and disease progression.
While experimental techniques like spatial transcriptomics now enable
high-resolution mapping of tissue organization across space and time, current
methods that model cellular evolution operate at the single-cell level,
overlooking the coordinated development of cellular states in a tissue. We
introduce NicheFlow, a flow-based generative model that infers the temporal
trajectory of cellular microenvironments across sequential spatial slides. By
representing local cell neighborhoods as point clouds, NicheFlow jointly models
the evolution of cell states and spatial coordinates using optimal transport
and Variational Flow Matching. Our approach successfully recovers both global
spatial architecture and local microenvironment composition across diverse
spatiotemporal datasets, from embryonic to brain development.

</details>


### [172] [Balanced Multimodal Learning via Mutual Information](https://arxiv.org/abs/2511.00987)
*Rongrong Xie,Guido Sanguinetti*

Main category: cs.LG

TL;DR: 提出了一种解决多模态学习中模态不平衡问题的新框架，通过互信息量化模态间交互，采用跨模态知识蒸馏和多任务式训练来平衡不同模态的贡献。


<details>
  <summary>Details</summary>
Motivation: 多模态学习面临模态不平衡问题，特别是在生物数据分析中，数据集有限、获取成本高且质量不均。传统方法难以同时利用模态间协同效应并有效解决模态冲突。

Method: 采用两阶段平衡多模态学习策略：1) 跨模态知识蒸馏预训练，用强模态增强弱模态预测能力；2) 多任务式主训练，基于模态特定性能指标和模态间互信息动态校准梯度贡献。

Result: 该方法有效缓解了模态不平衡问题，显著提升了多模态模型的整体性能。

Conclusion: 提出的统一框架通过互信息量化和平衡学习策略，成功解决了多模态学习中的模态不平衡挑战，为生物数据等多模态应用提供了有效解决方案。

Abstract: Multimodal learning has increasingly become a focal point in research,
primarily due to its ability to integrate complementary information from
diverse modalities. Nevertheless, modality imbalance, stemming from factors
such as insufficient data acquisition and disparities in data quality, has
often been inadequately addressed. This issue is particularly prominent in
biological data analysis, where datasets are frequently limited, costly to
acquire, and inherently heterogeneous in quality. Conventional multimodal
methodologies typically fall short in concurrently harnessing intermodal
synergies and effectively resolving modality conflicts.
  In this study, we propose a novel unified framework explicitly designed to
address modality imbalance by utilizing mutual information to quantify
interactions between modalities. Our approach adopts a balanced multimodal
learning strategy comprising two key stages: cross-modal knowledge distillation
(KD) and a multitask-like training paradigm. During the cross-modal KD
pretraining phase, stronger modalities are leveraged to enhance the predictive
capabilities of weaker modalities. Subsequently, our primary training phase
employs a multitask-like learning mechanism, dynamically calibrating gradient
contributions based on modality-specific performance metrics and intermodal
mutual information. This approach effectively alleviates modality imbalance,
thereby significantly improving overall multimodal model performance.

</details>


### [173] [Hydra: Dual Exponentiated Memory for Multivariate Time Series Analysis](https://arxiv.org/abs/2511.00989)
*Asal Meskin,Alireza Mirrokni,Ali Najar,Ali Behrouz*

Main category: cs.LG

TL;DR: Hydra是一个双头元上下文记忆模块，通过2维递归在时间和变量维度上学习模式，解决了现有时间序列模型缺乏时间归纳偏置、忽略变量间依赖关系以及长序列建模效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer、MLP和线性模型在多元时间序列建模中存在三个主要问题：(1)置换等变性导致缺乏时间归纳偏置；(2)自然设计为单变量设置，忽略了时间和变量维度的相互依赖；(3)长序列建模效率低。线性RNN虽然解决了效率和归纳偏置问题，但局限于单序列且会传播误差。

Method: Hydra采用双头元上下文记忆模块，通过2维递归在时间和变量维度上学习模式，优先记忆更信息丰富的时间序列模式。虽然2维特性使训练递归且不可并行，但提出了2D分块训练算法，在保持效果的同时实现10倍效率提升。

Result: 在时间序列预测、分类和异常检测等多个任务和数据集上的实验结果表明，Hydra相比最先进的基线方法具有优越性能。

Conclusion: Hydra通过2维递归设计有效解决了现有时间序列模型的局限性，在多个任务上表现出色，同时通过创新的训练算法保证了效率。

Abstract: In recent years, effectively modeling multivariate time series has gained
significant popularity, mainly due to its wide range of applications, ranging
from healthcare to financial markets and energy management. Transformers, MLPs,
and linear models as the de facto backbones of modern time series models have
shown promising results in single-variant and/or short-term forecasting. These
models, however: (1) are permutation equivariant and so lack temporal inductive
bias, being less expressive to capture the temporal dynamics; (2) are naturally
designed for univariate setup, missing the inter-dependencies of temporal and
variate dimensions; and/or (3) are inefficient for Long-term time series
modeling. To overcome training and inference efficiency as well as the lack of
temporal inductive bias, recently, linear Recurrent Neural Networks (RNNs) have
gained attention as an alternative to Transformer-based models. These models,
however, are inherently limited to a single sequence, missing inter-variate
dependencies, and can propagate errors due to their additive nature. In this
paper, we present Hydra, a by-design two-headed meta in-context memory module
that learns how to memorize patterns at test time by prioritizing time series
patterns that are more informative about the data. Hydra uses a 2-dimensional
recurrence across both time and variate at each step, which is more powerful
than mixing methods. Although the 2-dimensional nature of the model makes its
training recurrent and non-parallelizable, we present a new 2D-chunk-wise
training algorithm that approximates the actual recurrence with $\times 10$
efficiency improvement, while maintaining the effectiveness. Our experimental
results on a diverse set of tasks and datasets, including time series
forecasting, classification, and anomaly detection show the superior
performance of Hydra compared to state-of-the-art baselines.

</details>


### [174] [None To Optima in Few Shots: Bayesian Optimization with MDP Priors](https://arxiv.org/abs/2511.01006)
*Diantong Li,Kyunghyun Cho,Chong Liu*

Main category: cs.LG

TL;DR: 提出了ProfBO算法，通过MDP先验建模相关任务的优化轨迹，显著减少黑盒优化所需的函数评估次数


<details>
  <summary>Details</summary>
Motivation: 贝叶斯优化在现实应用中因评估成本高昂而不实用，需要开发能在少量评估下获得高质量解的方法

Method: 使用MDP先验建模源任务的优化轨迹，将先验嵌入神经网络，采用模型无关元学习快速适应新任务

Result: 在Covid、Cancer基准和超参数调优任务中，ProfBO始终优于最先进方法，用更少评估获得高质量解

Conclusion: ProfBO已准备好实际部署，能有效解决评估成本高昂的黑盒优化问题

Abstract: Bayesian Optimization (BO) is an efficient tool for optimizing black-box
functions, but its theoretical guarantees typically hold in the asymptotic
regime. In many critical real-world applications such as drug discovery or
materials design, where each evaluation can be very costly and time-consuming,
BO becomes impractical for many evaluations. In this paper, we introduce the
Procedure-inFormed BO (ProfBO) algorithm, which solves black-box optimization
with remarkably few function evaluations. At the heart of our algorithmic
design are Markov Decision Process (MDP) priors that model optimization
trajectories from related source tasks, thereby capturing procedural knowledge
on efficient optimization. We embed these MDP priors into a prior-fitted neural
network and employ model-agnostic meta-learning for fast adaptation to new
target tasks. Experiments on real-world Covid and Cancer benchmarks and
hyperparameter tuning tasks demonstrate that ProfBO consistently outperforms
state-of-the-art methods by achieving high-quality solutions with significantly
fewer evaluations, making it ready for practical deployment.

</details>


### [175] [Equality Graph Assisted Symbolic Regression](https://arxiv.org/abs/2511.01009)
*Fabricio Olivetti de Franca,Gabriel Kronberger*

Main category: cs.LG

TL;DR: 提出SymRegg算法，利用等式图(e-graph)结构避免符号回归中冗余表达式计算，提高搜索效率


<details>
  <summary>Details</summary>
Motivation: 遗传编程在符号回归中需要大量计算冗余表达式（可达总评估次数的60%），通过e-graph结构可以避免重复计算

Method: 基于e-graph结构的新搜索算法：从e-graph中选择表达式进行扰动，若生成未访问表达式则插入e-graph并生成等价形式

Result: SymRegg提高了搜索效率，在不同数据集上保持准确结果，且只需极简超参数选择

Conclusion: e-graph结构能有效提升符号回归搜索效率，减少冗余计算

Abstract: In Symbolic Regression (SR), Genetic Programming (GP) is a popular search
algorithm that delivers state-of-the-art results in term of accuracy. Its
success relies on the concept of neutrality, which induces large plateaus that
the search can safely navigate to more promising regions. Navigating these
plateaus, while necessary, requires the computation of redundant expressions,
up to 60% of the total number of evaluation, as noted in a recent study. The
equality graph (e-graph) structure can compactly store and group equivalent
expressions enabling us to verify if a given expression and their variations
were already visited by the search, thus enabling us to avoid unnecessary
computation. We propose a new search algorithm for symbolic regression called
SymRegg that revolves around the e-graph structure following simple steps:
perturb solutions sampled from a selection of expressions stored in the
e-graph, if it generates an unvisited expression, insert it into the e-graph
and generates its equivalent forms. We show that SymRegg is capable of
improving the efficiency of the search, maintaining consistently accurate
results across different datasets while requiring a choice of a minimalist set
of hyperparameters.

</details>


### [176] [What's the next frontier for Data-centric AI? Data Savvy Agents](https://arxiv.org/abs/2511.01015)
*Nabeel Seedat,Jiashuo Liu,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: 本文主张在智能代理系统设计中应将数据处理能力作为首要考量，提出了实现数据感知代理的四个关键能力：主动数据获取、复杂数据处理、交互式测试数据合成和持续适应。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理在自主通信、协作和使用工具方面取得进展，但在数据处理方面仍被忽视。可扩展的自主性需要代理能够持续获取、处理和发展数据，以确保在现实世界中的可靠部署。

Method: 提出了四个关键能力框架：(1)主动数据获取：代理自主收集任务关键知识或征求人类输入填补数据空白；(2)复杂数据处理：上下文感知和灵活处理多样化数据挑战和输入；(3)交互式测试数据合成：从静态基准转向动态生成的交互式测试数据进行代理评估；(4)持续适应：代理迭代优化数据和背景知识以适应变化环境。

Result: 构建了一个数据感知代理的理论框架，强调数据处理能力应成为代理设计的核心要素，而非仅仅关注推理能力。

Conclusion: 当前代理研究主要强调推理能力，本文旨在激发对数据感知代理作为以数据为中心AI下一个前沿领域的思考，推动代理系统向更可靠的实际部署方向发展。

Abstract: The recent surge in AI agents that autonomously communicate, collaborate with
humans and use diverse tools has unlocked promising opportunities in various
real-world settings. However, a vital aspect remains underexplored: how agents
handle data. Scalable autonomy demands agents that continuously acquire,
process, and evolve their data. In this paper, we argue that data-savvy
capabilities should be a top priority in the design of agentic systems to
ensure reliable real-world deployment. Specifically, we propose four key
capabilities to realize this vision: (1) Proactive data acquisition: enabling
agents to autonomously gather task-critical knowledge or solicit human input to
address data gaps; (2) Sophisticated data processing: requiring context-aware
and flexible handling of diverse data challenges and inputs; (3) Interactive
test data synthesis: shifting from static benchmarks to dynamically generated
interactive test data for agent evaluation; and (4) Continual adaptation:
empowering agents to iteratively refine their data and background knowledge to
adapt to shifting environments. While current agent research predominantly
emphasizes reasoning, we hope to inspire a reflection on the role of data-savvy
agents as the next frontier in data-centric AI.

</details>


### [177] [SARIMAX-Based Power Outage Prediction During Extreme Weather Events](https://arxiv.org/abs/2511.01017)
*Haoran Ye,Qiuzhuang Sun,Yang Yang*

Main category: cs.LG

TL;DR: 开发基于SARIMAX的短期停电预测系统，用于极端天气事件。通过两阶段特征工程和稳健优化策略，在24小时和48小时预测范围内相比基线方法提升8.4%的预测性能。


<details>
  <summary>Details</summary>
Motivation: 解决极端天气事件期间短期电力中断的准确预测问题，传统方法在数据不规则性和数值不稳定性方面存在挑战。

Method: 采用两阶段特征工程（数据清洗和相关性过滤），结合时间嵌入、多尺度滞后特征和天气变量作为外生输入，使用SARIMAX模型，并实施标准化、分层拟合策略和回退机制。

Result: 模型在RMSE指标上达到177.2，相比基线方法（193.4）提升了8.4%，验证了特征工程和优化策略的有效性。

Conclusion: 提出的SARIMAX预测系统在极端天气相关的停电预测中表现出色，特征工程和稳健优化策略显著提升了预测准确性。

Abstract: This study develops a SARIMAX-based prediction system for short-term power
outage forecasting during extreme weather events. Using hourly data from
Michigan counties with outage counts and comprehensive weather features, we
implement a systematic two-stage feature engineering pipeline: data cleaning to
remove zero-variance and unknown features, followed by correlation-based
filtering to eliminate highly correlated predictors. The selected features are
augmented with temporal embeddings, multi-scale lag features, and weather
variables with their corresponding lags as exogenous inputs to the SARIMAX
model. To address data irregularity and numerical instability, we apply
standardization and implement a hierarchical fitting strategy with sequential
optimization methods, automatic downgrading to ARIMA when convergence fails,
and historical mean-based fallback predictions as a final safeguard. The model
is optimized separately for short-term (24 hours) and medium-term (48 hours)
forecast horizons using RMSE as the evaluation metric. Our approach achieves an
RMSE of 177.2, representing an 8.4\% improvement over the baseline method (RMSE
= 193.4), thereby validating the effectiveness of our feature engineering and
robust optimization strategy for extreme weather-related outage prediction.

</details>


### [178] [MedEqualizer: A Framework Investigating Bias in Synthetic Medical Data and Mitigation via Augmentation](https://arxiv.org/abs/2511.01054)
*Sama Salarian,Yue Zhang,Swati Padhee,Srinivasan Parthasarathy*

Main category: cs.LG

TL;DR: 评估基于GAN的合成医疗数据公平性，提出MedEqualizer框架改善人口统计学平衡


<details>
  <summary>Details</summary>
Motivation: 合成医疗数据可提高数据可访问性，但需确保跨受保护属性的公平性，避免临床研究和决策中的偏见

Method: 使用MIMIC-III数据集评估多种GAN模型的公平性，引入MedEqualizer模型无关增强框架来丰富代表性不足的亚组

Result: 合成数据中存在显著亚组不平衡，MedEqualizer显著改善了合成数据的人口统计学平衡

Conclusion: MedEqualizer为更公平和代表性的医疗数据合成提供了可行路径

Abstract: Synthetic healthcare data generation presents a viable approach to enhance
data accessibility and support research by overcoming limitations associated
with real-world medical datasets. However, ensuring fairness across protected
attributes in synthetic data is critical to avoid biased or misleading results
in clinical research and decision-making. In this study, we assess the fairness
of synthetic data generated by multiple generative adversarial network
(GAN)-based models using the MIMIC-III dataset, with a focus on
representativeness across protected demographic attributes. We measure subgroup
representation using the logarithmic disparity metric and observe significant
imbalances, with many subgroups either underrepresented or overrepresented in
the synthetic data, compared to the real data. To mitigate these disparities,
we introduce MedEqualizer, a model-agnostic augmentation framework that
enriches the underrepresented subgroups prior to synthetic data generation. Our
results show that MedEqualizer significantly improves demographic balance in
the resulting synthetic datasets, offering a viable path towards more equitable
and representative healthcare data synthesis.

</details>


### [179] [Window-Based Feature Engineering for Cognitive Workload Detection](https://arxiv.org/abs/2511.01060)
*Andrew Hallam,R G Gayathri,Glory Lee,Atul Sajjanhar*

Main category: cs.LG

TL;DR: 使用COLET数据集，通过基于窗口的时间特征提取和机器学习/深度学习技术对认知负荷进行分类，深度学习模型特别是表格架构在各项指标上优于传统机器学习方法。


<details>
  <summary>Details</summary>
Motivation: 认知负荷在健康、心理学和国防应用等领域日益重要，需要开发有效的实时评估方法。

Method: 采用基于窗口的时间分区方法增强特征，然后使用机器学习和深度学习模型对认知负荷水平进行分类。

Result: 深度学习模型（特别是表格架构）在精确度、F1分数、准确率和分类精度方面都优于传统机器学习方法。

Conclusion: 基于窗口的时间特征提取和深度学习技术对于复杂动态任务中的实时认知负荷评估具有显著效果和潜力。

Abstract: Cognitive workload is a topic of increasing interest across various fields
such as health, psychology, and defense applications. In this research, we
focus on classifying cognitive workload using the COLET dataset, employing a
window-based approach for feature generation and machine/deep learning
techniques for classification. We apply window-based temporal partitioning to
enhance features used in existing research, followed by machine learning and
deep learning models to classify different levels of cognitive workload. The
results demonstrate that deep learning models, particularly tabular
architectures, outperformed traditional machine learning methods in precision,
F1-score, accuracy, and classification precision. This study highlights the
effectiveness of window-based temporal feature extraction and the potential of
deep learning techniques for real-time cognitive workload assessment in complex
and dynamic tasks.

</details>


### [180] [Energy-Efficient Deep Learning Without Backpropagation: A Rigorous Evaluation of Forward-Only Algorithms](https://arxiv.org/abs/2511.01061)
*Przemysław Spyra,Witold Dzwinel*

Main category: cs.LG

TL;DR: 该论文挑战了反向传播(BP)对实现最先进性能至关重要的长期假设，提出Mono-Forward(MF)算法在分类准确率上持续超越优化调参的BP基线，同时实现显著能效提升。


<details>
  <summary>Details</summary>
Motivation: 质疑反向传播在深度学习中的必要性，探索更高效、可持续的替代方法，解决BP在能效和训练速度方面的局限性。

Method: 提出Mono-Forward(MF)算法，这是从Forward-Forward(FF)到Cascaded Forward(CaFo)再到MF的演进路径，在相同架构和通用超参数优化框架下进行公平比较。

Result: MF在MLP架构上分类准确率持续超越BP基线，能耗降低高达41%，训练速度提升高达34%，但无BP方法的内存效率优势在实践中可能被开销抵消。

Conclusion: MF被确立为MLP中实用、高性能且可持续的BP替代方案，挑战了BP在深度学习中的核心地位。

Abstract: The long-held assumption that backpropagation (BP) is essential for
state-of-the-art performance is challenged by this work. We present rigorous,
hardware-validated evidence that the Mono-Forward (MF) algorithm, a
backpropagation-free method, consistently surpasses an optimally tuned BP
baseline in classification accuracy on its native Multi-Layer Perceptron (MLP)
architectures. This superior generalization is achieved with profound
efficiency gains, including up to 41% less energy consumption and up to 34%
faster training. Our analysis, which charts an evolutionary path from Geoffrey
Hinton's Forward-Forward (FF) to the Cascaded Forward (CaFo) and finally to MF,
is grounded in a fair comparative framework using identical architectures and
universal hyperparameter optimization. We further provide a critical
re-evaluation of memory efficiency in BP-free methods, empirically
demonstrating that practical overhead can offset theoretical gains. Ultimately,
this work establishes MF as a practical, high-performance, and sustainable
alternative to BP for MLPs.

</details>


### [181] [Happiness as a Measure of Fairness](https://arxiv.org/abs/2511.01069)
*Georg Pichler,Marco Romanelli,Pablo Piantanida*

Main category: cs.LG

TL;DR: 提出基于幸福感的公平性框架，通过线性规划计算最优公平后处理策略，统一并扩展了多种已知公平性定义


<details>
  <summary>Details</summary>
Motivation: 现有公平性定义缺乏直观性和人性化，需要一种更贴近人类感知的公平性度量方法

Method: 基于幸福感的公平框架，将公平性定义为各群体从决策结果中获得的效用，使用线性规划求解最优后处理策略

Result: 方法高效且可扩展，能够统一和扩展多种已知公平性定义，在多样化场景中表现出实用优势

Conclusion: 幸福感公平框架提供了既直观又数学严谨的公平性方法，具有良好的理论统一性和实践应用价值

Abstract: In this paper, we propose a novel fairness framework grounded in the concept
of happi- ness, a measure of the utility each group gains fromdecisionoutcomes.
Bycapturingfairness through this intuitive lens, we not only offer a more
human-centered approach, but also one that is mathematically rigorous: In order
to compute the optimal, fair post-processing strategy, only a linear program
needs to be solved. This makes our method both efficient and scalable with
existing optimization tools. Furthermore, it unifies and extends several
well-known fairness definitions, and our em- pirical results highlight its
practical strengths across diverse scenarios.

</details>


### [182] [AI Progress Should Be Measured by Capability-Per-Resource, Not Scale Alone: A Framework for Gradient-Guided Resource Allocation in LLMs](https://arxiv.org/abs/2511.01077)
*David McCoy,Yulun Wu,Zachary Butzin-Dozier*

Main category: cs.LG

TL;DR: 挑战AI研究中的"规模至上主义"，提出以能力-资源比为导向的LLM开发新范式，通过梯度影响模式指导资源分配决策，显著提升AI生命周期效率。


<details>
  <summary>Details</summary>
Motivation: 当前AI研究过度追求模型规模和计算量增长，导致不可持续的环境影响和资源不平等问题，需要从根本上重新定位LLM发展方向。

Method: 基于梯度影响模式的理论框架，识别transformer模型中具有重大影响的少数参数，提出仅更新高影响参数、使用梯度范数作为高效代理指标、协调参数和数据选择的方法。

Result: 理论分析表明，仅更新高影响参数在性能-资源比上严格优于全参数调优，简单梯度范数可高效识别关键组件，协调参数和数据选择可大幅降低资源需求。

Conclusion: 提出两阶段范式：基础开发者的边际收益预训练和下游用户的影响引导适应，通过梯度蓝图连接，将资源意识嵌入模型开发、适应和评估过程，实现更可持续和公平的AI发展。

Abstract: This position paper challenges the "scaling fundamentalism" dominating AI
research, where unbounded growth in model size and computation has led to
unsustainable environmental impacts and widening resource inequality. We argue
that LLM development should be fundamentally reoriented toward
capability-per-resource rather than capability alone. We present a theoretical
framework demonstrating that resource-allocation decisions guided by gradient
influence patterns can dramatically improve efficiency throughout the AI
lifecycle. Our analysis shows that in transformer-based models, where a small
fraction of parameters exert outsized influence (following heavy-tailed
distributions), three critical insights emerge: (1) updating only
high-influence parameters strictly outperforms full-parameter tuning on a
performance-per-resource basis; (2) simple gradient norms provide
computationally efficient proxies for identifying these high-influence
components; and (3) coordinated parameter and data selection yields
multiplicative efficiency gains, potentially reducing resource requirements by
orders of magnitude. Building on these theoretical foundations, we propose a
two stage paradigm marginal-return pretraining for foundation developers and
influence guided adaptation for downstream users bridged by gradient
blueprints, metadata describing which parameters matter most for various tasks.
This capability-per-resource perspective transforms what were once considered
pragmatic hardware workarounds into theoretically optimal strategies,
democratizing access to cutting-edge AI capabilities while significantly
reducing environmental impact. By embedding resource consciousness into how we
develop, adapt, and evaluate models, we can reshape AI progress toward a more
sustainable and equitable future.

</details>


### [183] [Continual Learning, Not Training: Online Adaptation For Agents](https://arxiv.org/abs/2511.01093)
*Aman Jaglan,Jarrod Barnes*

Main category: cs.LG

TL;DR: ATLAS是一个双智能体架构，通过分离推理（教师）和执行（学生）角色，结合持久学习记忆，实现无需梯度更新的持续学习，在推理时动态调整操作策略。


<details>
  <summary>Details</summary>
Motivation: 传统持续学习方法依赖基于梯度的重新训练，不适合需要实时适应的部署智能体。需要一种无需梯度更新的持续学习方案。

Method: 采用双智能体架构：教师负责推理，学生负责执行；包含持久学习记忆存储经验指导；通过编排层在推理时动态调整监督级别和初始计划选择。

Result: 在微软ExCyTIn-Bench基准测试中，使用GPT-5-mini作为学生，ATLAS达到54.1%成功率，比GPT-5（High）高13%，成本降低86%。跨事件验证显示泛化能力：冻结的小册子将准确率从28%提升至41%。

Conclusion: ATLAS确立了无梯度持续学习作为实现自适应、可部署AI系统的可行路径，提供了可用于训练显式世界模型的有价值因果注释轨迹。

Abstract: Continual Learning (CL) methods have traditionally focused on mitigating
catastrophic forgetting through gradient-based retraining, an approach
ill-suited for deployed agents that must adapt in real time. We introduce our
Adaptive Teaching and Learning System (ATLAS), a dual-agent architecture that
decouples reasoning (Teacher) from execution (Student) and incorporates a
persistent learning memory that stores distilled guidance from experience. This
informs the orchestration layer, enabling the system to dynamically adjust its
operational strategies, such as supervision level or initial plan selection, at
inference time. In doing so, ATLAS achieves gradient-free continual learning,
shifting the locus of adaptation from model parameters to system-level
orchestration. We formulate this as a system-centric paradigm for continual
learning, where the objective is adaptive efficiency: maximizing task success
while minimizing computational cost through inference-time orchestration rather
than parameter updates. Evaluated on Microsoft's ExCyTIn-Bench, an open-source
benchmark simulating complex cyberthreat investigation, ATLAS achieves 54.1%
success with GPT-5-mini as its Student, outperforming the larger GPT-5 (High)
by 13% while reducing cost by 86%. Cross-incident validation demonstrates
generalization: frozen pamphlets from Incident #5 improve accuracy from 28% to
41% with zero retraining, while shifting output composition from verbose
exploration to structured reasoning. Together, these findings establish
gradient-free continual learning as a viable path toward adaptive, deployable
AI systems and provide causally annotated traces valuable for training explicit
world models.

</details>


### [184] [One model to solve them all: 2BSDE families via neural operators](https://arxiv.org/abs/2511.01125)
*Takashi Furuya,Anastasis Kratsios,Dylan Possamaï,Bogdan Raonić*

Main category: cs.LG

TL;DR: 本文提出了一种基于Kolmogorov-Arnold网络的温和生成变体神经算子模型，用于求解具有随机终止时间的二阶倒向随机微分方程(2BSDEs)族。


<details>
  <summary>Details</summary>
Motivation: 解决无限族二阶倒向随机微分方程在正则有界欧几里得域上的求解问题，特别关注随机终止时间的情况。

Method: 利用Kolmogorov-Arnold网络构建神经算子模型，通过多项式参数数量实现高效近似。

Result: 证明了对于广泛的2BSDE族，解算子可由适当的神经算子模型近似；识别出一个结构化子类，其神经算子近似仅需多项式参数数量，而非一般最坏情况下的指数需求。

Conclusion: 该方法在保持近似精度的同时，显著降低了参数复杂度，为高效求解2BSDE族提供了新途径。

Abstract: We introduce a mild generative variant of the classical neural operator
model, which leverages Kolmogorov--Arnold networks to solve infinite families
of second-order backward stochastic differential equations ($2$BSDEs) on
regular bounded Euclidean domains with random terminal time. Our first main
result shows that the solution operator associated with a broad range of
$2$BSDE families is approximable by appropriate neural operator models. We then
identify a structured subclass of (infinite) families of $2$BSDEs whose neural
operator approximation requires only a polynomial number of parameters in the
reciprocal approximation rate, as opposed to the exponential requirement in
general worst-case neural operator guarantees.

</details>


### [185] [Stochastic Regret Guarantees for Online Zeroth- and First-Order Bilevel Optimization](https://arxiv.org/abs/2511.01126)
*Parvin Nazari,Bojian Hou,Davoud Ataee Tarzanagh,Li Shen,George Michailidis*

Main category: cs.LG

TL;DR: 本文提出了一种新的搜索方向，使一阶和零阶随机在线双层优化算法无需窗口平滑即可实现次线性随机双层遗憾，提高了效率并减少了超梯度估计的oracle依赖。


<details>
  <summary>Details</summary>
Motivation: 当前在线双层优化方法依赖确定性窗口平滑遗憾最小化，在函数快速变化时无法准确反映系统性能。

Method: 引入新的搜索方向，结合一阶和零阶随机算法，减少超梯度估计的oracle依赖，同时更新内外层变量和线性系统解，使用零阶方法估计Hessian、Jacobian和梯度。

Result: 算法实现了无需窗口平滑的次线性随机双层遗憾，在在线参数化损失调优和黑盒对抗攻击实验中验证了有效性。

Conclusion: 提出的框架在保证性能的同时显著提高了在线双层优化的效率。

Abstract: Online bilevel optimization (OBO) is a powerful framework for machine
learning problems where both outer and inner objectives evolve over time,
requiring dynamic updates. Current OBO approaches rely on deterministic
\textit{window-smoothed} regret minimization, which may not accurately reflect
system performance when functions change rapidly. In this work, we introduce a
novel search direction and show that both first- and zeroth-order (ZO)
stochastic OBO algorithms leveraging this direction achieve sublinear
{stochastic bilevel regret without window smoothing}. Beyond these guarantees,
our framework enhances efficiency by: (i) reducing oracle dependence in
hypergradient estimation, (ii) updating inner and outer variables alongside the
linear system solution, and (iii) employing ZO-based estimation of Hessians,
Jacobians, and gradients. Experiments on online parametric loss tuning and
black-box adversarial attacks validate our approach.

</details>


### [186] [Regularization Implies balancedness in the deep linear network](https://arxiv.org/abs/2511.01137)
*Kathryn Lindsey,Govind Menon*

Main category: cs.LG

TL;DR: 使用几何不变量理论研究深度线性网络，通过Kempf-Ness定理证明L2正则化器在平衡流形上最小化，将训练动态分解为纤维上的正则化流和平衡流形上的学习流，正则化流可通过矩映射精确求解。


<details>
  <summary>Details</summary>
Motivation: 为深度学习和线性系统理论中的平衡性提供统一的数学框架，从模型简化和贝叶斯原理角度解释平衡性。

Method: 应用几何不变量理论和Kempf-Ness定理，将训练动态分解为两个梯度流：纤维上的正则化流和平衡流形上的学习流，使用矩映射求解正则化流。

Result: 建立了L2正则化器在平衡流形上最小化的理论，实现了训练动态的精确分解，正则化流可解析求解。

Conclusion: 该方法为深度学习和线性系统理论中的平衡性概念提供了统一的数学解释框架，连接了模型简化和贝叶斯原理视角。

Abstract: We use geometric invariant theory (GIT) to study the deep linear network
(DLN). The Kempf-Ness theorem is used to establish that the $L^2$ regularizer
is minimized on the balanced manifold. This allows us to decompose the training
dynamics into two distinct gradient flows: a regularizing flow on fibers and a
learning flow on the balanced manifold. We show that the regularizing flow is
exactly solvable using the moment map.
  This approach provides a common mathematical framework for balancedness in
deep learning and linear systems theory. We use this framework to interpret
balancedness in terms of model reduction and Bayesian principles.

</details>


### [187] [Adapt under Attack and Domain Shift: Unified Adversarial Meta-Learning and Domain Adaptation for Robust Automatic Modulation Classification](https://arxiv.org/abs/2511.01172)
*Ali Owfi,Amirmohammad Bamdad,Tolunay Seyfi,Fatemeh Afghah*

Main category: cs.LG

TL;DR: 提出了一种结合元学习和领域自适应的统一框架，使自动调制分类系统能够同时抵抗对抗攻击和环境变化。


<details>
  <summary>Details</summary>
Motivation: 深度学习在自动调制分类中表现出色，但易受对抗攻击和数据分布变化的影响，阻碍了在实际动态环境中的部署。

Method: 采用两阶段策略：离线阶段使用元学习方法在单一源域上训练模型，使其能够泛化防御未知攻击；在线阶段应用领域自适应将模型特征与新目标域对齐，无需大量标注数据。

Result: 该框架显著提高了调制分类在面对组合威胁时的准确性。

Conclusion: 该框架为解决现代AMC系统的部署和操作挑战提供了关键解决方案。

Abstract: Deep learning has emerged as a leading approach for Automatic Modulation
Classification (AMC), demonstrating superior performance over traditional
methods. However, vulnerability to adversarial attacks and susceptibility to
data distribution shifts hinder their practical deployment in real-world,
dynamic environments. To address these threats, we propose a novel, unified
framework that integrates meta-learning with domain adaptation, making AMC
systems resistant to both adversarial attacks and environmental changes. Our
framework utilizes a two-phase strategy. First, in an offline phase, we employ
a meta-learning approach to train the model on clean and adversarially
perturbed samples from a single source domain. This method enables the model to
generalize its defense, making it resistant to a combination of previously
unseen attacks. Subsequently, in the online phase, we apply domain adaptation
to align the model's features with a new target domain, allowing it to adapt
without requiring substantial labeled data. As a result, our framework achieves
a significant improvement in modulation classification accuracy against these
combined threats, offering a critical solution to the deployment and
operational challenges of modern AMC systems.

</details>


### [188] [A Comparative Study of Model Adaptation Strategies for Multi-Treatment Uplift Modeling](https://arxiv.org/abs/2511.01185)
*Ruyue Zhang,Xiaopeng Ke,Ming Liu,Fangzhou Shi,Chang Men,Zhengdan Zhu*

Main category: cs.LG

TL;DR: 本文提出了一种基于函数逼近定理的正交函数适应方法，用于提升多治疗场景下的提升建模效果和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前多治疗提升建模方法通常从二元治疗工作改编而来，这些改编方法在不同数据特征下无法保持有效性。

Method: 将现有模型改编分为结构适应和特征适应两类，提出基于函数逼近定理的正交函数适应方法。

Result: 实验表明，OFA方法相比其他基础改编方法能显著提升模型性能，并具有最高的鲁棒性。

Conclusion: 正交函数适应方法能有效提升多治疗场景下提升建模的性能和鲁棒性。

Abstract: Uplift modeling has emerged as a crucial technique for individualized
treatment effect estimation, particularly in fields such as marketing and
healthcare. Modeling uplift effects in multi-treatment scenarios plays a key
role in real-world applications. Current techniques for modeling
multi-treatment uplift are typically adapted from binary-treatment works. In
this paper, we investigate and categorize all current model adaptations into
two types: Structure Adaptation and Feature Adaptation. Through our empirical
experiments, we find that these two adaptation types cannot maintain
effectiveness under various data characteristics (noisy data, mixed with
observational data, etc.). To enhance estimation ability and robustness, we
propose Orthogonal Function Adaptation (OFA) based on the function
approximation theorem. We conduct comprehensive experiments with multiple data
characteristics to study the effectiveness and robustness of all model
adaptation techniques. Our experimental results demonstrate that our proposed
OFA can significantly improve uplift model performance compared to other
vanilla adaptation methods and exhibits the highest robustness.

</details>


### [189] [Analyzing the Power of Chain of Thought through Memorization Capabilities](https://arxiv.org/abs/2511.01190)
*Lijia Yu,Xiao-Shan Gao,Lijun Zhang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: It has been shown that the chain of thought (CoT) can enhance the power of
large language models (LLMs) to solve certain mathematical reasoning problems.
However, the capacity of CoT is still not fully explored. As an important
instance, the following basic question has not yet been answered: Does CoT
expand the capability of transformers across all reasoning tasks? We
demonstrate that reasoning with transformers is essentially a memorization
problem for reasoning datasets. Thus, examining the power of CoT across all
reasoning tasks amounts to analyzing the memorization capabilities of CoT
transformers. In this paper, we give a complete description of the memorization
capabilities of fixed-precision transformers with or without CoT and give a
negative answer to the above-mentioned question. Precisely, we first give
necessary and sufficient conditions for fixed-precision transformers with and
without CoT to memorize a finite reasoning dataset and show that these two
conditions do not imply each other. Then, we give lower and upper bounds for
the number of parameters needed for transformers with or without CoT to
memorize a finite reasoning dataset with $N$ elements, which are
$\overline{\Theta}(N)$ in all cases. This implies that there exist reasoning
tasks for which CoT does not enhance the reasoning power of transformers,
leading to a negative answer to the above-mentioned question. Finally, we give
the first results on memorizing infinite reasoning datasets by CoT transformers
and show that some simple infinite datasets cannot be memorized by transformers
with or without CoT.

</details>


### [190] [Transmitter Identification and Protocol Categorization in Shared Spectrum via Multi-Task RF Classification at the Network Edge](https://arxiv.org/abs/2511.01198)
*Tariq Abdul-Quddoos,Tasnia Sharmin,Xiangfang Li,Lijun Qian*

Main category: cs.LG

TL;DR: 提出一个多任务RF信号分类框架，用于共享频谱环境中的发射机识别和协议分类，使用CNN处理信号重叠和环境变化问题，在POWDER平台数据上取得了高准确率。


<details>
  <summary>Details</summary>
Motivation: 随着频谱共享日益重要，频谱监测和发射机识别对于执行频谱使用政策、提高频谱利用效率和保障网络安全至关重要。

Method: 设计卷积神经网络(CNN)，采用多通道输入策略提取有意义的信号特征，解决信号特征重叠和环境变化等关键挑战。

Result: 在POWDER平台RF数据上取得显著准确率：协议分类90%，发射基站分类100%，联合分类任务92%。

Conclusion: 该方法在增强现代无线网络频谱监测、管理和安全方面具有重要潜力。

Abstract: As spectrum sharing becomes increasingly vital to meet rising wireless
demands in the future, spectrum monitoring and transmitter identification are
indispensable for enforcing spectrum usage policy, efficient spectrum
utilization, and net- work security. This study proposed a robust framework for
transmitter identification and protocol categorization via multi- task RF
signal classification in shared spectrum environments, where the spectrum
monitor will classify transmission protocols (e.g., 4G LTE, 5G-NR, IEEE
802.11a) operating within the same frequency bands, and identify different
transmitting base stations, as well as their combinations. A Convolutional
Neural Network (CNN) is designed to tackle critical challenges such as
overlapping signal characteristics and environmental variability. The proposed
method employs a multi-channel input strategy to extract meaningful signal
features, achieving remarkable accuracy: 90% for protocol classification, 100%
for transmitting base station classification, and 92% for joint classification
tasks, utilizing RF data from the POWDER platform. These results highlight the
significant potential of the proposed method to enhance spectrum monitoring,
management, and security in modern wireless networks.

</details>


### [191] [FEval-TTC: Fair Evaluation Protocol for Test-Time Compute](https://arxiv.org/abs/2511.01203)
*Pavel Rumiantsev,Soumyasundar Pal,Yingxue Zhang,Mark Coates*

Main category: cs.LG

TL;DR: 提出了FEval-TTC公平评估协议，用于确保测试时计算方法的评估一致性，不受LLM性能和API成本波动的影响。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的性能和API调用成本会随时间波动，这可能使先前研究的结论失效，需要一种公平的评估方法。

Method: 设计了FEval-TTC协议，标准化少样本提示和答案提取过程，支持跨多个LLM在数学和常识推理数据集上的评估，并提供成本建模程序。

Result: 开发了开源评估框架，减少了研究者的时间和金钱开销，便于公平比较不同的测试时计算方法。

Conclusion: FEval-TTC为测试时计算方法的评估提供了标准化和公平的基准，有助于确保研究结论的长期有效性。

Abstract: The performance of Large Language Models (LLMs) and the associated dollar
costs of API calls can fluctuate over time, potentially invalidating
conclusions drawn in prior research. To address this, we propose a Fair
Evaluation protocol for Test-Time Compute (FEval-TTC), designed to ensure
consistent assessment of test-time compute (TTC) methods, regardless of such
fluctuations. FEval-TTC focuses on the evaluation of TTC methods that utilize
underlying Chains-of-Thought (CoT). It supports evaluations across multiple
LLMs on a diverse set of mathematical and commonsense reasoning datasets. The
few-shot prompting and answer extraction processes are standardized across
datasets, reducing both time and monetary overhead for researchers.
Furthermore, we provide a cost modelling procedure that estimates both the
token and dollar cost per query, facilitating equitable comparisons of
prevalent TTC methods. We open-source FEval-TTC for public use at
https://github.com/networkslab/feval_ttc .

</details>


### [192] [Optimizing Electric Vehicle Charging Station Placement Using Reinforcement Learning and Agent-Based Simulations](https://arxiv.org/abs/2511.01218)
*Minh-Duc Nguyen,Dung D. Le,Phi Long Nguyen*

Main category: cs.LG

TL;DR: 提出了一种结合深度强化学习和基于代理模拟的新框架，用于优化电动汽车充电站布局，通过混合奖励函数和双Q网络显著减少了等待时间。


<details>
  <summary>Details</summary>
Motivation: 现有充电站布局方法使用确定性奖励系统，无法充分应对现实世界的动态和不确定性，导致评估成本高且不反映真实场景。

Method: 整合深度强化学习与基于代理的模拟，使用具有双Q网络的混合强化学习代理来选择最优位置和配置充电端口，采用结合确定性因素和模拟反馈的混合奖励函数。

Result: 在越南河内的案例研究中，该方法相比初始状态将平均等待时间减少了53.28%，优于静态基线方法。

Conclusion: 该可扩展和自适应解决方案增强了电动汽车基础设施规划，有效应对现实世界复杂性并改善了用户体验。

Abstract: The rapid growth of electric vehicles (EVs) necessitates the strategic
placement of charging stations to optimize resource utilization and minimize
user inconvenience. Reinforcement learning (RL) offers an innovative approach
to identifying optimal charging station locations; however, existing methods
face challenges due to their deterministic reward systems, which limit
efficiency. Because real-world conditions are dynamic and uncertain, a
deterministic reward structure cannot fully capture the complexities of
charging station placement. As a result, evaluation becomes costly and
time-consuming, and less reflective of real-world scenarios. To address this
challenge, we propose a novel framework that integrates deep RL with
agent-based simulations to model EV movement and estimate charging demand in
real time. Our approach employs a hybrid RL agent with dual Q-networks to
select optimal locations and configure charging ports, guided by a hybrid
reward function that combines deterministic factors with simulation-derived
feedback. Case studies in Hanoi, Vietnam, show that our method reduces average
waiting times by 53.28% compared to the initial state, outperforming static
baseline methods. This scalable and adaptive solution enhances EV
infrastructure planning, effectively addressing real-world complexities and
improving user experience.

</details>


### [193] [WindMiL: Equivariant Graph Learning for Wind Loading Prediction](https://arxiv.org/abs/2511.01226)
*Themistoklis Vargiemezis,Charilaos Kanatsoulis,Catherine Gorlé*

Main category: cs.LG

TL;DR: WindMiL是一个结合系统数据集生成和对称感知图神经网络的机器学习框架，用于高效预测建筑风荷载，解决了传统方法计算成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 传统风洞测试和大涡模拟计算成本过高，每个大涡模拟案例需要至少24小时计算时间，使得大规模参数研究不可行。

Method: 1) 通过符号距离函数插值生成屋顶几何形状，模拟462个不同形状和风向的大涡模拟案例，创建大规模风荷载数据集；2) 开发反射等变图神经网络，确保在镜像几何形状下的物理一致性预测。

Result: 在插值和外推评估中，WindMiL对表面压力系数的均值和标准差均实现高精度（如平均Cp的RMSE≤0.02），在反射测试评估中保持96%以上的命中率，而非等变基线模型下降超过10%。

Conclusion: 通过将系统数据集与等变代理模型相结合，WindMiL能够实现高效、可扩展且准确的建筑风荷载预测。

Abstract: Accurate prediction of wind loading on buildings is crucial for structural
safety and sustainable design, yet conventional approaches such as wind tunnel
testing and large-eddy simulation (LES) are prohibitively expensive for
large-scale exploration. Each LES case typically requires at least 24 hours of
computation, making comprehensive parametric studies infeasible. We introduce
WindMiL, a new machine learning framework that combines systematic dataset
generation with symmetry-aware graph neural networks (GNNs). First, we
introduce a large-scale dataset of wind loads on low-rise buildings by applying
signed distance function interpolation to roof geometries and simulating 462
cases with LES across varying shapes and wind directions. Second, we develop a
reflection-equivariant GNN that guarantees physically consistent predictions
under mirrored geometries. Across interpolation and extrapolation evaluations,
WindMiL achieves high accuracy for both the mean and the standard deviation of
surface pressure coefficients (e.g., RMSE $\leq 0.02$ for mean $C_p$) and
remains accurate under reflected-test evaluation, maintaining hit rates above
$96\%$ where the non-equivariant baseline model drops by more than $10\%$. By
pairing a systematic dataset with an equivariant surrogate, WindMiL enables
efficient, scalable, and accurate predictions of wind loads on buildings.

</details>


### [194] [A Saddle Point Remedy: Power of Variable Elimination in Non-convex Optimization](https://arxiv.org/abs/2511.01234)
*Min Gan,Guang-Yong Chen,Yang Yi,Lin Yang*

Main category: cs.LG

TL;DR: 变量消除算法通过重塑优化景观，将原始问题中的鞍点转化为简化问题中的局部最大值，从而显著改善非凸优化的收敛性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 理解变量消除算法（如VarPro）在实践中表现出的优越收敛性和鲁棒性的原理机制，特别是在处理大规模非凸优化中的鞍点问题时。

Method: 基于Hessian惯性和Schur补的严格几何分析，比较原始和简化公式的优化景观，证明变量消除如何重塑临界点结构。

Result: 证明局部最大值在简化景观中是由原始公式中的鞍点创建并直接对应的，在非凸矩阵分解、双参数神经网络和深度残差网络训练中验证了方法的有效性。

Conclusion: 通过鞍点变换进行景观简化是一个强大原则，可以指导设计更鲁棒高效的优化算法，超越了仅仅解释现有方法的范畴。

Abstract: The proliferation of saddle points, rather than poor local minima, is
increasingly understood to be a primary obstacle in large-scale non-convex
optimization for machine learning. Variable elimination algorithms, like
Variable Projection (VarPro), have long been observed to exhibit superior
convergence and robustness in practice, yet a principled understanding of why
they so effectively navigate these complex energy landscapes has remained
elusive. In this work, we provide a rigorous geometric explanation by comparing
the optimization landscapes of the original and reduced formulations. Through a
rigorous analysis based on Hessian inertia and the Schur complement, we prove
that variable elimination fundamentally reshapes the critical point structure
of the objective function, revealing that local maxima in the reduced landscape
are created from, and correspond directly to, saddle points in the original
formulation. Our findings are illustrated on the canonical problem of
non-convex matrix factorization, visualized directly on two-parameter neural
networks, and finally validated in training deep Residual Networks, where our
approach yields dramatic improvements in stability and convergence to superior
minima. This work goes beyond explaining an existing method; it establishes
landscape simplification via saddle point transformation as a powerful
principle that can guide the design of a new generation of more robust and
efficient optimization algorithms.

</details>


### [195] [KAT-GNN: A Knowledge-Augmented Temporal Graph Neural Network for Risk Prediction in Electronic Health Records](https://arxiv.org/abs/2511.01249)
*Kun-Wei Lin,Yu-Chen Kuo,Hsin-Yao Wang,Yi-Ju Tseng*

Main category: cs.LG

TL;DR: KAT-GNN是一个结合临床知识和时序动态的图神经网络框架，用于电子健康记录的风险预测，在冠状动脉疾病和院内死亡率预测任务上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录数据具有异构性和不规则时序性，建模这些数据面临挑战。需要整合临床知识和时序动态来提高风险预测准确性。

Method: 构建模态特定的患者图，使用SNOMED CT本体和共现先验进行知识增强，采用时间感知transformer捕捉纵向动态。

Result: 在CAD预测中AUROC达到0.9269±0.0029，在MIMIC-III和MIMIC-IV死亡率预测中分别达到0.9230±0.0070和0.8849±0.0089，优于GRASP和RETAIN等基线方法。

Conclusion: 将临床知识整合到图表示中，结合时间感知注意力机制，为跨不同临床任务和数据集的风险预测提供了有效且可推广的方法。

Abstract: Clinical risk prediction using electronic health records (EHRs) is vital to
facilitate timely interventions and clinical decision support. However,
modeling heterogeneous and irregular temporal EHR data presents significant
challenges. We propose \textbf{KAT-GNN} (Knowledge-Augmented Temporal Graph
Neural Network), a graph-based framework that integrates clinical knowledge and
temporal dynamics for risk prediction. KAT-GNN first constructs
modality-specific patient graphs from EHRs. These graphs are then augmented
using two knowledge sources: (1) ontology-driven edges derived from SNOMED CT
and (2) co-occurrence priors extracted from EHRs. Subsequently, a time-aware
transformer is employed to capture longitudinal dynamics from the graph-encoded
patient representations. KAT-GNN is evaluated on three distinct datasets and
tasks: coronary artery disease (CAD) prediction using the Chang Gung Research
Database (CGRD) and in-hospital mortality prediction using the MIMIC-III and
MIMIC-IV datasets. KAT-GNN achieves state-of-the-art performance in CAD
prediction (AUROC: 0.9269 $\pm$ 0.0029) and demonstrated strong results in
mortality prediction in MIMIC-III (AUROC: 0.9230 $\pm$ 0.0070) and MIMIC-IV
(AUROC: 0.8849 $\pm$ 0.0089), consistently outperforming established baselines
such as GRASP and RETAIN. Ablation studies confirm that both knowledge-based
augmentation and the temporal modeling component are significant contributors
to performance gains. These findings demonstrate that the integration of
clinical knowledge into graph representations, coupled with a time-aware
attention mechanism, provides an effective and generalizable approach for risk
prediction across diverse clinical tasks and datasets.

</details>


### [196] [A Spatio-Temporal Online Robust Tensor Recovery Approach for Streaming Traffic Data Imputation](https://arxiv.org/abs/2511.01267)
*Yiyang Yang,Xiejian Chi,Shanxing Gao,Kaidong Wang,Yao Wang*

Main category: cs.LG

TL;DR: 提出一种新颖的在线鲁棒张量恢复算法，用于智能交通系统中的交通数据恢复，能够同时处理缺失值和异常值，在保持高恢复精度的同时显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统批量张量恢复方法在计算和存储资源需求高，难以应对持续增长的交通数据量；现有在线方法在复杂现实场景中性能下降严重，未能充分利用交通数据的固有结构特性。

Method: 将交通数据恢复问题重新表述为流式框架，提出同时利用交通数据全局时空相关性和局部一致性的在线鲁棒张量恢复算法。

Result: 在三个真实交通数据集上的实验表明，该方法实现了高恢复精度，同时计算效率相比最先进的批量方法提升了三个数量级。

Conclusion: 该方法作为智能交通系统中交通数据质量增强的可扩展有效解决方案具有巨大潜力。

Abstract: Data quality is critical to Intelligent Transportation Systems (ITS), as
complete and accurate traffic data underpin reliable decision-making in traffic
control and management. Recent advances in low-rank tensor recovery algorithms
have shown strong potential in capturing the inherent structure of
high-dimensional traffic data and restoring degraded observations. However,
traditional batch-based methods demand substantial computational and storage
resources, which limits their scalability in the face of continuously expanding
traffic data volumes. Moreover, recent online tensor recovery methods often
suffer from severe performance degradation in complex real-world scenarios due
to their insufficient exploitation of the intrinsic structural properties of
traffic data. To address these challenges, we reformulate the traffic data
recovery problem within a streaming framework, and propose a novel online
robust tensor recovery algorithm that simultaneously leverages both the global
spatio-temporal correlations and local consistency of traffic data, achieving
high recovery accuracy and significantly improved computational efficiency in
large-scale scenarios. Our method is capable of simultaneously handling missing
and anomalous values in traffic data, and demonstrates strong adaptability
across diverse missing patterns. Experimental results on three real-world
traffic datasets demonstrate that the proposed approach achieves high recovery
accuracy while significantly improving computational efficiency by up to three
orders of magnitude compared to state-of-the-art batch-based methods. These
findings highlight the potential of the proposed approach as a scalable and
effective solution for traffic data quality enhancement in ITS.

</details>


### [197] [Adversarial Spatio-Temporal Attention Networks for Epileptic Seizure Forecasting](https://arxiv.org/abs/2511.01275)
*Zan Li,Kyongmin Yeo,Wesley Gifford,Lara Marcuse,Madeline Fields,Bülent Yener*

Main category: cs.LG

TL;DR: STAN是一个对抗性时空注意力网络，用于从多变量EEG信号预测癫痫发作，通过级联注意力块联合建模空间脑连接和时间神经动态，实现高灵敏度和低误报率。


<details>
  <summary>Details</summary>
Motivation: 癫痫发作预测在医疗时间序列预测中面临关键挑战，需要高灵敏度、低误报率和个体特异性适应能力。现有方法假设固定的发作前持续时间或分别处理时空特征，无法有效捕捉双向依赖关系。

Method: 提出STAN网络，通过交替的空间和时间模块级联注意力块，联合建模空间脑连接和时间神经动态。采用带梯度惩罚的对抗训练，从明确定义的15分钟发作前窗口学习区分发作间期和发作前期状态。

Result: 在两个基准EEG数据集上达到最先进性能：CHB-MIT头皮数据集96.6%灵敏度、0.011次/小时误报；MSSM颅内数据集94.2%灵敏度、0.063次/小时误报。模型计算高效（230万参数，45ms延迟，180MB内存），适合实时边缘部署。

Conclusion: STAN能够捕捉细微的发作前动态，在个体特异性时间（通常发作前15-45分钟）触发可靠警报，无需个体化训练。该框架为医疗和其他时间序列领域的时空预测提供了通用范例。

Abstract: Forecasting epileptic seizures from multivariate EEG signals represents a
critical challenge in healthcare time series prediction, requiring high
sensitivity, low false alarm rates, and subject-specific adaptability. We
present STAN, an Adversarial Spatio-Temporal Attention Network that jointly
models spatial brain connectivity and temporal neural dynamics through cascaded
attention blocks with alternating spatial and temporal modules. Unlike existing
approaches that assume fixed preictal durations or separately process spatial
and temporal features, STAN captures bidirectional dependencies between spatial
and temporal patterns through a unified cascaded architecture. Adversarial
training with gradient penalty enables robust discrimination between interictal
and preictal states learned from clearly defined 15-minute preictal windows.
Continuous 90-minute pre-seizure monitoring reveals that the learned
spatio-temporal attention patterns enable early detection: reliable alarms
trigger at subject-specific times (typically 15-45 minutes before onset),
reflecting the model's capacity to capture subtle preictal dynamics without
requiring individualized training. Experiments on two benchmark EEG datasets
(CHB-MIT scalp: 8 subjects, 46 events; MSSM intracranial: 4 subjects, 14
events) demonstrate state-of-the-art performance: 96.6% sensitivity with 0.011
false detections per hour and 94.2% sensitivity with 0.063 false detections per
hour, respectively, while maintaining computational efficiency (2.3M
parameters, 45 ms latency, 180 MB memory) for real-time edge deployment. Beyond
epilepsy, the proposed framework provides a general paradigm for
spatio-temporal forecasting in healthcare and other time series domains where
individual heterogeneity and interpretability are crucial.

</details>


### [198] [Identification of Capture Phases in Nanopore Protein Sequencing Data Using a Deep Learning Model](https://arxiv.org/abs/2511.01277)
*Annabelle Martin,Daphne Kontogiorgos-Heintz,Jeff Nivala*

Main category: cs.LG

TL;DR: 开发了一个轻量级1D CNN模型CaptureNet-Deep，用于自动检测纳米孔蛋白测序中的捕获阶段，将分析时间从数天缩短到30分钟以内。


<details>
  <summary>Details</summary>
Motivation: 手动识别纳米孔蛋白测序中的捕获阶段耗时且需要专业知识，需要自动化解决方案来提高效率。

Method: 使用轻量级一维卷积神经网络(1D CNN)在降采样信号窗口中检测捕获阶段，并与CNN-LSTM混合模型、基于直方图的分类器和其他CNN变体进行比较。

Result: 最佳模型CaptureNet-Deep在测试数据上达到F1分数0.94和精度93.39%，支持低延迟推理并集成到实验仪表板中。

Conclusion: 使用简单可解释的轻量级机器学习模型可以实现高效的实时捕获检测，在测序工作流程中具有广泛应用前景。

Abstract: Nanopore protein sequencing produces long, noisy ionic current traces in
which key molecular phases, such as protein capture and translocation, are
embedded. Capture phases mark the successful entry of a protein into the pore
and serve as both a checkpoint and a signal that a channel merits further
analysis. However, manual identification of capture phases is time-intensive,
often requiring several days for expert reviewers to annotate the data due to
the need for domain-specific interpretation of complex signal patterns. To
address this, a lightweight one-dimensional convolutional neural network (1D
CNN) was developed and trained to detect capture phases in down-sampled signal
windows. Evaluated against CNN-LSTM (Long Short-Term Memory) hybrids,
histogram-based classifiers, and other CNN variants using run-level data
splits, our best model, CaptureNet-Deep, achieved an F1 score of 0.94 and
precision of 93.39% on held-out test data. The model supports low-latency
inference and is integrated into a dashboard for Oxford Nanopore experiments,
reducing the total analysis time from several days to under thirty minutes.
These results show that efficient, real-time capture detection is possible
using simple, interpretable architectures and suggest a broader role for
lightweight ML models in sequencing workflows.

</details>


### [199] [Lyapunov Stability Learning with Nonlinear Control via Inductive Biases](https://arxiv.org/abs/2511.01283)
*Yupu Lu,Shijie Lin,Hao Xu,Zeqing Zhang,Jia Pan*

Main category: cs.LG

TL;DR: 提出了一种将Lyapunov条件作为归纳偏置的神经网络控制Lyapunov函数（CLF）设计方法，通过端到端学习同时优化CLF和控制器，相比现有方法提高了收敛率和吸引域（ROA）


<details>
  <summary>Details</summary>
Motivation: 现有基于学习-验证框架的深度CLF方法将Lyapunov条件作为复杂约束进行优化，难以实现全局收敛且验证过程复杂，需要改进这一框架

Method: 将Lyapunov条件作为归纳偏置，设计神经网络CLF和基于CLF的控制器，实现有限约束下的稳定优化过程，支持CLF和控制器的端到端学习

Result: 在大量实验案例中，该方法相比现有方法在CLF学习上实现了更高的收敛率和更大的吸引域（ROA），并深入揭示了先前方法在学习过程中成功率下降的原因

Conclusion: 提出的基于归纳偏置的CLF设计方法有效改进了学习-验证框架，实现了更稳定和高效的CLF学习

Abstract: Finding a control Lyapunov function (CLF) in a dynamical system with a
controller is an effective way to guarantee stability, which is a crucial issue
in safety-concerned applications. Recently, deep learning models representing
CLFs have been applied into a learner-verifier framework to identify
satisfiable candidates. However, the learner treats Lyapunov conditions as
complex constraints for optimisation, which is hard to achieve global
convergence. It is also too complicated to implement these Lyapunov conditions
for verification. To improve this framework, we treat Lyapunov conditions as
inductive biases and design a neural CLF and a CLF-based controller guided by
this knowledge. This design enables a stable optimisation process with limited
constraints, and allows end-to-end learning of both the CLF and the controller.
Our approach achieves a higher convergence rate and larger region of attraction
(ROA) in learning the CLF compared to existing methods among abundant
experiment cases. We also thoroughly reveal why the success rate decreases with
previous methods during learning.

</details>


### [200] [Koopman-based Prediction of Connectivity for Flying Ad Hoc Networks](https://arxiv.org/abs/2511.01286)
*Sivaram Krishnan,Jinho Choi,Jihong Park,Gregory Sherman,Benjamin Campbell*

Main category: cs.LG

TL;DR: 本文探讨了使用数据驱动的Koopman方法来解决飞行自组织网络(FANETs)中高度动态环境下的通信挑战，通过集中式和分布式两种方法预测无人机轨迹动态和SINR，以改善网络性能。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习技术在静态无线环境中表现良好，但在高度动态的FANETs环境中存在局限性，需要开发能够适应不断变化拓扑的新方法。

Method: 利用Koopman算子理论，提出了集中式和分布式两种方法，通过预测无人机轨迹动态和信号干扰加噪声比(SINR)来建模FANETs通信。

Result: 结果表明，这些方法能够准确预测导致通信中断的连接和隔离事件，为无人机基于预测调度传输提供了可能。

Conclusion: 数据驱动的Koopman方法能够有效应对FANETs的动态环境挑战，提高通信可靠性，为下一代AI无线网络提供了有前景的解决方案。

Abstract: The application of machine learning (ML) to communication systems is expected
to play a pivotal role in future artificial intelligence (AI)-based
next-generation wireless networks. While most existing works focus on ML
techniques for static wireless environments, they often face limitations when
applied to highly dynamic environments, such as flying ad hoc networks
(FANETs). This paper explores the use of data-driven Koopman approaches to
address these challenges. Specifically, we investigate how these approaches can
model UAV trajectory dynamics within FANETs, enabling more accurate predictions
and improved network performance. By leveraging Koopman operator theory, we
propose two possible approaches -- centralized and distributed -- to
efficiently address the challenges posed by the constantly changing topology of
FANETs. To demonstrate this, we consider a FANET performing surveillance with
UAVs following pre-determined trajectories and predict
signal-to-interference-plus-noise ratios (SINRs) to ensure reliable
communication between UAVs. Our results show that these approaches can
accurately predict connectivity and isolation events that lead to modelled
communication outages. This capability could help UAVs schedule their
transmissions based on these predictions.

</details>


### [201] [LSHFed: Robust and Communication-Efficient Federated Learning with Locally-Sensitive Hashing Gradient Mapping](https://arxiv.org/abs/2511.01296)
*Guanjie Cheng,Mengzhen Yang,Xinkui Zhao,Shuyi Yu,Tianyu Du,Yangyang Wu,Mengying Zhu,Shuiguang Deng*

Main category: cs.LG

TL;DR: LSHFed是一个鲁棒且通信高效的联邦学习框架，通过LSHGM梯度验证机制将高维梯度投影为紧凑的二进制表示，在保护隐私的同时有效检测恶意梯度，显著降低通信开销。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在分布式环境中面临推理攻击和投毒攻击的威胁，现有防御方法存在通信计算成本高、检测精度有限的问题。

Method: 提出LSHFed框架，核心是LSHGM梯度验证机制，使用多超平面局部敏感哈希将高维梯度投影为紧凑的二进制表示，仅通过不可逆的哈希形式检测恶意梯度。

Result: 实验表明，即使50%参与者是恶意攻击者，LSHFed仍能保持高模型性能，梯度验证通信量相比全梯度方法减少高达1000倍。

Conclusion: LSHFed在增强联邦学习鲁棒性和隐私保护的同时，显著降低了通信开销，为信任缺失环境下的安全联邦学习提供了有效解决方案。

Abstract: Federated learning (FL) enables collaborative model training across
distributed nodes without exposing raw data, but its decentralized nature makes
it vulnerable in trust-deficient environments. Inference attacks may recover
sensitive information from gradient updates, while poisoning attacks can
degrade model performance or induce malicious behaviors. Existing defenses
often suffer from high communication and computation costs, or limited
detection precision. To address these issues, we propose LSHFed, a robust and
communication-efficient FL framework that simultaneously enhances aggregation
robustness and privacy preservation. At its core, LSHFed incorporates LSHGM, a
novel gradient verification mechanism that projects high-dimensional gradients
into compact binary representations via multi-hyperplane locally-sensitive
hashing. This enables accurate detection and filtering of malicious gradients
using only their irreversible hash forms, thus mitigating privacy leakage risks
and substantially reducing transmission overhead. Extensive experiments
demonstrate that LSHFed maintains high model performance even when up to 50% of
participants are collusive adversaries while achieving up to a 1000x reduction
in gradient verification communication compared to full-gradient methods.

</details>


### [202] [Diffusion-Based Solver for CNF Placement on the Cloud-Continuum](https://arxiv.org/abs/2511.01343)
*Álvaro Vázquez Rodríguez,Manuel Fernández-Veiga,Carlos Giraldo-Rodríguez*

Main category: cs.LG

TL;DR: 提出基于去噪扩散概率模型(DDPM)的云原生网络功能(CNF)放置新框架，将放置问题重新定义为生成图到分配任务，通过图神经网络迭代优化分配矩阵，实现快速可行的解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统方法（混合整数非线性规划、启发式算法、强化学习）在可扩展性、约束处理和泛化能力方面存在局限，需要新的解决方案来应对5G/6G网络中CNF在云连续体上的放置挑战。

Method: 将CNF放置问题编码为异构图，训练图神经网络去噪器迭代优化噪声CNF到云的分配矩阵，在损失函数中直接整合约束特定损失来学习可行解空间。

Result: 在多种拓扑结构上的广泛评估表明，该方法能持续产生可行解，推理速度比MINLP求解器快几个数量级。

Conclusion: 基于扩散的生成建模在约束网络嵌入问题中具有巨大潜力，为实现分布式云原生网络功能的实用、可扩展编排做出了贡献。

Abstract: The placement of Cloud-Native Network Functions (CNFs) across the
Cloud-Continuum represents a core challenge in the orchestration of current 5G
and future 6G networks. The process involves the placement of interdependent
computing tasks, structured as Service Function Chains, over distributed cloud
infrastructures. This is achieved while satisfying strict resource, bandwidth
and latency constraints. It is acknowledged that classical approaches,
including mixed-integer nonlinear programming, heuristics and reinforcement
learning are limited in terms of scalability, constraint handling and
generalisation capacity. In the present study, a novel theoretical framework is
proposed, which is based on Denoising Diffusion Probabilistic Models (DDPM) for
CNF placement. The present approach proposes a reconceptualisation of placement
as a generative graph to assignment task, where the placement problem is
encoded as a heterogeneous graph, and a Graph Neural Network denoiser is
trained to iteratively refine noisy CNF-to-cloud assignment matrices. The model
incorporates constraint-specific losses directly into the loss function,
thereby allowing it to learn feasible solution spaces. The integration of the
DDPM formulation with structured combinatorial constraints is achieved through
a rigorous and systematic approach. Extensive evaluations across diverse
topologies have been conducted, which have confirmed that the model
consistently produces feasible solutions with orders of magnitude faster
inference than MINLP solvers. The results obtained demonstrate the potential of
diffusion-based generative modelling for constrained network embedding
problems, making an impact towards the practical, scalable orchestration of
distributed Cloud-Native Network Functions.

</details>


### [203] [MiniFool - Physics-Constraint-Aware Minimizer-Based Adversarial Attacks in Deep Neural Networks](https://arxiv.org/abs/2511.01352)
*Lucie Flek,Oliver Janik,Philipp Alexander Jung,Akbar Karimi,Timo Saala,Alexander Schmidt,Matthias Schott,Philipp Soldin,Matthias Thiesmeyer,Christopher Wiebusch,Ulrich Willemsen*

Main category: cs.LG

TL;DR: 提出了MiniFool算法，这是一种基于物理启发的对抗攻击方法，用于测试粒子物理和天体粒子物理中的神经网络分类任务。该算法通过最小化结合χ²检验统计量和目标分数偏差的成本函数来工作，可量化网络决策的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 开发一种通用的对抗攻击算法来测试神经网络在粒子物理和天体粒子物理分类任务中的鲁棒性，特别是在IceCube中微子观测站的天体τ中微子搜索等应用场景。

Method: 基于最小化成本函数的方法，该函数结合了χ²检验统计量（基于实验不确定性的扰动概率）与期望目标分数的偏差。通过调整攻击参数来缩放实验不确定性，测试分类变化。

Result: 研究发现，对于正确分类和错误分类的事件，分类翻转的可能性不同。通过攻击参数测试分类变化，可以量化网络决策的鲁棒性，并测试未标记实验数据的分类鲁棒性。

Conclusion: MiniFool算法在多个科学领域（包括MNIST数据集和CMS实验数据）都表现出通用适用性，为测试神经网络在物理应用中的鲁棒性提供了有效工具。

Abstract: In this paper, we present a new algorithm, MiniFool, that implements
physics-inspired adversarial attacks for testing neural network-based
classification tasks in particle and astroparticle physics. While we initially
developed the algorithm for the search for astrophysical tau neutrinos with the
IceCube Neutrino Observatory, we apply it to further data from other science
domains, thus demonstrating its general applicability. Here, we apply the
algorithm to the well-known MNIST data set and furthermore, to Open Data data
from the CMS experiment at the Large Hadron Collider. The algorithm is based on
minimizing a cost function that combines a $\chi^2$ based test-statistic with
the deviation from the desired target score. The test statistic quantifies the
probability of the perturbations applied to the data based on the experimental
uncertainties. For our studied use cases, we find that the likelihood of a
flipped classification differs for both the initially correctly and incorrectly
classified events. When testing changes of the classifications as a function of
an attack parameter that scales the experimental uncertainties, the robustness
of the network decision can be quantified. Furthermore, this allows testing the
robustness of the classification of unlabeled experimental data.

</details>


### [204] [Verifiable Split Learning via zk-SNARKs](https://arxiv.org/abs/2511.01356)
*Rana Alaa,Darío González-Ferreiro,Carlos Beis-Penedo,Manuel Fernández-Veiga,Rebeca P. Díaz-Redondo,Ana Fernández-Vilas*

Main category: cs.LG

TL;DR: 提出可验证的分割学习框架，通过集成zk-SNARK证明来确保分割学习中计算的正确定性和可验证性。


<details>
  <summary>Details</summary>
Motivation: 分割学习在协作学习中很有用，但缺乏验证计算正确性和诚实性的能力。

Method: 在服务器端的前向传播和反向传播中为双方生成zk-SNARK证明和验证，确保双方的可验证性。

Result: 与基于区块链的系统相比，应用zk-SNARK测试实现了可验证性和正确性，而区块链虽然轻量但不可验证。

Conclusion: zk-SNARK证明能够有效解决分割学习中的可验证性问题，确保计算过程的正确性和诚实性。

Abstract: Split learning is an approach to collaborative learning in which a deep
neural network is divided into two parts: client-side and server-side at a cut
layer. The client side executes its model using its raw input data and sends
the intermediate activation to the server side. This configuration architecture
is very useful for enabling collaborative training when data or resources are
separated between devices. However, split learning lacks the ability to verify
the correctness and honesty of the computations that are performed and
exchanged between the parties. To this purpose, this paper proposes a
verifiable split learning framework that integrates a zk-SNARK proof to ensure
correctness and verifiability. The zk-SNARK proof and verification are
generated for both sides in forward propagation and backward propagation on the
server side, guaranteeing verifiability on both sides. The verifiable split
learning architecture is compared to a blockchain-enabled system for the same
deep learning network, one that records updates but without generating the
zero-knowledge proof. From the comparison, it can be deduced that applying the
zk-SNARK test achieves verifiability and correctness, while blockchains are
lightweight but unverifiable.

</details>


### [205] [Learning Intractable Multimodal Policies with Reparameterization and Diversity Regularization](https://arxiv.org/abs/2511.01374)
*Ziqi Wang,Jiashun Liu,Ling Pan*

Main category: cs.LG

TL;DR: 提出了一种基于重参数化的多模态强化学习算法，通过距离多样性正则化解决传统确定性或单峰高斯策略在多模态决策场景中的局限性，在多样性关键领域表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统连续深度强化学习算法使用确定性或单峰高斯策略，无法表达复杂的多模态决策分布，这在多样性关键场景中会限制性能表现。

Method: 首先将现有难处理的多模态策略统一框架化，证明可通过重参数化直接优化；然后提出基于距离的多样性正则化，无需显式计算决策概率。

Result: 在多目标达成和生成式RL等多样性关键领域展示了多模态策略的优势，特别是在少样本鲁棒性方面；在MuJoCo基准测试中也表现出竞争力。

Conclusion: 摊销策略是一种有前景的策略模型类别，具有强大的多模态表达能力和高性能；提出的方法在性能、决策多样性和效率之间实现了良好平衡。

Abstract: Traditional continuous deep reinforcement learning (RL) algorithms employ
deterministic or unimodal Gaussian actors, which cannot express complex
multimodal decision distributions. This limitation can hinder their performance
in diversity-critical scenarios. There have been some attempts to design online
multimodal RL algorithms based on diffusion or amortized actors. However, these
actors are intractable, making existing methods struggle with balancing
performance, decision diversity, and efficiency simultaneously. To overcome
this challenge, we first reformulate existing intractable multimodal actors
within a unified framework, and prove that they can be directly optimized by
policy gradient via reparameterization. Then, we propose a distance-based
diversity regularization that does not explicitly require decision
probabilities. We identify two diversity-critical domains, namely multi-goal
achieving and generative RL, to demonstrate the advantages of multimodal
policies and our method, particularly in terms of few-shot robustness. In
conventional MuJoCo benchmarks, our algorithm also shows competitive
performance. Moreover, our experiments highlight that the amortized actor is a
promising policy model class with strong multimodal expressivity and high
performance. Our code is available at https://github.com/PneuC/DrAC

</details>


### [206] [Protecting the Neural Networks against FGSM Attack Using Machine Unlearning](https://arxiv.org/abs/2511.01377)
*Amir Hossein Khorasani,Ali Jahanian,Maryam Rastgarpour*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Machine learning is a powerful tool for building predictive models. However,
it is vulnerable to adversarial attacks. Fast Gradient Sign Method (FGSM)
attacks are a common type of adversarial attack that adds small perturbations
to input data to trick a model into misclassifying it. In response to these
attacks, researchers have developed methods for "unlearning" these attacks,
which involves retraining a model on the original data without the added
perturbations. Machine unlearning is a technique that tries to "forget"
specific data points from the training dataset, to improve the robustness of a
machine learning model against adversarial attacks like FGSM. In this paper, we
focus on applying unlearning techniques to the LeNet neural network, a popular
architecture for image classification. We evaluate the efficacy of unlearning
FGSM attacks on the LeNet network and find that it can significantly improve
its robustness against these types of attacks.

</details>


### [207] [Memory-Efficient Training with In-Place FFT Implementation](https://arxiv.org/abs/2511.01385)
*Xinyu Ding,Bangtian Liu,Siyu Liao,Zhongfeng Wang*

Main category: cs.LG

TL;DR: 提出了首个实域完全原位FFT框架(rdFFT)，通过隐式复数编码消除中间缓存使用，减少训练内存成本


<details>
  <summary>Details</summary>
Motivation: 现有FFT实现（包括标准FFT和实FFT）无法实现真正的原位计算，特别是实FFT会导致维度不匹配和额外内存分配

Method: 利用蝴蝶操作对称性和频域共轭特性，设计隐式复数编码方案，保持输入输出内存空间一致性

Result: 在多个自然语言理解任务上的实验证明了该方法在减少训练内存成本方面的有效性

Conclusion: 该框架为频域轻量级适配提供了有前景的方向

Abstract: Fast Fourier Transforms (FFT) are widely used to reduce memory and
computational costs in deep learning. However, existing implementations,
including standard FFT and real FFT (rFFT), cannot achieve true in-place
computation. In particular, rFFT maps an input of size n to a complex output of
size n/2+1, causing dimensional mismatch and requiring additional memory
allocation. We propose the first real-domain, fully in-place FFT framework
(rdFFT) that preserves input-output memory space consistency. By leveraging
butterfly operation symmetry and conjugate properties in the frequency domain,
we design an implicit complex encoding scheme that eliminates intermediate
cache usage entirely. Experiments on multiple natural language understanding
tasks demonstrate the method effectiveness in reducing training memory cost,
offering a promising direction for frequency-domain lightweight adaptation.

</details>


### [208] [Leveraging Compact Satellite Embeddings and Graph Neural Networks for Large-Scale Poverty Mapping](https://arxiv.org/abs/2511.01408)
*Markus B. Pettersson,Adel Daoud*

Main category: cs.LG

TL;DR: 提出基于图的卫星嵌入方法预测撒哈拉以南非洲的贫困指数，通过建模空间关系和模糊标签损失处理坐标位移问题，在37个DHS数据集上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 全球南方地区缺乏精细化的贫困地图，DHS调查数据虽然质量高但空间覆盖有限且坐标因隐私保护而被随机位移，降低了数据质量。

Method: 使用低维AlphaEarth卫星嵌入，通过图结构建模调查点和未标记位置的空间关系，引入概率性"模糊标签"损失函数来处理坐标位移问题。

Result: 在37个DHS数据集（2017-2023）上的实验表明，相比仅使用图像的基线方法，加入图结构略微提高了预测准确性。

Conclusion: 紧凑的地球观测嵌入在大规模社会经济制图方面具有潜力，图结构方法能够改善财富预测的泛化能力。

Abstract: Accurate, fine-grained poverty maps remain scarce across much of the Global
South. While Demographic and Health Surveys (DHS) provide high-quality
socioeconomic data, their spatial coverage is limited and reported coordinates
are randomly displaced for privacy, further reducing their quality. We propose
a graph-based approach leveraging low-dimensional AlphaEarth satellite
embeddings to predict cluster-level wealth indices across Sub-Saharan Africa.
By modeling spatial relations between surveyed and unlabeled locations, and by
introducing a probabilistic "fuzzy label" loss to account for coordinate
displacement, we improve the generalization of wealth predictions beyond
existing surveys. Our experiments on 37 DHS datasets (2017-2023) show that
incorporating graph structure slightly improves accuracy compared to
"image-only" baselines, demonstrating the potential of compact EO embeddings
for large-scale socioeconomic mapping.

</details>


### [209] [CG-FKAN: Compressed-Grid Federated Kolmogorov-Arnold Networks for Communication Constrained Environment](https://arxiv.org/abs/2511.01433)
*Seunghun Yu,Youngjoon Lee,Jinu Gong,Joonhyuk Kang*

Main category: cs.LG

TL;DR: 提出CG-FKAN方法，通过稀疏化传输关键系数来压缩KAN网络中的扩展网格，在通信受限的联邦学习环境中降低通信开销并保持性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习(FL)在隐私敏感应用中广泛使用但可解释性有限，而KAN网络通过可学习样条函数解决了这个问题。然而现有FL研究应用KAN时忽略了网格扩展带来的通信开销问题。

Method: 提出CG-FKAN方法，在通信预算约束下通过稀疏化处理并仅传输必要的系数来压缩扩展网格。

Result: 实验表明CG-FKAN在通信受限设置下比固定网格KAN的RMSE降低达13.6%。

Conclusion: CG-FKAN有效解决了KAN在联邦学习中的通信开销问题，并推导了其近似误差的理论上界。

Abstract: Federated learning (FL), widely used in privacy-critical applications,
suffers from limited interpretability, whereas Kolmogorov-Arnold Networks (KAN)
address this limitation via learnable spline functions. However, existing FL
studies applying KAN overlook the communication overhead introduced by grid
extension, which is essential for modeling complex functions. In this letter,
we propose CG-FKAN, which compresses extended grids by sparsifying and
transmitting only essential coefficients under a communication budget.
Experiments show that CG-FKAN achieves up to 13.6% lower RMSE than fixed-grid
KAN in communication-constrained settings. In addition, we derive a theoretical
upper bound on its approximation error.

</details>


### [210] [The Curvature Rate λ: A Scalar Measure of Input-Space Sharpness in Neural Networks](https://arxiv.org/abs/2511.01438)
*Jacob Poschl*

Main category: cs.LG

TL;DR: 提出了一种在输入空间中定义的曲率度量——曲率率λ，通过高阶输入导数的指数增长率来衡量神经网络的平滑性，比参数空间的曲率度量更易解释且不受参数化影响。


<details>
  <summary>Details</summary>
Motivation: 现有的曲率度量通常在参数空间定义（如Hessian特征值），计算昂贵、对重新参数化敏感，且难以在功能层面解释。需要一种直接在输入空间定义的曲率度量。

Method: 引入曲率率λ，定义为高阶输入导数的指数增长率，通过log ||D^n f||与n的斜率来估计。提出曲率率正则化(CRR)来直接塑造输入空间几何。

Result: 在解析函数和神经网络上的实验表明，λ在训练过程中可预测地演化，CRR能达到与SAM相似的准确率，同时产生更平坦的输入空间几何和更好的置信度校准。

Conclusion: λ通过微分动力学将曲率概念基础化，为学习模型的功能平滑性提供了一个紧凑、可解释且参数化不变的描述符。

Abstract: Curvature influences generalization, robustness, and how reliably neural
networks respond to small input perturbations. Existing sharpness metrics are
typically defined in parameter space (e.g., Hessian eigenvalues) and can be
expensive, sensitive to reparameterization, and difficult to interpret in
functional terms. We introduce a scalar curvature measure defined directly in
input space: the curvature rate {\lambda}, given by the exponential growth rate
of higher-order input derivatives. Empirically, {\lambda} is estimated as the
slope of log ||D^n f|| versus n for small n. This growth-rate perspective
unifies classical analytic quantities: for analytic functions, {\lambda}
corresponds to the inverse radius of convergence, and for bandlimited signals,
it reflects the spectral cutoff. The same principle extends to neural networks,
where {\lambda} tracks the emergence of high-frequency structure in the
decision boundary. Experiments on analytic functions and neural networks (Two
Moons and MNIST) show that {\lambda} evolves predictably during training and
can be directly shaped using a simple derivative-based regularizer, Curvature
Rate Regularization (CRR). Compared to Sharpness-Aware Minimization (SAM), CRR
achieves similar accuracy while yielding flatter input-space geometry and
improved confidence calibration. By grounding curvature in differentiation
dynamics, {\lambda} provides a compact, interpretable, and
parameterization-invariant descriptor of functional smoothness in learned
models.

</details>


### [211] [Efficient Curvature-aware Graph Network](https://arxiv.org/abs/2511.01443)
*Chaoqun Fei,Tinglve Zhou,Tianyong Hao,Yangyang Li*

Main category: cs.LG

TL;DR: 提出了一种新的图曲率度量——有效电阻曲率，用于替代计算复杂度高的Ollivier-Ricci曲率，在保持几何表达能力的同时显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有Ollivier-Ricci曲率虽然具有强几何可解释性，但其计算复杂度过高，限制了在大规模图数据集上的应用。

Method: 使用节点对之间的有效电阻来量化沿图边传递消息的难易程度，替代最优传输距离，从而定义有效电阻曲率。

Result: 理论证明了有效电阻曲率的低计算复杂度，实验表明该方法在多种GNN任务中与Ollivier-Ricci曲率性能相当，同时大幅降低计算开销。

Conclusion: 有效电阻曲率是Ollivier-Ricci曲率的有效替代方案，在保持几何表达能力的同时显著提升了计算效率。

Abstract: Graph curvature provides geometric priors for Graph Neural Networks (GNNs),
enhancing their ability to model complex graph structures, particularly in
terms of structural awareness, robustness, and theoretical interpretability.
Among existing methods, Ollivier-Ricci curvature has been extensively studied
due to its strong geometric interpretability, effectively characterizing the
local geometric distribution between nodes. However, its prohibitively high
computational complexity limits its applicability to large-scale graph
datasets. To address this challenge, we propose a novel graph curvature
measure--Effective Resistance Curvature--which quantifies the ease of message
passing along graph edges using the effective resistance between node pairs,
instead of the optimal transport distance. This method significantly
outperforms Ollivier-Ricci curvature in computational efficiency while
preserving comparable geometric expressiveness. Theoretically, we prove the low
computational complexity of effective resistance curvature and establish its
substitutability for Ollivier-Ricci curvature. Furthermore, extensive
experiments on diverse GNN tasks demonstrate that our method achieves
competitive performance with Ollivier-Ricci curvature while drastically
reducing computational overhead.

</details>


### [212] [DAMBench: A Multi-Modal Benchmark for Deep Learning-based Atmospheric Data Assimilation](https://arxiv.org/abs/2511.01468)
*Hao Wang,Zixuan Weng,Jindong Han,Wei Fan,Hao Liu*

Main category: cs.LG

TL;DR: 提出了DAMBench，首个大规模多模态基准测试，用于在真实大气条件下评估数据驱动的数据同化模型，解决了现有研究依赖简化场景和缺乏标准化基准的问题。


<details>
  <summary>Details</summary>
Motivation: 传统数据同化方法虽然有效，但深度学习提供了更可扩展、高效和灵活的替代方案。然而现有深度学习数据同化研究存在两个关键限制：(1)依赖合成扰动的简化场景，(2)缺乏标准化基准进行公平模型比较。

Method: 整合了来自先进预报系统的高质量背景状态和真实世界的多模态观测数据（气象站和卫星图像），所有数据重采样到统一网格并进行时间对齐，提供统一评估协议并基准测试代表性数据同化方法。

Result: 建立了严谨的研究基础，促进可重复性、公平比较和扩展到真实世界多模态场景的能力，并展示了集成真实观测如何增强简单基线模型。

Conclusion: DAMBench为未来研究提供了标准化基准，解决了现有数据同化研究的局限性，数据集和代码已公开可用。

Abstract: Data Assimilation is a cornerstone of atmospheric system modeling, tasked
with reconstructing system states by integrating sparse, noisy observations
with prior estimation. While traditional approaches like variational and
ensemble Kalman filtering have proven effective, recent advances in deep
learning offer more scalable, efficient, and flexible alternatives better
suited for complex, real-world data assimilation involving large-scale and
multi-modal observations. However, existing deep learning-based DA research
suffers from two critical limitations: (1) reliance on oversimplified scenarios
with synthetically perturbed observations, and (2) the absence of standardized
benchmarks for fair model comparison. To address these gaps, in this work, we
introduce DAMBench, the first large-scale multi-modal benchmark designed to
evaluate data-driven DA models under realistic atmospheric conditions. DAMBench
integrates high-quality background states from state-of-the-art forecasting
systems and real-world multi-modal observations (i.e., real-world weather
stations and satellite imagery). All data are resampled to a common grid and
temporally aligned to support systematic training, validation, and testing. We
provide unified evaluation protocols and benchmark representative data
assimilation approaches, including latent generative models and neural process
frameworks. Additionally, we propose a lightweight multi-modal plugin to
demonstrate how integrating realistic observations can enhance even simple
baselines. Through comprehensive experiments, DAMBench establishes a rigorous
foundation for future research, promoting reproducibility, fair comparison, and
extensibility to real-world multi-modal scenarios. Our dataset and code are
publicly available at https://github.com/figerhaowang/DAMBench.

</details>


### [213] [Real-time Continual Learning on Intel Loihi 2](https://arxiv.org/abs/2511.01553)
*Elvin Hajizada,Danielle Rager,Timothy Shea,Leobardo Campos-Macias,Andreas Wild,Eyke Hüllermeier,Yulia Sandamirskaya,Mike Davies*

Main category: cs.LG

TL;DR: CLP-SNN是一种基于脉冲神经网络的在线持续学习架构，在英特尔Loihi 2芯片上实现，通过事件驱动稀疏学习、自归一化学习规则和神经发生机制，在OpenLORIS数据集上达到与重放方法相当的准确率，同时实现70倍速度和5600倍能效提升。


<details>
  <summary>Details</summary>
Motivation: 解决边缘设备在开放世界中面临的数据分布变化和新类别出现的挑战，克服传统离线训练范式在功率受限环境中的局限性，实现高效的在线持续学习。

Method: 提出CLP-SNN架构，包含三个创新：事件驱动时空稀疏局部学习、自归一化三因子学习规则维持权重归一化、集成神经发生和元可塑性实现容量扩展和遗忘缓解。

Result: 在OpenLORIS少样本学习实验中，CLP-SNN达到与重放方法竞争的准确率，同时无需排练。相比边缘GPU上的最佳替代OCL方法，速度提升70倍（0.33ms vs 23.2ms），能效提升5600倍（0.05mJ vs 281mJ）。

Conclusion: 共同设计的脑启发算法和神经形态硬件能够打破传统精度-效率权衡，为未来边缘AI系统提供突破性解决方案。

Abstract: AI systems on edge devices face a critical challenge in open-world
environments: adapting when data distributions shift and novel classes emerge.
While offline training dominates current paradigms, online continual learning
(OCL)--where models learn incrementally from non-stationary streams without
catastrophic forgetting--remains challenging in power-constrained settings. We
present a neuromorphic solution called CLP-SNN: a spiking neural network
architecture for Continually Learning Prototypes and its implementation on
Intel's Loihi 2 chip. Our approach introduces three innovations: (1)
event-driven and spatiotemporally sparse local learning, (2) a self-normalizing
three-factor learning rule maintaining weight normalization, and (3) integrated
neurogenesis and metaplasticity for capacity expansion and forgetting
mitigation. On OpenLORIS few-shot learning experiments, CLP-SNN achieves
accuracy competitive with replay methods while being rehearsal-free. CLP-SNN
delivers transformative efficiency gains: 70\times faster (0.33ms vs 23.2ms),
and 5,600\times more energy efficient (0.05mJ vs 281mJ) than the best
alternative OCL on edge GPU. This demonstrates that co-designed brain-inspired
algorithms and neuromorphic hardware can break traditional accuracy-efficiency
trade-offs for future edge AI systems.

</details>


### [214] [Gated Fusion Enhanced Multi-Scale Hierarchical Graph Convolutional Network for Stock Movement Prediction](https://arxiv.org/abs/2511.01570)
*Xiaosha Xue,Peibo Duan,Zhipeng Liu,Qi Chu,Changsheng Zhang,Bin zhang*

Main category: cs.LG

TL;DR: MS-HGFN是一种多尺度分层图融合网络，通过动态图学习和多尺度时空特征融合，显著提升了股票市场预测的准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有多尺度图神经网络在股票预测中忽视了两个关键问题：股票内部属性模式对股票间相关性的影响，以及多尺度采样中对粗粒度和细粒度特征的偏置关注。

Method: 提出分层GNN模块，通过学习不同时间尺度上的内部属性模式和外部属性特征构建动态图，并采用自上而下的门控方法融合多尺度时空特征。

Result: 在美国和中国股市真实数据集上的实验表明，MS-HGFN优于传统和先进模型，预测准确率提升高达1.4%，且在收益模拟中表现出更好的稳定性。

Conclusion: MS-HGFN通过全面捕捉时空依赖关系和多尺度特征融合，有效解决了股票预测中的关键挑战，为金融市场分析提供了有力工具。

Abstract: Accurately predicting stock market movements remains a formidable challenge
due to the inherent volatility and complex interdependencies among stocks.
Although multi-scale Graph Neural Networks (GNNs) hold potential for modeling
these relationships, they frequently neglect two key points: the subtle
intra-attribute patterns within each stock affecting inter-stock correlation,
and the biased attention to coarse- and fine-grained features during
multi-scale sampling. To overcome these challenges, we introduce MS-HGFN
(Multi-Scale Hierarchical Graph Fusion Network). The model features a
hierarchical GNN module that forms dynamic graphs by learning patterns from
intra-attributes and features from inter-attributes over different time scales,
thus comprehensively capturing spatio-temporal dependencies. Additionally, a
top-down gating approach facilitates the integration of multi-scale
spatio-temporal features, preserving critical coarse- and fine-grained features
without too much interference. Experiments utilizing real-world datasets from
U.S. and Chinese stock markets demonstrate that MS-HGFN outperforms both
traditional and advanced models, yielding up to a 1.4% improvement in
prediction accuracy and enhanced stability in return simulations. The code is
available at https://anonymous.4open.science/r/MS-HGFN.

</details>


### [215] [HIT-ROCKET: Hadamard-vector Inner-product Transformer for ROCKET](https://arxiv.org/abs/2511.01572)
*Wang Hao,Kuang Zhang,Hou Chengyu,Yuan Zhonghao,Tan Chenxing,Fu Weifeng,Zhu Yangying*

Main category: cs.LG

TL;DR: 提出基于Hadamard卷积变换的时间序列分类方法，相比现有SOTA方法显著提升计算效率，在保持性能的同时减少50%训练时间


<details>
  <summary>Details</summary>
Motivation: 现有时间序列分类方法如HIVE-COTE、Proximity Forest等计算复杂度高、参数调优和训练周期长，而轻量级方法如ROCKET在核选择和计算开销方面仍有改进空间

Method: 使用Hadamard矩阵的列或行向量作为不同长度的卷积核，利用核正交性提升计算效率、鲁棒性和适应性，与现有方法完全兼容

Result: 在UCR时间序列数据集上的实验显示，F1分数比ROCKET至少提升5%，训练时间比最快的miniROCKET缩短50%，可在超低功耗嵌入式设备上部署

Conclusion: 提出的Hadamard卷积变换方法在保持高性能的同时显著提升了计算效率，为时间序列分类提供了更高效的解决方案

Abstract: Time series classification holds broad application value in communications,
information countermeasures, finance, and medicine. However, state-of-the-art
(SOTA) methods-including HIVE-COTE, Proximity Forest, and TS-CHIEF-exhibit high
computational complexity, coupled with lengthy parameter tuning and training
cycles. In contrast, lightweight solutions like ROCKET (Random Convolutional
Kernel Transform) offer greater efficiency but leave substantial room for
improvement in kernel selection and computational overhead. To address these
challenges, we propose a feature extraction approach based on Hadamard
convolutional transform, utilizing column or row vectors of Hadamard matrices
as convolution kernels with extended lengths of varying sizes. This enhancement
maintains full compatibility with existing methods (e.g., ROCKET) while
leveraging kernel orthogonality to boost computational efficiency, robustness,
and adaptability. Comprehensive experiments on multi-domain datasets-focusing
on the UCR time series dataset-demonstrate SOTA performance: F1-score improved
by at least 5% vs. ROCKET, with 50% shorter training time than miniROCKET
(fastest ROCKET variant) under identical hyperparameters, enabling deployment
on ultra-low-power embedded devices. All code is available on GitHub.

</details>


### [216] [Explore More, Learn Better: Parallel MLLM Embeddings under Mutual Information Minimization](https://arxiv.org/abs/2511.01588)
*Zhicheng Wang,Chen Ju,Xu Chen,Shuai Xiao,Jinsong Lan,Xiaoyong Zhu,Ying Chen,Zhiguo Cao*

Main category: cs.LG

TL;DR: 提出并行解耦框架(PDF)用于多模态嵌入学习，通过在多模态大语言模型(MLLM)上使用可学习前缀生成并行嵌入路径，结合互信息最小化和对比监督实现多样化的语义覆盖。


<details>
  <summary>Details</summary>
Motivation: 当前多模态嵌入模型受限于SSC范式（单一输入、单一嵌入、对比监督），无法充分利用MLLM的能力，将丰富的多面输入压缩为单一嵌入。

Method: 使用共享MLLM骨干网络，通过不同的可学习前缀生成多个并行路径，结合互信息最小化约束和每路径对比监督来促进嵌入多样性。

Result: 在MMEB基准测试中显著提升性能：VLM2Vec-LLaVA-1.6-LR模型提升+8.9%(7B)，VLM2Vec-Qwen2VL模型提升+4.2%(2B)和+3.1%(7B)。2B模型仅用一半计算预算就超越基线+2.6%。

Conclusion: PDF框架能够有效利用MLLM的可控性生成多样化嵌入，在保持高效推理的同时显著提升多模态嵌入性能。

Abstract: Embedding models are a cornerstone of modern AI. Driven by Multimodal Large
Language Models (MLLMs), they have made great progress in architecture and data
curation, while the holistic paradigm is still limited to SSC, i.e., single
input, singular embedding, contrastive supervision, which collapses rich,
multifaceted inputs into monolithic embeddings and fails to fully exploit MLLM
capabilities. In this paper, we tailor one Parallel Decoupling Framework (PDF)
for multimodal embedding learning, by utilizing the proprietary steerability of
MLLMs, i.e., their ability to flexibly generate quite differentiated response
under explicit instructions. Concretely, PDF conditions a shared MLLM backbone
on distinct, learnable prefixes to roll out multiple parallel paths for one
input, then relies on these paths to obtain parallel embeddings. To promote
full parallel diversity, we employ Mutual Information Minimization (MIM) as an
explicit constraint, coupled with per-path contrastive supervision to maintain
semantic alignment. Such dual-objectives force PDF to yield robust semantic
coverage and a generalizable embedding space. Ultimately, the remarkable
embedding space are accessible at inference via one single forward pass,
incurring negligible computational overhead. We instantiate PDF on multiple
MLLM backbones and prove its effectiveness on MMEB benchmark. Significant gains
are consistently achieved across various resolutions and model sizes, e.g.,
boosting the VLM2Vec-LLaVA-1.6-LR model by a remarkable +8.9% (7B), while the
VLM2Vec-Qwen2VL models by +4.2% (2B) and +3.1% (7B). In terms of efficiency,
our 2B model surpasses its baseline by +2.6% using only half the computational
budget.

</details>


### [217] [Defining Energy Indicators for Impact Identification on Aerospace Composites: A Physics-Informed Machine Learning Perspective](https://arxiv.org/abs/2511.01592)
*Natália Ribeiro Marinho,Richard Loendersloot,Frank Grooteman,Jan Willem Wiegman,Uraz Odyurt,Tiedo Tinga*

Main category: cs.LG

TL;DR: 提出了一种物理信息驱动的机器学习框架，通过专用输入空间将领域知识嵌入到机器学习中，显著提高了航空航天复合材料冲击能量预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前冲击能量预测方法受限于数据稀疏性、信号噪声、复杂特征依赖关系、非线性动力学、大规模设计空间和反问题的不适定性，需要更可靠的能量估计方法。

Method: 结合观测偏差和针对性特征选择，从时域、频域和时频域提取特征，通过统计显著性、相关性过滤、降维和噪声鲁棒性的结构化特征选择过程，生成紧凑的能量敏感指标集。

Result: 使用优化的输入空间训练全连接神经网络，在多个冲击场景下验证，冲击能量预测精度显著提高，误差比传统时间序列技术和纯数据驱动模型减少三倍。

Conclusion: 该物理信息框架产生了具有统计鲁棒性和物理意义的紧凑特征集，实现了可解释且可追溯到可测量结构响应的冲击能量预测。

Abstract: Energy estimation is critical to impact identification on aerospace
composites, where low-velocity impacts can induce internal damage that is
undetectable at the surface. Current methodologies for energy prediction are
often constrained by data sparsity, signal noise, complex feature
interdependencies, non-linear dynamics, massive design spaces, and the
ill-posed nature of the inverse problem. This study introduces a
physics-informed framework that embeds domain knowledge into machine learning
through a dedicated input space. The approach combines observational biases,
which guide the design of physics-motivated features, with targeted feature
selection to retain only the most informative indicators. Features are
extracted from time, frequency, and time-frequency domains to capture
complementary aspects of the structural response. A structured feature
selection process integrating statistical significance, correlation filtering,
dimensionality reduction, and noise robustness ensures physical relevance and
interpretability. Exploratory data analysis further reveals domain-specific
trends, yielding a reduced feature set that captures essential dynamic
phenomena such as amplitude scaling, spectral redistribution, and transient
signal behaviour. Together, these steps produce a compact set of
energy-sensitive indicators with both statistical robustness and physical
significance, resulting in impact energy predictions that remain interpretable
and traceable to measurable structural responses. Using this optimised input
space, a fully-connected neural network is trained and validated with
experimental data from multiple impact scenarios, including pristine and
damaged states. The resulting model demonstrates significantly improved impact
energy prediction accuracy, reducing errors by a factor of three compared to
conventional time-series techniques and purely data-driven models.

</details>


### [218] [Estimation of Toeplitz Covariance Matrices using Overparameterized Gradient Descent](https://arxiv.org/abs/2511.01605)
*Daniel Busbib,Ami Wiesel*

Main category: cs.LG

TL;DR: 该论文重新审视了托普利茨协方差估计问题，通过过参数化梯度下降方法，证明了当参数数量为2P或4P时，可以从随机初始化实现全局收敛，并提出了一种加速梯度下降变体。


<details>
  <summary>Details</summary>
Motivation: 受到深度学习中使用过参数化梯度下降取得成功的启发，重新审视传统托普利茨协方差估计问题，探索简单梯度下降方法在该问题上的潜力。

Method: 将P×P协方差建模为K个复正弦波的叠加，使用梯度下降优化参数。当K=2P或4P时实现过参数化，并提出具有分离学习率的加速梯度下降变体。

Result: 数值实验表明，过参数化梯度下降在挑战性设置下可以达到或超过最先进方法的精度，同时保持简单性和可扩展性。

Conclusion: 过参数化梯度下降为托普利茨协方差估计提供了一种简单有效的替代方案，证明了在适当过参数化下，梯度下降可以实现全局收敛并获得竞争性性能。

Abstract: We consider covariance estimation under Toeplitz structure. Numerous
sophisticated optimization methods have been developed to maximize the Gaussian
log-likelihood under Toeplitz constraints. In contrast, recent advances in deep
learning demonstrate the surprising power of simple gradient descent (GD)
applied to overparameterized models. Motivated by this trend, we revisit
Toeplitz covariance estimation through the lens of overparameterized GD. We
model the $P\times P$ covariance as a sum of $K$ complex sinusoids with
learnable parameters and optimize them via GD. We show that when $K = P$, GD
may converge to suboptimal solutions. However, mild overparameterization ($K =
2P$ or $4P$) consistently enables global convergence from random
initializations. We further propose an accelerated GD variant with separate
learning rates for amplitudes and frequencies. When frequencies are fixed and
only amplitudes are optimized, we prove that the optimization landscape is
asymptotically benign and any stationary point recovers the true covariance.
Finally, numerical experiments demonstrate that overparameterized GD can match
or exceed the accuracy of state-of-the-art methods in challenging settings,
while remaining simple and scalable.

</details>


### [219] [Scaling Graph Chain-of-Thought Reasoning: A Multi-Agent Framework with Efficient LLM Serving](https://arxiv.org/abs/2511.01633)
*Chengying Huan,Ziheng Meng,Yongchao Liu,Zhengyi Yang,Yun Zhu,Yue Yun,Shipeng Li,Rong Gu,Xiabao Wu,Haitao Zhang,Chuntao Hong,Shaonan Ma,Guihai Chen,Chen Tian*

Main category: cs.LG

TL;DR: GLM是一个多智能体图推理系统，通过分解推理任务、优化LLM服务架构，显著提升了图推理的准确性、效率和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有的图推理系统存在准确性低、token使用过多、延迟高和吞吐量低的问题，主要由于单智能体提示、重复上下文编码和低效服务执行。

Method: 将推理分解为分类、推理、动作生成和图检索等专门智能体，采用分支和选择性上下文共享减少提示长度和推理迭代；引入图推理感知的LLM推理机制，包括图特定KV缓存管理、基于优先级的淘汰和流水线执行。

Result: GLM将答案准确性提升高达38%，token成本降低95.7%，推理延迟降低90.3%，吞吐量提高15.1倍。

Conclusion: GLM通过多智能体架构和优化的LLM服务设计，实现了高效的大规模复杂图推理，为现实世界推理应用提供了可行的解决方案。

Abstract: Graph Chain-of-Thought (Graph-CoT) enables large language models (LLMs) to
perform step-by-step reasoning over graph-structured knowledge, but existing
pipelines suffer from low accuracy, excessive token usage, high latency, and
low throughput due to single-agent monolithic prompts, repeated context
re-encoding, and inefficient serving execution. We present GLM, the first
multi-agent Graph-CoT system co-designed with an optimized LLM serving
architecture. GLM decomposes reasoning into specialized agents for
classification, reasoning, action generation, and graph retrieval, enabling
branching and selective context sharing to reduce prompt length and reasoning
iterations while preserving reasoning quality, thereby improving accuracy and
reducing overall token consumption. To scale inference, we introduce a
Graph-CoT-aware LLM inference mechanism with graph-specific KV-cache
management, priority-based eviction, and pipelined execution to improve serving
efficiency. Experiments demonstrate that GLM improves answer accuracy by up to
38%, reduces token cost by up to 95.7%, lowers inference latency by 90.3%, and
achieves up to 15.1x higher throughput compared to state-of-the-art Graph-CoT
baselines, enabling efficient adoption for complex real-world reasoning at
scale.

</details>


### [220] [Cross-Treatment Effect Estimation for Multi-Category, Multi-Valued Causal Inference via Dynamic Neural Masking](https://arxiv.org/abs/2511.01641)
*Xiaopeng Ke,Yihan Yu,Ruyue Zhang,Zhishuo Zhou,Fangzhou Shi,Chang Men,Zhengdan Zhu*

Main category: cs.LG

TL;DR: XTNet是一个用于多类别多值处理效应估计的新型网络架构，通过交叉效应估计模块和动态掩码机制捕捉处理交互，无需限制性结构假设。


<details>
  <summary>Details</summary>
Motivation: 反事实因果推断在扩展到多类别多值处理时面临重大挑战，现有方法局限于二元或单类型处理，存在假设限制、可扩展性不足和评估框架不完善等问题。

Method: 提出XTNet架构，采用分解策略将基本效应与交叉处理交互分离，引入动态掩码机制的交叉效应估计模块，并提出了考虑处理成本和交互效应的MCMV-AUCC评估指标。

Result: 在合成和真实数据集上的广泛实验表明，XTNet在排序准确性和效应估计质量方面始终优于最先进的基线方法，真实A/B测试结果进一步证实了其有效性。

Conclusion: XTNet能够有效建模复杂组合处理空间，为多类别多值处理效应估计提供了可行的解决方案。

Abstract: Counterfactual causal inference faces significant challenges when extended to
multi-category, multi-valued treatments, where complex cross-effects between
heterogeneous interventions are difficult to model. Existing methodologies
remain constrained to binary or single-type treatments and suffer from
restrictive assumptions, limited scalability, and inadequate evaluation
frameworks for complex intervention scenarios.
  We present XTNet, a novel network architecture for multi-category,
multi-valued treatment effect estimation. Our approach introduces a
cross-effect estimation module with dynamic masking mechanisms to capture
treatment interactions without restrictive structural assumptions. The
architecture employs a decomposition strategy separating basic effects from
cross-treatment interactions, enabling efficient modeling of combinatorial
treatment spaces. We also propose MCMV-AUCC, a suitable evaluation metric that
accounts for treatment costs and interaction effects. Extensive experiments on
synthetic and real-world datasets demonstrate that XTNet consistently
outperforms state-of-the-art baselines in both ranking accuracy and effect
estimation quality. The results of the real-world A/B test further confirm its
effectiveness.

</details>


### [221] [Bayesian Natural Gradient Fine-Tuning of CLIP Models via Kalman Filtering](https://arxiv.org/abs/2511.01694)
*Hossein Abdi,Mingfei Sun,Wei Pan*

Main category: cs.LG

TL;DR: 提出基于卡尔曼滤波的贝叶斯自然梯度下降方法，用于CLIP模型的小样本微调，在保持ID性能的同时提升OOD鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决CLIP模型在少样本微调中面临的一阶优化方法收敛慢、对超参数敏感、OOD泛化差的问题，以及二阶方法计算复杂度高的问题。

Method: 使用卡尔曼滤波器对自然梯度下降进行贝叶斯近似，结合二阶优化的优势和贝叶斯推断，提供不确定性量化。

Result: 在多个图像分类数据集上的实验表明，该方法在ID性能上达到或优于现有方法，并在OOD鲁棒性方面表现更好。

Conclusion: 这是首次成功将卡尔曼滤波应用于CLIP模型微调，实现了更鲁棒和高效的视觉语言任务学习。

Abstract: Vision-language pre-trained models, such as CLIP, have established new
benchmarks in multimodal data mining. In such models, few-shot fine-tuning is a
major challenge to achieve optimal performance on both in-distribution (ID) and
out-of-distribution (OOD) datasets, especially when labeled data is scarce.
Most existing fine-tuning approaches rely on first-order gradient-based
optimizers, which typically suffer from slow convergence, sensitivity to
step-size hyperparameters, and poor generalization in OOD settings. In
contrast, second-order methods utilize local curvature information of the loss
landscape to adjust the update step size. This is particularly beneficial for
CLIP models, whose non-convex loss functions often contain sharp critical
points. In such cases, natural gradient direction can offer more substantial
and efficient per-iteration updates when fine-tuning with limited data. Natural
Gradient Descent (NGD) is obtained by preconditioning the standard gradient
with the inverse Fisher Information Matrix (FIM), which is computationally
expensive for large models. To address this, we propose a Bayesian
approximation of NGD using a Kalman filter for CLIP models. Our method combines
the benefits of second-order optimization with Bayesian inference, which
enhances generalization while providing uncertainty quantification. Extensive
experiments conducted on diverse image classification datasets demonstrate that
our algorithm consistently achieves superior--or comparable--ID performance and
improved OOD robustness compared to state-of-the-art baselines. To the best of
our knowledge, this work represents the first successful application of Kalman
filtering to fine-tuning CLIP-based models, which enables more robust and
efficient learning in vision-language tasks.

</details>


### [222] [Collaborative Large Language Model Inference via Resource-Aware Parallel Speculative Decoding](https://arxiv.org/abs/2511.01695)
*Jungyeon Koh,Hyun Jong Yang*

Main category: cs.LG

TL;DR: 提出了一个联合优化用户关联和资源分配的统一框架，支持高效的并行推测解码，在移动边缘计算系统中实现可扩展和低延迟的LLM服务。


<details>
  <summary>Details</summary>
Motivation: 设备上大语言模型推理需求增长，需要高效的移动边缘计算解决方案。推测解码虽然能通过轻量级草稿模型和强大目标模型的分工来提升效率，但面临通信开销和异步延迟问题。

Method: 使用多智能体深度强化学习算法解决用户关联和资源分配问题，并在Sionna模拟器中进行实验评估。

Result: 该方法在不影响推理准确性的情况下，实现了最高28.0%、平均23.7%的端到端延迟降低。

Conclusion: 该框架能够有效支持移动边缘计算系统中可扩展和低延迟的LLM服务。

Abstract: The growing demand for on-device large language model (LLM) inference
highlights the need for efficient mobile edge computing (MEC) solutions,
especially in resource-constrained settings. Speculative decoding offers a
promising solution by partitioning token generation between a lightweight draft
model on mobile devices and a powerful target model on edge servers, but
suffers from communication overhead and asynchronous delays. This paper is the
first to propose a unified framework that jointly optimizes user association
and resource allocation (UARA) to support efficient parallel speculative
decoding. We solve the UARA problem using a multi-agent deep reinforcement
learning algorithm. To evaluate our approach under realistic conditions, we
conduct experiments using the Sionna simulator. Results show that our method
achieves up to 28.0% and an average of 23.7% reduction in end-to-end latency
without compromising inference accuracy, enabling scalable and low-latency LLM
services in MEC systems.

</details>


### [223] [Edge AI in Highly Volatile Environments: Is Fairness Worth the Accuracy Trade-off?](https://arxiv.org/abs/2511.01737)
*Obaidullah Zaland,Feras M. Awaysheh,Sawsan Al Zubi,Abdul Rahman Safi,Monowar Bhuyan*

Main category: cs.LG

TL;DR: 本文研究了在高度波动的边缘环境中联邦学习的模型准确性与公平性之间的权衡，通过实证评估不同客户端选择算法在公平性、模型性能和时间方面的表现。


<details>
  <summary>Details</summary>
Motivation: 边缘环境的固有波动性（动态资源可用性和异构客户端能力）给实现高精度和公平的客户端参与带来了重大挑战，需要研究准确性与公平性的权衡关系。

Method: 在三个基准数据集（CIFAR10、FashionMNIST和EMNIST）上，对基于公平性的客户端选择算法（如RBFF和RBCSF）与随机和贪婪选择算法进行了广泛的实证评估。

Result: 结果表明，更公平的客户端选择算法虽然能为客户端提供稍好的参与机会，但在波动环境中会导致全局训练速度变慢。

Conclusion: 这项工作揭示了在波动边缘环境中公平性与性能、公平性与速度之间的权衡关系，并探索了解决联邦学习中公平客户端选择策略现有缺陷的未来研究方向。

Abstract: Federated learning (FL) has emerged as a transformative paradigm for edge
intelligence, enabling collaborative model training while preserving data
privacy across distributed personal devices. However, the inherent volatility
of edge environments, characterized by dynamic resource availability and
heterogeneous client capabilities, poses significant challenges for achieving
high accuracy and fairness in client participation. This paper investigates the
fundamental trade-off between model accuracy and fairness in highly volatile
edge environments. This paper provides an extensive empirical evaluation of
fairness-based client selection algorithms such as RBFF and RBCSF against
random and greedy client selection regarding fairness, model performance, and
time, in three benchmarking datasets (CIFAR10, FashionMNIST, and EMNIST). This
work aims to shed light on the fairness-performance and fairness-speed
trade-offs in a volatile edge environment and explore potential future research
opportunities to address existing pitfalls in \textit{fair client selection}
strategies in FL. Our results indicate that more equitable client selection
algorithms, while providing a marginally better opportunity among clients, can
result in slower global training in volatile environments\footnote{The code for
our experiments can be found at
https://github.com/obaidullahzaland/FairFL_FLTA.

</details>


### [224] [Game-theoretic distributed learning of generative models for heterogeneous data collections](https://arxiv.org/abs/2511.01740)
*Dmitrij Schlesinger,Boris Flach*

Main category: cs.LG

TL;DR: 提出一种基于合成数据交换而非模型参数共享的分布式学习方法，解决了本地模型和数据异质性问题，将本地模型视为黑盒，通过博弈论方法实现协同学习。


<details>
  <summary>Details</summary>
Motivation: 解决分布式学习中处理异构本地模型和数据的挑战，利用生成模型的成功，通过交换合成数据来避免直接共享模型参数。

Method: 将本地模型视为黑盒，能够从数据中学习参数并生成数据；支持半监督学习，允许不同概率空间的本地模型处理多模态异构数据；基于博弈论将本地模型学习制定为合作博弈。

Result: 证明了指数族本地模型存在唯一纳什均衡，提出的学习方法收敛到该均衡；在标准视觉数据集上的图像分类和条件生成任务中展示了优势。

Conclusion: 提出的基于合成数据交换的分布式学习方法有效解决了异构性问题，通过博弈论框架实现了本地模型的协同学习，在视觉任务中表现出良好性能。

Abstract: One of the main challenges in distributed learning arises from the difficulty
of handling heterogeneous local models and data. In light of the recent success
of generative models, we propose to meet this challenge by building on the idea
of exchanging synthetic data instead of sharing model parameters. Local models
can then be treated as ``black boxes'' with the ability to learn their
parameters from data and to generate data according to these parameters.
Moreover, if the local models admit semi-supervised learning, we can extend the
approach by enabling local models on different probability spaces. This allows
to handle heterogeneous data with different modalities. We formulate the
learning of the local models as a cooperative game starting from the principles
of game theory. We prove the existence of a unique Nash equilibrium for
exponential family local models and show that the proposed learning approach
converges to this equilibrium. We demonstrate the advantages of our approach on
standard benchmark vision datasets for image classification and conditional
generation.

</details>


### [225] [Towards Efficient Federated Learning of Networked Mixture-of-Experts for Mobile Edge Computing](https://arxiv.org/abs/2511.01743)
*Song Gao,Shusen Jing,Shuai Zhang,Yue Wang,Xiangwei Zhou,Songyang Zhang*

Main category: cs.LG

TL;DR: 提出了Networked Mixture-of-Experts (NMoE)系统，通过联邦学习框架在移动边缘网络中协同训练和部署大型AI模型，解决边缘设备资源有限的问题。


<details>
  <summary>Details</summary>
Motivation: 大型AI模型在移动边缘计算中的计算资源和训练数据需求与边缘设备有限的存储和计算能力之间存在冲突，这给边缘训练和部署带来了重大挑战。

Method: 引入NMoE系统，客户端基于专家能力将任务分发给合适的邻居并聚合返回结果；提出结合监督学习和自监督学习的联邦学习框架，平衡个性化和泛化能力。

Result: 通过广泛实验证明了NMoE系统的有效性，为NMoE训练算法提供了见解和基准。

Conclusion: NMoE系统能够有效解决边缘设备资源限制问题，在保持通信效率和数据隐私的同时实现大型AI模型的边缘部署。

Abstract: Recent advancements in large artificial intelligence models (LAMs) are
driving significant innovations in mobile edge computing within next-generation
wireless networks. However, the substantial demands for computational resources
and large-scale training data required to train LAMs conflict with the limited
storage and computational capacity of edge devices, posing significant
challenges to training and deploying LAMs at the edge. In this work, we
introduce the Networked Mixture-of-Experts (NMoE) system, in which clients
infer collaboratively by distributing tasks to suitable neighbors based on
their expertise and aggregate the returned results. For training the NMoE, we
propose a federated learning framework that integrates both supervised and
self-supervised learning to balance personalization and generalization, while
preserving communication efficiency and data privacy. We conduct extensive
experiments to demonstrate the efficacy of the proposed NMoE system, providing
insights and benchmarks for the NMoE training algorithms.

</details>


### [226] [An Open-Access Benchmark of Statistical and Machine-Learning Anomaly Detection Methods for Battery Applications](https://arxiv.org/abs/2511.01745)
*Mei-Chin Pang,Suraj Adhikari,Takuma Kasahara,Nagihiro Haba,Saneyuki Ohno*

Main category: cs.LG

TL;DR: OSBAD是一个用于电池应用异常检测的开源基准测试框架，通过比较15种不同算法，结合物理和统计特征工程提升异常检测性能，并提出了基于贝叶斯优化的超参数自动调优方法。


<details>
  <summary>Details</summary>
Motivation: 电池安全在消费电子、电动汽车和航空等领域至关重要，未检测到的异常可能引发安全隐患或高昂停机时间。现有异常检测方法缺乏统一基准和系统比较。

Method: 开发OSBAD开源基准，包含15种统计、距离和无监督机器学习算法；提出物理和统计特征转换工作流分解集体异常；设计基于贝叶斯优化的超参数自动调优管道。

Result: 在涵盖液态和固态化学电池的数据集上验证，展示了跨化学体系的泛化能力；特征工程显著提升了异常可分离性；超参数调优解决了无监督学习中标签不完整的问题。

Conclusion: OSBAD为电池分析建立了统一的异常检测开发基础，强调了物理统计特征工程和概率超参数调优在安全关键能源系统中可信数据驱动诊断的重要性。

Abstract: Battery safety is critical in applications ranging from consumer electronics
to electric vehicles and aircraft, where undetected anomalies could trigger
safety hazards or costly downtime. In this study, we present OSBAD as an
open-source benchmark for anomaly detection frameworks in battery applications.
By benchmarking 15 diverse algorithms encompassing statistical, distance-based,
and unsupervised machine-learning methods, OSBAD enables a systematic
comparison of anomaly detection methods across heterogeneous datasets. In
addition, we demonstrate how a physics- and statistics-informed feature
transformation workflow enhances anomaly separability by decomposing collective
anomalies into point anomalies. To address a major bottleneck in unsupervised
anomaly detection due to incomplete labels, we propose a Bayesian optimization
pipeline that facilitates automated hyperparameter tuning based on
transfer-learning and regression proxies. Through validation on datasets
covering both liquid and solid-state chemistries, we further demonstrate the
cross-chemistry generalization capability of OSBAD to identify irregularities
across different electrochemical systems. By making benchmarking database with
open-source reproducible anomaly detection workflows available to the
community, OSBAD establishes a unified foundation for developing safe,
scalable, and transferable anomaly detection tools in battery analytics. This
research underscores the significance of physics- and statistics-informed
feature engineering as well as model selection with probabilistic
hyperparameter tuning, in advancing trustworthy, data-driven diagnostics for
safety-critical energy systems.

</details>


### [227] [RLAC: Reinforcement Learning with Adversarial Critic for Free-Form Generation Tasks](https://arxiv.org/abs/2511.01758)
*Mian Wu,Gavin Zhang,Sewon Min,Sergey Levine,Aviral Kumar*

Main category: cs.LG

TL;DR: RLAC是一种使用动态批评器的强化学习后训练方法，通过识别最可能的失败模式来减少验证成本，同时提升生成器和批评器的性能。


<details>
  <summary>Details</summary>
Motivation: 开放生成任务需要满足多样化的评估标准，但验证成本过高且评估不完整，使得基于标准的强化学习难以扩展。

Method: 使用LLM作为动态批评器识别最可能的失败模式，通过外部验证器验证，联合优化生成器和批评器。

Result: RLAC在文本生成中提高了事实准确性，在代码生成中提高了正确性，优于穷举验证和奖励模型方法。

Conclusion: 动态批评器比固定批评器更有效，RLAC有潜力将强化学习后训练扩展到自由形式生成任务。

Abstract: Open-ended generation tasks require outputs to satisfy diverse and often
implicit task-specific evaluation rubrics. The sheer number of relevant rubrics
leads to prohibitively high verification costs and incomplete assessments of a
response, making reinforcement learning (RL) post-training with rubric-based
rewards difficult to scale. This problem is exacerbated by the fact that often
the best way to combine these rubrics into one single reward is also highly
prompt-specific. We propose Reinforcement Learning with Adversarial Critic
(RLAC), a post-training approach that addresses these challenges via dynamic
rubric verification. Our approach employs a large language model (LLM) as a
critic that dynamically identifies only the most likely failure modes (e.g., a
factual error or unhandled edge case), which are then verified by an external
validator to optimize both generator and critic jointly. By training both the
generator and the critic, this game enhances the critic's error detection and
the generator's output quality while reducing required verifications. Our
experiments demonstrate that RLAC improves factual accuracy in text generation
and correctness in code generation, while also outperforming exhaustive
verification and reward model methods. We show that dynamic critics are more
effective than fixed critics, showcasing the potential of RLAC for scaling RL
post-training to free-form generation tasks.

</details>


### [228] [Random Initialization of Gated Sparse Adapters](https://arxiv.org/abs/2511.01794)
*Vi Retault,Yohaï-Eliel Berreby*

Main category: cs.LG

TL;DR: RIGSA是一种新的参数高效微调方法，通过随机初始化全秩适配器、ReZero门控和迭代幅度剪枝来解决语言模型微调中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型在新任务上微调时出现的灾难性遗忘问题，现有PEFT方法如LoRA存在秩约束限制，稀疏适配提供了不施加秩约束的替代方案。

Method: RIGSA方法：从随机初始化的全秩适配器开始，使用ReZero类似的门控机制，并通过迭代幅度剪枝进行稀疏化处理。

Result: 在SmolLM2-1.7B-Instruct模型上测试，RIGSA能够学习新的Textual MNIST任务，且相比QLoRA显示出更少的遗忘，特别是在GSM8k任务上表现更好，但与随机掩码方法性能相当。

Conclusion: RIGSA作为一种稀疏适配方法，在减少灾难性遗忘方面优于QLoRA，为参数高效微调提供了有前景的替代方案。

Abstract: When fine-tuning language models on new tasks, catastrophic forgetting --
performance degradation on previously-learned tasks -- is a ubiquitous problem.
While Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA address this
through low-rank adapters, sparse adaptation offers an alternative that doesn't
impose rank constraints. We introduce Random Initialization of Gated Sparse
Adapters (RIGSA), which starts from randomly-initialized full-rank adapters,
gates them with a ReZero analog, and sparsifies them with iterative magnitude
pruning. We evaluate RIGSA on SmolLM2-1.7B-Instruct using a novel
vision-in-text task (Textual MNIST) and measure forgetting on PIQA, HellaSwag,
and GSM8k. SmolLM2-1.7B-Instruct initially performs around chance level on
Textual MNIST, and is capable of learning the task through RIGSA, 4-bit QLoRA
and random masking. In spite of having more trainable parameters than QLoRA,
the RIGSA configurations that we studied displayed less forgetting than QLoRA,
particularly on GSM8k, though it performs comparably to random masking.

</details>


### [229] [Fractional Diffusion Bridge Models](https://arxiv.org/abs/2511.01795)
*Gabriel Nobis,Maximilian Springenberg,Arina Belova,Rembert Daems,Christoph Knochenhauer,Manfred Opper,Tolga Birdal,Wojciech Samek*

Main category: cs.LG

TL;DR: 提出分数扩散桥模型(FDBM)，这是一种基于分数布朗运动近似的新型生成扩散桥框架，能够捕捉真实随机过程中的记忆效应、长程依赖性和异常扩散现象。


<details>
  <summary>Details</summary>
Motivation: 真实随机过程具有记忆效应、时间相关性、长程依赖性和异常扩散等特性，这些在标准扩散或桥模型中无法被捕捉，因为标准模型使用布朗运动。

Method: 利用分数布朗运动的马尔可夫近似(MA-fBM)构建FDBM，保持分数布朗运动的非马尔可夫性质同时实现可处理的推理。扩展到Schrödinger桥问题并推导出学习非配对数据转换的原则性损失函数。

Result: 在蛋白质构象预测和图像翻译任务中，FDBM相比布朗运动基线表现更优：蛋白质结构预测中Cα原子位置的RMSD更低，非配对图像翻译中FID分数更低。

Conclusion: FDBM框架能够有效建模真实随机过程的复杂特性，在多个任务中优于传统基于布朗运动的方法。

Abstract: We present Fractional Diffusion Bridge Models (FDBM), a novel generative
diffusion bridge framework driven by an approximation of the rich and
non-Markovian fractional Brownian motion (fBM). Real stochastic processes
exhibit a degree of memory effects (correlations in time), long-range
dependencies, roughness and anomalous diffusion phenomena that are not captured
in standard diffusion or bridge modeling due to the use of Brownian motion
(BM). As a remedy, leveraging a recent Markovian approximation of fBM (MA-fBM),
we construct FDBM that enable tractable inference while preserving the
non-Markovian nature of fBM. We prove the existence of a coupling-preserving
generative diffusion bridge and leverage it for future state prediction from
paired training data. We then extend our formulation to the Schr\"{o}dinger
bridge problem and derive a principled loss function to learn the unpaired data
translation. We evaluate FDBM on both tasks: predicting future protein
conformations from aligned data, and unpaired image translation. In both
settings, FDBM achieves superior performance compared to the Brownian
baselines, yielding lower root mean squared deviation (RMSD) of C$_\alpha$
atomic positions in protein structure prediction and lower Fr\'echet Inception
Distance (FID) in unpaired image translation.

</details>


### [230] [Bayesian Coreset Optimization for Personalized Federated Learning](https://arxiv.org/abs/2511.01800)
*Prateek Chanda,Shrey Modi,Ganesh Ramakrishnan*

Main category: cs.LG

TL;DR: 提出了一种基于个性化核心集加权的联邦学习方法，通过选择代表性数据点而非全部客户端数据进行训练，在减少计算负担的同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中客户端训练整个数据集的计算负担问题，通过核心集选择代表性数据点来优化训练过程。

Method: 个性化核心集加权联邦学习，使用核心集代表性数据点而非全部客户端数据来更新中央服务器，通过理论分析推导泛化误差边界。

Result: 理论分析显示平均泛化误差在极小极大意义下最优，实验在多个基准数据集上相比随机采样方法有显著提升，在医疗数据集上优于基于子模优化的子集选择方法。

Conclusion: 智能选择训练样本能有效提升联邦学习性能，提出的方法在减少计算负担的同时保持或提升了模型质量。

Abstract: In a distributed machine learning setting like Federated Learning where there
are multiple clients involved which update their individual weights to a single
central server, often training on the entire individual client's dataset for
each client becomes cumbersome. To address this issue we propose $\methodprop$:
a personalized coreset weighted federated learning setup where the training
updates for each individual clients are forwarded to the central server based
on only individual client coreset based representative data points instead of
the entire client data. Through theoretical analysis we present how the average
generalization error is minimax optimal up to logarithm bounds (upper bounded
by $\mathcal{O}(n_k^{-\frac{2 \beta}{2 \beta+\boldsymbol{\Lambda}}} \log ^{2
\delta^{\prime}}(n_k))$) and lower bounds of $\mathcal{O}(n_k^{-\frac{2
\beta}{2 \beta+\boldsymbol{\Lambda}}})$, and how the overall generalization
error on the data likelihood differs from a vanilla Federated Learning setup as
a closed form function ${\boldsymbol{\Im}}(\boldsymbol{w}, n_k)$ of the coreset
weights $\boldsymbol{w}$ and coreset sample size $n_k$. Our experiments on
different benchmark datasets based on a variety of recent personalized
federated learning architectures show significant gains as compared to random
sampling on the training data followed by federated learning, thereby
indicating how intelligently selecting such training samples can help in
performance. Additionally, through experiments on medical datasets our proposed
method showcases some gains as compared to other submodular optimization based
approaches used for subset selection on client's data.

</details>


### [231] [Dynamic Reconstruction of Ultrasound-Derived Flow Fields With Physics-Informed Neural Fields](https://arxiv.org/abs/2511.01804)
*Viraj Patel,Lisa Kreusser,Katharine Fraser*

Main category: cs.LG

TL;DR: 提出了一种基于物理知识的神经场模型，使用多尺度傅里叶特征编码从稀疏噪声超声数据中估计血流，无需真实监督即可实现去噪和修复。


<details>
  <summary>Details</summary>
Motivation: 超声血流分析对疾病诊断有价值，但超声存在深度衰减问题，传统EchoPIV技术测量血流速度存在挑战。物理知识机器学习可以增强准确性和鲁棒性，特别是在噪声或不完整数据场景下。

Method: 使用物理知识神经场模型结合多尺度傅里叶特征编码，从稀疏噪声超声数据重建血流，无需地面真实监督。

Result: 模型在合成和真实数据集上均实现了低均方误差的去噪和修复效果，验证了相对于参考流场和真实流量测量的准确性。

Conclusion: 将其他成像模态中证明有效的方法应用于超声血流重建，展示了物理知识神经场在医学血流重建中的潜力。

Abstract: Blood flow is sensitive to disease and provides insight into cardiac
function, making flow field analysis valuable for diagnosis. However, while
safer than radiation-based imaging and more suitable for patients with medical
implants, ultrasound suffers from attenuation with depth, limiting the quality
of the image. Despite advances in echocardiographic particle image velocimetry
(EchoPIV), accurately measuring blood velocity remains challenging due to the
technique's limitations and the complexity of blood flow dynamics.
Physics-informed machine learning can enhance accuracy and robustness,
particularly in scenarios where noisy or incomplete data challenge purely
data-driven approaches. We present a physics-informed neural field model with
multi-scale Fourier Feature encoding for estimating blood flow from sparse and
noisy ultrasound data without requiring ground truth supervision. We
demonstrate that this model achieves consistently low mean squared error in
denoising and inpainting both synthetic and real datasets, verified against
reference flow fields and ground truth flow rate measurements. While
physics-informed neural fields have been widely used to reconstruct medical
images, applications to medical flow reconstruction are mostly prominent in
Flow MRI. In this work, we adapt methods that have proven effective in other
imaging modalities to address the specific challenge of ultrasound-based flow
reconstruction.

</details>


### [232] [No-rank Tensor Decomposition Using Metric Learning](https://arxiv.org/abs/2511.01816)
*Maryam Bagherian*

Main category: cs.LG

TL;DR: 提出了一种基于度量学习的无秩张量分解框架，用判别性相似度优化替代传统重构目标，在数据稀缺场景下优于传统方法


<details>
  <summary>Details</summary>
Motivation: 传统张量分解方法基于重构和固定秩约束，难以捕捉高维数据的语义结构，需要一种能直接反映语义相似度的新方法

Method: 基于度量学习的框架，通过优化三元组损失和多样性、均匀性正则化，学习数据驱动的嵌入空间，使距离直接反映语义相似度

Result: 在多个领域（人脸识别、脑连接分析、模拟数据）均优于PCA、t-SNE、UMAP和传统张量分解方法，在聚类指标上显著提升，且在小数据集上表现优于基于Transformer的方法

Conclusion: 度量学习为张量分析提供了新范式，优先考虑语义相关性而非像素级保真度，在数据稀缺场景下具有计算优势

Abstract: Tensor decomposition faces fundamental challenges in analyzing
high-dimensional data, where traditional methods based on reconstruction and
fixed-rank constraints often fail to capture semantically meaningful
structures. This paper introduces a no-rank tensor decomposition framework
grounded in metric learning, which replaces reconstruction objectives with a
discriminative, similarity-based optimization. The proposed approach learns
data-driven embeddings by optimizing a triplet loss with diversity and
uniformity regularization, creating a feature space where distance directly
reflects semantic similarity. We provide theoretical guarantees for the
framework's convergence and establish bounds on its metric properties.
Evaluations across diverse domains --including face recognition (LFW,
Olivetti), brain connectivity analysis (ABIDE), and simulated data (galaxy
morphology, crystal structures)-- demonstrate that our method outperforms
baseline techniques, including PCA, t-SNE, UMAP, and tensor decomposition
baselines (CP and Tucker). Results show substantial improvements in clustering
metrics (Silhouette Score, Davies--Bouldin Index, Calinski--Harabasz Index,
Separation Ratio, Adjusted Rand Index, Normalized Mutual Information) and
reveal a fundamental trade-off: while metric learning optimizes global class
separation, it deliberately transforms local geometry to align with semantic
relationships. Crucially, our approach achieves superior performance with
smaller training datasets compared to transformer-based methods, offering an
efficient alternative for domains with limited labeled data. This work
establishes metric learning as a paradigm for tensor-based analysis,
prioritizing semantic relevance over pixel-level fidelity while providing
computational advantages in data-scarce scenarios.

</details>


### [233] [Machine and Deep Learning for Indoor UWB Jammer Localization](https://arxiv.org/abs/2511.01819)
*Hamed Fard,Mahsa Kholghi,Benedikt Groß,Gerhard Wunder*

Main category: cs.LG

TL;DR: 本文提出了一种基于域对抗ConvNeXt自编码器(A-CNT)的方法，用于在室内环境变化下实现鲁棒的恶意干扰源定位，解决了UWB定位系统在环境布局改变时的性能退化问题。


<details>
  <summary>Details</summary>
Motivation: UWB定位系统虽然能提供厘米级精度，但容易受到干扰攻击，且在室内环境布局改变时定位性能会严重下降。现有方法在跨房间布局的干扰源定位方面研究不足。

Method: 引入两个UWB数据集，提出域对抗ConvNeXt自编码器(A-CNT)，利用梯度反转层对齐跨域的CIR衍生特征，实现特征层面的域适应。

Result: 在原始数据集上，Random Forest达到最高F1-macro分数0.95，XGBoost达到最低平均欧几里得误差20.16cm。但在修改布局后，XGBoost误差增至207.99cm。A-CNT方法将误差降至34.67cm，比非对抗迁移学习提升77%，比最佳基线提升83%。

Conclusion: 域对抗特征对齐能够在环境变化下实现鲁棒且可迁移的室内干扰源定位，为智能建筑中的资产跟踪和入侵检测提供安全保障。

Abstract: Ultra-wideband (UWB) localization delivers centimeter-scale accuracy but is
vulnerable to jamming attacks, creating security risks for asset tracking and
intrusion detection in smart buildings. Although machine learning (ML) and deep
learning (DL) methods have improved tag localization, localizing malicious
jammers within a single room and across changing indoor layouts remains largely
unexplored. Two novel UWB datasets, collected under original and modified room
configurations, are introduced to establish comprehensive ML/DL baselines.
Performance is rigorously evaluated using a variety of classification and
regression metrics. On the source dataset with the collected UWB features,
Random Forest achieves the highest F1-macro score of 0.95 and XGBoost achieves
the lowest mean Euclidean error of 20.16 cm. However, deploying these
source-trained models in the modified room layout led to severe performance
degradation, with XGBoost's mean Euclidean error increasing tenfold to 207.99
cm, demonstrating significant domain shift. To mitigate this degradation, a
domain-adversarial ConvNeXt autoencoder (A-CNT) is proposed that leverages a
gradient-reversal layer to align CIR-derived features across domains. The A-CNT
framework restores localization performance by reducing the mean Euclidean
error to 34.67 cm. This represents a 77 percent improvement over
non-adversarial transfer learning and an 83 percent improvement over the best
baseline, restoring the fraction of samples within 30 cm to 0.56. Overall, the
results demonstrate that adversarial feature alignment enables robust and
transferable indoor jammer localization despite environmental changes. Code and
dataset available at https://github.com/afbf4c8996f/Jammer-Loc

</details>


### [234] [Towards Multi-Fidelity Scaling Laws of Neural Surrogates in CFD](https://arxiv.org/abs/2511.01830)
*Paul Setinek,Gianluca Galletti,Johannes Brandstetter*

Main category: cs.LG

TL;DR: 本文研究了科学机器学习中数据保真度与计算成本之间的权衡，通过重新制定经典缩放定律，将数据集轴分解为计算预算和数据集组成，揭示了计算-性能缩放行为。


<details>
  <summary>Details</summary>
Motivation: 科学机器学习通常受限于通过数值模拟生成训练数据的高昂成本，但可以通过调整建模假设和近似来在模拟保真度和计算成本之间进行权衡，这是其他领域所不具备的特点。

Method: 使用低保真度和高保真度的雷诺平均纳维-斯托克斯(RANS)模拟来研究神经代理模型中数据保真度与成本之间的权衡，重新制定经典缩放定律，将数据集轴分解为计算预算和数据集组成。

Result: 实验揭示了计算-性能缩放行为，并展示了在给定数据集配置下预算依赖的最优保真度混合。

Conclusion: 这些发现为多保真度神经代理数据集提供了首个经验缩放定律研究，并为科学机器学习中计算高效的数据集生成提供了实际考虑。

Abstract: Scaling laws describe how model performance grows with data, parameters and
compute. While large datasets can usually be collected at relatively low cost
in domains such as language or vision, scientific machine learning is often
limited by the high expense of generating training data through numerical
simulations. However, by adjusting modeling assumptions and approximations,
simulation fidelity can be traded for computational cost, an aspect absent in
other domains. We investigate this trade-off between data fidelity and cost in
neural surrogates using low- and high-fidelity Reynolds-Averaged Navier-Stokes
(RANS) simulations. Reformulating classical scaling laws, we decompose the
dataset axis into compute budget and dataset composition. Our experiments
reveal compute-performance scaling behavior and exhibit budget-dependent
optimal fidelity mixes for the given dataset configuration. These findings
provide the first study of empirical scaling laws for multi-fidelity neural
surrogate datasets and offer practical considerations for compute-efficient
dataset generation in scientific machine learning.

</details>


### [235] [Dynamic Routing Between Experts: A Data-Efficient Approach to Continual Learning in Vision-Language Models](https://arxiv.org/abs/2511.01831)
*Jay Mohta,Kenan Emir Ak,Dimitrios Dimitriadis,Yan Xu,Mingwei Shen*

Main category: cs.LG

TL;DR: 提出基于路由的方法解决视觉语言模型在顺序微调时的灾难性遗忘问题，无需同时访问所有任务数据，保持基础能力的同时提升专业任务性能。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在顺序微调新任务时会出现灾难性遗忘，传统多任务学习需要同时访问所有数据集且计算开销随任务数量线性增长。

Method: 采用基于路由的方法，在InternVL-2模型(2B和8B参数)上实现新任务集成，同时保持预训练获得的基础知识。

Result: 路由方法在ChartQA、MMBench和DocVQA等通用基准上保持性能，同时提升专业任务准确率，无需所有任务数据并发访问。

Conclusion: 路由机制具有良好可扩展性和鲁棒性，特别适用于语义相关的新任务，并实现跨模态知识迁移，优于现有持续学习方法。

Abstract: Vision-Language Models (VLMs) suffer from catastrophic forgetting when
sequentially fine-tuned on new tasks, degrading performance on previously
learned foundational and task-specific capabilities. While multi-task learning
can mitigate forgetting, it requires simultaneous access to all datasets and
imposes computational overhead that scales linearly with the number of tasks.
In this work, we introduce a routing-based approach that enables the
integration of new tasks while preserving the foundational knowledge acquired
during pretraining. We evaluate our method using InternVL-2 models (2B and 8B
parameters) and demonstrate that routing preserves the model's foundational
capabilities by maintaining performance on general-purpose benchmarks such as
ChartQA, MMBench, and DocVQA, while simultaneously improving accuracy on
specialized tasks. Importantly, our approach achieves this without requiring
concurrent access to data from all tasks, avoiding the significant
computational and data overhead associated with traditional multi-task
learning. We further conduct extensive ablation studies to evaluate the
scalability and robustness of routing-based learning, showing that the approach
is resilient to a growing number of tasks and performs particularly well when
new tasks are semantically related. Finally, we show that the routing mechanism
enables superior cross-modal transfer between language and vision capabilities,
allowing knowledge learned in one modality to enhance performance in another
capability not achieved by existing continual learning methods.

</details>


### [236] [Priors in Time: Missing Inductive Biases for Language Model Interpretability](https://arxiv.org/abs/2511.01836)
*Ekdeep Singh Lubana,Can Rager,Sai Sumedh R. Hindupur,Valerie Costa,Greta Tuckute,Oam Patel,Sonia Krishna Murthy,Thomas Fel,Daniel Wurgaft,Eric J. Bigelow,Johnny Lin,Demba Ba,Martin Wattenberg,Fernanda Viegas,Melanie Weber,Aaron Mueller*

Main category: cs.LG

TL;DR: 提出了时间特征分析（Temporal Feature Analysis）方法，通过考虑语言模型激活中的时间动态特性，将表示分解为可预测成分和残差成分，解决了传统稀疏自编码器在捕捉语言时序结构方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有特征提取方法假设概念是独立方向，但语言模型表示具有丰富的时间动态特性（包括概念维度的系统性增长、上下文相关相关性和非平稳性），这与稀疏自编码器的独立性先验相冲突。

Method: 引入时间特征分析目标，具有时间归纳偏置，将给定时间的表示分解为两部分：可从上下文推断的可预测成分，以及捕捉上下文无法解释的新信息的残差成分。

Result: 时间特征分析器能正确解析花园路径句子、识别事件边界，并更广泛地区分抽象慢速信息和新颖快速信息，而现有稀疏自编码器在上述所有任务中都显示出显著缺陷。

Conclusion: 结果强调了在设计稳健可解释性工具时需要与数据匹配的归纳偏置。

Abstract: Recovering meaningful concepts from language model activations is a central
aim of interpretability. While existing feature extraction methods aim to
identify concepts that are independent directions, it is unclear if this
assumption can capture the rich temporal structure of language. Specifically,
via a Bayesian lens, we demonstrate that Sparse Autoencoders (SAEs) impose
priors that assume independence of concepts across time, implying stationarity.
Meanwhile, language model representations exhibit rich temporal dynamics,
including systematic growth in conceptual dimensionality, context-dependent
correlations, and pronounced non-stationarity, in direct conflict with the
priors of SAEs. Taking inspiration from computational neuroscience, we
introduce a new interpretability objective -- Temporal Feature Analysis --
which possesses a temporal inductive bias to decompose representations at a
given time into two parts: a predictable component, which can be inferred from
the context, and a residual component, which captures novel information
unexplained by the context. Temporal Feature Analyzers correctly parse garden
path sentences, identify event boundaries, and more broadly delineate abstract,
slow-moving information from novel, fast-moving information, while existing
SAEs show significant pitfalls in all the above tasks. Overall, our results
underscore the need for inductive biases that match the data in designing
robust interpretability tools.

</details>


### [237] [Interpretable Machine Learning for Reservoir Water Temperatures in the U.S. Red River Basin of the South](https://arxiv.org/abs/2511.01837)
*Isabela Suaza-Sierra,Hernan A. Moreno,Luis A De la Fuente,Thomas M. Neeson*

Main category: cs.LG

TL;DR: 该研究结合可解释机器学习和符号建模，开发了Kolmogorov-Arnold网络(KANs)来预测和解释水库水温动态，在红河流域10个水库的10,000多个温度剖面数据上取得了高精度预测(RMSE=1.20°C, R²=0.97)，并推导出可解释的解析表达式。


<details>
  <summary>Details</summary>
Motivation: 准确预测水库水温对水资源管理、生态系统健康和气候韧性至关重要，但单纯预测无法揭示背后的物理机制。研究旨在填补这一空白，将预测能力与过程理解相结合。

Method: 使用集成学习(RF、XGBoost)和神经网络(MLP)进行预测，通过SHAP分析量化物理驱动因素的贡献，然后开发KANs网络符号化近似水库水温，推导出渐进复杂的解析方程。

Result: 最佳预测精度RMSE=1.20°C，R²=0.97。KANs方程从单预测因子(R²=0.84)逐步提升到10个预测因子(R²=0.92)，但超过5个因子后增益递减。深度是关键次要预测因子，降水影响有限。

Conclusion: 该框架通过结合KANs和可解释机器学习，成功将黑盒模型转化为透明替代模型，既提高了预测准确性，又增强了对水库热动力学的理解，平衡了简洁性与准确性。

Abstract: Accurate prediction of Reservoir Water Temperature (RWT) is vital for
sustainable water management, ecosystem health, and climate resilience. Yet,
prediction alone offers limited insight into the governing physical processes.
To bridge this gap, we integrated explainable machine learning (ML) with
symbolic modeling to uncover the drivers of RWT dynamics across ten reservoirs
in the Red River Basin, USA, using over 10,000 depth-resolved temperature
profiles. We first employed ensemble and neural models, including Random Forest
(RF), Extreme Gradient Boosting (XGBoost), and Multilayer Perceptron (MLP),
achieving high predictive skill (best RMSE = 1.20 degree Celsius, R^2 = 0.97).
Using SHAP (SHapley Additive exPlanations), we quantified the contribution of
physical drivers such as air temperature, depth, wind, and lake volume,
revealing consistent patterns across reservoirs. To translate these data-driven
insights into compact analytical expressions, we developed Kolmogorov Arnold
Networks (KANs) to symbolically approximate RWT. Ten progressively complex KAN
equations were derived, improving from R^2 = 0.84 using a single predictor
(7-day antecedent air temperature) to R^2 = 0.92 with ten predictors, though
gains diminished beyond five, highlighting a balance between simplicity and
accuracy. The resulting equations, dominated by linear and rational forms,
incrementally captured nonlinear behavior while preserving interpretability.
Depth consistently emerged as a secondary but critical predictor, whereas
precipitation had limited effect. By coupling predictive accuracy with
explanatory power, this framework demonstrates how KANs and explainable ML can
transform black-box models into transparent surrogates that advance both
prediction and understanding of reservoir thermal dynamics.

</details>


### [238] [Bridging Lifelong and Multi-Task Representation Learning via Algorithm and Complexity Measure](https://arxiv.org/abs/2511.01847)
*Zhi Wang,Chicheng Zhang,Ramya Korlakai Vinayak*

Main category: cs.LG

TL;DR: 提出了一个终身表示学习框架，使用多任务经验风险最小化作为子程序，并基于新引入的任务规避维度建立了样本复杂度界限。


<details>
  <summary>Details</summary>
Motivation: 终身学习需要学习者在面对连续任务时识别和利用共享结构来加速学习，这与多任务学习或元学习不同，后者需要预先获得所有任务来学习表示。

Method: 使用多任务经验风险最小化作为子程序，引入任务规避维度来分析样本复杂度，适用于涉及一般函数类的广泛学习问题。

Result: 建立了一个样本复杂度界限，该界限适用于分类和回归等具体任务，即使在存在噪声的情况下也有效。

Conclusion: 提出的终身表示学习框架和算法能够有效利用共享表示结构，在连续任务学习中实现加速学习，并通过理论分析验证了其有效性。

Abstract: In lifelong learning, a learner faces a sequence of tasks with shared
structure and aims to identify and leverage it to accelerate learning. We study
the setting where such structure is captured by a common representation of
data. Unlike multi-task learning or learning-to-learn, where tasks are
available upfront to learn the representation, lifelong learning requires the
learner to make use of its existing knowledge while continually gathering
partial information in an online fashion. In this paper, we consider a
generalized framework of lifelong representation learning. We propose a simple
algorithm that uses multi-task empirical risk minimization as a subroutine and
establish a sample complexity bound based on a new notion we introduce--the
task-eluder dimension. Our result applies to a wide range of learning problems
involving general function classes. As concrete examples, we instantiate our
result on classification and regression tasks under noise.

</details>


### [239] [Coordinate ascent neural Kalman-MLE for state estimation](https://arxiv.org/abs/2511.01855)
*Bettina Hanlon,Angel Garcia Fernandez*

Main category: cs.LG

TL;DR: 提出一种坐标上升算法，通过最大似然估计监督学习动态状态估计中的动态和测量模型，包括神经网络参数和噪声协方差矩阵，然后结合非线性卡尔曼滤波器进行状态估计。


<details>
  <summary>Details</summary>
Motivation: 传统动态状态估计方法通常假设模型已知，但在实际应用中动态和测量模型往往未知或难以精确建模，需要从数据中学习这些模型。

Method: 使用坐标上升算法，在最大似然估计框架下交替学习动态和测量模型的神经网络参数以及噪声协方差矩阵，假设模型为高斯分布。

Result: 训练得到的动态和测量模型能够与非线性卡尔曼滤波器结合，在测试阶段有效估计系统状态。

Conclusion: 该方法能够从数据中学习动态状态估计所需的完整模型，为实际应用提供了一种有效的端到端解决方案。

Abstract: This paper presents a coordinate ascent algorithm to learn dynamic and
measurement models in dynamic state estimation using maximum likelihood
estimation in a supervised manner. In particular, the dynamic and measurement
models are assumed to be Gaussian and the algorithm learns the neural network
parameters that model the dynamic and measurement functions, and also the noise
covariance matrices. The trained dynamic and measurement models are then used
with a non-linear Kalman filter algorithm to estimate the state during the
testing phase.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [240] [Conserved Quantities in Expanding Gödel Cosmology](https://arxiv.org/abs/2511.00238)
*Alexander Leithes*

Main category: gr-qc

TL;DR: 该论文研究了在包含膨胀和旋转的Gödel背景时空中的线性扰动，构建了规范不变量，发现在膨胀Gödel宇宙学中存在守恒量，特别是对于无压尘埃在大尺度上守恒的空间度规迹扰动ζ_SMTP。


<details>
  <summary>Details</summary>
Motivation: 研究膨胀Gödel宇宙学中的扰动行为，探索在包含旋转和膨胀的背景下是否存在守恒量，这对于理解这类宇宙模型的性质具有重要意义。

Method: 在线性阶研究Gödel背景时空的扰动，分析它们在规范变换下的行为，构建规范不变量，利用扰动能量守恒方程寻找守恒量。

Result: 发现在膨胀Gödel宇宙学中存在守恒量，特别是对于无压尘埃，空间度规迹扰动ζ_SMTP在大尺度上是守恒的。

Conclusion: 膨胀Gödel宇宙学中存在守恒量，作者计划将研究扩展到完美流体物质内容，以在该背景下也获得守恒量。

Abstract: At linear order we study perturbations to a G\"odel background spacetime
which includes expansion in addition to rotation. We investigate the
transformation behaviour of these perturbations under gauge transformations and
construct gauge invariant quantities. Using the perturbed energy conservation
equation we find that there are conserved quantities in Expanding G\"odel (EG)
Cosmology, in particular a spatial metric trace perturbation, {\zeta} SMTP ,
which is conserved on large scales for pressureless dust. We intend to extend
our discussion to a perfect fluid matter content with a view to also obtaining
conserved quantities in this context.

</details>


### [241] [Spin-up and mass-gain in hyperbolic encounters of spinning black holes](https://arxiv.org/abs/2511.00307)
*Healey Kogan,Frederick C. L. Pardoe,Helvi Witek*

Main category: gr-qc

TL;DR: 通过数值相对论模拟研究等质量黑洞散射过程中的自旋增长和质量增加，发现最大自旋增长为0.3，最大质量增加为15%。


<details>
  <summary>Details</summary>
Motivation: 研究黑洞在散射过程中通过吸收引力波辐射的轨道角动量和能量而获得自旋增长和质量增加的物理机制。

Method: 进行一系列数值相对论模拟，涵盖等质量黑洞的广泛初始自旋范围（χi∈[-0.7,0.7]），考虑不同的初始动量、入射角度，并识别散射与合并配置的阈值。

Result: 在接近阈值角度、大动量和负初始自旋的系统中，自旋增长和质量增加最为显著。初始自旋为0.7的系统有时会出现自旋下降，尽管黑洞角动量增加，但由于质量相应增加。

Conclusion: 黑洞散射过程中的自旋增长和质量增加受到初始自旋、动量和入射角度的显著影响，最大自旋增长为0.3，最大质量增加为15%。

Abstract: Scattering black holes spin up and gain mass through the re-absorption of
orbital angular momentum and energy radiated in gravitational waves during
their encounter. In this work, we perform a series of numerical relativity
simulations to investigate the spin-up and mass-gain for equal-mass black holes
with a wide range of equal initial spins, $\chi_{\rm i}\in[-0.7,0.7]$, aligned
(or anti-aligned) to the orbital angular momentum. We also consider a variety
of initial momenta. Furthermore, we explore a range of incident angles and
identify the threshold between scattering and merging configurations. The
spin-up and mass-gain are typically largest in systems with incident angles
close to the threshold value, large momenta, and negative (i.e. anti-aligned)
initial spins. When evaluated at the threshold angle, we find that the spin-up
decreases linearly with initial spin. Intriguingly, systems with initial spin
$\chi_{\rm i}=0.7$ sometimes experience a spin-down, in spite of an increase in
the black-hole angular momentum, due to a corresponding gain in the black-hole
mass. Across the simulation suite, we find a maximum spin-up of $0.3$ and a
maximum increase in the black-hole mass of $15\%$.

</details>


### [242] [The thermal view of $f(R)$ cosmology](https://arxiv.org/abs/2511.00347)
*Valerio Faraoni,Santiago Novoa Cattivelli*

Main category: gr-qc

TL;DR: 该论文将标量-张量引力理论的热力学观点应用于f(R)引力理论，特别是均匀各向同性宇宙，研究其向爱因斯坦宇宙学的收敛性。


<details>
  <summary>Details</summary>
Motivation: 将广义相对论视为引力零温状态的新热力学观点应用于f(R)引力理论，探索这类理论如何收敛到爱因斯坦宇宙学。

Method: 在热力学形式适用的范围内，首先对一般f(R)理论进行分析，然后通过幂律和Starobinsky f(R)引力模型进行具体说明。

Result: 获得了关于f(R)理论向爱因斯坦宇宙学收敛（或不收敛）的结果，并通过具体模型验证了这些发现。

Conclusion: 该研究为理解f(R)引力理论在热力学框架下的行为提供了新视角，揭示了其向标准宇宙学演化的特性。

Abstract: A new thermal view of scalar-tensor gravity, in which general relativity is
the zero-temperature state of gravity, is applied to the specific subclass of
$f(R)$ gravity theories and, specifically, to spatially homogeneous and
isotropic universes. Within the limits of application of the new thermal
formalism, results on the convergence to Einstein cosmology (or lack thereof)
are first obtained for general $f(R)$ theories, and then illustrated with
power-law and Starobinsky $f(R)$ gravity.

</details>


### [243] [Effective spacetime description of light propagation in linear magnetoelectric media](https://arxiv.org/abs/2511.00353)
*Lucas T. de Paula,Caio C. Holanda Ribeiro,Vitorio A. De Lorenci*

Main category: gr-qc

TL;DR: 该论文利用引力与光学的形式类比，研究线性磁电介质中的光传播，从有效时空角度构建了类比模型，并发现可以建立类似事件视界的单向传播区域。


<details>
  <summary>Details</summary>
Motivation: 探索引力与光学现象之间的形式类比，为理解广义相对论的动力学方面提供新视角，并将这种类比应用于线性磁电介质中的光传播研究。

Method: 从协变形式的麦克斯韦方程出发，识别线性非色散磁电材料的有效度量，并在几何光学极限下构建类比模型。

Result: 在线性磁电材料中总能识别出有效度量，在合理假设磁电响应强度下，可以建立类似事件视界的单向传播区域。

Conclusion: 引力-光学类比可成功应用于磁电介质研究，为模拟引力现象提供了新的光学平台，特别是在事件视界等引力效应方面。

Abstract: Formal analogies between gravitational and optical phenomena have been
explored for over a century, providing valuable insights into kinematic aspects
of general relativity. Here, this analogy is employed to study light
propagation in linear magnetoelectric media from an effective spacetime
perspective. Starting from Maxwell's equations in covariant form, it is shown
that an effective metric can always be identified for linear, non-dispersive
magnetoelectric materials. The effective metric is then used to construct
analog models in the limit of geometric optics. Among the optical effects
analyzed, it is shown that under reasonable assumptions on the magnitude of the
magnetoelectric response, a one-way propagation region can be established,
which behaves analogously to an event horizon.

</details>


### [244] [Structural Properties of Magnetized Neutron Stars under f (R, T ) Gravity Framework](https://arxiv.org/abs/2511.00425)
*Charul Rathod,M. Mishra,Prasanta Kumar Das*

Main category: gr-qc

TL;DR: 研究在强磁场和f(R,T)修正引力框架下中子星的结构特性，发现修正引力参数负值越大，最大引力质量越高；强磁场仅轻微降低最大质量而不破坏球对称性。


<details>
  <summary>Details</summary>
Motivation: 在f(R,T)修正引力框架下研究强磁场对中子星结构的影响，探索物质-几何耦合在高密度物质条件下如何偏离广义相对论。

Method: 数值求解修正的Tolman-Oppenheimer-Volkoff方程，使用各向同性状态方程（APR、FPS、SLy模型），分析不同修正引力参数和中心磁场值下的质量-半径序列、质量分布和压力分布。

Result: 修正引力参数负值越大，最大引力质量越高；强磁场（高达10^18高斯）仅轻微降低最大质量而不破坏球对称性；结果与GW170817、PSR和NICER观测数据一致。

Conclusion: f(R,T)修正引力框架下，物质-几何耦合显著影响中子星结构，而强磁场影响相对较小；研究结果与现有观测数据相符。

Abstract: The current work investigates the structural properties of neutron stars in
the presence of a strong magnetic field within the framework of f(R,T) modified
gravity, where the matter-geometry coupling leads to deviations from general
relativity at high matter densities. We present here the mass-radius sequences,
as well as the mass and pressure distributions for various values of the
modified gravity parameter and the central magnetic field. The modified
Tolman-Oppenheimer- Volkoff equations are numerically solved using isotropic
equations of state, specifically the APR, FPS, and SLy models. Comparing the
corresponding results in the context of general relativity suggests that more
negative values of the modified gravity parameter result in higher maximum
gravitational masses. In contrast, strong central magnetic fields of up to 1018
Gauss cause only a slight decrease in maximum mass without disrupting spherical
symmetry. Our findings are in agreement with the observed data from GW170817,
PSR and NICER.

</details>


### [245] [Reconstructing $f(T)$ Gravity From Hubble Parameterization Constraints](https://arxiv.org/abs/2511.00464)
*Suraj Kumar Behera,Pratik P. Ray,B. Mishra*

Main category: gr-qc

TL;DR: 本文提出了基于挠率引力理论f(T)重力的宇宙学模型，通过参数化哈勃参数并利用宇宙学数据集约束自由参数，分析了宇宙的几何和动力学特性，发现宇宙呈现晚期幻象行为。


<details>
  <summary>Details</summary>
Motivation: 研究f(T)重力理论下的宇宙学模型，解释宇宙晚期加速膨胀现象，通过参数化方法分析几何和动力学参数。

Method: 引入哈勃参数的参数化形式，使用宇宙学数据集约束自由参数，分析减速参数、急动参数和snap参数等几何参数，研究非线性f(T)形式的动力学参数和能量条件。

Result: 模型显示宇宙呈现晚期幻象行为，强能量条件被违反，几何参数与宇宙学观测值一致。

Conclusion: f(T)重力理论能够成功描述宇宙晚期加速膨胀，模型与观测数据相符，验证了强能量条件的违反和幻象行为的存在。

Abstract: In this paper, we have presented the cosmological model of the Universe that
represents late time cosmic acceleration in torsion based gravitational theory,
the $f(T)$ gravity. A well motivated parametrization for the Hubble parameter
has been introduced and the free parameters involved are constrained using the
cosmological datasets. With the constrained values of the free parameters,
other geometrical parameters such as deceleration parameter, jerk parameter,
and snap parameter are analyzed and confronted with the prescribed value of the
cosmological observations. In addition, the dynamical parameters are analyzed
in some non-linear form of $f(T)$ and the energy conditions are also studied
and confirmed with the violation of the strong energy condition. The obtained
cosmological model provides late time phantom behavior of the Universe.

</details>


### [246] [Testing general relativity with gravitational waves -- improving and extending Modified Dispersion Relation tests](https://arxiv.org/abs/2511.00497)
*Tomasz Baka,Balázs Cirok,K. Haris,Johannes Noller,N. V. Krishnendu*

Main category: gr-qc

TL;DR: 本文改进了LIGO-Virgo-KAGRA合作组测试修正色散关系的方法，包括引入群速度参数化、改进采样程序以及扩展到负指数α，并重新分析了GWTC-3数据。


<details>
  <summary>Details</summary>
Motivation: 改进对修正色散关系的测试方法，以更精确地检验广义相对论并约束可能的修正项幅度。

Method: 采用群速度参数化、改进的采样方法，并将测试扩展到负指数α ∈ {-1, -2, -3}，然后重新分析GWTC-3目录中的引力波事件。

Result: 与GWTC-3结果相比，改进方法使后验分布宽度平均缩小19%，引力子质量的90%上限从2.42×10⁻¹¹ peV改进到2.21×10⁻¹¹ peV。对于负指数扩展测试，未发现支持广义相对论破坏的证据。

Conclusion: 改进的测试方法显著提高了对修正色散关系约束的精度，特别是通过改进采样方法，但对负指数α的测试未发现偏离广义相对论的证据。

Abstract: Searching for a modified dispersion relation is one of the general relativity
tests performed by the LIGO-Virgo-KAGRA collaboration with each new cumulative
Gravitational Wave Transient Catalog (GWTC). It considers classes of theories
that modify the dispersion of gravitational waves by introducing a massive
graviton or breaking Lorentz invariance. The symmetry breaking is parameterized
phenomenologically by a momentum power law term $p^\alpha$ added to the
dispersion relation, with the test placing constraints on the amplitude
$A_\alpha$ of the introduced deviation. In this work, we implement improvements
to the test, chief among them group velocity parametrization, a better sampling
procedure, and extension to negative exponents $\alpha$ of $p$. We then
reanalyze the events from the third catalog, GWTC-3, with our improved method.
Compared with GWTC-3 results, we find significant improvement, mostly from the
improved sampling method, in the posteriors obtained by analyzing individual
event and more modest improvements in the combined bounds on amplitude
parameters $A_\alpha$ -- on average, we observe 19% shrinking of posterior
width. The 90% upper bound on the graviton mass changes from $2.42 \times
10^{-11}$ peV to $2.21 \times 10^{-11}$ peV. For the extension of our test to
$\alpha \in \{-1, -2, -3\}$, we find no evidence in favor of general relativity
violation.

</details>


### [247] [Reflectionless and echo modes in asymmetric Damour-Solodukhin wormholes](https://arxiv.org/abs/2511.00565)
*Wei-Liang Qian,Qiyuan Pan,Ramin G. Daghigh,Bean Wang,Rui-Hong Yue*

Main category: gr-qc

TL;DR: 该论文研究了超致密天体回波现象中反射无反射模式和回波模式在复频率平面上的相似性，发现两种谱的渐近特性高度相似，在实轴附近具有均匀分布且模式间距相同。


<details>
  <summary>Details</summary>
Motivation: 理解超致密天体回波现象的物理机制，澄清回波模式与反射无反射模式之间的关系，探索描述回波现象的有效理论工具。

Method: 通过将准反射无反射模式扩展到完全反射无反射模式，并将对称Damour-Solodukhin虫洞推广到非对称情况，使用散射矩阵和格林函数两种互补方法进行主要解析处理。

Result: 发现回波模式和反射无反射模式的渐近谱在复频率平面上高度相似，都具有平行于实轴的近似均匀分布，且模式间距相同。对称虫洞的反射无反射模式精确位于实频率轴上。

Conclusion: 反射无反射模式和回波模式都提供了描述回波现象的有效工具，两种视角在复频率平面上展现出强相似性，反射无反射模式的偏离可作为虫洞不对称程度的度量。

Abstract: It is understood that the echo waveforms in ultracompact objects can be
regarded as composed mainly of the asymptotic high-overtone quasinormal modes,
dubbed echo modes, which predominantly lie parallel to the real frequency axis.
Alternatively, Rosato {\it et al.} recently suggested that high-frequency
quasi-reflectionless scattering modes are primarily responsible for the echo
phenomenon. This identification relies on greybody factors as stable
observables, despite the apparent spectral instability of quasinormal modes. In
this work, by extending the definition of quasi-reflectionless modes to
reflectionless ones and generalizing symmetric Damour-Solodukhin wormholes to
asymmetric cases, we examine the underlying similarity between the
reflectionless and echo mode spectra in the complex frequency plane. Through a
primarily analytical treatment, we demonstrate that the asymptotic properties
of these two spectra exhibit a strong resemblance, featuring an approximately
uniform distribution parallel to the real frequency axis with the same spacing
between successive modes. Specifically, the real parts of echo modes coincide
with those of reflectionless modes at the limit $|\mathrm{Re}\omega| \gg
|\mathrm{Im}\omega|$. While echo modes typically possess non-vanishing
imaginary parts, the reflectionless modes of symmetric Damour-Solodukhin
wormholes lie precisely on the real frequency axis, with any deviation serving
as a measure of the degree of asymmetry of the wormhole. We support our
derivations by employing two complementary approaches, based on the scattering
matrix and the Green's function, and argue that both perspectives provide
effective tools for describing the echo phenomenon.

</details>


### [248] [Taming singularities and chaos in conformal gravity](https://arxiv.org/abs/2511.00585)
*Jiale Gu,Leonardo Modesto,Cosimo Bambi*

Main category: gr-qc

TL;DR: 该论文通过Weyl共形变换解决了Bianchi IX时空中的宇宙学奇点问题，证明在无限类共形框架下，初始（大爆炸）和最终（大挤压）奇点都消失了。


<details>
  <summary>Details</summary>
Motivation: 解决广义引力理论中的宇宙学奇点问题，特别是Bianchi IX时空中的初始和最终奇点问题。

Method: 采用Weyl共形不变性理论，在无限类共形框架下分析时空的测地完备性，分别研究无质量粒子、有质量粒子和共形耦合粒子的情况。

Result: 证明过去和未来奇点在有限仿射参数（无质量粒子）或有限固有时间（有质量和共形耦合粒子）内都无法达到，时空在共形变换下变得测地完备。

Conclusion: 通过共形重新标度，Bianchi IX度量可以转化为准FLRW时空，从而驯服了奇点附近的混沌行为，解决了奇点问题。

Abstract: We hereby address the cosmological singularity problem in a general
gravitational theory invariant under Weyl conformal transformations. In
particular, we focus on the Bianchi IX spacetime and we show that both the
initial (big bang) and final (big crunch) singularities disappear in an
infinite class of conformal frames naturally selected according to analyticity.
It turns out that the past and future singularities are both unattainable
within a finite affine parameter (for massless particles) or within a finite
proper time (for massive and conformally coupled particles). In order to prove
such a statement, we show the geodesic completion of the spacetime when probed
by massless, massive, and conformally coupled particles. Finally, the chaotic
behavior of the spacetime near the singularity is tamed by a conformal
rescaling that turns the Bianchi IX metric into a quasi-FLRW spacetime.

</details>


### [249] [Probing Non-rotating Black Hole in Kalb-Ramond Gravity: Imaging and Polarized Signatures Surrounded by Different Thick Accretion Flows](https://arxiv.org/abs/2511.00586)
*Xiao-Xiong Zeng,Chen-Yu Yang,Muhammad Israr Aslam,Rabia Saleem*

Main category: gr-qc

TL;DR: 研究Kalb-Ramond引力中球形对称静态黑洞的阴影和偏振图像，分析参数对高阶图像和初级图像的影响。


<details>
  <summary>Details</summary>
Motivation: 探讨KR引力中黑洞参数对阴影和偏振图像的影响，理解不同吸积盘模型下的成像特征。

Method: 使用两种几何厚吸积盘模型（RIAF-like模型和HOU盘模型）分析黑洞阴影，在HOU盘模型下研究各向异性辐射的偏振图像。

Result: 参数增加会减小高阶图像尺寸，观测者倾角改变图像形状。HOU模型中高倾角时视界轮廓遮蔽减弱。偏振强度在高阶图像区域最强，远离时快速衰减。

Conclusion: 参数变化揭示了时空内在结构，观测者取向与参数共同塑造偏振特征，不同吸积盘模型显示显著亮度差异。

Abstract: In this work, we consider a spherically symmetric static black hole metric in
Kalb-Ramond (KR) gravity, and investigate the impact of relevant parameters on
the black hole shadow and polarization images. For black hole shadow images, we
consider two geometrically thick accretion disk models such as a
phenomenological RIAF-like model and an analytical HOU disk model. In each
case, we observe a bright ring-like structure corresponding to the higher-order
images with a surrounding region of non-zero intensity that represents the
primary image. The increasing values of $\hat{\lambda}$ or $\hat{\gamma}$
results in decrease the size of the higher-order image, while increasing values
of observer inclination $\theta_{o}$ alter its shape and cause the horizons
outline to be obscured. On the other hand, in HOU disk model, at high observer
inclinations, the obscuration of the horizons outline by radiation from outside
the equatorial plane is weakened. Consequently, the brightness of the primary
image in the phenomenological model is significantly greater than that in the
HOU disk model, indicating the strong gravitational lensing effect. For the
polarized images, we use only the HOU disk model with anisotropic radiation,
assuming an infalling accretion flow matter. The obtained results illustrate
that the polarization intensity $P_{o}$ in the higher-order image region is
significantly stronger than as compare to other regions, and it is rapidly
decreases away from this region. The variation in $\hat{\lambda}$ and
$\hat{\gamma}$ depicts the intrinsic structure of the space-time and $\theta_o$
depends on the observers orientation, together they shape the polarization
features.

</details>


### [250] [Impact of a third body on binary neutron star tidal interactions](https://arxiv.org/abs/2511.00621)
*Meet Khatri,Ankur Renduchintala,Sayak Datta,Sajal Mukherjee*

Main category: gr-qc

TL;DR: 该论文研究了在紧凑双星并合波形建模中引入第三体扰动的影响，重点分析了第三体对双星动力学和潮汐相互作用的影响。


<details>
  <summary>Details</summary>
Motivation: 传统的紧凑双星并合波形建模假设双星处于孤立状态，但现实中可能存在第三体扰动。本研究旨在打破这一假设，探讨遥远第三体对双星动力学的影响。

Method: 采用微扰方法处理三体问题，研究第三体存在引起的潮汐相互作用。修改了轨道运动方程和双星组分四极矩演化方程，计算了辐射能量和累积相位差。

Result: 研究发现对于b-EMRI系统，潮汐效应较弱；而对于b-IMRI系统，这些效应最为显著，值得深入研究。

Conclusion: 第三体扰动对紧凑双星并合系统的影响因系统类型而异，b-IMRI系统是研究这些效应的最相关对象。

Abstract: For waveform modelling of compact binary coalescence, it is conventionally
assumed that the binary is in isolation. In this work, we break that assumption
and introduce a third body at a distance. The primary goal is to understand how
the distant third body would affect the binary dynamics. However, in the
present work, we treat the three-body problem perturbatively and study tidal
interaction in the binary due to the third body's presence. We introduce
appropriate modifications to the equations governing the orbital motions and
the evolution equations of the binary component's quadrupole moment. Further,
we obtain the radiated energy and accumulated dephasing for the binary. We show
that for b-EMRI, the effect is weak in the tidal sector, while for systems such
as b-IMRIs, it would be most relevant to study these effects.

</details>


### [251] [Long-lived modes and grey-body factors of massive fields in quantum-corrected (Hayward) black holes](https://arxiv.org/abs/2511.00778)
*Alexey Dubinsky*

Main category: gr-qc

TL;DR: 研究海沃德黑洞背景下大质量标量场的动力学，发现场质量显著抑制准正规振荡的衰减率，产生长寿命模式，并在特定临界质量下接近准共振态。


<details>
  <summary>Details</summary>
Motivation: 探索海沃德黑洞（既可解释为规则时空，也可作为渐近安全引力的有效几何）中大质量标量场的动力学行为，特别是准正规模和灰体因子的特性。

Method: 使用带Padé改进的WKB方法计算准正规谱和灰体因子，并通过时域积分和Prony分析进行验证。

Result: 场质量显著抑制准正规振荡的衰减率，产生长寿命模式；时域中标准指数衰减被具有幂律包络的振荡尾部取代；灰体因子显示传输峰向高频移动且低频部分被抑制。

Conclusion: 准正规模与灰体因子之间的对应关系对大质量场仍然有效，对于大多极数具有高精度，但随着场质量增加或多极数减少而逐渐失去精度。

Abstract: We study the dynamics of a massive scalar field in the background of the
Hayward black hole, which can be interpreted both as a regular spacetime and as
an effective geometry arising from Asymptotically Safe gravity. The quasinormal
spectrum and grey-body factors are computed using the WKB method with Pad\'e
improvements and confirmed through time-domain integration followed by Prony
analysis. We find that the mass of the field significantly suppresses the
damping rate of quasinormal oscillations, giving rise to long-lived modes that
continuously approach arbitrarily long-lived states (quasi-resonances) at
certain critical field masses. In the time domain, the standard exponentially
decaying ringdown is replaced by oscillatory tails with a power-law envelope.
The corresponding grey-body factors reveal a pronounced shift of the
transmission peak toward higher frequencies and a suppression of the
low-frequency part of the spectrum. Finally, we show that the correspondence
between quasinormal modes and grey-body factors remains valid for massive
fields, being highly accurate for large multipole numbers and gradually losing
precision as either the field mass increases or the multipole number decreases.

</details>


### [252] [Scalar perturbation around a rotating Kalb-Ramond BTZ black hole](https://arxiv.org/abs/2511.00784)
*Zhong-Wu Xia,Sheng Long,Huajie Gong,Qiyuan Pan,Jiliang Jing*

Main category: gr-qc

TL;DR: 本文研究了Kalb-Ramond BTZ类黑洞的标量扰动，发现KR参数显著改变准正规模谱，并识别出左分支基模的不稳定性。


<details>
  <summary>Details</summary>
Motivation: 研究KR场如何影响黑洞扰动，特别是对QNM谱和超辐射现象的修改作用。

Method: 通过分离变量法将Klein-Gordon方程径向部分化为广义Heun方程，使用Heun函数计算准正规模，并分析能量和角动量通量。

Result: KR参数显著修改QNM谱，左分支基模存在不稳定性，且KR场改变了超辐射发生的阈值和Robin耦合参数范围。

Conclusion: KR场在黑洞扰动建模中具有重要作用，能够显著影响QNM谱和超辐射特性。

Abstract: We investigate the scalar perturbation of a newly proposed Kalb-Ramond (KR)
BTZ-like black hole. After the separation of variables for the Klein-Gordon
equation, we find that the radial part reduces to the general Heun equation.
Using the Heun function, we compute quasinormal modes (QNMs) subject to generic
Robin boundary conditions, which shows that the KR parameter substantially
modifies the QNM spectrum and only the fundamental mode on the left branch has
an instability. To ascertain whether the instability is superradiant, we
further analyze how the KR field changes the energy and angular momentum
fluxes. Our results show that the KR parameter shifts the threshold and the
range of the Robin coupling parameter where the superradiance occurs,
underscoring the importance of the KR field in modeling black hole
perturbations.

</details>


### [253] [Wormhole geometries in Einstein-aether theory](https://arxiv.org/abs/2511.00825)
*Hanif Golchin,Hamid R. Bakhtiarizadeh,Mohammad Reza Mehdizadeh*

Main category: gr-qc

TL;DR: 首次在爱因斯坦-以太理论背景下研究可穿越虫洞解，发现某些参数组合下存在虫洞几何结构，且能同时满足零能量条件和弱能量条件。


<details>
  <summary>Details</summary>
Motivation: 探索爱因斯坦-以太理论中是否存在可穿越虫洞解，并研究这些解是否能够满足能量条件，这是传统爱因斯坦引力理论中难以实现的目标。

Method: 通过求解爱因斯坦-以太理论的场方程，分析三种不同类型虫洞形状函数下的零能量条件和弱能量条件。

Result: 发现通过选择合适的模型参数，虫洞几何结构能够在虫洞喉部及整个空间都满足能量条件，这为爱因斯坦-以太理论提供了新的约束条件。

Conclusion: 能量条件的满足对爱因斯坦-以太耦合参数施加了比先前理论和观测考虑更严格的限制，表明该理论在描述虫洞几何方面具有独特优势。

Abstract: We perform the first study of traversable wormhole solutions in the
background of Einstein-Aether theory. We show that the field equations admit
some wormhole geometries, for several combinations of values of the aether
coupling constants. We investigating the null and weak energy conditions for
wormhole solutions with three types of the worm hole shape function. In
contrast to Einstein gravity, we find that by choosing adequate values for the
parameters of the models, wormhole geometries respect the energy conditions at
the wormhole throat and also throughout the whole space. Satisfaction of these
energy conditions led to some constraints on the value of the Einstein-Aether
theory. Comparing these constraints with those previously obtained from
theoretical and observational considerations, we find that the satisfaction of
energy condition put more limitations on the values of the Einstein-Aether
couplings.

</details>


### [254] [Reconstruction of Black Hole Ringdown Signals with Data Gaps using a Deep-Learning Framework](https://arxiv.org/abs/2511.00834)
*Jing-Qi Lai,Jia-Geng Jiao,Cai-Ying Shao,Jun-Xi Shi,Yu Tian*

Main category: gr-qc

TL;DR: 提出了DenoiseGapFiller (DGF)深度学习框架，专门用于重建被数据间隙和仪器噪声破坏的引力波环降信号，通过双分支编码器-解码器架构实现高质量信号重建。


<details>
  <summary>Details</summary>
Motivation: 引力波环降信号经常受到数据间隙和仪器噪声的干扰，影响信号检测和参数估计的准确性，需要开发专门的方法来恢复这些被破坏的信号。

Method: 采用双分支编码器-解码器架构，通过混合层和Transformer风格块进行融合，在包含高达20%数据间隙的合成环降波形上进行端到端训练。

Result: 实现了平均波形失配为0.002，时域残差幅度降低约一个数量级，0.01-1 Hz频段的功率谱密度被抑制1-2个数量级，恢复了0.01-0.1 Hz附近的准正则模式峰值。

Conclusion: DGF能够忠实地重建原始信号，为后续科学工作奠定了基础，这意味着在检测证据中惩罚更轻，参数估计的可信区域更紧。

Abstract: We introduce DenoiseGapFiller (DGF), a deep-learning framework specifically
designed to reconstruct gravitational-wave ringdown signals corrupted by data
gaps and instrumental noise. DGF employs a dual-branch encoder-decoder
architecture, which is fused via mixing layers and Transformer-style blocks.
Trained end-to-end on synthetic ringdown waveforms with gaps up to 20% of the
segment length, DGF can achieve a mean waveform mismatch of 0.002. The residual
amplitudes of the Time-domain shrink by roughly an order of magnitude and the
power spectral density in the 0.01-1 Hz band is suppressed by 1-2 orders of
magnitude, restoring the peak of quasi-normal mode(QNM) in the time-frequency
representation around 0.01-0.1 Hz. The ability of the model to faithfully
reconstruct the original signals, which implies milder penalties in the
detection evidence and tighter credible regions for parameter estimation, lay a
foundation for the following scientific work.

</details>


### [255] [The final fate of anisotropic-dissipative gravitational collapse](https://arxiv.org/abs/2511.00890)
*Kanabar Jay*

Main category: gr-qc

TL;DR: 研究高度磁化辐射主导的球对称耗散恒星构型的最终命运，通过引入辐射不透明度的角度依赖性几何因子，推导出决定坍缩启动、停止或逆转的方向敏感阈值条件。


<details>
  <summary>Details</summary>
Motivation: 探索恒星最终命运不仅取决于物质总量，还取决于物质在不同方向上抵抗引力的能力，特别是高度磁化辐射主导恒星构型的坍缩动力学。

Method: 引入由辐射不透明度角度依赖性定义的几何因子f(θ,φ)，利用场方程推导方向敏感的阈值条件，建立几何控制框架统一分析黑洞形成、反弹行为和测地线延迟捕获。

Result: 获得了决定坍缩启动、停止或逆转的方向敏感阈值条件，将黑洞形成、反弹行为和测地线延迟捕获统一到单一几何控制框架中。

Conclusion: 该理论分析有助于通过预先计算的不同磁场、温度和密度剖面的不透明度表，以及其他可用数据来分析坍缩过程。

Abstract: The final fate of a collapsing star depends not only on how much matter it
contains but also on how that matter resists gravity in different directions.
In this work, we investigate the final fate of highly magnetized
radiation-dominated spherically symmetric dissipative stellar configurations.
We study the dynamics of collapse by introducing a dimensionless geometric
factor $f(\theta, \phi)$ defined by the angular dependence of the radiative
opacity. Using the field equations, we derive direction-sensitive threshold
conditions that determine whether collapse initiates, halts, or reverses. The
resulting inequalities unify black hole formation, bounce behavior, and delayed
trapping of geodesics into a single geometrically controlled framework. This
theoretical analysis would help analyze the collapse through pre-computed
opacity tables for different magnetic field, temperature, and density profiles,
along with other data available for such considered profiles.

</details>


### [256] [From Spray to Metric: The Geometric Construction of the Jacobi Metric](https://arxiv.org/abs/2511.01004)
*Zonghai Li*

Main category: gr-qc

TL;DR: 提出了一种从测地线方程视角系统化几何化动力学的方法，通过施加动力学约束将半喷射提升为喷射，并通过重新参数化提取相关度量结构。


<details>
  <summary>Details</summary>
Motivation: 为动力学系统提供一种独立于传统变分框架的几何视角，建立从运动方程到度量结构的直接路径。

Method: 通过施加合适的动力学约束将半喷射提升为喷射，然后通过重新参数化提取相关的度量结构。

Result: 在静态时空下恢复了光学度量、大质量粒子的雅可比度量及其在电磁场中带电粒子的推广；在平面圆形限制三体问题中自然产生了Randers型Finsler度量。

Conclusion: 该方法为动力学系统的几何研究提供了新视角，可能成为进一步研究的基础。

Abstract: This paper develops a systematic approach to the geometrization of dynamics
from the viewpoint of the geodesic equation. The method promotes a semispray to
a spray through the imposition of suitable dynamical constraints, and the
associated metric structure is extracted via reparameterization. When applied
to static spacetimes, this spray-to-metric framework recovers the optical
metric, the Jacobi metric for massive particles, and its generalization for
charged particles in electromagnetic fields. We further show that a
Randers-type Finsler metric arises naturally in the planar circular restricted
three-body problem. By establishing a direct pathway from equations of motion
to metric structures, this work offers a geometric perspective, independent of
the traditional variational framework, may provide a basis for further studies
on dynamical systems.

</details>


### [257] [Noncommutative dyonic black holes sourced by nonlinear electromagnetic fields](https://arxiv.org/abs/2511.01020)
*Ana Bokulić,Filip Požar*

Main category: gr-qc

TL;DR: 该论文研究了非交换(NC)修正对非线性电动力学(NLE)拉格朗日量的影响，通过Drinfel'd扭曲和Seiberg-Witten映射实现爱因斯坦-NLE理论的NC变形，并求解了静态球对称双荷黑洞的微扰解。


<details>
  <summary>Details</summary>
Motivation: 探索非交换几何对非线性电动力学的修正效应，将NC变形引入爱因斯坦-NLE理论框架，研究两种非线性来源的相互作用。

Method: 使用∂t∧∂φ Drinfel'd扭曲和Seiberg-Witten映射实现NC变形，在交换极限下假设静态球对称双荷黑洞作为种子解，对NC参数a进行一阶微扰求解运动方程。

Result: 获得了对度量张量和规范势的一阶NC修正，并在多个著名NLE理论中评估了这些修正的具体形式。

Conclusion: NC变形在物质部分引入了额外的非线性效应，与NLE本身非线性共同构成了该框架中的双重非线性来源，为理解NC几何对引力-电磁相互作用的影响提供了新视角。

Abstract: We introduce the first-order noncommutative (NC) corrections to the general
nonlinear electrodynamics (NLE) Lagrangian depending on two electromagnetic
invariants. The NC deformation of Einstein-NLE theory is implemented using the
$\partial_t\wedge\partial_\varphi$ Drinfel'd twist and the NC effects are
encoded in the matter sector through the Seiberg-Witten map. The resulting
equations of motion reflect two distinct sources of nonlinearity in this
framework; one arising from replacing Maxwell's electrodynamics with its
nonlinear modifications and another from the NC deformations. Assuming a
general form of static, spherically symmetric dyonic black hole as a seed
solution in the commutative limit, we solve the equations of motion
perturbatively to the first order in the NC parameter $a$. Finally, we evaluate
the obtained corrections to the metric tensor and gauge potential for several
prominent NLE theories.

</details>


### [258] [Various metric forms of all type D black holes and their application](https://arxiv.org/abs/2511.01029)
*Jiri Podolsky*

Main category: gr-qc

TL;DR: 总结了爱因斯坦-麦克斯韦-Λ方程D型双对齐电磁场精确解的全类，包括带旋转、NUT参数和加速的带电黑洞，讨论了不同度量表示及其物理几何性质。


<details>
  <summary>Details</summary>
Motivation: 系统整理和研究爱因斯坦-麦克斯韦-Λ方程D型双对齐电磁场的精确解全类，特别是包含旋转、NUT参数和加速的带电黑洞解。

Method: 总结Plebanski-Demianski、Griffiths-Podolsky-Vratny和Astorino三种度量表示，分析它们之间的相互关系，并用于研究物理和几何性质。

Result: 证明了这类黑洞当且仅当加速时才会发射引力辐射，并扩展到非对齐麦克斯韦场的D型黑洞。

Conclusion: 建立了D型双对齐电磁场精确解的统一框架，揭示了加速与引力辐射发射的关联，为研究黑洞物理性质提供了有力工具。

Abstract: Several recent results concerning the complete class of exact solutions to
the Einstein-Maxwell-$\Lambda$ equations of type D with a double-aligned
electromagnetic field are summarized. This large class of spacetimes includes
charged black holes with rotation, NUT parameter, and acceleration. In
particular, we present their Plebanski-Demianski, Griffiths-Podolsky-Vratny,
and Astorino metric representations, we discuss mutual relations between them,
and demonstrate their usefulness for investigation of physical and geometrical
properties (singularities, horizons, conformal diagrams, ergoregions, rotating
strings causing the acceleration, thermodynamics). It includes the proof that
such black holes emit gravitational radiation if, and only if, they accelerate.
Very recent extension to type D black holes with non-aligned Maxwell field is
also mentioned.

</details>


### [259] [Can we identify primordial black holes? The role of subsolar gravitational wave events](https://arxiv.org/abs/2511.01051)
*Francesco Crescimbeni*

Main category: gr-qc

TL;DR: 该论文批判性地分析了如何区分亚太阳质量致密双星系统中的原初黑洞与中子星，通过研究潮汐效应在引力波信号中的差异来识别原初黑洞。


<details>
  <summary>Details</summary>
Motivation: 亚太阳质量致密双星合并的探测被认为是原初黑洞存在的最有力证据之一，但需要与可能存在于亚太阳质量范围内的中子星等恒星双星系统进行区分。

Method: 通过预测当前和未来引力波探测器对亚太阳质量双星中潮汐效应的约束能力，分析引力波信号中潮汐效应的差异。

Result: 研究发现，与PBHs不同，恒星双星的引力波信号会受到潮汐效应的影响，且随着质量减小，这种效应会增强数个数量级。

Conclusion: 亚太阳质量合并的探测对宇宙学和核物理都具有重要意义，通过潮汐效应分析可以有效区分原初黑洞和恒星双星系统。

Abstract: The detection of a subsolar object in a compact binary merger is regarded as
one of the most compelling signatures of a population of primordial black holes
(PBHs). We critically examine whether such systems can be distinguished from
stellar binaries, such as those composed of neutron stars (NSs), which could
also populate the subsolar mass range. Unlike PBHs, the gravitational-wave
signal from stellar binaries is affected by tidal effects, which increase by
several orders of magnitude as the mass decreases. We forecast the capability
of current and future gravitational-wave (GW) detectors to constrain tidal
effects in putative subsolar binaries. We also discuss the broader implications
that the detection of a subsolar merger would have for both cosmology and
nuclear physics.

</details>


### [260] [Cosmological spacetimes with spatially constant sign-changing curvature](https://arxiv.org/abs/2511.01128)
*Miguel Sánchez*

Main category: gr-qc

TL;DR: 该论文构建了全局双曲时空模型，其中时间函数的空间切片具有变化的曲率符号和拓扑结构，突破了经典宇宙学原理的限制。


<details>
  <summary>Details</summary>
Motivation: 挑战经典宇宙学原理关于空间切片曲率符号和拓扑结构必须保持不变的假设，探索更丰富的宇宙学可能性。

Method: 通过时间函数构造全局双曲时空，使得空间切片的曲率k(t₀)和拓扑结构随t₀变化。

Result: 成功构建了空间切片曲率符号和拓扑结构可变的宇宙学模型。

Conclusion: 这些模型展示了宇宙学中超越经典宇宙学原理限制的新可能性，为宇宙演化提供了更丰富的理论框架。

Abstract: Globally hyperbolic spacetimes endowed with a time function $t$ whose
spacelike slices $t=t_0$ have constant curvature $k(t_0)$ and where the sign of
$k(t_0)$ (as well as the topology of the slice) varies with $t_0$, can be
constructed despite some common claims about the implications of the classical
Cosmological Principle. Here, we stress the possibilities of these cosmologies
and announce the development of new models obtained in collaboration with G.
Garc\'{\i}a-Moreno, B. Janssen, A. Jim\'enez-Cano, M. Mars and R. Vera

</details>


### [261] [Black holes in a dense infinite medium: a toy-model regularizing the Schwarzschild metric](https://arxiv.org/abs/2511.01326)
*Aurélien Barrau,Killian Martineau,Hanane Zelgoum*

Main category: gr-qc

TL;DR: 本文重新研究了黑洞从周围均匀无限空间吸积能量的动力学，提出了当介质密度不可忽略时对史瓦西近似的简单启发式修正，解决了质量在有限时间内发散的问题，并显著改变了热力学性质。


<details>
  <summary>Details</summary>
Motivation: 重新审视黑洞吸积动力学，特别是当周围介质密度不可忽略时，传统史瓦西近似需要修正，以解决质量发散等不合理现象。

Method: 采用牛顿力学方法，对史瓦西近似进行简单的启发式修正，主要是一种指导性方法而非严格处理。

Result: 修正后的行为显著改善：质量在有限时间内发散的问题得到解决，热力学性质发生深刻变化，结果比通常获得的行为更令人信服。

Conclusion: 这种修正方法为量子引力和反弹模型提供了潜在启示，虽然基于牛顿方法，但为更严格处理提供了指导方向。

Abstract: We revisit the dynamics of a black hole accreting energy from a surrounding
homogeneous and infinite space. We argue for a simple heuristic modification of
the Schwarzschild approximation when the density of the medium is not
negligible anymore. The resulting behavior is drastically modified: the mass
divergence at finite time is cured and the thermodynamical properties are
deeply changed. Some potential consequences for quantum gravity and bouncing
models are also pointed out. Those conclusions being mostly obtained from a
Newtonian approach, they only aim at guiding toward a more rigorous treatment.
Still, interestingly, the behavior is far more convincing that the one usually
obtained.

</details>


### [262] [Analytical sensitivity curves of the second-generation time-delay interferometry](https://arxiv.org/abs/2511.01330)
*Chunyu Zhang*

Main category: gr-qc

TL;DR: 该论文引入逆光程算子简化第二代时间延迟干涉测量(TDI)组合的表示，推导了各种TDI组合的响应函数、噪声功率谱密度和灵敏度曲线的解析表达式，证明了第二代TDI与第一代具有相同的灵敏度。


<details>
  <summary>Details</summary>
Motivation: 为未来天基引力波探测器开发简化的数学工具来描述第二代时间延迟干涉测量(TDI)组合，提供分析表达式和近似公式用于仪器优化和数据分析研究。

Method: 引入逆光程算子来简化第二代TDI组合的表示，推导了TDI Michelson、(α,β,γ)、Monitor、Beacon、Relay和Sagnac组合及其正交A、E、T通道的响应函数、噪声功率谱密度和灵敏度曲线的解析表达式和高精度近似公式。

Result: 发现第二代TDI与第一代具有相同灵敏度；A、E、T通道的灵敏度和最优灵敏度与TDI代数和具体组合无关；A和E通道具有相等的平均响应、噪声PSD和灵敏度；T通道在低频响应较弱；除(α,β,γ)和ζ组合及T通道外，所有灵敏度曲线在特定频率范围内呈现平坦段。

Conclusion: 这些分析和近似公式为未来天基引力波探测器的仪器优化和数据分析研究提供了有用的基准，证明了第二代TDI组合的有效性和性能特征。

Abstract: Forthcoming space-based gravitational-wave (GW) detectors will employ
second-generation time-delay interferometry (TDI) to suppress laser frequency
noise and achieve the sensitivity required for GW detection. We introduce an
inverse light-path operator $\mathcal{P}_{i_{1}i_{2}i_{3}\ldots i_{n-1}i_{n}}$,
which enables simple representation of second-generation TDI combinations and a
concise description of light propagation. Analytical expressions and
high-accuracy approximate formulas are derived for the sky- and
polarization-averaged response functions, noise power spectral densities
(PSDs), and sensitivity curves of TDI Michelson, ($\alpha,\beta,\gamma$),
Monitor, Beacon, Relay, and Sagnac combinations, as well as their orthogonal
$A, E, T$ channels. Our results show that: (i) second-generation TDIs have the
same sensitivities as their first-generation counterparts; (ii) the $A, E, T$
sensitivities and the optimal sensitivity are independent of the TDI generation
and specific combination; (iii) the $A$ and $E$ channels have equal averaged
responses, noise PSDs, and sensitivities, while the $T$ channel has much weaker
response and sensitivity at low frequencies ($2\pi fL/c\lesssim3$); (iv) except
for the $(\alpha,\beta,\gamma)$ and $\zeta$ combinations and the $T$ channel,
all sensitivity curves exhibit a flat section in the range $f_{n}<f\lesssim
1.5/(2\pi L/c)$, where the noise-balance frequency $f_{n}$ separates the
proof-mass- and optical-path-dominated regimes, while the response-transition
frequency $\sim 1.5/(2\pi L/c)$ separates the response function's low- and
high-frequency behaviors; (v) the averaged response, noise PSD, and sensitivity
of $\zeta$ scales with those of the $T$ channel. These analytical and
approximate formulations provide useful benchmarks for instrument optimization
and data-analysis studies for future space-based GW detectors.

</details>


### [263] [Hotspot Images Driven by Magnetic Reconnection in Kerr-Sen black hole](https://arxiv.org/abs/2511.01342)
*Ke Wang,Xiao-Xiong Zeng*

Main category: gr-qc

TL;DR: 研究Kerr-Sen黑洞中磁重联前后热点图像的变化，发现第一耀斑可作为能量提取的潜在特征，观测者方位角、黑洞膨胀参数和自旋都会影响能量提取信号的检测。


<details>
  <summary>Details</summary>
Motivation: 研究Kerr-Sen黑洞中磁重联过程对热点图像的影响，探索能量提取的观测特征。

Method: 回顾Comisso-Asenjo磁重联过程，引入热点成像方法，分析热点强度的时间演化，包括能量提取发生与不发生的情况，以及观测者方位角变化的影响。

Result: 第一耀斑可能作为能量提取的潜在特征；改变观测者方位角可能改变第一和第二耀斑之间的时间间隔；较大的膨胀参数和较高的自旋都使能量提取信号更难识别。

Conclusion: 磁重联过程在Kerr-Sen黑洞中会产生可观测的热点图像变化，第一耀斑可作为能量提取的指示器，但黑洞参数会影响信号的可检测性。

Abstract: In the Kerr-Sen black hole, this study investigates the changes in hotspot
images before and after the occurrence of magnetic reconnection. After
reviewing the Comisso-Asenjo magnetic reconnection process and introducing the
hotspot imaging method, we examine the temporal evolution of hotspot intensity,
including when energy extraction occurs, when it does not occur, and when the
observer's azimuthal angle is altered. We also discuss the influence of the
black hole's expansion parameter and spin on hotspot imaging. The results
indicate that the first flare may serve as a potential signature of ongoing
energy extraction: changing the observer's azimuthal angle may alter the time
interval between the first and second flares: a larger expansion parameter
makes it more difficult to identify the energy extraction signal, and a higher
spin also makes it more challenging to detect the energy extraction signal.

</details>


### [264] [Novel topological subclass in Hourava-Lifshitz black holes](https://arxiv.org/abs/2511.01367)
*Hao Chen,Meng-Yao Zhang,Hassan Hassanabadi,Qihong Huang,Zheng-Wen Long*

Main category: gr-qc

TL;DR: 本文在z=3 Hořava-Lifshitz引力理论中，对带电静态黑洞的热力学拓扑进行了普遍分类，发现了一个新的拓扑子类$\ddot{W}^{1-}$，扩展了现有的分类体系。


<details>
  <summary>Details</summary>
Motivation: 探索带电黑洞在Hořava-Lifshitz引力理论中的热力学拓扑性质，完善现有的拓扑分类框架。

Method: 在正则系综和大正则系综下，分析z=3 Hořava-Lifshitz引力中带电静态黑洞的热力学拓扑结构。

Result: 发现了一个新的拓扑子类$\ddot{W}^{1-}$，该子类表现出独特的稳定性特征：低温下相空间中存在一个不稳定的小黑洞，高温下则有两个不稳定的小黑洞和一个稳定的大黑洞共存。

Conclusion: 带电黑洞的稳定性依赖于系综的选择，这一发现有助于完善黑洞热力学的拓扑框架，为理解黑洞和引力的本质提供了重要视角。

Abstract: This work explores the universal classification of thermodynamic topology for
charged static black holes within the $z=3$ Ho\u{r}ava-Lifshitz gravity theory,
considering both canonical and grand canonical ensembles. We introduce a new
topological subclass, denoted as $\ddot{W}^{1-}$. This finding expands the
existing topological classification, going beyond the five previously defined
classes and their respective subclasses. The $\ddot{W}^{1-}$ subclass presents
a distinct and previously unobserved stability profile: In the low-temperature
regime, an unstable small black hole appears in the phase space, whereas, while
in the high temperature regime, two unstable small black holes exist together
with a stable large black hole. Our study underscores the dependence of charged
black hole stability on the selection of the ensemble. These results contribute
to refining and expanding the topological framework in black hole
thermodynamics, providing key perspectives on the underlying nature of black
holes and gravity.

</details>


### [265] [Scalar self-force effects in neutral $W$-soliton backgrounds](https://arxiv.org/abs/2511.01402)
*Massimo Bianchi,Donato Bini,Giorgio Di Russo*

Main category: gr-qc

TL;DR: 研究W-孤子解的几何和物理性质，包括5维解及其4维约化，分析粒子散射过程和准正规模谱。


<details>
  <summary>Details</summary>
Motivation: 探索新发现的W-孤子解在5维和4维情况下的几何与物理特性，特别是散射过程和准正规模行为。

Method: 研究5维W-孤子解及其4维约化，分析质量/无质量粒子散射，计算规范不变散射角，研究标量场传播和准正规模谱。

Result: 获得了散射角的精确表达式和大角动量展开式，计算了准正规模谱和辐射能量损失，后者以全解析的后牛顿展开形式给出。

Conclusion: W-孤子解在5维和4维情况下展现出相似的物理特性，但存在重要差异，散射过程和准正规模分析为理解该解提供了新见解。

Abstract: We investigate several geometrical and physical properties of the recently
found $W$-soliton solution (neutral case). We discuss both the genuine 5d
solution and its reduction to 4d and highlight similarities and differences. In
both cases, we study scattering processes of massless and massive particles in
the background, reconstructing the gauge-invariant scattering angle, either
with exact expressions or with large-angular momentum expansion expressions,
which we show how to resum in a useful form. Finally, we analyze the
propagation of a test scalar field in the $W$-soliton background and compute
the spectrum of Quasi Normal Modes in the case of (non-)minimal coupling and
the radiated energy in the case of minimal coupling. Our result for the energy
loss is fully analytic and presented in a Post-Newtonian expansion, following
the approach termed gravitational self force.

</details>


### [266] [Robustness of timelike circular orbit topology against particle spin](https://arxiv.org/abs/2511.01447)
*Yong Song,Jiaqi Fu,Yiting Cen*

Main category: gr-qc

TL;DR: 该论文通过拓扑方法研究了静态球对称时空中旋转测试粒子的类时圆轨道的拓扑性质，发现在相邻视界之间拓扑数为W=-1（存在不稳定轨道），在最外层视界外W=0（稳定-不稳定轨道成对出现或不存在），这些结果与粒子自旋方向无关。


<details>
  <summary>Details</summary>
Motivation: 研究旋转测试粒子在广义相对论中的运动特性，特别是类时圆轨道的拓扑性质，以揭示时空结构对粒子轨道的基本影响。

Method: 使用Mathisson-Papapetrou-Dixon形式体系和Tulczyjew自旋补充条件，在静态球对称时空中构造辅助势场和矢量场，计算拓扑绕数W。

Result: 在相邻视界之间W=-1（至少存在一个不稳定轨道），在最外层视界外W=0（轨道成对出现或不存在），这些结果在Schwarzschild、Schwarzschild-AdS和Schwarzschild-dS时空中得到验证。

Conclusion: 拓扑方法揭示了即使包含自旋效应，轨道拓扑性质仍保持不变的特性，突出了时空结构本身对粒子轨道的基本影响。

Abstract: Based on a detailed study of the motion of spinning test particles within the
Mathisson-Papapetrou-Dixon formalism under the Tulczyjew spin-supplementary
condition in static, spherically symmetric spacetimes, we investigate the
topological properties of timelike circular orbits (TCOs) for such particles.
By constructing an auxiliary potential and an associated vector field on the
equatorial plane, we compute the topological winding number W for regions
between horizons and outside the outermost horizon in asymptotically flat,
anti-de Sitter (AdS), and de Sitter (dS) black hole spacetimes. Our results
show that between two neighboring horizons (including the cosmological horizon
in the dS case), the topological number is W=-1, indicating the presence of at
least one unstable TCO. Outside the outermost horizon, we find W=0 for both
asymptotically flat and AdS black holes, implying that any TCOs must appear in
stable-unstable pairs or be absent. These conclusions are independent of the
spin orientation (co-rotating or counter-rotating) of the test particle. The
analysis is supported by explicit examples in Schwarzschild, Schwarzschild-AdS,
and Schwarzschild-dS spacetimes, confirming the general topological
predictions. While the effective potential for spinning particles has been
previously studied, the topological approach employed here reveals invariant
properties that remain robust even when spin is included, thereby highlighting
the fundamental influence of the spacetime structure itself.

</details>


### [267] [The images of Brans-Dicke-Kerr type naked singularities](https://arxiv.org/abs/2511.01478)
*Fen Long,Weike Deng,Xin Qin,Songbai Chen,Jiliang Jing*

Main category: gr-qc

TL;DR: 研究了Brans-Dicke-Kerr时空的图像，发现当ω>-3/2时表现为裸奇点，阴影随ω减小而变平变小，ω<1/2时呈现特殊的水母形状和分形结构，a>M时出现独特的灰色区域，可用于区分Kerr时空和测试Brans-Dicke理论。


<details>
  <summary>Details</summary>
Motivation: 研究Brans-Dicke-Kerr时空的图像特征，探索Brans-Dicke参数ω对时空结构和观测图像的影响，为通过高精度观测测试Brans-Dicke理论提供基础。

Method: 分析Brans-Dicke-Kerr时空的图像，研究不同ω参数值下时空的阴影形状和结构特征，特别关注a≤M和a>M两种情况。

Result: 当ω>-3/2时表现为裸奇点；ω减小导致阴影变平变小；ω<1/2时出现特殊的水母形状和分形结构；a>M时图像中出现由两个分离斑块组成的独特灰色区域。

Conclusion: Brans-Dicke-Kerr时空的图像特征明显区别于Kerr和Kerr-de Sitter情况，Brans-Dicke参数的影响有助于揭示时空内在结构，为通过观测测试该理论提供可能。

Abstract: We have studied the images of the Brans-Dicke-Kerr spacetime with a
dimensionless Brans-Dicke parameter $\omega$, which belongs to axisymmetric
rotating solutions in the Brans-Dicke theory. Our results show that the
Brans-Dicke-Kerr spacetime with the parameter $\omega>-3/2$ represents naked
singularities with distinct structures. For the case with $a \leq M$, the
shadow in the Brans-Dicke-Kerr spacetime persists, gradually becomes flatter
and smaller as $\omega$ decreases. Especially when $\omega<1/2$, the shadow in
the image exhibit a very special ``jellyfish" shape and possesses a
self-similar fractal structure. For the case with $a > M$, a distinct gray
region consisting of two separate patches appears in the image observed by
equatorial observers. This indicating that the Brans-Dicke-Kerr spacetime can
be distinguished from the Kerr and Kerr-de Sitter cases based on its image.
These effects of the Brans-Dicke parameter could help us to reveal the
intrinsic structure of the Brans-Dicke-Kerr spacetimes and provide a foundation
for testing Brans-Dicke theory through future high-precision observations.

</details>


### [268] [Shadow of Extreme Compact Charged Objects in Consistent 4-Dimensional Einstein-Gauss-Bonnet Gravity](https://arxiv.org/abs/2511.01533)
*Sara Saghafi,Kourosh Nozari,Maryam Kaveh*

Main category: gr-qc

TL;DR: 本文研究四维爱因斯坦-高斯-博内引力框架下的极端致密带电天体，通过分析其阴影、光线偏折角等光学特性，与事件视界望远镜观测数据对比，来约束高斯-博内耦合常数α。


<details>
  <summary>Details</summary>
Motivation: 为了更好地描述小尺度和宇宙尺度的引力现象，需要扩展广义相对论。四维爱因斯坦-高斯-博内理论通过引入曲率相关的时空修正项来实现这一目标。

Method: 在四维爱因斯坦-高斯-博内引力框架下研究极端致密带电天体，分析其测地线结构和光学特性（包括阴影、光线偏折角等可观测参数），并与事件视界望远镜的观测数据进行对比。

Result: 通过理论预测与观测数据的对比分析，获得了对高斯-博内耦合常数α的约束条件。

Conclusion: 四维爱因斯坦-高斯-博内引力理论能够通过极端致密带电天体的光学观测特性来约束其耦合参数，为测试修正引力理论提供了有效途径。

Abstract: In order to better describe gravitational phenomena on both very small and
cosmological scales, there have been constant attempts to generalize and expand
the theory of General Relativity (GR) since its inception. The Einstein Gauss
Bonnet (EGB) theory is one such extension that adds spacetime corrections
related to curvature. Since the standard Gauss Bonnet term is purely
topological, it does not contribute to the field equations in four dimensions.
To get around this restriction, however, an invariant four dimensional limit
has been developed. In this work, we study Extreme Compact Charged Objects
(ECCOs), which can resemble black holes, in a gravity framework that is
compatible with Einstein Gauss Bonnet in four dimensions. Our main goal is to
compare theoretical predictions with Event Horizon Telescope (EHT)
observational data in order to constrain the Gauss Bonnet coupling constant
{\alpha}. In order to achieve this, we investigate important optical
characteristics like the shadow, light bending angle, and other associated
observables, as well as the geodesic structure of ECCO spacetimes in EGB
gravity. Finally, we apply these findings to constrain the Gauss Bonnet
constant.

</details>


### [269] [A General Framework for the Spontaneous Scalarization of Regular Black Holes](https://arxiv.org/abs/2511.01544)
*Ernesto Contreras,Mikaela Carrasco-Hidalgo,Pedro Bargueño,Arthur G. Suvorov*

Main category: gr-qc

TL;DR: 研究非线性电动力学支持的规则黑洞的自发标量化，通过P-对偶形式重构电磁场部分并耦合标量场，发现标量化分支在熵方面更优，观测偏差在10%以内。


<details>
  <summary>Details</summary>
Motivation: 探索超越Reissner-Nordström情况的更一般非线性电动力学时空中的标量化现象，构建通用框架来研究规则黑洞的标量化行为。

Method: 从任意种子度量出发，使用P-对偶形式重构电磁场部分，然后非最小耦合实标量场，以Balart-Vagenas规则黑洞为例进行具体分析。

Result: 发现标量化和无标量分支可以共存，标量化构型在熵方面更优；观测偏差在阴影尺寸和准正规模方面均小于10%，当前观测无法排除这些解。

Conclusion: 该构造为探索非线性电动力学支持的时空标量化提供了通用路径，扩展了超出特定Reissner-Nordström类情况的研究范围。

Abstract: We investigate the spontaneous scalarization of generic, static, and
spherically symmetric regular black holes supported by nonlinear
electrodynamics. Starting from an arbitrary seed metric, we employ the P-dual
formalism to reconstruct the electromagnetic sector and subsequently couple a
real scalar field nonminimally. As a worked example, we apply the framework to
the regular Balart-Vagenas black hole, showing that scalarized and scalar-free
branches can coexist in a region where the scalarized configurations are
entropically preferred. We further assess possible observational imprints,
finding percent-level deviations in both the shadow size and the fundamental
scalar quasi-normal modes ($< 10\%$ for small charge-to-mass ratios),
indicating that current electromagnetic and gravitational-wave observations do
not rule out these solutions. Our construction thus provides a general route to
explore scalarization on top of nonlinear-electrodynamics-supported spacetimes,
extending beyond specific Reissner-Nordstr\"om-like cases.

</details>


### [270] [Does the survival and sudden death of quadripartite steering in curved spacetime truly depend on multi-directionality?](https://arxiv.org/abs/2511.01561)
*Xiaobao Liu,Wentao Liu,Si-Han Shang,Shu-Min Wu*

Main category: gr-qc

TL;DR: 研究了史瓦西黑洞背景下高斯四体量子导引的方向依赖性及其在不同模式间的重新分布，发现了三种不同的行为模式，揭示了量子导引对时空曲率的方向敏感性。


<details>
  <summary>Details</summary>
Motivation: 探索黑洞引力场对量子导引这一非经典量子关联的影响，特别是其方向依赖性在引力背景下的表现。

Method: 在史瓦西黑洞背景下系统研究高斯四体量子导引，分析可访问和不可访问区域中的导引行为，考虑霍金温度的影响。

Result: 发现三种不同的导引行为：(1)从非引力观测者到引力观测者的导引在最大不对称性时突然死亡；(2)反向导引单调衰减；(3)混合分区到非引力模式的导引保持有限渐近值。

Conclusion: 量子导引在黑洞背景下表现出强烈的方向敏感性，霍金效应产生的不可访问导引具有内在不对称性，导引行为强烈依赖于方向。

Abstract: We systematically investigate the directional dependence of Gaussian
quadripartite quantum steering and its redistribution among different modes in
the background of a Schwarzschild black hole. For physically accessible
sectors, we identify three distinct behaviors: (i) steering from
non-gravitational to gravitational observers undergoes sudden death at maximal
asymmetry with the Hawking temperature, marking the crossover from two-way to
one-way steerability; (ii) steering in the opposite direction decays
monotonically and vanishes only in the extreme black hole limit, highlighting
its directional sensitivity to spacetime curvature; (iii) steering from hybrid
gravitational-non-gravitational partitions to non-gravitational mode persists
at a finite asymptotic value set by the initial squeezing parameter. Moreover,
all inaccessible steerings generated by the Hawking effect exhibit an intrinsic
asymmetry, with their specific behavior being strongly dependent on the
steering direction.

</details>


### [271] [Accretion Process as a Probe of Extra Dimensions in MOG Compact Object Spacetimes](https://arxiv.org/abs/2511.01677)
*Kourosh Nozari,Sara Saghafi,Zeynab Ramezanpasandi*

Main category: gr-qc

TL;DR: 研究高维修正引力理论中规则球对称致密天体的粒子动力学和吸积过程，发现额外维度减小最内稳定圆轨道半径，增强能量通量和温度，且这些修正可能接近当前观测探测阈值。


<details>
  <summary>Details</summary>
Motivation: 通过研究高维修正引力理论中的天体物理过程（如黑洞吸积），来探索额外维度如何修改引力理论，并检验修改引力理论和更高维框架。

Method: 分析高维规则球对称MOG致密天体周围中性粒子的动力学，研究最内稳定圆轨道、能量通量、温度和微分光度，并分析完美流体在该天体上的吸积过程。

Result: 额外维度减小最内稳定圆轨道半径，同时增强对应的通量和温度；通过比较有效盘温度与事件视界望远镜对Sgr A*的观测，发现MOG和高维修正对吸积盘性质的影响可能接近当前探测阈值。

Conclusion: 高维修正引力理论中的天体吸积过程为探测额外维度提供了有力工具，这些修正效应可能已接近当前观测设备的探测能力。

Abstract: The idea of extra spatial dimensions arises from attempts to unify gravity
with other fundamental interactions, develop a consistent theory of quantum
gravity, and address open problems in particle physics and cosmology.
Considerable attention has been devoted to understanding how such dimensions
modify gravitational theories. One way to probe their impact is through the
analytical study of astrophysical processes such as black hole accretion. Since
accretion efficiently converts gravitational energy into radiation, this makes
it a powerful tool to test modified gravity (MOG) theories and
higher-dimensional frameworks via the behavior of dark compact objects like
black holes, neutron stars, and white dwarfs. In this work, we investigate the
dynamics of neutral particles around a higher-dimensional, regular, spherically
symmetric MOG compact object, focusing on the innermost stable circular orbit
(ISCO), energy flux, temperature, and differential luminosity. We further
analyze the accretion of a perfect fluid onto the same object, deriving
analytical expressions for the four-velocity and proper energy density of the
inflowing matter. Our findings show that extra dimensions reduce the ISCO
radius while enhancing the corresponding flux and temperature. Finally, by
comparing the effective disk temperature $T_{\text{eff}}$ with Event Horizon
Telescope (EHT) observations of Sgr A*, we argue that MOG and
higher-dimensional corrections to the accretion disk properties could be close
to the current threshold of detectability.

</details>


### [272] [Semiclassical Gravity Beyond General Relativity: Insights from Torsion](https://arxiv.org/abs/2511.01684)
*R. Morales-Cabrera,Y. Bonder*

Main category: gr-qc

TL;DR: 本文发展了具有非平凡时空挠率的修正引力的半经典理论，特别在四维爱因斯坦-嘉当理论中建立了半经典处理的公理化框架。


<details>
  <summary>Details</summary>
Motivation: 研究挠率存在下的修正引力理论，特别是在爱因斯坦-嘉当理论框架下，处理非最小耦合自由克莱因-戈登场的情况。

Method: 使用Hadamard重整化方法，获得能量-动量和自旋密度算符的明确定义期望值，并通过微分形式构建重整化拉格朗日量来识别重整化模糊性。

Result: 获得了能量-动量和自旋密度算符的明确定义期望值，识别了尺度重整化模糊性，并分析了在挠率存在下持续存在的共形反常。

Conclusion: 成功建立了挠率修正引力理论的半经典处理框架，解决了重整化问题并确认了共形反常的持续性。

Abstract: We develop a semiclassical theory of modified gravity with nontrivial
spacetime torsion. In particular, we show that the semiclassical treatment can
be axiomatized in the case of Einstein--Cartan theory with a nonminimally
coupled, free Klein--Gordon field, in four dimensions. Using Hadamard
renormalization, we obtain well-defined expectation values for both, the
energy--momentum and spin--density operators. These objects exhibit scale and
renormalization ambiguities; we identify the latter by constructing a
renormalization Lagrangian in terms of differential forms, which are
particularly well suited for this purpose. Furthermore, we analyze the
conformal anomaly, which persists in the presence of torsion.

</details>


### [273] [Baryogenesis constraints on generalized mass-to-horizon entropy](https://arxiv.org/abs/2511.01693)
*Giuseppe Gaetano Luciano,Emmanuel N. Saridakis*

Main category: gr-qc

TL;DR: 该论文研究了在广义质量-视界熵的宇宙学框架下生成重子不对称性。这种熵是Bekenstein-Hawking面积定律的幂律扩展，通过应用引力-热力学猜想，修正了弗里德曼方程和哈勃参数的演化，使得标准超引力耦合可以产生非零的物质-反物质不对称性。


<details>
  <summary>Details</summary>
Motivation: 探索在修正的引力-热力学框架下如何解释宇宙中观测到的重子不对称性，特别是研究偏离标准Bekenstein-Hawking熵的微小修正对重子生成的影响。

Method: 采用广义质量-视界熵作为Bekenstein-Hawking面积定律的幂律扩展，应用引力-热力学猜想推导修正的弗里德曼方程，分析哈勃参数演化对重子不对称性生成的影响。

Result: 即使使用标准的超引力耦合，修正的哈勃参数演化也能产生非零的重子不对称性。与观测数据比较，对熵指数给出了严格约束：0 < 1 - n ≲ O(10^-2)，在退耦温度T_D ≃ 10^16 GeV时。

Conclusion: 研究结果表明，从标准Bekenstein-Hawking熵（n=1）的微小但物理上显著的偏离，可能是实现与当前宇宙学观测完全一致性所必需的。

Abstract: We investigate the generation of the baryon asymmetry within the cosmological
framework based on a generalized mass-to-horizon entropy. This entropy,
recently proposed as a power-law extension of the Bekenstein-Hawking area law,
arises from a modified mass-horizon relation constructed to ensure consistency
with the Clausius relation. By applying the gravity-thermodynamics conjecture,
the resulting corrections to the Friedmann equations modify the evolution of
the Hubble parameter. Consequently, even the standard supergravity coupling
between the Ricci scalar and the baryon current can generate a non-vanishing
matter-antimatter asymmetry. Comparison with observational data yields a
stringent constraint on the entropic exponent, namely $0 < 1 - n \lesssim
\mathcal{O}(10^{-2})$, at the decoupling temperature $T_D \simeq
10^{16}\,\text{GeV}$, corresponding to the current upper limit on tensor-mode
fluctuations at the inflationary scale. These findings indicate that minor,
subtle, yet physically significant departures, from the standard
Bekenstein-Hawking entropy ($n = 1$) may be required to achieve full
consistency with present cosmological observations.

</details>


### [274] [On energy and its positivity in spacetimes with an expanding flat de Sitter background](https://arxiv.org/abs/2511.01713)
*Rodrigo Avalos,Eric Ling,Annachiara Piubello*

Main category: gr-qc

TL;DR: 本文提出了在膨胀宇宙背景下（以德西特空间为背景时空）的能量定义，将正能定理扩展到具有脐点第二基本形式的初始数据集，并建立了该能量的正定性。


<details>
  <summary>Details</summary>
Motivation: 传统正能定理基于渐近平坦流形，假设第二基本形式在无穷远处衰减为零，用于模拟孤立引力系统。但实际天体存在于膨胀宇宙中，其第二基本形式是脐点的，因此需要为这种宇宙学背景定义能量概念。

Method: 以德西特空间为背景时空，采用平坦膨胀坐标，该空间被脐点超曲面叶状化。由于德西特空间中宇宙视界的存在阻碍了全局能量定义，因此采用准局域能量定义方法，将Liu-Yau能量适配到该框架中。

Result: 建立了在膨胀宇宙学背景下的准局域能量定义，并证明了对于宇宙学常数的某些有界值，该能量是正的。

Conclusion: 成功将正能定理扩展到宇宙学背景，为研究膨胀宇宙中的引力系统提供了理论基础，证明了在适当条件下能量仍然保持正定性。

Abstract: The positive energy theorems are a fundamental pillar in mathematical general
relativity. Originally proved by Schoen-Yau and later Witten, these theorems
were established for asymptotically flat manifolds where the metric tends to
the standard Euclidean metric and whose second fundamental form decays to zero
at infinity. This ansatz on the metric and second fundamental form is motivated
by the desire to model an isolated gravitational system with a Minkowski space
background for the spacetime. However, actual astrophysical massive objects are
not truly isolated but rather exist within an expanding cosmological universe,
where the second fundamental form is umbilic. With this in mind, we seek a
notion of energy for initial data sets with an umbilic second fundamental form.
In this work, we present a definition of energy in such an expanding
cosmological setting. Instead of Minkowski space, we take de Sitter space as
the background spacetime, which, when written in flat-expanding coordinates, is
foliated by umbilic hypersurfaces each isometric to Euclidean 3-space. This
cosmological setting necessitates a quasi-local energy definition, as the
presence of a cosmological horizon in de Sitter space obstructs a global one.
We define energy in this quasi-local setting by adapting the Liu-Yau energy to
our framework and establish positivity of this energy for certain bounded
values of the cosmological constant.

</details>
